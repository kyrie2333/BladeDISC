exec ${PAGER:-/usr/bin/less} "$0" || exit 1
Executing tests from //mlir/disc/tests/tensorflow_ops:max.cpp.test
-----------------------------------------------------------------------------
[==========] Running 1 test from 1 test suite.
[----------] Global test environment set-up.
[----------] 1 test from TFMaxOpTest
[ RUN      ] TFMaxOpTest.ColReduceStaticShape3DF32
2023-06-28 15:08:16.332652: I ./mlir/disc/tests/mlir_feature_test.h:29] Apply env setting:
2023-06-28 15:08:16.332711: I ./mlir/disc/tests/mlir_feature_test.h:31] 	DISC_MEM_INTENSIVE_OPT_EXPERIMENTAL = true
2023-06-28 15:08:16.332715: I ./mlir/disc/tests/mlir_feature_test.h:31] 	DISC_ENABLE_STITCH = true
2023-06-28 15:08:16.332719: I mlir/disc/tests/mlir_feature_test.cc:271] Testing for CUDA backend
2023-06-28 15:08:16.333516: I mlir/disc/tests/mlir_feature_test.cc:145] Original TF code: func.func @main(%arg0: tensor<110x100x13xf32>) -> tensor<100x13xf32> attributes {tf.entry_function = {inputs = "{{INPUTS}}", outputs = "{{OUTPUTS}}", input_placements="{{INPUT_PLACEMENTS}}", output_placements="{{OUTPUT_PLACEMENTS}}"}} {
  %graph = tf_executor.graph {
    %1:2 = tf_executor.island wraps "tf.Const"() {value = dense<[0]> : tensor<1xi32>} : () -> tensor<1xi32>
    %2:2 = tf_executor.island wraps "tf.Max"(%arg0, %1) {keep_dims = false} : (tensor<110x100x13xf32>, tensor<1xi32>) -> tensor<100x13xf32>
    tf_executor.fetch %2 : tensor<100x13xf32>
  }
  return %graph : tensor<100x13xf32>
}
2023-06-28 15:08:16.333542: I mlir/disc/tests/mlir_feature_test.cc:149] New TF code: func.func @main(%arg0: tensor<110x100x13xf32>) -> tensor<100x13xf32> attributes {tf.entry_function = {inputs = "input0", outputs = "output0", input_placements="gpu", output_placements="gpu"}} {
  %graph = tf_executor.graph {
    %1:2 = tf_executor.island wraps "tf.Const"() {value = dense<[0]> : tensor<1xi32>} : () -> tensor<1xi32>
    %2:2 = tf_executor.island wraps "tf.Max"(%arg0, %1) {keep_dims = false} : (tensor<110x100x13xf32>, tensor<1xi32>) -> tensor<100x13xf32>
    tf_executor.fetch %2 : tensor<100x13xf32>
  }
  return %graph : tensor<100x13xf32>
}
2023-06-28 15:08:16.333881: I mlir/disc/tests/mlir_test.cc:233] tf_opt_pat: external/org_tensorflow/tensorflow/compiler/mlir/tf-opt
2023-06-28 15:08:16.333889: I mlir/disc/tests/mlir_test.cc:234] mlir_file_path: /root/.cache/bazel/_bazel_root/54ece412abe75fa85ab728a2c061e33c/execroot/org_disc_compiler/_tmp/b247c1654ecfc81b23b002417f617bea/tempfile-adf59ec6ac82-139e1c75-42725-5ff31f40a0ef9
2023-06-28 15:08:16.333892: I mlir/disc/tests/mlir_test.cc:235] tmp_dir: /root/.cache/bazel/_bazel_root/54ece412abe75fa85ab728a2c061e33c/execroot/org_disc_compiler/_tmp/b247c1654ecfc81b23b002417f617bea
2023-06-28 15:08:16.333895: I mlir/disc/tests/mlir_test.cc:236] test_name: ColReduceStaticShape3DF32_0
2023-06-28 15:08:16.333904: I mlir/disc/tests/mlir_test.cc:284] tf_opt_path: external/org_tensorflow/tensorflow/compiler/mlir/tf-opt

2023-06-28 15:08:16.333910: I mlir/disc/tests/mlir_test.cc:257] program_path: external/org_tensorflow/tensorflow/compiler/mlir/tf-opt

2023-06-28 15:08:16.387234: I mlir/disc/tests/mlir_test.cc:269] Executed: external/org_tensorflow/tensorflow/compiler/mlir/tf-opt --tf-standard-pipeline /root/.cache/bazel/_bazel_root/54ece412abe75fa85ab728a2c061e33c/execroot/org_disc_compiler/_tmp/b247c1654ecfc81b23b002417f617bea/tempfile-adf59ec6ac82-139e1c75-42725-5ff31f40a0ef9 -o /root/.cache/bazel/_bazel_root/54ece412abe75fa85ab728a2c061e33c/execroot/org_disc_compiler/_tmp/b247c1654ecfc81b23b002417f617beaColReduceStaticShape3DF32_0_tf_dialect.mlir 
2023-06-28 15:08:16.387247: I mlir/disc/tests/mlir_test.cc:270] external/org_tensorflow/tensorflow/compiler/mlir/tf-opt: 0
2023-06-28 15:08:16.387250: I mlir/disc/tests/mlir_test.cc:271] -- stdout:

============ END ============

2023-06-28 15:08:16.387253: I mlir/disc/tests/mlir_test.cc:273] -- stderr:
2023-06-28 15:08:16.376335: I external/org_tensorflow/tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: SSE3 SSE4.1 SSE4.2 AVX AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.

============ END ============

2023-06-28 15:08:16.387267: I mlir/disc/tests/mlir_test.cc:275] ret: 0

2023-06-28 15:08:16.387280: I mlir/disc/tests/mlir_test.cc:257] program_path: mlir/disc/disc_compiler_main

2023-06-28 15:08:17.672095: I mlir/disc/tests/mlir_test.cc:269] Executed: mlir/disc/disc_compiler_main --mlir-print-elementsattrs-with-hex-if-larger -1 --mlir-elide-elementsattrs-if-larger 8 /root/.cache/bazel/_bazel_root/54ece412abe75fa85ab728a2c061e33c/execroot/org_disc_compiler/_tmp/b247c1654ecfc81b23b002417f617beaColReduceStaticShape3DF32_0_tf_dialect.mlir /root/.cache/bazel/_bazel_root/54ece412abe75fa85ab728a2c061e33c/execroot/org_disc_compiler/_tmp/b247c1654ecfc81b23b002417f617beaColReduceStaticShape3DF32_0.so 
2023-06-28 15:08:17.672119: I mlir/disc/tests/mlir_test.cc:270] mlir/disc/disc_compiler_main: 0
2023-06-28 15:08:17.672123: I mlir/disc/tests/mlir_test.cc:271] -- stdout:

============ END ============

2023-06-28 15:08:17.672503: I mlir/disc/tests/mlir_test.cc:273] -- stderr:
======== BEGIN Original Module =========
module {
  func.func @main(%arg0: tensor<110x100x13xf32>) -> tensor<100x13xf32> attributes {tf.entry_function = {input_placements = "gpu", inputs = "input0", output_placements = "gpu", outputs = "output0"}} {
    %cst = "tf.Const"() {value = dense<0> : tensor<1xi32>} : () -> tensor<1xi32>
    %0 = "tf.Max"(%arg0, %cst) {keep_dims = false} : (tensor<110x100x13xf32>, tensor<1xi32>) -> tensor<100x13xf32>
    return %0 : tensor<100x13xf32>
  }
}

======= END Original Module ==========
[DISC] Load Input IR takes: 1.909000e-03 s.
[[ INFO ]] Running TF2XLA
2023-06-28 15:08:16.433851: I external/org_tensorflow/tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: SSE3 SSE4.1 SSE4.2 AVX AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
// -----// IR Dump After SCCP (sccp) //----- //
module {
  func.func @main(%arg0: tensor<110x100x13xf32>) -> tensor<100x13xf32> attributes {tf.entry_function = {input_placements = "gpu", inputs = "input0", output_placements = "gpu", outputs = "output0"}} {
    %cst = "tf.Const"() {value = dense<0> : tensor<1xi32>} : () -> tensor<1xi32>
    %0 = "tf.Max"(%arg0, %cst) {keep_dims = false} : (tensor<110x100x13xf32>, tensor<1xi32>) -> tensor<100x13xf32>
    return %0 : tensor<100x13xf32>
  }
}


// -----// IR Dump After SCCP (sccp) //----- //
module {
  func.func @main(%arg0: tensor<110x100x13xf32>) -> tensor<100x13xf32> attributes {tf.entry_function = {input_placements = "gpu", inputs = "input0", output_placements = "gpu", outputs = "output0"}} {
    %cst = "tf.Const"() {value = dense<0> : tensor<1xi32>} : () -> tensor<1xi32>
    %0 = "tf.Max"(%arg0, %cst) {keep_dims = false} : (tensor<110x100x13xf32>, tensor<1xi32>) -> tensor<100x13xf32>
    return %0 : tensor<100x13xf32>
  }
}


// -----// IR Dump After LegalizeTF (xla-legalize-tf) //----- //
func.func @main(%arg0: tensor<110x100x13xf32>) -> tensor<100x13xf32> attributes {tf.entry_function = {input_placements = "gpu", inputs = "input0", output_placements = "gpu", outputs = "output0"}} {
  %0 = mhlo.constant dense<0> : tensor<1xi32>
  %1 = mhlo.convert %arg0 : tensor<110x100x13xf32>
  %2 = mhlo.constant dense<0xFF800000> : tensor<f32>
  %3 = mhlo.reduce(%1 init: %2) applies mhlo.maximum across dimensions = [0] : (tensor<110x100x13xf32>, tensor<f32>) -> tensor<100x13xf32>
  %4 = mhlo.convert %3 : tensor<100x13xf32>
  return %4 : tensor<100x13xf32>
}

// -----// IR Dump After DiscLowerTfPass (disc-lower-tf) //----- //
func.func @main(%arg0: tensor<110x100x13xf32>) -> tensor<100x13xf32> attributes {tf.entry_function = {input_placements = "gpu", inputs = "input0", output_placements = "gpu", outputs = "output0"}} {
  %0 = mhlo.constant dense<0xFF800000> : tensor<f32>
  %1 = mhlo.reduce(%arg0 init: %0) applies mhlo.maximum across dimensions = [0] : (tensor<110x100x13xf32>, tensor<f32>) -> tensor<100x13xf32>
  return %1 : tensor<100x13xf32>
}

===-------------------------------------------------------------------------===
                         ... Execution time report ...
===-------------------------------------------------------------------------===
  Total Execution Time: 0.0078 seconds

  ----Wall Time----  ----Name----
    0.0000 (  0.6%)  ReviseArgumentsForStaticRankPass
    0.0000 (  0.1%)  FunctionalControlFlowToRegionsPass
    0.0040 ( 51.6%)  Inliner
    0.0000 (  0.3%)    (A) CallGraph
    0.0038 ( 48.7%)  'func.func' Pipeline
    0.0038 ( 48.7%)    Canonicalizer
    0.0001 (  1.0%)  'func.func' Pipeline
    0.0000 (  0.0%)    DropWhileShapeInvariantPass
    0.0000 (  0.0%)    ReplicateTensorListInitOpsPass
    0.0001 (  0.9%)    Canonicalizer
    0.0002 (  2.3%)  SCCP
    0.0000 (  0.2%)  GuaranteeAllFuncsOneUsePass
    0.0000 (  0.0%)    (A) CallGraph
    0.0000 (  0.1%)  TensorFlowShapeInferencePass
    0.0001 (  1.7%)  SCCP
    0.0000 (  0.1%)  TensorListOpsDecompositionPass
    0.0000 (  0.1%)  StackOpsDecompositionPass
    0.0000 (  0.1%)  TensorArrayOpsDecompositionPass
    0.0000 (  0.5%)  'func.func' Pipeline
    0.0000 (  0.5%)    DecomposeResourceOpsPass
    0.0000 (  0.2%)  PromoteResourcesToArgsPass
    0.0000 (  0.1%)  SymbolDCE
    0.0000 (  0.1%)  'func.func' Pipeline
    0.0000 (  0.1%)    SinkConstantsToControlFlowPass
    0.0000 (  0.1%)  TensorFlowShapeInferencePass
    0.0002 (  2.9%)  StablehloLegalizeToHloPass
    0.0000 (  0.6%)  'func.func' Pipeline
    0.0000 (  0.4%)    DiscLowerTfPass
    0.0000 (  0.2%)    LowerQuantizedPass
    0.0000 (  0.2%)  LegalizeTfTypesPass
    0.0009 ( 11.4%)  'func.func' Pipeline
    0.0007 (  9.4%)    LegalizeTF
    0.0001 (  1.8%)    DiscLowerTfPass
    0.0000 (  0.1%)    mlir::mhlo::{anonymous}::AdjustLayout
    0.0000 (  0.2%)  LegalizeTFCollective
    0.0001 (  0.9%)  'func.func' Pipeline
    0.0001 (  0.8%)    Canonicalizer
    0.0000 (  0.1%)  TensorFlowShapeInferencePass
    0.0004 (  4.6%)  'func.func' Pipeline
    0.0004 (  4.6%)    LegalizeTF
    0.0000 (  0.1%)  LegalizeTFCommunicationPass
    0.0000 (  0.5%)  'func.func' Pipeline
    0.0000 (  0.4%)    DiscDynamicSliceConverterPass
    0.0000 (  0.1%)    SinkConstantsToControlFlowPass
   -0.0023 (-29.4%)  Rest
    0.0078 (100.0%)  Total
======== BEGIN After TF2HLO =========
module {
  func.func @main(%arg0: tensor<110x100x13xf32>) -> tensor<100x13xf32> attributes {tf.entry_function = {input_placements = "gpu", inputs = "input0", output_placements = "gpu", outputs = "output0"}} {
    %0 = mhlo.constant dense<0xFF800000> : tensor<f32>
    %1 = mhlo.reduce(%arg0 init: %0) applies mhlo.maximum across dimensions = [0] : (tensor<110x100x13xf32>, tensor<f32>) -> tensor<100x13xf32>
    return %1 : tensor<100x13xf32>
  }
}

======= END After TF2HLO ==========
[DISC] tf2hlo takes: 8.119000e-03 s.
SymbolicDimMgr::save walkRankedTensorValue takes: 1 us
SymbolicDimMgr::save update attributes takes: 0 us
SymbolicDimMgr::updateProductEqualityMap simplifySymbolicDimProductPair takes: 0 us
productSet.size() = 0
SymbolicDimMgr::updateProductEqualityMap propagate graph takes: 1 us
SymbolicDimMgr::updateProductEqualityMap remove multiply takes: 0 us
SymbolicDimMgr::updateProductEqualityMap build toRemove  takes: 0 us
SymbolicDimMgr::updateProductEqualityMap apply toRemove  takes: 0 us
SymbolicDimMgr::save updateProductEqualityMap takes: 11 us
SymbolicDimMgr::save updateFunctionType takes: 1 us
SymbolicDimMgr::save collect symbolicDim ops takes: 1 us
SymbolicDimMgr::save remove symbolicDim ops takes: 0 us
SymbolicDimMgr::save remove unused production takes: 0 us
SymbolicDimMgr::save remove unused production #2 takes: 0 us
SymbolicDimMgr::save canonicalize the name takes: 0 us
SymbolicDimMgr::save replace the name takes: 0 us
SymbolicDimMgr::save updateFunctionType takes: 0 us
// -----// IR Dump After DiscShapeOptimizationPass (disc-shape-optimization) //----- //
module {
  func.func @main(%arg0: tensor<110x100x13xf32>) -> tensor<100x13xf32> attributes {tf.entry_function = {input_placements = "gpu", inputs = "input0", output_placements = "gpu", outputs = "output0"}} {
    %0 = mhlo.constant dense<0xFF800000> : tensor<f32>
    %1 = mhlo.reduce(%arg0 init: %0) applies mhlo.maximum across dimensions = [0] : (tensor<110x100x13xf32>, tensor<f32>) -> tensor<100x13xf32>
    return %1 : tensor<100x13xf32>
  }
  func.func @shape_constraint_graph() {
    return
  }
}


SymbolicDimMgr::save walkRankedTensorValue takes: 0 us
SymbolicDimMgr::save update attributes takes: 0 us
SymbolicDimMgr::updateProductEqualityMap simplifySymbolicDimProductPair takes: 0 us
productSet.size() = 0
SymbolicDimMgr::updateProductEqualityMap propagate graph takes: 1 us
SymbolicDimMgr::updateProductEqualityMap remove multiply takes: 0 us
SymbolicDimMgr::updateProductEqualityMap build toRemove  takes: 0 us
SymbolicDimMgr::updateProductEqualityMap apply toRemove  takes: 0 us
SymbolicDimMgr::save updateProductEqualityMap takes: 12 us
SymbolicDimMgr::save updateFunctionType takes: 1 us
SymbolicDimMgr::save collect symbolicDim ops takes: 1 us
SymbolicDimMgr::save remove symbolicDim ops takes: 0 us
SymbolicDimMgr::save remove unused production takes: 0 us
SymbolicDimMgr::save remove unused production #2 takes: 0 us
SymbolicDimMgr::save canonicalize the name takes: 0 us
SymbolicDimMgr::save replace the name takes: 0 us
SymbolicDimMgr::save updateFunctionType takes: 0 us
// -----// IR Dump After DiscShapeOptimizationPass (disc-shape-optimization) //----- //
module {
  func.func @main(%arg0: tensor<110x100x13xf32>) -> tensor<100x13xf32> attributes {tf.entry_function = {input_placements = "gpu", inputs = "input0", output_placements = "gpu", outputs = "output0"}} {
    %0 = mhlo.constant dense<0xFF800000> : tensor<f32>
    %1 = mhlo.reduce(%arg0 init: %0) applies mhlo.maximum across dimensions = [0] : (tensor<110x100x13xf32>, tensor<f32>) -> tensor<100x13xf32>
    return %1 : tensor<100x13xf32>
  }
  func.func @shape_constraint_graph() {
    return
  }
}


SymbolicDimMgr::save walkRankedTensorValue takes: 0 us
SymbolicDimMgr::save update attributes takes: 0 us
SymbolicDimMgr::updateProductEqualityMap simplifySymbolicDimProductPair takes: 0 us
productSet.size() = 0
SymbolicDimMgr::updateProductEqualityMap propagate graph takes: 1 us
SymbolicDimMgr::updateProductEqualityMap remove multiply takes: 0 us
SymbolicDimMgr::updateProductEqualityMap build toRemove  takes: 0 us
SymbolicDimMgr::updateProductEqualityMap apply toRemove  takes: 0 us
SymbolicDimMgr::save updateProductEqualityMap takes: 13 us
SymbolicDimMgr::save updateFunctionType takes: 0 us
SymbolicDimMgr::save collect symbolicDim ops takes: 1 us
SymbolicDimMgr::save remove symbolicDim ops takes: 0 us
SymbolicDimMgr::save remove unused production takes: 0 us
SymbolicDimMgr::save remove unused production #2 takes: 0 us
SymbolicDimMgr::save canonicalize the name takes: 0 us
SymbolicDimMgr::save replace the name takes: 0 us
SymbolicDimMgr::save updateFunctionType takes: 0 us
// -----// IR Dump After DiscShapeOptimizationPass (disc-shape-optimization) //----- //
module {
  func.func @main(%arg0: tensor<110x100x13xf32>) -> tensor<100x13xf32> attributes {tf.entry_function = {input_placements = "gpu", inputs = "input0", output_placements = "gpu", outputs = "output0"}} {
    %0 = mhlo.constant dense<0xFF800000> : tensor<f32>
    %1 = mhlo.reduce(%arg0 init: %0) applies mhlo.maximum across dimensions = [0] : (tensor<110x100x13xf32>, tensor<f32>) -> tensor<100x13xf32>
    return %1 : tensor<100x13xf32>
  }
  func.func @shape_constraint_graph() {
    return
  }
}


// -----// IR Dump After HloCanonicalizeReductionPass (hlo-canonicalize-reduction) //----- //
func.func @main(%arg0: tensor<110x100x13xf32>) -> tensor<100x13xf32> attributes {tf.entry_function = {input_placements = "gpu", inputs = "input0", output_placements = "gpu", outputs = "output0"}} {
  %0 = mhlo.constant dense<0xFF800000> : tensor<f32>
  %c1_i32 = arith.constant 1 : i32
  %c0 = arith.constant 0 : index
  %dim = tensor.dim %arg0, %c0 : tensor<110x100x13xf32>
  %1 = arith.index_cast %dim : index to i32
  %2 = arith.muli %c1_i32, %1 : i32
  %c1 = arith.constant 1 : index
  %dim_0 = tensor.dim %arg0, %c1 : tensor<110x100x13xf32>
  %3 = arith.index_cast %dim_0 : index to i32
  %4 = arith.muli %c1_i32, %3 : i32
  %c2 = arith.constant 2 : index
  %dim_1 = tensor.dim %arg0, %c2 : tensor<110x100x13xf32>
  %5 = arith.index_cast %dim_1 : index to i32
  %6 = arith.muli %4, %5 : i32
  %from_elements = tensor.from_elements %2, %6 : tensor<2xi32>
  %7 = mhlo.dynamic_reshape %arg0, %from_elements : (tensor<110x100x13xf32>, tensor<2xi32>) -> tensor<?x?xf32>
  %8 = mhlo.reduce(%7 init: %0) applies mhlo.maximum across dimensions = [0] : (tensor<?x?xf32>, tensor<f32>) -> tensor<?xf32>
  %c1_2 = arith.constant 1 : index
  %dim_3 = tensor.dim %arg0, %c1_2 : tensor<110x100x13xf32>
  %9 = arith.index_cast %dim_3 : index to i32
  %c2_4 = arith.constant 2 : index
  %dim_5 = tensor.dim %arg0, %c2_4 : tensor<110x100x13xf32>
  %10 = arith.index_cast %dim_5 : index to i32
  %from_elements_6 = tensor.from_elements %9, %10 : tensor<2xi32>
  %11 = mhlo.dynamic_reshape %8, %from_elements_6 : (tensor<?xf32>, tensor<2xi32>) -> tensor<100x13xf32>
  return %11 : tensor<100x13xf32>
}

SymbolicDimMgr::save walkRankedTensorValue takes: 1 us
SymbolicDimMgr::save update attributes takes: 1 us
SymbolicDimMgr::updateProductEqualityMap simplifySymbolicDimProductPair takes: 2 us
productSet.size() = 0
SymbolicDimMgr::updateProductEqualityMap propagate graph takes: 1 us
SymbolicDimMgr::updateProductEqualityMap remove multiply takes: 0 us
SymbolicDimMgr::updateProductEqualityMap build toRemove  takes: 0 us
SymbolicDimMgr::updateProductEqualityMap apply toRemove  takes: 0 us
SymbolicDimMgr::save updateProductEqualityMap takes: 15 us
SymbolicDimMgr::save updateFunctionType takes: 1 us
SymbolicDimMgr::save collect symbolicDim ops takes: 2 us
SymbolicDimMgr::save remove symbolicDim ops takes: 0 us
SymbolicDimMgr::save remove unused production takes: 0 us
SymbolicDimMgr::save remove unused production #2 takes: 0 us
SymbolicDimMgr::save canonicalize the name takes: 0 us
SymbolicDimMgr::save replace the name takes: 1 us
SymbolicDimMgr::save updateFunctionType takes: 0 us
SymbolicDimMgr::save walkRankedTensorValue takes: 0 us
SymbolicDimMgr::save update attributes takes: 0 us
SymbolicDimMgr::updateProductEqualityMap simplifySymbolicDimProductPair takes: 0 us
productSet.size() = 0
SymbolicDimMgr::updateProductEqualityMap propagate graph takes: 1 us
SymbolicDimMgr::updateProductEqualityMap remove multiply takes: 0 us
SymbolicDimMgr::updateProductEqualityMap build toRemove  takes: 0 us
SymbolicDimMgr::updateProductEqualityMap apply toRemove  takes: 0 us
SymbolicDimMgr::save updateProductEqualityMap takes: 11 us
SymbolicDimMgr::save updateFunctionType takes: 0 us
SymbolicDimMgr::save collect symbolicDim ops takes: 1 us
SymbolicDimMgr::save remove symbolicDim ops takes: 0 us
SymbolicDimMgr::save remove unused production takes: 0 us
SymbolicDimMgr::save remove unused production #2 takes: 0 us
SymbolicDimMgr::save canonicalize the name takes: 0 us
SymbolicDimMgr::save replace the name takes: 0 us
SymbolicDimMgr::save updateFunctionType takes: 0 us
// -----// IR Dump After DiscShapeOptimizationPass (disc-shape-optimization) //----- //
module {
  func.func @main(%arg0: tensor<110x100x13xf32>) -> tensor<100x13xf32> attributes {tf.entry_function = {input_placements = "gpu", inputs = "input0", output_placements = "gpu", outputs = "output0"}} {
    %0 = mhlo.constant dense<0xFF800000> : tensor<f32>
    %1 = mhlo.reshape %arg0 : (tensor<110x100x13xf32>) -> tensor<110x1300xf32>
    %2 = mhlo.reduce(%1 init: %0) applies mhlo.maximum across dimensions = [0] : (tensor<110x1300xf32>, tensor<f32>) -> tensor<1300xf32>
    %3 = mhlo.reshape %2 : (tensor<1300xf32>) -> tensor<100x13xf32>
    return %3 : tensor<100x13xf32>
  }
  func.func @shape_constraint_graph() {
    return
  }
}


// -----// IR Dump After PlaceOpsPass (mhlo-place-ops) //----- //
module {
  func.func @main(%arg0: tensor<110x100x13xf32>) -> tensor<100x13xf32> attributes {tf.entry_function = {input_placements = "gpu", inputs = "input0", output_placements = "gpu", outputs = "output0"}} {
    %0 = mhlo.constant {disc.device = "gpu"} dense<0xFF800000> : tensor<f32>
    %1 = mhlo.reshape %arg0 {disc.device = "gpu"} : (tensor<110x100x13xf32>) -> tensor<110x1300xf32>
    %2 = mhlo.reduce(%1 init: %0) applies mhlo.maximum across dimensions = [0] : (tensor<110x1300xf32>, tensor<f32>) -> tensor<1300xf32>
    %3 = mhlo.reshape %2 {disc.device = "gpu"} : (tensor<1300xf32>) -> tensor<100x13xf32>
    return %3 : tensor<100x13xf32>
  }
  func.func @shape_constraint_graph() {
    return
  }
}


SymbolicDimMgr::save walkRankedTensorValue takes: 0 us
SymbolicDimMgr::save update attributes takes: 0 us
SymbolicDimMgr::updateProductEqualityMap simplifySymbolicDimProductPair takes: 0 us
productSet.size() = 0
SymbolicDimMgr::updateProductEqualityMap propagate graph takes: 1 us
SymbolicDimMgr::updateProductEqualityMap remove multiply takes: 0 us
SymbolicDimMgr::updateProductEqualityMap build toRemove  takes: 0 us
SymbolicDimMgr::updateProductEqualityMap apply toRemove  takes: 0 us
SymbolicDimMgr::save updateProductEqualityMap takes: 11 us
SymbolicDimMgr::save updateFunctionType takes: 1 us
SymbolicDimMgr::save collect symbolicDim ops takes: 1 us
SymbolicDimMgr::save remove symbolicDim ops takes: 0 us
SymbolicDimMgr::save remove unused production takes: 0 us
SymbolicDimMgr::save remove unused production #2 takes: 0 us
SymbolicDimMgr::save canonicalize the name takes: 0 us
SymbolicDimMgr::save replace the name takes: 0 us
SymbolicDimMgr::save updateFunctionType takes: 0 us
// -----// IR Dump After DiscShapeOptimizationPass (disc-shape-optimization) //----- //
module {
  func.func @main(%arg0: tensor<110x100x13xf32>) -> tensor<100x13xf32> attributes {tf.entry_function = {input_placements = "gpu", inputs = "input0", output_placements = "gpu", outputs = "output0"}} {
    %0 = mhlo.constant {disc.device = "gpu"} dense<0xFF800000> : tensor<f32>
    %1 = mhlo.reshape %arg0 {disc.device = "gpu"} : (tensor<110x100x13xf32>) -> tensor<110x1300xf32>
    %2 = mhlo.reduce(%1 init: %0) applies mhlo.maximum across dimensions = [0] : (tensor<110x1300xf32>, tensor<f32>) -> tensor<1300xf32>
    %3 = mhlo.reshape %2 {disc.device = "gpu"} : (tensor<1300xf32>) -> tensor<100x13xf32>
    return %3 : tensor<100x13xf32>
  }
  func.func @shape_constraint_graph() {
    return
  }
}


SymbolicDimMgr::save walkRankedTensorValue takes: 0 us
SymbolicDimMgr::save update attributes takes: 0 us
SymbolicDimMgr::updateProductEqualityMap simplifySymbolicDimProductPair takes: 0 us
productSet.size() = 0
SymbolicDimMgr::updateProductEqualityMap propagate graph takes: 1 us
SymbolicDimMgr::updateProductEqualityMap remove multiply takes: 0 us
SymbolicDimMgr::updateProductEqualityMap build toRemove  takes: 0 us
SymbolicDimMgr::updateProductEqualityMap apply toRemove  takes: 0 us
SymbolicDimMgr::save updateProductEqualityMap takes: 12 us
SymbolicDimMgr::save updateFunctionType takes: 1 us
SymbolicDimMgr::save collect symbolicDim ops takes: 1 us
SymbolicDimMgr::save remove symbolicDim ops takes: 0 us
SymbolicDimMgr::save remove unused production takes: 0 us
SymbolicDimMgr::save remove unused production #2 takes: 0 us
SymbolicDimMgr::save canonicalize the name takes: 0 us
SymbolicDimMgr::save replace the name takes: 0 us
SymbolicDimMgr::save updateFunctionType takes: 0 us
// -----// IR Dump After DiscShapeOptimizationPass (disc-shape-optimization) //----- //
module {
  func.func @main(%arg0: tensor<110x100x13xf32>) -> tensor<100x13xf32> attributes {tf.entry_function = {input_placements = "gpu", inputs = "input0", output_placements = "gpu", outputs = "output0"}} {
    %0 = mhlo.constant {disc.device = "gpu"} dense<0xFF800000> : tensor<f32>
    %1 = mhlo.reshape %arg0 {disc.device = "gpu"} : (tensor<110x100x13xf32>) -> tensor<110x1300xf32>
    %2 = mhlo.reduce(%1 init: %0) applies mhlo.maximum across dimensions = [0] : (tensor<110x1300xf32>, tensor<f32>) -> tensor<1300xf32>
    %3 = mhlo.reshape %2 {disc.device = "gpu"} : (tensor<1300xf32>) -> tensor<100x13xf32>
    return %3 : tensor<100x13xf32>
  }
  func.func @shape_constraint_graph() {
    return
  }
}


SymbolicDimMgr::save walkRankedTensorValue takes: 1 us
SymbolicDimMgr::save update attributes takes: 1 us
SymbolicDimMgr::updateProductEqualityMap simplifySymbolicDimProductPair takes: 0 us
productSet.size() = 0
SymbolicDimMgr::updateProductEqualityMap propagate graph takes: 1 us
SymbolicDimMgr::updateProductEqualityMap remove multiply takes: 0 us
SymbolicDimMgr::updateProductEqualityMap build toRemove  takes: 0 us
SymbolicDimMgr::updateProductEqualityMap apply toRemove  takes: 0 us
SymbolicDimMgr::save updateProductEqualityMap takes: 12 us
SymbolicDimMgr::save updateFunctionType takes: 1 us
SymbolicDimMgr::save collect symbolicDim ops takes: 1 us
SymbolicDimMgr::save remove symbolicDim ops takes: 0 us
SymbolicDimMgr::save remove unused production takes: 0 us
SymbolicDimMgr::save remove unused production #2 takes: 0 us
SymbolicDimMgr::save canonicalize the name takes: 0 us
SymbolicDimMgr::save replace the name takes: 1 us
SymbolicDimMgr::save updateFunctionType takes: 0 us
// -----// IR Dump After DiscShapeOptimizationPass (disc-shape-optimization) //----- //
module {
  func.func @main(%arg0: tensor<110x100x13xf32>) -> tensor<100x13xf32> attributes {tf.entry_function = {input_placements = "gpu", inputs = "input0", output_placements = "gpu", outputs = "output0"}} {
    %0 = mhlo.constant {disc.device = "gpu"} dense<0xFF800000> : tensor<f32>
    %1 = mhlo.reshape %arg0 {disc.device = "gpu"} : (tensor<110x100x13xf32>) -> tensor<110x1300xf32>
    %2 = mhlo.reduce(%1 init: %0) applies mhlo.maximum across dimensions = [0] : (tensor<110x1300xf32>, tensor<f32>) -> tensor<1300xf32>
    %3 = mhlo.reshape %2 {disc.device = "gpu"} : (tensor<1300xf32>) -> tensor<100x13xf32>
    return %3 : tensor<100x13xf32>
  }
  func.func @shape_constraint_graph() {
    return
  }
}


SymbolicDimMgr::save walkRankedTensorValue takes: 0 us
SymbolicDimMgr::save update attributes takes: 0 us
SymbolicDimMgr::updateProductEqualityMap simplifySymbolicDimProductPair takes: 0 us
productSet.size() = 0
SymbolicDimMgr::updateProductEqualityMap propagate graph takes: 1 us
SymbolicDimMgr::updateProductEqualityMap remove multiply takes: 0 us
SymbolicDimMgr::updateProductEqualityMap build toRemove  takes: 0 us
SymbolicDimMgr::updateProductEqualityMap apply toRemove  takes: 0 us
SymbolicDimMgr::save updateProductEqualityMap takes: 13 us
SymbolicDimMgr::save updateFunctionType takes: 1 us
SymbolicDimMgr::save collect symbolicDim ops takes: 1 us
SymbolicDimMgr::save remove symbolicDim ops takes: 0 us
SymbolicDimMgr::save remove unused production takes: 0 us
SymbolicDimMgr::save remove unused production #2 takes: 0 us
SymbolicDimMgr::save canonicalize the name takes: 0 us
SymbolicDimMgr::save replace the name takes: 0 us
SymbolicDimMgr::save updateFunctionType takes: 0 us
// -----// IR Dump After DiscShapeOptimizationPass (disc-shape-optimization) //----- //
module {
  func.func @main(%arg0: tensor<110x100x13xf32>) -> tensor<100x13xf32> attributes {tf.entry_function = {input_placements = "gpu", inputs = "input0", output_placements = "gpu", outputs = "output0"}} {
    %c110 = arith.constant 110 : index
    %c100 = arith.constant 100 : index
    %c13 = arith.constant 13 : index
    %c1300 = arith.constant 1300 : index
    %0 = mhlo.constant {disc.device = "gpu"} dense<0xFF800000> : tensor<f32>
    %1 = "disc_shape.tie_shape"(%arg0, %c110, %c100, %c13) : (tensor<110x100x13xf32>, index, index, index) -> tensor<110x100x13xf32>
    %2 = mhlo.reshape %1 {disc.device = "gpu"} : (tensor<110x100x13xf32>) -> tensor<110x1300xf32>
    %3 = "disc_shape.tie_shape"(%2, %c110, %c1300) : (tensor<110x1300xf32>, index, index) -> tensor<110x1300xf32>
    %4 = mhlo.reduce(%3 init: %0) applies mhlo.maximum across dimensions = [0] : (tensor<110x1300xf32>, tensor<f32>) -> tensor<1300xf32>
    %5 = "disc_shape.tie_shape"(%4, %c1300) : (tensor<1300xf32>, index) -> tensor<1300xf32>
    %6 = mhlo.reshape %5 {disc.device = "gpu"} : (tensor<1300xf32>) -> tensor<100x13xf32>
    %7 = "disc_shape.tie_shape"(%6, %c100, %c13) : (tensor<100x13xf32>, index, index) -> tensor<100x13xf32>
    return %7 : tensor<100x13xf32>
  }
  func.func @shape_constraint_graph() {
    return
  }
}


// -----// IR Dump After Canonicalizer (canonicalize) //----- //
func.func @main(%arg0: tensor<110x100x13xf32>) -> tensor<100x13xf32> attributes {tf.entry_function = {input_placements = "gpu", inputs = "input0", output_placements = "gpu", outputs = "output0"}} {
  %0 = mhlo.constant {disc.device = "gpu"} dense<0xFF800000> : tensor<f32>
  %1 = mhlo.reshape %arg0 {disc.device = "gpu"} : (tensor<110x100x13xf32>) -> tensor<110x1300xf32>
  %2 = mhlo.reduce(%1 init: %0) applies mhlo.maximum across dimensions = [0] : (tensor<110x1300xf32>, tensor<f32>) -> tensor<1300xf32>
  %3 = mhlo.reshape %2 {disc.device = "gpu"} : (tensor<1300xf32>) -> tensor<100x13xf32>
  return %3 : tensor<100x13xf32>
}

// -----// IR Dump After FuncBufferize (func-bufferize) //----- //
module {
  func.func @main(%arg0: memref<110x100x13xf32>) -> memref<100x13xf32> attributes {tf.entry_function = {input_placements = "gpu", inputs = "input0", output_placements = "gpu", outputs = "output0"}} {
    %0 = bufferization.to_tensor %arg0 : memref<110x100x13xf32>
    %1 = mhlo.constant {disc.device = "gpu"} dense<0xFF800000> : tensor<f32>
    %2 = mhlo.reshape %0 {disc.device = "gpu"} : (tensor<110x100x13xf32>) -> tensor<110x1300xf32>
    %3 = mhlo.reduce(%2 init: %1) applies mhlo.maximum across dimensions = [0] : (tensor<110x1300xf32>, tensor<f32>) -> tensor<1300xf32>
    %4 = mhlo.reshape %3 {disc.device = "gpu"} : (tensor<1300xf32>) -> tensor<100x13xf32>
    %5 = bufferization.to_memref %4 : memref<100x13xf32>
    return %5 : memref<100x13xf32>
  }
  func.func @shape_constraint_graph() {
    return
  }
}


// -----// IR Dump After HloLegalizeToLhloPass (hlo-legalize-to-lhlo) //----- //
module {
  func.func @main(%arg0: memref<110x100x13xf32>) -> memref<100x13xf32> attributes {tf.entry_function = {input_placements = "gpu", inputs = "input0", output_placements = "gpu", outputs = "output0"}} {
    %0 = bufferization.to_tensor %arg0 : memref<110x100x13xf32>
    %1 = bufferization.to_memref %0 : memref<110x100x13xf32>
    %alloc = memref.alloc() : memref<f32>
    "lmhlo.constant"(%alloc) {disc.device = "gpu", value = dense<0xFF800000> : tensor<f32>} : (memref<f32>) -> ()
    %alloc_0 = memref.alloc() : memref<110x1300xf32>
    "lmhlo.reshape"(%1, %alloc_0) {disc.device = "gpu"} : (memref<110x100x13xf32>, memref<110x1300xf32>) -> ()
    %alloc_1 = memref.alloc() : memref<1300xf32>
    "lmhlo.reduce"(%alloc_0, %alloc, %alloc_1) ({
    ^bb0(%arg1: memref<f32>, %arg2: memref<f32>, %arg3: memref<f32>):
      %alloc_3 = memref.alloc() : memref<f32>
      "lmhlo.maximum"(%arg1, %arg2, %alloc_3) {disc.device = "gpu"} : (memref<f32>, memref<f32>, memref<f32>) -> ()
      "lmhlo.copy"(%alloc_3, %arg3) : (memref<f32>, memref<f32>) -> ()
      "lmhlo.terminator"() : () -> ()
    }) {dimensions = dense<0> : tensor<1xi64>, disc.device = "gpu"} : (memref<110x1300xf32>, memref<f32>, memref<1300xf32>) -> ()
    %alloc_2 = memref.alloc() : memref<100x13xf32>
    "lmhlo.reshape"(%alloc_1, %alloc_2) {disc.device = "gpu"} : (memref<1300xf32>, memref<100x13xf32>) -> ()
    %2 = bufferization.to_tensor %alloc_2 : memref<100x13xf32>
    %3 = bufferization.to_memref %2 : memref<100x13xf32>
    return %3 : memref<100x13xf32>
  }
  func.func @shape_constraint_graph() {
    return
  }
}


// -----// IR Dump After Canonicalizer (canonicalize) //----- //
func.func @main(%arg0: memref<110x100x13xf32>) -> memref<100x13xf32> attributes {tf.entry_function = {input_placements = "gpu", inputs = "input0", output_placements = "gpu", outputs = "output0"}} {
  %alloc = memref.alloc() : memref<f32>
  "lmhlo.constant"(%alloc) {disc.device = "gpu", value = dense<0xFF800000> : tensor<f32>} : (memref<f32>) -> ()
  %alloc_0 = memref.alloc() : memref<110x1300xf32>
  "lmhlo.reshape"(%arg0, %alloc_0) {disc.device = "gpu"} : (memref<110x100x13xf32>, memref<110x1300xf32>) -> ()
  %alloc_1 = memref.alloc() : memref<1300xf32>
  "lmhlo.reduce"(%alloc_0, %alloc, %alloc_1) ({
  ^bb0(%arg1: memref<f32>, %arg2: memref<f32>, %arg3: memref<f32>):
    "lmhlo.maximum"(%arg1, %arg2, %arg3) {disc.device = "gpu"} : (memref<f32>, memref<f32>, memref<f32>) -> ()
    "lmhlo.terminator"() : () -> ()
  }) {dimensions = dense<0> : tensor<1xi64>, disc.device = "gpu"} : (memref<110x1300xf32>, memref<f32>, memref<1300xf32>) -> ()
  %alloc_2 = memref.alloc() : memref<100x13xf32>
  "lmhlo.reshape"(%alloc_1, %alloc_2) {disc.device = "gpu"} : (memref<1300xf32>, memref<100x13xf32>) -> ()
  return %alloc_2 : memref<100x13xf32>
}

// -----// IR Dump After DiscAssignMemorySpacePass (disc-assign-memory-space) //----- //
module {
  func.func @main(%arg0: memref<110x100x13xf32, "gpu">) -> memref<100x13xf32, "gpu"> attributes {tf.entry_function = {input_placements = "gpu", inputs = "input0", output_placements = "gpu", outputs = "output0"}} {
    %alloc = memref.alloc() : memref<f32, "gpu">
    "lmhlo.constant"(%alloc) {disc.device = "gpu", value = dense<0xFF800000> : tensor<f32>} : (memref<f32, "gpu">) -> ()
    %alloc_0 = memref.alloc() : memref<110x1300xf32, "gpu">
    "lmhlo.reshape"(%arg0, %alloc_0) {disc.device = "gpu"} : (memref<110x100x13xf32, "gpu">, memref<110x1300xf32, "gpu">) -> ()
    %alloc_1 = memref.alloc() : memref<1300xf32, "gpu">
    "lmhlo.reduce"(%alloc_0, %alloc, %alloc_1) ({
    ^bb0(%arg1: memref<f32>, %arg2: memref<f32>, %arg3: memref<f32>):
      "lmhlo.maximum"(%arg1, %arg2, %arg3) {disc.device = "gpu"} : (memref<f32>, memref<f32>, memref<f32>) -> ()
      "lmhlo.terminator"() : () -> ()
    }) {dimensions = dense<0> : tensor<1xi64>, disc.device = "gpu"} : (memref<110x1300xf32, "gpu">, memref<f32, "gpu">, memref<1300xf32, "gpu">) -> ()
    %alloc_2 = memref.alloc() : memref<100x13xf32, "gpu">
    "lmhlo.reshape"(%alloc_1, %alloc_2) {disc.device = "gpu"} : (memref<1300xf32, "gpu">, memref<100x13xf32, "gpu">) -> ()
    return %alloc_2 : memref<100x13xf32, "gpu">
  }
  func.func @shape_constraint_graph() {
    return
  }
}


SymbolicDimMgr::save walkRankedTensorValue takes: 0 us
SymbolicDimMgr::save update attributes takes: 0 us
SymbolicDimMgr::save updateProductEqualityMap takes: 0 us
SymbolicDimMgr::save updateFunctionType takes: 1 us
SymbolicDimMgr::save collect symbolicDim ops takes: 1 us
SymbolicDimMgr::save remove symbolicDim ops takes: 0 us
SymbolicDimMgr::save remove unused production takes: 0 us
SymbolicDimMgr::save remove unused production #2 takes: 0 us
SymbolicDimMgr::save canonicalize the name takes: 0 us
SymbolicDimMgr::save replace the name takes: 1 us
SymbolicDimMgr::save updateFunctionType takes: 0 us
// -----// IR Dump After DiscFusionPass (disc-fusion) //----- //
func.func @main(%arg0: memref<110x100x13xf32, "gpu">) -> memref<100x13xf32, "gpu"> attributes {tf.entry_function = {input_placements = "gpu", inputs = "input0", output_placements = "gpu", outputs = "output0"}} {
  %alloc = memref.alloc() : memref<f32, "gpu">
  %alloc_0 = memref.alloc() : memref<110x1300xf32, "gpu">
  %alloc_1 = memref.alloc() : memref<1300xf32, "gpu">
  "lmhlo.fusion"() ({
    "lmhlo.constant"(%alloc) {disc.device = "gpu", value = dense<0xFF800000> : tensor<f32>} : (memref<f32, "gpu">) -> ()
    "lmhlo.reshape"(%arg0, %alloc_0) {disc.device = "gpu"} : (memref<110x100x13xf32, "gpu">, memref<110x1300xf32, "gpu">) -> ()
    "lmhlo.reduce"(%alloc_0, %alloc, %alloc_1) ({
    ^bb0(%arg1: memref<f32>, %arg2: memref<f32>, %arg3: memref<f32>):
      "lmhlo.maximum"(%arg1, %arg2, %arg3) {disc.device = "gpu"} : (memref<f32>, memref<f32>, memref<f32>) -> ()
      "lmhlo.terminator"() : () -> ()
    }) {dimensions = dense<0> : tensor<1xi64>, disc.device = "gpu"} : (memref<110x1300xf32, "gpu">, memref<f32, "gpu">, memref<1300xf32, "gpu">) -> ()
    "lmhlo.terminator"() : () -> ()
  }) {disc.device = "gpu", disc.fusion.name = "main_kColReduction_reduce__3_1_0", disc.fusion_type = "kColReduction"} : () -> ()
  %alloc_2 = memref.alloc() : memref<100x13xf32, "gpu">
  "lmhlo.reshape"(%alloc_1, %alloc_2) {disc.device = "gpu"} : (memref<1300xf32, "gpu">, memref<100x13xf32, "gpu">) -> ()
  return %alloc_2 : memref<100x13xf32, "gpu">
}

SymbolicDimMgr::save walkRankedTensorValue takes: 0 us
SymbolicDimMgr::save update attributes takes: 1 us
SymbolicDimMgr::save updateProductEqualityMap takes: 0 us
SymbolicDimMgr::save updateFunctionType takes: 1 us
SymbolicDimMgr::save collect symbolicDim ops takes: 1 us
SymbolicDimMgr::save remove symbolicDim ops takes: 0 us
SymbolicDimMgr::save remove unused production takes: 0 us
SymbolicDimMgr::save remove unused production #2 takes: 0 us
SymbolicDimMgr::save canonicalize the name takes: 0 us
SymbolicDimMgr::save replace the name takes: 1 us
SymbolicDimMgr::save updateFunctionType takes: 0 us
// -----// IR Dump After DiscSpecializeFusionWithSpeculationPass (disc-specialize-fusion-with-speculation) //----- //
func.func @main(%arg0: memref<110x100x13xf32, "gpu">) -> memref<100x13xf32, "gpu"> attributes {tf.entry_function = {input_placements = "gpu", inputs = "input0", output_placements = "gpu", outputs = "output0"}} {
  %alloc = memref.alloc() : memref<f32, "gpu">
  %alloc_0 = memref.alloc() : memref<110x1300xf32, "gpu">
  %alloc_1 = memref.alloc() : memref<1300xf32, "gpu">
  %c0 = arith.constant 0 : index
  %dim = memref.dim %alloc_0, %c0 : memref<110x1300xf32, "gpu">
  %c1 = arith.constant 1 : index
  %dim_2 = memref.dim %alloc_0, %c1 : memref<110x1300xf32, "gpu">
  %0 = arith.muli %dim, %dim_2 : index
  %c256 = arith.constant 256 : index
  %1 = arith.ceildivsi %0, %c256 : index
  %c108 = arith.constant 108 : index
  %2 = arith.cmpi sgt, %1, %c108 : index
  scf.if %2 {
    "lmhlo.fusion"() ({
      "lmhlo.constant"(%alloc) {disc.device = "gpu", value = dense<0xFF800000> : tensor<f32>} : (memref<f32, "gpu">) -> ()
      "lmhlo.reshape"(%arg0, %alloc_0) {disc.device = "gpu"} : (memref<110x100x13xf32, "gpu">, memref<110x1300xf32, "gpu">) -> ()
      "lmhlo.reduce"(%alloc_0, %alloc, %alloc_1) ({
      ^bb0(%arg1: memref<f32>, %arg2: memref<f32>, %arg3: memref<f32>):
        "lmhlo.maximum"(%arg1, %arg2, %arg3) {disc.device = "gpu"} : (memref<f32>, memref<f32>, memref<f32>) -> ()
        "lmhlo.terminator"() : () -> ()
      }) {dimensions = dense<0> : tensor<1xi64>, disc.device = "gpu"} : (memref<110x1300xf32, "gpu">, memref<f32, "gpu">, memref<1300xf32, "gpu">) -> ()
      "lmhlo.terminator"() : () -> ()
    }) {disc.device = "gpu", disc.fusion.name = "main_kColReduction_reduce__3_1_0", disc.fusion.tag = "8w32h", disc.fusion_type = "kColReduction", disc_col_reduction_schedule_hint = 1 : i32, disc_thread_per_block_hint = 256 : i32} : () -> ()
  } else {
    "lmhlo.fusion"() ({
      "lmhlo.constant"(%alloc) {disc.device = "gpu", value = dense<0xFF800000> : tensor<f32>} : (memref<f32, "gpu">) -> ()
      "lmhlo.reshape"(%arg0, %alloc_0) {disc.device = "gpu"} : (memref<110x100x13xf32, "gpu">, memref<110x1300xf32, "gpu">) -> ()
      "lmhlo.reduce"(%alloc_0, %alloc, %alloc_1) ({
      ^bb0(%arg1: memref<f32>, %arg2: memref<f32>, %arg3: memref<f32>):
        "lmhlo.maximum"(%arg1, %arg2, %arg3) {disc.device = "gpu"} : (memref<f32>, memref<f32>, memref<f32>) -> ()
        "lmhlo.terminator"() : () -> ()
      }) {dimensions = dense<0> : tensor<1xi64>, disc.device = "gpu"} : (memref<110x1300xf32, "gpu">, memref<f32, "gpu">, memref<1300xf32, "gpu">) -> ()
      "lmhlo.terminator"() : () -> ()
    }) {disc.device = "gpu", disc.fusion.name = "main_kColReduction_reduce__3_1_0", disc.fusion.tag = "8w16h", disc.fusion_type = "kColReduction", disc_col_reduction_schedule_hint = 2 : i32, disc_thread_per_block_hint = 128 : i32} : () -> ()
  }
  %alloc_3 = memref.alloc() : memref<100x13xf32, "gpu">
  "lmhlo.reshape"(%alloc_1, %alloc_3) {disc.device = "gpu"} : (memref<1300xf32, "gpu">, memref<100x13xf32, "gpu">) -> ()
  return %alloc_3 : memref<100x13xf32, "gpu">
}

// -----// IR Dump After Canonicalizer (canonicalize) //----- //
func.func @main(%arg0: memref<110x100x13xf32, "gpu">) -> memref<100x13xf32, "gpu"> attributes {tf.entry_function = {input_placements = "gpu", inputs = "input0", output_placements = "gpu", outputs = "output0"}} {
  %alloc = memref.alloc() : memref<f32, "gpu">
  %alloc_0 = memref.alloc() : memref<110x1300xf32, "gpu">
  %alloc_1 = memref.alloc() : memref<1300xf32, "gpu">
  "lmhlo.fusion"() ({
    "lmhlo.constant"(%alloc) {disc.device = "gpu", value = dense<0xFF800000> : tensor<f32>} : (memref<f32, "gpu">) -> ()
    "lmhlo.reshape"(%arg0, %alloc_0) {disc.device = "gpu"} : (memref<110x100x13xf32, "gpu">, memref<110x1300xf32, "gpu">) -> ()
    "lmhlo.reduce"(%alloc_0, %alloc, %alloc_1) ({
    ^bb0(%arg1: memref<f32>, %arg2: memref<f32>, %arg3: memref<f32>):
      "lmhlo.maximum"(%arg1, %arg2, %arg3) {disc.device = "gpu"} : (memref<f32>, memref<f32>, memref<f32>) -> ()
      "lmhlo.terminator"() : () -> ()
    }) {dimensions = dense<0> : tensor<1xi64>, disc.device = "gpu"} : (memref<110x1300xf32, "gpu">, memref<f32, "gpu">, memref<1300xf32, "gpu">) -> ()
    "lmhlo.terminator"() : () -> ()
  }) {disc.device = "gpu", disc.fusion.name = "main_kColReduction_reduce__3_1_0", disc.fusion.tag = "8w32h", disc.fusion_type = "kColReduction", disc_col_reduction_schedule_hint = 1 : i32, disc_thread_per_block_hint = 256 : i32} : () -> ()
  %alloc_2 = memref.alloc() : memref<100x13xf32, "gpu">
  "lmhlo.reshape"(%alloc_1, %alloc_2) {disc.device = "gpu"} : (memref<1300xf32, "gpu">, memref<100x13xf32, "gpu">) -> ()
  return %alloc_2 : memref<100x13xf32, "gpu">
}

// -----// IR Dump After BufferDeallocation (buffer-deallocation) //----- //
func.func @main(%arg0: memref<110x100x13xf32, "gpu">) -> memref<100x13xf32, "gpu"> attributes {tf.entry_function = {input_placements = "gpu", inputs = "input0", output_placements = "gpu", outputs = "output0"}} {
  %alloc = memref.alloc() : memref<f32, "gpu">
  %alloc_0 = memref.alloc() : memref<110x1300xf32, "gpu">
  %alloc_1 = memref.alloc() : memref<1300xf32, "gpu">
  "lmhlo.fusion"() ({
    "lmhlo.constant"(%alloc) {disc.device = "gpu", value = dense<0xFF800000> : tensor<f32>} : (memref<f32, "gpu">) -> ()
    "lmhlo.reshape"(%arg0, %alloc_0) {disc.device = "gpu"} : (memref<110x100x13xf32, "gpu">, memref<110x1300xf32, "gpu">) -> ()
    "lmhlo.reduce"(%alloc_0, %alloc, %alloc_1) ({
    ^bb0(%arg1: memref<f32>, %arg2: memref<f32>, %arg3: memref<f32>):
      "lmhlo.maximum"(%arg1, %arg2, %arg3) {disc.device = "gpu"} : (memref<f32>, memref<f32>, memref<f32>) -> ()
      "lmhlo.terminator"() : () -> ()
    }) {dimensions = dense<0> : tensor<1xi64>, disc.device = "gpu"} : (memref<110x1300xf32, "gpu">, memref<f32, "gpu">, memref<1300xf32, "gpu">) -> ()
    "lmhlo.terminator"() : () -> ()
  }) {disc.device = "gpu", disc.fusion.name = "main_kColReduction_reduce__3_1_0", disc.fusion.tag = "8w32h", disc.fusion_type = "kColReduction", disc_col_reduction_schedule_hint = 1 : i32, disc_thread_per_block_hint = 256 : i32} : () -> ()
  memref.dealloc %alloc_0 : memref<110x1300xf32, "gpu">
  memref.dealloc %alloc : memref<f32, "gpu">
  %alloc_2 = memref.alloc() : memref<100x13xf32, "gpu">
  "lmhlo.reshape"(%alloc_1, %alloc_2) {disc.device = "gpu"} : (memref<1300xf32, "gpu">, memref<100x13xf32, "gpu">) -> ()
  memref.dealloc %alloc_1 : memref<1300xf32, "gpu">
  return %alloc_2 : memref<100x13xf32, "gpu">
}

// -----// IR Dump After RalInjectExecutionContextPass (disc-ral-inject-execution-context) //----- //
module {
  func.func @main(%arg0: !disc_ral.context) attributes {tf.entry_function = {input_placements = "gpu", inputs = "input0", output_placements = "gpu", outputs = "output0"}} {
    %c0 = arith.constant 0 : index
    %0 = "disc_ral.recv_input"(%arg0, %c0) : (!disc_ral.context, index) -> memref<110x100x13xf32, "gpu">
    %alloc = memref.alloc() : memref<f32, "gpu">
    %alloc_0 = memref.alloc() : memref<110x1300xf32, "gpu">
    %alloc_1 = memref.alloc() : memref<1300xf32, "gpu">
    "lmhlo.fusion"() ({
      "lmhlo.constant"(%alloc) {disc.device = "gpu", value = dense<0xFF800000> : tensor<f32>} : (memref<f32, "gpu">) -> ()
      "lmhlo.reshape"(%0, %alloc_0) {disc.device = "gpu"} : (memref<110x100x13xf32, "gpu">, memref<110x1300xf32, "gpu">) -> ()
      "lmhlo.reduce"(%alloc_0, %alloc, %alloc_1) ({
      ^bb0(%arg1: memref<f32>, %arg2: memref<f32>, %arg3: memref<f32>):
        "lmhlo.maximum"(%arg1, %arg2, %arg3) {disc.device = "gpu"} : (memref<f32>, memref<f32>, memref<f32>) -> ()
        "lmhlo.terminator"() : () -> ()
      }) {dimensions = dense<0> : tensor<1xi64>, disc.device = "gpu"} : (memref<110x1300xf32, "gpu">, memref<f32, "gpu">, memref<1300xf32, "gpu">) -> ()
      "lmhlo.terminator"() : () -> ()
    }) {disc.device = "gpu", disc.fusion.name = "main_kColReduction_reduce__3_1_0", disc.fusion.tag = "8w32h", disc.fusion_type = "kColReduction", disc_col_reduction_schedule_hint = 1 : i32, disc_thread_per_block_hint = 256 : i32} : () -> ()
    memref.dealloc %alloc_0 : memref<110x1300xf32, "gpu">
    memref.dealloc %alloc : memref<f32, "gpu">
    %alloc_2 = memref.alloc() : memref<100x13xf32, "gpu">
    "lmhlo.reshape"(%alloc_1, %alloc_2) {disc.device = "gpu"} : (memref<1300xf32, "gpu">, memref<100x13xf32, "gpu">) -> ()
    memref.dealloc %alloc_1 : memref<1300xf32, "gpu">
    %c0_3 = arith.constant 0 : index
    "disc_ral.send_output"(%arg0, %c0_3, %alloc_2) : (!disc_ral.context, index, memref<100x13xf32, "gpu">) -> ()
    return
  }
  func.func @shape_constraint_graph() {
    return
  }
}


// -----// IR Dump After DiscLowerToLibraryCallPass (disc-lower-to-library-call) //----- //
func.func @main(%arg0: !disc_ral.context) attributes {tf.entry_function = {input_placements = "gpu", inputs = "input0", output_placements = "gpu", outputs = "output0"}} {
  %0 = llvm.mlir.constant(0 : i32) : i32
  %c0 = arith.constant 0 : index
  %c100 = arith.constant 100 : index
  %c1 = arith.constant 1 : index
  %c13 = arith.constant 13 : index
  %1 = "disc_ral.dispatch"(%arg0, %c0) {backend_config = "", call_target_name = "ral_recv_input", device = "cpu", has_side_effect = false} : (!disc_ral.context, index) -> memref<110x100x13xf32, "gpu">
  %alloc = memref.alloc() : memref<f32, "gpu">
  %alloc_0 = memref.alloc() : memref<110x1300xf32, "gpu">
  %alloc_1 = memref.alloc() : memref<1300xf32, "gpu">
  "lmhlo.fusion"() ({
    "lmhlo.constant"(%alloc) {disc.device = "gpu", value = dense<0xFF800000> : tensor<f32>} : (memref<f32, "gpu">) -> ()
    "lmhlo.reshape"(%1, %alloc_0) {disc.device = "gpu"} : (memref<110x100x13xf32, "gpu">, memref<110x1300xf32, "gpu">) -> ()
    "lmhlo.reduce"(%alloc_0, %alloc, %alloc_1) ({
    ^bb0(%arg1: memref<f32>, %arg2: memref<f32>, %arg3: memref<f32>):
      "lmhlo.maximum"(%arg1, %arg2, %arg3) {disc.device = "gpu"} : (memref<f32>, memref<f32>, memref<f32>) -> ()
      "lmhlo.terminator"() : () -> ()
    }) {dimensions = dense<0> : tensor<1xi64>, disc.device = "gpu"} : (memref<110x1300xf32, "gpu">, memref<f32, "gpu">, memref<1300xf32, "gpu">) -> ()
    "lmhlo.terminator"() : () -> ()
  }) {disc.device = "gpu", disc.fusion.name = "main_kColReduction_reduce__3_1_0", disc.fusion.tag = "8w32h", disc.fusion_type = "kColReduction", disc_col_reduction_schedule_hint = 1 : i32, disc_thread_per_block_hint = 256 : i32} : () -> ()
  memref.dealloc %alloc_0 : memref<110x1300xf32, "gpu">
  memref.dealloc %alloc : memref<f32, "gpu">
  %2 = llvm.inttoptr %0 : i32 to !llvm.ptr<i8>
  %alloca = memref.alloca() : memref<2xindex, "cpu">
  memref.store %c100, %alloca[%c0] : memref<2xindex, "cpu">
  memref.store %c13, %alloca[%c1] : memref<2xindex, "cpu">
  %3 = "disc_ral.dispatch"(%arg0, %2, %alloc_1, %alloca) {backend_config = "", call_target_name = "inc_ref", device = "gpu", has_side_effect = false} : (!disc_ral.context, !llvm.ptr<i8>, memref<1300xf32, "gpu">, memref<2xindex, "cpu">) -> memref<100x13xf32, "gpu">
  %reinterpret_cast = memref.reinterpret_cast %3 to offset: [0], sizes: [100, 13], strides: [13, 1] : memref<100x13xf32, "gpu"> to memref<100x13xf32, "gpu">
  memref.dealloc %alloc_1 : memref<1300xf32, "gpu">
  "disc_ral.dispatch"(%arg0, %c0, %reinterpret_cast) {backend_config = "", call_target_name = "ral_send_output", device = "cpu", has_side_effect = false} : (!disc_ral.context, index, memref<100x13xf32, "gpu">) -> ()
  return
}

kColReduction <main_kColReduction_reduce__3_1_0___8w32h>, use_new: 0 schedule_hint: 1
SymbolicDimMgr::save walkRankedTensorValue takes: 2 us
SymbolicDimMgr::save update attributes takes: 2 us
SymbolicDimMgr::save updateProductEqualityMap takes: 0 us
SymbolicDimMgr::save updateFunctionType takes: 2 us
SymbolicDimMgr::save collect symbolicDim ops takes: 4 us
SymbolicDimMgr::save remove symbolicDim ops takes: 0 us
SymbolicDimMgr::save remove unused production takes: 0 us
SymbolicDimMgr::save remove unused production #2 takes: 0 us
SymbolicDimMgr::save canonicalize the name takes: 0 us
SymbolicDimMgr::save replace the name takes: 3 us
SymbolicDimMgr::save updateFunctionType takes: 1 us
// -----// IR Dump After DiscLhloLegalizeRootsToParallelLoopsPass (disc-lhlo-legalize-roots-to-parallel-loops) //----- //
func.func @main(%arg0: !disc_ral.context) attributes {tf.entry_function = {input_placements = "gpu", inputs = "input0", output_placements = "gpu", outputs = "output0"}} {
  %c110 = arith.constant 110 : index
  %c41 = arith.constant 41 : index
  %c2 = arith.constant 2 : index
  %c4 = arith.constant 4 : index
  %c256 = arith.constant 256 : index
  %c64 = arith.constant 64 : index
  %c8 = arith.constant 8 : index
  %c32 = arith.constant 32 : index
  %c1300 = arith.constant 1300 : index
  %0 = llvm.mlir.constant(0 : i32) : i32
  %c0 = arith.constant 0 : index
  %c100 = arith.constant 100 : index
  %c1 = arith.constant 1 : index
  %c13 = arith.constant 13 : index
  %1 = "disc_ral.dispatch"(%arg0, %c0) {backend_config = "", call_target_name = "ral_recv_input", device = "cpu", has_side_effect = false} : (!disc_ral.context, index) -> memref<110x100x13xf32, "gpu">
  %alloc = memref.alloc() : memref<f32, "gpu">
  %alloc_0 = memref.alloc() : memref<110x1300xf32, "gpu">
  %alloc_1 = memref.alloc() : memref<1300xf32, "gpu">
  "lmhlo.fusion"() ({
    "lmhlo.constant"(%alloc) {disc.device = "gpu", value = dense<0xFF800000> : tensor<f32>} : (memref<f32, "gpu">) -> ()
    "lmhlo.reshape"(%1, %alloc_0) {disc.device = "gpu"} : (memref<110x100x13xf32, "gpu">, memref<110x1300xf32, "gpu">) -> ()
    scf.parallel (%arg1) = (%c0) to (%c1300) step (%c1) {
      %4 = "disc_shape.delinearize"(%arg1, %c1300) : (index, index) -> index
      %5 = memref.load %alloc[] : memref<f32, "gpu">
      memref.store %5, %alloc_1[%4] : memref<1300xf32, "gpu">
      scf.yield
    }
    scf.parallel (%arg1, %arg2) = (%c0, %c0) to (%c41, %c256) step (%c1, %c1) {
      %4 = memref.load %alloc[] : memref<f32, "gpu">
      %5 = arith.divui %arg1, %c41 : index
      %6 = arith.remui %arg1, %c41 : index
      %7 = arith.divui %arg2, %c32 : index
      %8 = arith.remui %arg2, %c32 : index
      %9 = arith.muli %8, %c8 : index
      %10 = arith.addi %7, %9 : index
      %11 = arith.muli %6, %c32 : index
      %12 = arith.addi %8, %11 : index
      %alloc_2 = memref.alloc() : memref<256xf32, #gpu.address_space<workgroup>>
      %13 = arith.cmpi ult, %12, %c1300 : index
      %14 = scf.if %13 -> (f32) {
        %19 = scf.for %arg3 = %c0 to %c64 step %c1 iter_args(%arg4 = %4) -> (f32) {
          %20 = arith.muli %5, %c8 : index
          %21 = arith.addi %7, %20 : index
          %22 = arith.muli %21, %c64 : index
          %23 = arith.addi %arg3, %22 : index
          %24 = arith.cmpi slt, %23, %c110 : index
          %25 = scf.if %24 -> (f32) {
            %26 = memref.load %alloc_0[%23, %12] : memref<110x1300xf32, "gpu">
            %27 = arith.maxf %arg4, %26 : f32
            scf.yield %27 : f32
          } else {
            scf.yield %arg4 : f32
          }
          scf.yield %25 : f32
        }
        scf.yield %19 : f32
      } else {
        scf.yield %4 : f32
      }
      memref.store %14, %alloc_2[%10] : memref<256xf32, #gpu.address_space<workgroup>>
      gpu.barrier
      %15 = arith.cmpi slt, %7, %c4 : index
      scf.if %15 {
        %19 = arith.addi %10, %c4 : index
        %20 = memref.load %alloc_2[%10] : memref<256xf32, #gpu.address_space<workgroup>>
        %21 = memref.load %alloc_2[%19] : memref<256xf32, #gpu.address_space<workgroup>>
        %22 = arith.maxf %20, %21 : f32
        memref.store %22, %alloc_2[%10] : memref<256xf32, #gpu.address_space<workgroup>>
      }
      gpu.barrier
      %16 = arith.cmpi slt, %7, %c2 : index
      scf.if %16 {
        %19 = arith.addi %10, %c2 : index
        %20 = memref.load %alloc_2[%10] : memref<256xf32, #gpu.address_space<workgroup>>
        %21 = memref.load %alloc_2[%19] : memref<256xf32, #gpu.address_space<workgroup>>
        %22 = arith.maxf %20, %21 : f32
        memref.store %22, %alloc_2[%10] : memref<256xf32, #gpu.address_space<workgroup>>
      }
      gpu.barrier
      %17 = arith.cmpi eq, %7, %c0 : index
      %18 = arith.andi %17, %13 : i1
      scf.if %18 {
        %19 = arith.addi %10, %c1 : index
        %20 = memref.load %alloc_2[%10] : memref<256xf32, #gpu.address_space<workgroup>>
        %21 = memref.load %alloc_2[%19] : memref<256xf32, #gpu.address_space<workgroup>>
        %22 = arith.maxf %20, %21 : f32
        %23 = memref.atomic_rmw maxf %22, %alloc_1[%12] : (f32, memref<1300xf32, "gpu">) -> f32
      }
      scf.yield
    }
    "lmhlo.terminator"() : () -> ()
  }) {disc.device = "gpu", disc.fusion.name = "main_kColReduction_reduce__3_1_0", disc.fusion.tag = "8w32h", disc.fusion_type = "kColReduction", disc_col_reduction_schedule_hint = 1 : i32, disc_thread_per_block_hint = 256 : i32} : () -> ()
  memref.dealloc %alloc_0 : memref<110x1300xf32, "gpu">
  memref.dealloc %alloc : memref<f32, "gpu">
  %2 = llvm.inttoptr %0 : i32 to !llvm.ptr<i8>
  %alloca = memref.alloca() : memref<2xindex, "cpu">
  memref.store %c100, %alloca[%c0] : memref<2xindex, "cpu">
  memref.store %c13, %alloca[%c1] : memref<2xindex, "cpu">
  %3 = "disc_ral.dispatch"(%arg0, %2, %alloc_1, %alloca) {backend_config = "", call_target_name = "inc_ref", device = "gpu", has_side_effect = false} : (!disc_ral.context, !llvm.ptr<i8>, memref<1300xf32, "gpu">, memref<2xindex, "cpu">) -> memref<100x13xf32, "gpu">
  %reinterpret_cast = memref.reinterpret_cast %3 to offset: [0], sizes: [100, 13], strides: [13, 1] : memref<100x13xf32, "gpu"> to memref<100x13xf32, "gpu">
  memref.dealloc %alloc_1 : memref<1300xf32, "gpu">
  "disc_ral.dispatch"(%arg0, %c0, %reinterpret_cast) {backend_config = "", call_target_name = "ral_send_output", device = "cpu", has_side_effect = false} : (!disc_ral.context, index, memref<100x13xf32, "gpu">) -> ()
  return
}

// -----// IR Dump After ExpandOps (memref-expand) //----- //
func.func @main(%arg0: !disc_ral.context) attributes {tf.entry_function = {input_placements = "gpu", inputs = "input0", output_placements = "gpu", outputs = "output0"}} {
  %c110 = arith.constant 110 : index
  %c41 = arith.constant 41 : index
  %c2 = arith.constant 2 : index
  %c4 = arith.constant 4 : index
  %c256 = arith.constant 256 : index
  %c64 = arith.constant 64 : index
  %c8 = arith.constant 8 : index
  %c32 = arith.constant 32 : index
  %c1300 = arith.constant 1300 : index
  %0 = llvm.mlir.constant(0 : i32) : i32
  %c0 = arith.constant 0 : index
  %c100 = arith.constant 100 : index
  %c1 = arith.constant 1 : index
  %c13 = arith.constant 13 : index
  %1 = "disc_ral.dispatch"(%arg0, %c0) {backend_config = "", call_target_name = "ral_recv_input", device = "cpu", has_side_effect = false} : (!disc_ral.context, index) -> memref<110x100x13xf32, "gpu">
  %alloc = memref.alloc() : memref<f32, "gpu">
  %alloc_0 = memref.alloc() : memref<110x1300xf32, "gpu">
  %alloc_1 = memref.alloc() : memref<1300xf32, "gpu">
  "lmhlo.fusion"() ({
    "lmhlo.constant"(%alloc) {disc.device = "gpu", value = dense<0xFF800000> : tensor<f32>} : (memref<f32, "gpu">) -> ()
    "lmhlo.reshape"(%1, %alloc_0) {disc.device = "gpu"} : (memref<110x100x13xf32, "gpu">, memref<110x1300xf32, "gpu">) -> ()
    scf.parallel (%arg1) = (%c0) to (%c1300) step (%c1) {
      %4 = "disc_shape.delinearize"(%arg1, %c1300) : (index, index) -> index
      %5 = memref.load %alloc[] : memref<f32, "gpu">
      memref.store %5, %alloc_1[%4] : memref<1300xf32, "gpu">
      scf.yield
    }
    scf.parallel (%arg1, %arg2) = (%c0, %c0) to (%c41, %c256) step (%c1, %c1) {
      %4 = memref.load %alloc[] : memref<f32, "gpu">
      %5 = arith.divui %arg1, %c41 : index
      %6 = arith.remui %arg1, %c41 : index
      %7 = arith.divui %arg2, %c32 : index
      %8 = arith.remui %arg2, %c32 : index
      %9 = arith.muli %8, %c8 : index
      %10 = arith.addi %7, %9 : index
      %11 = arith.muli %6, %c32 : index
      %12 = arith.addi %8, %11 : index
      %alloc_2 = memref.alloc() : memref<256xf32, #gpu.address_space<workgroup>>
      %13 = arith.cmpi ult, %12, %c1300 : index
      %14 = scf.if %13 -> (f32) {
        %19 = scf.for %arg3 = %c0 to %c64 step %c1 iter_args(%arg4 = %4) -> (f32) {
          %20 = arith.muli %5, %c8 : index
          %21 = arith.addi %7, %20 : index
          %22 = arith.muli %21, %c64 : index
          %23 = arith.addi %arg3, %22 : index
          %24 = arith.cmpi slt, %23, %c110 : index
          %25 = scf.if %24 -> (f32) {
            %26 = memref.load %alloc_0[%23, %12] : memref<110x1300xf32, "gpu">
            %27 = arith.maxf %arg4, %26 : f32
            scf.yield %27 : f32
          } else {
            scf.yield %arg4 : f32
          }
          scf.yield %25 : f32
        }
        scf.yield %19 : f32
      } else {
        scf.yield %4 : f32
      }
      memref.store %14, %alloc_2[%10] : memref<256xf32, #gpu.address_space<workgroup>>
      gpu.barrier
      %15 = arith.cmpi slt, %7, %c4 : index
      scf.if %15 {
        %19 = arith.addi %10, %c4 : index
        %20 = memref.load %alloc_2[%10] : memref<256xf32, #gpu.address_space<workgroup>>
        %21 = memref.load %alloc_2[%19] : memref<256xf32, #gpu.address_space<workgroup>>
        %22 = arith.maxf %20, %21 : f32
        memref.store %22, %alloc_2[%10] : memref<256xf32, #gpu.address_space<workgroup>>
      }
      gpu.barrier
      %16 = arith.cmpi slt, %7, %c2 : index
      scf.if %16 {
        %19 = arith.addi %10, %c2 : index
        %20 = memref.load %alloc_2[%10] : memref<256xf32, #gpu.address_space<workgroup>>
        %21 = memref.load %alloc_2[%19] : memref<256xf32, #gpu.address_space<workgroup>>
        %22 = arith.maxf %20, %21 : f32
        memref.store %22, %alloc_2[%10] : memref<256xf32, #gpu.address_space<workgroup>>
      }
      gpu.barrier
      %17 = arith.cmpi eq, %7, %c0 : index
      %18 = arith.andi %17, %13 : i1
      scf.if %18 {
        %19 = arith.addi %10, %c1 : index
        %20 = memref.load %alloc_2[%10] : memref<256xf32, #gpu.address_space<workgroup>>
        %21 = memref.load %alloc_2[%19] : memref<256xf32, #gpu.address_space<workgroup>>
        %22 = arith.maxf %20, %21 : f32
        %23 = memref.generic_atomic_rmw %alloc_1[%12] : memref<1300xf32, "gpu"> {
        ^bb0(%arg3: f32):
          %24 = arith.cmpf ogt, %arg3, %22 : f32
          %25 = arith.select %24, %arg3, %22 : f32
          memref.atomic_yield %25 : f32
        }
      }
      scf.yield
    }
    "lmhlo.terminator"() : () -> ()
  }) {disc.device = "gpu", disc.fusion.name = "main_kColReduction_reduce__3_1_0", disc.fusion.tag = "8w32h", disc.fusion_type = "kColReduction", disc_col_reduction_schedule_hint = 1 : i32, disc_thread_per_block_hint = 256 : i32} : () -> ()
  memref.dealloc %alloc_0 : memref<110x1300xf32, "gpu">
  memref.dealloc %alloc : memref<f32, "gpu">
  %2 = llvm.inttoptr %0 : i32 to !llvm.ptr<i8>
  %alloca = memref.alloca() : memref<2xindex, "cpu">
  memref.store %c100, %alloca[%c0] : memref<2xindex, "cpu">
  memref.store %c13, %alloca[%c1] : memref<2xindex, "cpu">
  %3 = "disc_ral.dispatch"(%arg0, %2, %alloc_1, %alloca) {backend_config = "", call_target_name = "inc_ref", device = "gpu", has_side_effect = false} : (!disc_ral.context, !llvm.ptr<i8>, memref<1300xf32, "gpu">, memref<2xindex, "cpu">) -> memref<100x13xf32, "gpu">
  %reinterpret_cast = memref.reinterpret_cast %3 to offset: [0], sizes: [100, 13], strides: [13, 1] : memref<100x13xf32, "gpu"> to memref<100x13xf32, "gpu">
  memref.dealloc %alloc_1 : memref<1300xf32, "gpu">
  "disc_ral.dispatch"(%arg0, %c0, %reinterpret_cast) {backend_config = "", call_target_name = "ral_send_output", device = "cpu", has_side_effect = false} : (!disc_ral.context, index, memref<100x13xf32, "gpu">) -> ()
  return
}

// -----// IR Dump After InputInlineFusionPass (disc-input-inline-fusion) //----- //
func.func @main(%arg0: !disc_ral.context) attributes {tf.entry_function = {input_placements = "gpu", inputs = "input0", output_placements = "gpu", outputs = "output0"}} {
  %cst = arith.constant 0xFF800000 : f32
  %c110 = arith.constant 110 : index
  %c41 = arith.constant 41 : index
  %c2 = arith.constant 2 : index
  %c4 = arith.constant 4 : index
  %c256 = arith.constant 256 : index
  %c64 = arith.constant 64 : index
  %c8 = arith.constant 8 : index
  %c32 = arith.constant 32 : index
  %c1300 = arith.constant 1300 : index
  %0 = llvm.mlir.constant(0 : i32) : i32
  %c0 = arith.constant 0 : index
  %c100 = arith.constant 100 : index
  %c1 = arith.constant 1 : index
  %c13 = arith.constant 13 : index
  %1 = "disc_ral.dispatch"(%arg0, %c0) {backend_config = "", call_target_name = "ral_recv_input", device = "cpu", has_side_effect = false} : (!disc_ral.context, index) -> memref<110x100x13xf32, "gpu">
  %alloc = memref.alloc() : memref<f32, "gpu">
  %alloc_0 = memref.alloc() : memref<110x1300xf32, "gpu">
  %alloc_1 = memref.alloc() : memref<1300xf32, "gpu">
  "lmhlo.fusion"() ({
    scf.parallel (%arg1) = (%c0) to (%c1300) step (%c1) {
      %4 = "disc_shape.delinearize"(%arg1, %c1300) : (index, index) -> index
      memref.store %cst, %alloc_1[%4] : memref<1300xf32, "gpu">
      scf.yield
    }
    scf.parallel (%arg1, %arg2) = (%c0, %c0) to (%c41, %c256) step (%c1, %c1) {
      %4 = arith.divui %arg1, %c41 : index
      %5 = arith.remui %arg1, %c41 : index
      %6 = arith.divui %arg2, %c32 : index
      %7 = arith.remui %arg2, %c32 : index
      %8 = arith.muli %7, %c8 : index
      %9 = arith.addi %6, %8 : index
      %10 = arith.muli %5, %c32 : index
      %11 = arith.addi %7, %10 : index
      %alloc_2 = memref.alloc() : memref<256xf32, #gpu.address_space<workgroup>>
      %12 = arith.cmpi ult, %11, %c1300 : index
      %13 = scf.if %12 -> (f32) {
        %18 = scf.for %arg3 = %c0 to %c64 step %c1 iter_args(%arg4 = %cst) -> (f32) {
          %19 = arith.muli %4, %c8 : index
          %20 = arith.addi %6, %19 : index
          %21 = arith.muli %20, %c64 : index
          %22 = arith.addi %arg3, %21 : index
          %23 = arith.cmpi slt, %22, %c110 : index
          %24 = scf.if %23 -> (f32) {
            %25 = "disc_shape.linearize"(%22, %11, %c110, %c1300) {operand_segment_sizes = array<i32: 2, 2>} : (index, index, index, index) -> index
            %26:3 = "disc_shape.delinearize"(%25, %c110, %c100, %c13) : (index, index, index, index) -> (index, index, index)
            %27 = memref.load %1[%26#0, %26#1, %26#2] : memref<110x100x13xf32, "gpu">
            %28 = arith.maxf %arg4, %27 : f32
            scf.yield %28 : f32
          } else {
            scf.yield %arg4 : f32
          }
          scf.yield %24 : f32
        }
        scf.yield %18 : f32
      } else {
        scf.yield %cst : f32
      }
      memref.store %13, %alloc_2[%9] : memref<256xf32, #gpu.address_space<workgroup>>
      gpu.barrier
      %14 = arith.cmpi slt, %6, %c4 : index
      scf.if %14 {
        %18 = arith.addi %9, %c4 : index
        %19 = memref.load %alloc_2[%9] : memref<256xf32, #gpu.address_space<workgroup>>
        %20 = memref.load %alloc_2[%18] : memref<256xf32, #gpu.address_space<workgroup>>
        %21 = arith.maxf %19, %20 : f32
        memref.store %21, %alloc_2[%9] : memref<256xf32, #gpu.address_space<workgroup>>
      }
      gpu.barrier
      %15 = arith.cmpi slt, %6, %c2 : index
      scf.if %15 {
        %18 = arith.addi %9, %c2 : index
        %19 = memref.load %alloc_2[%9] : memref<256xf32, #gpu.address_space<workgroup>>
        %20 = memref.load %alloc_2[%18] : memref<256xf32, #gpu.address_space<workgroup>>
        %21 = arith.maxf %19, %20 : f32
        memref.store %21, %alloc_2[%9] : memref<256xf32, #gpu.address_space<workgroup>>
      }
      gpu.barrier
      %16 = arith.cmpi eq, %6, %c0 : index
      %17 = arith.andi %16, %12 : i1
      scf.if %17 {
        %18 = arith.addi %9, %c1 : index
        %19 = memref.load %alloc_2[%9] : memref<256xf32, #gpu.address_space<workgroup>>
        %20 = memref.load %alloc_2[%18] : memref<256xf32, #gpu.address_space<workgroup>>
        %21 = arith.maxf %19, %20 : f32
        %22 = memref.generic_atomic_rmw %alloc_1[%11] : memref<1300xf32, "gpu"> {
        ^bb0(%arg3: f32):
          %23 = arith.cmpf ogt, %arg3, %21 : f32
          %24 = arith.select %23, %arg3, %21 : f32
          memref.atomic_yield %24 : f32
        }
      }
      scf.yield
    }
    "lmhlo.terminator"() : () -> ()
  }) {disc.device = "gpu", disc.fusion.name = "main_kColReduction_reduce__3_1_0", disc.fusion.tag = "8w32h", disc.fusion_type = "kColReduction", disc_col_reduction_schedule_hint = 1 : i32, disc_thread_per_block_hint = 256 : i32} : () -> ()
  memref.dealloc %alloc_0 : memref<110x1300xf32, "gpu">
  memref.dealloc %alloc : memref<f32, "gpu">
  %2 = llvm.inttoptr %0 : i32 to !llvm.ptr<i8>
  %alloca = memref.alloca() : memref<2xindex, "cpu">
  memref.store %c100, %alloca[%c0] : memref<2xindex, "cpu">
  memref.store %c13, %alloca[%c1] : memref<2xindex, "cpu">
  %3 = "disc_ral.dispatch"(%arg0, %2, %alloc_1, %alloca) {backend_config = "", call_target_name = "inc_ref", device = "gpu", has_side_effect = false} : (!disc_ral.context, !llvm.ptr<i8>, memref<1300xf32, "gpu">, memref<2xindex, "cpu">) -> memref<100x13xf32, "gpu">
  %reinterpret_cast = memref.reinterpret_cast %3 to offset: [0], sizes: [100, 13], strides: [13, 1] : memref<100x13xf32, "gpu"> to memref<100x13xf32, "gpu">
  memref.dealloc %alloc_1 : memref<1300xf32, "gpu">
  "disc_ral.dispatch"(%arg0, %c0, %reinterpret_cast) {backend_config = "", call_target_name = "ral_send_output", device = "cpu", has_side_effect = false} : (!disc_ral.context, index, memref<100x13xf32, "gpu">) -> ()
  return
}

// -----// IR Dump After ArithExpandOps (arith-expand) //----- //
func.func @main(%arg0: !disc_ral.context) attributes {tf.entry_function = {input_placements = "gpu", inputs = "input0", output_placements = "gpu", outputs = "output0"}} {
  %cst = arith.constant 0xFF800000 : f32
  %c110 = arith.constant 110 : index
  %c41 = arith.constant 41 : index
  %c2 = arith.constant 2 : index
  %c4 = arith.constant 4 : index
  %c256 = arith.constant 256 : index
  %c64 = arith.constant 64 : index
  %c8 = arith.constant 8 : index
  %c32 = arith.constant 32 : index
  %c1300 = arith.constant 1300 : index
  %0 = llvm.mlir.constant(0 : i32) : i32
  %c0 = arith.constant 0 : index
  %c100 = arith.constant 100 : index
  %c1 = arith.constant 1 : index
  %c13 = arith.constant 13 : index
  %1 = "disc_ral.dispatch"(%arg0, %c0) {backend_config = "", call_target_name = "ral_recv_input", device = "cpu", has_side_effect = false} : (!disc_ral.context, index) -> memref<110x100x13xf32, "gpu">
  %alloc = memref.alloc() : memref<f32, "gpu">
  %alloc_0 = memref.alloc() : memref<110x1300xf32, "gpu">
  %alloc_1 = memref.alloc() : memref<1300xf32, "gpu">
  "lmhlo.fusion"() ({
    scf.parallel (%arg1) = (%c0) to (%c1300) step (%c1) {
      %4 = "disc_shape.delinearize"(%arg1, %c1300) : (index, index) -> index
      memref.store %cst, %alloc_1[%4] : memref<1300xf32, "gpu">
      scf.yield
    }
    scf.parallel (%arg1, %arg2) = (%c0, %c0) to (%c41, %c256) step (%c1, %c1) {
      %4 = arith.divui %arg1, %c41 : index
      %5 = arith.remui %arg1, %c41 : index
      %6 = arith.divui %arg2, %c32 : index
      %7 = arith.remui %arg2, %c32 : index
      %8 = arith.muli %7, %c8 : index
      %9 = arith.addi %6, %8 : index
      %10 = arith.muli %5, %c32 : index
      %11 = arith.addi %7, %10 : index
      %alloc_2 = memref.alloc() : memref<256xf32, #gpu.address_space<workgroup>>
      %12 = arith.cmpi ult, %11, %c1300 : index
      %13 = scf.if %12 -> (f32) {
        %18 = scf.for %arg3 = %c0 to %c64 step %c1 iter_args(%arg4 = %cst) -> (f32) {
          %19 = arith.muli %4, %c8 : index
          %20 = arith.addi %6, %19 : index
          %21 = arith.muli %20, %c64 : index
          %22 = arith.addi %arg3, %21 : index
          %23 = arith.cmpi slt, %22, %c110 : index
          %24 = scf.if %23 -> (f32) {
            %25 = "disc_shape.linearize"(%22, %11, %c110, %c1300) {operand_segment_sizes = array<i32: 2, 2>} : (index, index, index, index) -> index
            %26:3 = "disc_shape.delinearize"(%25, %c110, %c100, %c13) : (index, index, index, index) -> (index, index, index)
            %27 = memref.load %1[%26#0, %26#1, %26#2] : memref<110x100x13xf32, "gpu">
            %28 = arith.cmpf ugt, %arg4, %27 : f32
            %29 = arith.select %28, %arg4, %27 : f32
            %30 = arith.cmpf uno, %27, %27 : f32
            %31 = arith.select %30, %27, %29 : f32
            scf.yield %31 : f32
          } else {
            scf.yield %arg4 : f32
          }
          scf.yield %24 : f32
        }
        scf.yield %18 : f32
      } else {
        scf.yield %cst : f32
      }
      memref.store %13, %alloc_2[%9] : memref<256xf32, #gpu.address_space<workgroup>>
      gpu.barrier
      %14 = arith.cmpi slt, %6, %c4 : index
      scf.if %14 {
        %18 = arith.addi %9, %c4 : index
        %19 = memref.load %alloc_2[%9] : memref<256xf32, #gpu.address_space<workgroup>>
        %20 = memref.load %alloc_2[%18] : memref<256xf32, #gpu.address_space<workgroup>>
        %21 = arith.cmpf ugt, %19, %20 : f32
        %22 = arith.select %21, %19, %20 : f32
        %23 = arith.cmpf uno, %20, %20 : f32
        %24 = arith.select %23, %20, %22 : f32
        memref.store %24, %alloc_2[%9] : memref<256xf32, #gpu.address_space<workgroup>>
      }
      gpu.barrier
      %15 = arith.cmpi slt, %6, %c2 : index
      scf.if %15 {
        %18 = arith.addi %9, %c2 : index
        %19 = memref.load %alloc_2[%9] : memref<256xf32, #gpu.address_space<workgroup>>
        %20 = memref.load %alloc_2[%18] : memref<256xf32, #gpu.address_space<workgroup>>
        %21 = arith.cmpf ugt, %19, %20 : f32
        %22 = arith.select %21, %19, %20 : f32
        %23 = arith.cmpf uno, %20, %20 : f32
        %24 = arith.select %23, %20, %22 : f32
        memref.store %24, %alloc_2[%9] : memref<256xf32, #gpu.address_space<workgroup>>
      }
      gpu.barrier
      %16 = arith.cmpi eq, %6, %c0 : index
      %17 = arith.andi %16, %12 : i1
      scf.if %17 {
        %18 = arith.addi %9, %c1 : index
        %19 = memref.load %alloc_2[%9] : memref<256xf32, #gpu.address_space<workgroup>>
        %20 = memref.load %alloc_2[%18] : memref<256xf32, #gpu.address_space<workgroup>>
        %21 = arith.cmpf ugt, %19, %20 : f32
        %22 = arith.select %21, %19, %20 : f32
        %23 = arith.cmpf uno, %20, %20 : f32
        %24 = arith.select %23, %20, %22 : f32
        %25 = memref.generic_atomic_rmw %alloc_1[%11] : memref<1300xf32, "gpu"> {
        ^bb0(%arg3: f32):
          %26 = arith.cmpf ogt, %arg3, %24 : f32
          %27 = arith.select %26, %arg3, %24 : f32
          memref.atomic_yield %27 : f32
        }
      }
      scf.yield
    }
    "lmhlo.terminator"() : () -> ()
  }) {disc.device = "gpu", disc.fusion.name = "main_kColReduction_reduce__3_1_0", disc.fusion.tag = "8w32h", disc.fusion_type = "kColReduction", disc_col_reduction_schedule_hint = 1 : i32, disc_thread_per_block_hint = 256 : i32} : () -> ()
  memref.dealloc %alloc_0 : memref<110x1300xf32, "gpu">
  memref.dealloc %alloc : memref<f32, "gpu">
  %2 = llvm.inttoptr %0 : i32 to !llvm.ptr<i8>
  %alloca = memref.alloca() : memref<2xindex, "cpu">
  memref.store %c100, %alloca[%c0] : memref<2xindex, "cpu">
  memref.store %c13, %alloca[%c1] : memref<2xindex, "cpu">
  %3 = "disc_ral.dispatch"(%arg0, %2, %alloc_1, %alloca) {backend_config = "", call_target_name = "inc_ref", device = "gpu", has_side_effect = false} : (!disc_ral.context, !llvm.ptr<i8>, memref<1300xf32, "gpu">, memref<2xindex, "cpu">) -> memref<100x13xf32, "gpu">
  %reinterpret_cast = memref.reinterpret_cast %3 to offset: [0], sizes: [100, 13], strides: [13, 1] : memref<100x13xf32, "gpu"> to memref<100x13xf32, "gpu">
  memref.dealloc %alloc_1 : memref<1300xf32, "gpu">
  "disc_ral.dispatch"(%arg0, %c0, %reinterpret_cast) {backend_config = "", call_target_name = "ral_send_output", device = "cpu", has_side_effect = false} : (!disc_ral.context, index, memref<100x13xf32, "gpu">) -> ()
  return
}

// -----// IR Dump After DiscFlattenMemrefAccessPass (disc-flatten-memref-access) //----- //
func.func @main(%arg0: !disc_ral.context) attributes {tf.entry_function = {input_placements = "gpu", inputs = "input0", output_placements = "gpu", outputs = "output0"}} {
  %cst = arith.constant 0xFF800000 : f32
  %c110 = arith.constant 110 : index
  %c41 = arith.constant 41 : index
  %c2 = arith.constant 2 : index
  %c4 = arith.constant 4 : index
  %c256 = arith.constant 256 : index
  %c64 = arith.constant 64 : index
  %c8 = arith.constant 8 : index
  %c32 = arith.constant 32 : index
  %c1300 = arith.constant 1300 : index
  %0 = llvm.mlir.constant(0 : i32) : i32
  %c0 = arith.constant 0 : index
  %c100 = arith.constant 100 : index
  %c1 = arith.constant 1 : index
  %c13 = arith.constant 13 : index
  %1 = "disc_ral.dispatch"(%arg0, %c0) {backend_config = "", call_target_name = "ral_recv_input", device = "cpu", has_side_effect = false} : (!disc_ral.context, index) -> memref<110x100x13xf32, "gpu">
  %alloc = memref.alloc() : memref<f32, "gpu">
  %alloc_0 = memref.alloc() : memref<110x1300xf32, "gpu">
  %alloc_1 = memref.alloc() : memref<1300xf32, "gpu">
  "lmhlo.fusion"() ({
    scf.parallel (%arg1) = (%c0) to (%c1300) step (%c1) {
      %4 = "disc_shape.delinearize"(%arg1, %c1300) : (index, index) -> index
      %c1300_2 = arith.constant 1300 : index
      %5 = "disc_shape.linearize"(%4, %c1300_2) {operand_segment_sizes = array<i32: 1, 1>} : (index, index) -> index
      %reinterpret_cast_3 = memref.reinterpret_cast %alloc_1 to offset: [0], sizes: [1300], strides: [1] : memref<1300xf32, "gpu"> to memref<1300xf32, "gpu">
      memref.store %cst, %reinterpret_cast_3[%5] : memref<1300xf32, "gpu">
      scf.yield
    }
    scf.parallel (%arg1, %arg2) = (%c0, %c0) to (%c41, %c256) step (%c1, %c1) {
      %4 = arith.divui %arg1, %c41 : index
      %5 = arith.remui %arg1, %c41 : index
      %6 = arith.divui %arg2, %c32 : index
      %7 = arith.remui %arg2, %c32 : index
      %8 = arith.muli %7, %c8 : index
      %9 = arith.addi %6, %8 : index
      %10 = arith.muli %5, %c32 : index
      %11 = arith.addi %7, %10 : index
      %alloc_2 = memref.alloc() : memref<256xf32, #gpu.address_space<workgroup>>
      %12 = arith.cmpi ult, %11, %c1300 : index
      %13 = scf.if %12 -> (f32) {
        %19 = scf.for %arg3 = %c0 to %c64 step %c1 iter_args(%arg4 = %cst) -> (f32) {
          %20 = arith.muli %4, %c8 : index
          %21 = arith.addi %6, %20 : index
          %22 = arith.muli %21, %c64 : index
          %23 = arith.addi %arg3, %22 : index
          %24 = arith.cmpi slt, %23, %c110 : index
          %25 = scf.if %24 -> (f32) {
            %26 = "disc_shape.linearize"(%23, %11, %c110, %c1300) {operand_segment_sizes = array<i32: 2, 2>} : (index, index, index, index) -> index
            %27:3 = "disc_shape.delinearize"(%26, %c110, %c100, %c13) : (index, index, index, index) -> (index, index, index)
            %c110_5 = arith.constant 110 : index
            %c100_6 = arith.constant 100 : index
            %c13_7 = arith.constant 13 : index
            %28 = "disc_shape.linearize"(%27#0, %27#1, %27#2, %c110_5, %c100_6, %c13_7) {operand_segment_sizes = array<i32: 3, 3>} : (index, index, index, index, index, index) -> index
            %reinterpret_cast_8 = memref.reinterpret_cast %1 to offset: [0], sizes: [143000], strides: [1] : memref<110x100x13xf32, "gpu"> to memref<143000xf32, "gpu">
            %29 = memref.load %reinterpret_cast_8[%28] : memref<143000xf32, "gpu">
            %30 = arith.cmpf ugt, %arg4, %29 : f32
            %31 = arith.select %30, %arg4, %29 : f32
            %32 = arith.cmpf uno, %29, %29 : f32
            %33 = arith.select %32, %29, %31 : f32
            scf.yield %33 : f32
          } else {
            scf.yield %arg4 : f32
          }
          scf.yield %25 : f32
        }
        scf.yield %19 : f32
      } else {
        scf.yield %cst : f32
      }
      %c256_3 = arith.constant 256 : index
      %14 = "disc_shape.linearize"(%9, %c256_3) {operand_segment_sizes = array<i32: 1, 1>} : (index, index) -> index
      %reinterpret_cast_4 = memref.reinterpret_cast %alloc_2 to offset: [0], sizes: [256], strides: [1] : memref<256xf32, #gpu.address_space<workgroup>> to memref<256xf32, #gpu.address_space<workgroup>>
      memref.store %13, %reinterpret_cast_4[%14] : memref<256xf32, #gpu.address_space<workgroup>>
      gpu.barrier
      %15 = arith.cmpi slt, %6, %c4 : index
      scf.if %15 {
        %19 = arith.addi %9, %c4 : index
        %c256_5 = arith.constant 256 : index
        %20 = "disc_shape.linearize"(%9, %c256_5) {operand_segment_sizes = array<i32: 1, 1>} : (index, index) -> index
        %reinterpret_cast_6 = memref.reinterpret_cast %alloc_2 to offset: [0], sizes: [256], strides: [1] : memref<256xf32, #gpu.address_space<workgroup>> to memref<256xf32, #gpu.address_space<workgroup>>
        %21 = memref.load %reinterpret_cast_6[%20] : memref<256xf32, #gpu.address_space<workgroup>>
        %c256_7 = arith.constant 256 : index
        %22 = "disc_shape.linearize"(%19, %c256_7) {operand_segment_sizes = array<i32: 1, 1>} : (index, index) -> index
        %reinterpret_cast_8 = memref.reinterpret_cast %alloc_2 to offset: [0], sizes: [256], strides: [1] : memref<256xf32, #gpu.address_space<workgroup>> to memref<256xf32, #gpu.address_space<workgroup>>
        %23 = memref.load %reinterpret_cast_8[%22] : memref<256xf32, #gpu.address_space<workgroup>>
        %24 = arith.cmpf ugt, %21, %23 : f32
        %25 = arith.select %24, %21, %23 : f32
        %26 = arith.cmpf uno, %23, %23 : f32
        %27 = arith.select %26, %23, %25 : f32
        %c256_9 = arith.constant 256 : index
        %28 = "disc_shape.linearize"(%9, %c256_9) {operand_segment_sizes = array<i32: 1, 1>} : (index, index) -> index
        %reinterpret_cast_10 = memref.reinterpret_cast %alloc_2 to offset: [0], sizes: [256], strides: [1] : memref<256xf32, #gpu.address_space<workgroup>> to memref<256xf32, #gpu.address_space<workgroup>>
        memref.store %27, %reinterpret_cast_10[%28] : memref<256xf32, #gpu.address_space<workgroup>>
      }
      gpu.barrier
      %16 = arith.cmpi slt, %6, %c2 : index
      scf.if %16 {
        %19 = arith.addi %9, %c2 : index
        %c256_5 = arith.constant 256 : index
        %20 = "disc_shape.linearize"(%9, %c256_5) {operand_segment_sizes = array<i32: 1, 1>} : (index, index) -> index
        %reinterpret_cast_6 = memref.reinterpret_cast %alloc_2 to offset: [0], sizes: [256], strides: [1] : memref<256xf32, #gpu.address_space<workgroup>> to memref<256xf32, #gpu.address_space<workgroup>>
        %21 = memref.load %reinterpret_cast_6[%20] : memref<256xf32, #gpu.address_space<workgroup>>
        %c256_7 = arith.constant 256 : index
        %22 = "disc_shape.linearize"(%19, %c256_7) {operand_segment_sizes = array<i32: 1, 1>} : (index, index) -> index
        %reinterpret_cast_8 = memref.reinterpret_cast %alloc_2 to offset: [0], sizes: [256], strides: [1] : memref<256xf32, #gpu.address_space<workgroup>> to memref<256xf32, #gpu.address_space<workgroup>>
        %23 = memref.load %reinterpret_cast_8[%22] : memref<256xf32, #gpu.address_space<workgroup>>
        %24 = arith.cmpf ugt, %21, %23 : f32
        %25 = arith.select %24, %21, %23 : f32
        %26 = arith.cmpf uno, %23, %23 : f32
        %27 = arith.select %26, %23, %25 : f32
        %c256_9 = arith.constant 256 : index
        %28 = "disc_shape.linearize"(%9, %c256_9) {operand_segment_sizes = array<i32: 1, 1>} : (index, index) -> index
        %reinterpret_cast_10 = memref.reinterpret_cast %alloc_2 to offset: [0], sizes: [256], strides: [1] : memref<256xf32, #gpu.address_space<workgroup>> to memref<256xf32, #gpu.address_space<workgroup>>
        memref.store %27, %reinterpret_cast_10[%28] : memref<256xf32, #gpu.address_space<workgroup>>
      }
      gpu.barrier
      %17 = arith.cmpi eq, %6, %c0 : index
      %18 = arith.andi %17, %12 : i1
      scf.if %18 {
        %19 = arith.addi %9, %c1 : index
        %c256_5 = arith.constant 256 : index
        %20 = "disc_shape.linearize"(%9, %c256_5) {operand_segment_sizes = array<i32: 1, 1>} : (index, index) -> index
        %reinterpret_cast_6 = memref.reinterpret_cast %alloc_2 to offset: [0], sizes: [256], strides: [1] : memref<256xf32, #gpu.address_space<workgroup>> to memref<256xf32, #gpu.address_space<workgroup>>
        %21 = memref.load %reinterpret_cast_6[%20] : memref<256xf32, #gpu.address_space<workgroup>>
        %c256_7 = arith.constant 256 : index
        %22 = "disc_shape.linearize"(%19, %c256_7) {operand_segment_sizes = array<i32: 1, 1>} : (index, index) -> index
        %reinterpret_cast_8 = memref.reinterpret_cast %alloc_2 to offset: [0], sizes: [256], strides: [1] : memref<256xf32, #gpu.address_space<workgroup>> to memref<256xf32, #gpu.address_space<workgroup>>
        %23 = memref.load %reinterpret_cast_8[%22] : memref<256xf32, #gpu.address_space<workgroup>>
        %24 = arith.cmpf ugt, %21, %23 : f32
        %25 = arith.select %24, %21, %23 : f32
        %26 = arith.cmpf uno, %23, %23 : f32
        %27 = arith.select %26, %23, %25 : f32
        %28 = memref.generic_atomic_rmw %alloc_1[%11] : memref<1300xf32, "gpu"> {
        ^bb0(%arg3: f32):
          %29 = arith.cmpf ogt, %arg3, %27 : f32
          %30 = arith.select %29, %arg3, %27 : f32
          memref.atomic_yield %30 : f32
        }
      }
      scf.yield
    }
    "lmhlo.terminator"() : () -> ()
  }) {disc.device = "gpu", disc.fusion.name = "main_kColReduction_reduce__3_1_0", disc.fusion.tag = "8w32h", disc.fusion_type = "kColReduction", disc_col_reduction_schedule_hint = 1 : i32, disc_thread_per_block_hint = 256 : i32} : () -> ()
  memref.dealloc %alloc_0 : memref<110x1300xf32, "gpu">
  memref.dealloc %alloc : memref<f32, "gpu">
  %2 = llvm.inttoptr %0 : i32 to !llvm.ptr<i8>
  %alloca = memref.alloca() : memref<2xindex, "cpu">
  memref.store %c100, %alloca[%c0] : memref<2xindex, "cpu">
  memref.store %c13, %alloca[%c1] : memref<2xindex, "cpu">
  %3 = "disc_ral.dispatch"(%arg0, %2, %alloc_1, %alloca) {backend_config = "", call_target_name = "inc_ref", device = "gpu", has_side_effect = false} : (!disc_ral.context, !llvm.ptr<i8>, memref<1300xf32, "gpu">, memref<2xindex, "cpu">) -> memref<100x13xf32, "gpu">
  %reinterpret_cast = memref.reinterpret_cast %3 to offset: [0], sizes: [100, 13], strides: [13, 1] : memref<100x13xf32, "gpu"> to memref<100x13xf32, "gpu">
  memref.dealloc %alloc_1 : memref<1300xf32, "gpu">
  "disc_ral.dispatch"(%arg0, %c0, %reinterpret_cast) {backend_config = "", call_target_name = "ral_send_output", device = "cpu", has_side_effect = false} : (!disc_ral.context, index, memref<100x13xf32, "gpu">) -> ()
  return
}

// -----// IR Dump After Canonicalizer (disc-canonicalize) //----- //
func.func @main(%arg0: !disc_ral.context) attributes {tf.entry_function = {input_placements = "gpu", inputs = "input0", output_placements = "gpu", outputs = "output0"}} {
  %cst = arith.constant 0xFF800000 : f32
  %c110 = arith.constant 110 : index
  %c41 = arith.constant 41 : index
  %c2 = arith.constant 2 : index
  %c4 = arith.constant 4 : index
  %c256 = arith.constant 256 : index
  %c64 = arith.constant 64 : index
  %c8 = arith.constant 8 : index
  %c32 = arith.constant 32 : index
  %c1300 = arith.constant 1300 : index
  %0 = llvm.mlir.constant(0 : i32) : i32
  %c0 = arith.constant 0 : index
  %c100 = arith.constant 100 : index
  %c1 = arith.constant 1 : index
  %c13 = arith.constant 13 : index
  %1 = "disc_ral.dispatch"(%arg0, %c0) {backend_config = "", call_target_name = "ral_recv_input", device = "cpu", has_side_effect = false} : (!disc_ral.context, index) -> memref<110x100x13xf32, "gpu">
  %alloc = memref.alloc() : memref<1300xf32, "gpu">
  "lmhlo.fusion"() ({
    scf.parallel (%arg1) = (%c0) to (%c1300) step (%c1) {
      %reinterpret_cast_0 = memref.reinterpret_cast %alloc to offset: [0], sizes: [1300], strides: [1] : memref<1300xf32, "gpu"> to memref<1300xf32, "gpu">
      memref.store %cst, %reinterpret_cast_0[%arg1] : memref<1300xf32, "gpu">
      scf.yield
    }
    scf.parallel (%arg1, %arg2) = (%c0, %c0) to (%c41, %c256) step (%c1, %c1) {
      %4 = arith.divui %arg1, %c41 : index
      %5 = arith.remui %arg1, %c41 : index
      %6 = arith.divui %arg2, %c32 : index
      %7 = arith.remui %arg2, %c32 : index
      %8 = arith.muli %7, %c8 : index
      %9 = arith.addi %6, %8 : index
      %10 = arith.muli %5, %c32 : index
      %11 = arith.addi %7, %10 : index
      %alloc_0 = memref.alloc() : memref<256xf32, #gpu.address_space<workgroup>>
      %12 = arith.cmpi ult, %11, %c1300 : index
      %13 = scf.if %12 -> (f32) {
        %19 = scf.for %arg3 = %c0 to %c64 step %c1 iter_args(%arg4 = %cst) -> (f32) {
          %20 = arith.muli %4, %c8 : index
          %21 = arith.addi %6, %20 : index
          %22 = arith.muli %21, %c64 : index
          %23 = arith.addi %arg3, %22 : index
          %24 = arith.cmpi slt, %23, %c110 : index
          %25 = scf.if %24 -> (f32) {
            %26 = "disc_shape.linearize"(%23, %11, %c110, %c1300) {operand_segment_sizes = array<i32: 2, 2>} : (index, index, index, index) -> index
            %reinterpret_cast_2 = memref.reinterpret_cast %1 to offset: [0], sizes: [143000], strides: [1] : memref<110x100x13xf32, "gpu"> to memref<143000xf32, "gpu">
            %27 = memref.load %reinterpret_cast_2[%26] : memref<143000xf32, "gpu">
            %28 = arith.cmpf ugt, %arg4, %27 : f32
            %29 = arith.select %28, %arg4, %27 : f32
            %30 = arith.cmpf uno, %27, %27 : f32
            %31 = arith.select %30, %27, %29 : f32
            scf.yield %31 : f32
          } else {
            scf.yield %arg4 : f32
          }
          scf.yield %25 : f32
        }
        scf.yield %19 : f32
      } else {
        scf.yield %cst : f32
      }
      %14 = "disc_shape.linearize"(%9, %c256) {operand_segment_sizes = array<i32: 1, 1>} : (index, index) -> index
      %reinterpret_cast_1 = memref.reinterpret_cast %alloc_0 to offset: [0], sizes: [256], strides: [1] : memref<256xf32, #gpu.address_space<workgroup>> to memref<256xf32, #gpu.address_space<workgroup>>
      memref.store %13, %reinterpret_cast_1[%14] : memref<256xf32, #gpu.address_space<workgroup>>
      gpu.barrier
      %15 = arith.cmpi slt, %6, %c4 : index
      scf.if %15 {
        %19 = arith.addi %9, %c4 : index
        %20 = "disc_shape.linearize"(%9, %c256) {operand_segment_sizes = array<i32: 1, 1>} : (index, index) -> index
        %reinterpret_cast_2 = memref.reinterpret_cast %alloc_0 to offset: [0], sizes: [256], strides: [1] : memref<256xf32, #gpu.address_space<workgroup>> to memref<256xf32, #gpu.address_space<workgroup>>
        %21 = memref.load %reinterpret_cast_2[%20] : memref<256xf32, #gpu.address_space<workgroup>>
        %22 = "disc_shape.linearize"(%19, %c256) {operand_segment_sizes = array<i32: 1, 1>} : (index, index) -> index
        %reinterpret_cast_3 = memref.reinterpret_cast %alloc_0 to offset: [0], sizes: [256], strides: [1] : memref<256xf32, #gpu.address_space<workgroup>> to memref<256xf32, #gpu.address_space<workgroup>>
        %23 = memref.load %reinterpret_cast_3[%22] : memref<256xf32, #gpu.address_space<workgroup>>
        %24 = arith.cmpf ugt, %21, %23 : f32
        %25 = arith.select %24, %21, %23 : f32
        %26 = arith.cmpf uno, %23, %23 : f32
        %27 = arith.select %26, %23, %25 : f32
        %28 = "disc_shape.linearize"(%9, %c256) {operand_segment_sizes = array<i32: 1, 1>} : (index, index) -> index
        %reinterpret_cast_4 = memref.reinterpret_cast %alloc_0 to offset: [0], sizes: [256], strides: [1] : memref<256xf32, #gpu.address_space<workgroup>> to memref<256xf32, #gpu.address_space<workgroup>>
        memref.store %27, %reinterpret_cast_4[%28] : memref<256xf32, #gpu.address_space<workgroup>>
      }
      gpu.barrier
      %16 = arith.cmpi slt, %6, %c2 : index
      scf.if %16 {
        %19 = arith.addi %9, %c2 : index
        %20 = "disc_shape.linearize"(%9, %c256) {operand_segment_sizes = array<i32: 1, 1>} : (index, index) -> index
        %reinterpret_cast_2 = memref.reinterpret_cast %alloc_0 to offset: [0], sizes: [256], strides: [1] : memref<256xf32, #gpu.address_space<workgroup>> to memref<256xf32, #gpu.address_space<workgroup>>
        %21 = memref.load %reinterpret_cast_2[%20] : memref<256xf32, #gpu.address_space<workgroup>>
        %22 = "disc_shape.linearize"(%19, %c256) {operand_segment_sizes = array<i32: 1, 1>} : (index, index) -> index
        %reinterpret_cast_3 = memref.reinterpret_cast %alloc_0 to offset: [0], sizes: [256], strides: [1] : memref<256xf32, #gpu.address_space<workgroup>> to memref<256xf32, #gpu.address_space<workgroup>>
        %23 = memref.load %reinterpret_cast_3[%22] : memref<256xf32, #gpu.address_space<workgroup>>
        %24 = arith.cmpf ugt, %21, %23 : f32
        %25 = arith.select %24, %21, %23 : f32
        %26 = arith.cmpf uno, %23, %23 : f32
        %27 = arith.select %26, %23, %25 : f32
        %28 = "disc_shape.linearize"(%9, %c256) {operand_segment_sizes = array<i32: 1, 1>} : (index, index) -> index
        %reinterpret_cast_4 = memref.reinterpret_cast %alloc_0 to offset: [0], sizes: [256], strides: [1] : memref<256xf32, #gpu.address_space<workgroup>> to memref<256xf32, #gpu.address_space<workgroup>>
        memref.store %27, %reinterpret_cast_4[%28] : memref<256xf32, #gpu.address_space<workgroup>>
      }
      gpu.barrier
      %17 = arith.cmpi eq, %6, %c0 : index
      %18 = arith.andi %17, %12 : i1
      scf.if %18 {
        %19 = arith.addi %9, %c1 : index
        %20 = "disc_shape.linearize"(%9, %c256) {operand_segment_sizes = array<i32: 1, 1>} : (index, index) -> index
        %reinterpret_cast_2 = memref.reinterpret_cast %alloc_0 to offset: [0], sizes: [256], strides: [1] : memref<256xf32, #gpu.address_space<workgroup>> to memref<256xf32, #gpu.address_space<workgroup>>
        %21 = memref.load %reinterpret_cast_2[%20] : memref<256xf32, #gpu.address_space<workgroup>>
        %22 = "disc_shape.linearize"(%19, %c256) {operand_segment_sizes = array<i32: 1, 1>} : (index, index) -> index
        %reinterpret_cast_3 = memref.reinterpret_cast %alloc_0 to offset: [0], sizes: [256], strides: [1] : memref<256xf32, #gpu.address_space<workgroup>> to memref<256xf32, #gpu.address_space<workgroup>>
        %23 = memref.load %reinterpret_cast_3[%22] : memref<256xf32, #gpu.address_space<workgroup>>
        %24 = arith.cmpf ugt, %21, %23 : f32
        %25 = arith.select %24, %21, %23 : f32
        %26 = arith.cmpf uno, %23, %23 : f32
        %27 = arith.select %26, %23, %25 : f32
        %28 = memref.generic_atomic_rmw %alloc[%11] : memref<1300xf32, "gpu"> {
        ^bb0(%arg3: f32):
          %29 = arith.cmpf ogt, %arg3, %27 : f32
          %30 = arith.select %29, %arg3, %27 : f32
          memref.atomic_yield %30 : f32
        }
      }
      scf.yield
    }
    "lmhlo.terminator"() : () -> ()
  }) {disc.device = "gpu", disc.fusion.name = "main_kColReduction_reduce__3_1_0", disc.fusion.tag = "8w32h", disc.fusion_type = "kColReduction", disc_col_reduction_schedule_hint = 1 : i32, disc_thread_per_block_hint = 256 : i32} : () -> ()
  %2 = llvm.inttoptr %0 : i32 to !llvm.ptr<i8>
  %alloca = memref.alloca() : memref<2xindex, "cpu">
  memref.store %c100, %alloca[%c0] : memref<2xindex, "cpu">
  memref.store %c13, %alloca[%c1] : memref<2xindex, "cpu">
  %3 = "disc_ral.dispatch"(%arg0, %2, %alloc, %alloca) {backend_config = "", call_target_name = "inc_ref", device = "gpu", has_side_effect = false} : (!disc_ral.context, !llvm.ptr<i8>, memref<1300xf32, "gpu">, memref<2xindex, "cpu">) -> memref<100x13xf32, "gpu">
  %reinterpret_cast = memref.reinterpret_cast %3 to offset: [0], sizes: [100, 13], strides: [13, 1] : memref<100x13xf32, "gpu"> to memref<100x13xf32, "gpu">
  memref.dealloc %alloc : memref<1300xf32, "gpu">
  "disc_ral.dispatch"(%arg0, %c0, %reinterpret_cast) {backend_config = "", call_target_name = "ral_send_output", device = "cpu", has_side_effect = false} : (!disc_ral.context, index, memref<100x13xf32, "gpu">) -> ()
  return
}

// -----// IR Dump After CSE (cse) //----- //
func.func @main(%arg0: !disc_ral.context) attributes {tf.entry_function = {input_placements = "gpu", inputs = "input0", output_placements = "gpu", outputs = "output0"}} {
  %cst = arith.constant 0xFF800000 : f32
  %c110 = arith.constant 110 : index
  %c41 = arith.constant 41 : index
  %c2 = arith.constant 2 : index
  %c4 = arith.constant 4 : index
  %c256 = arith.constant 256 : index
  %c64 = arith.constant 64 : index
  %c8 = arith.constant 8 : index
  %c32 = arith.constant 32 : index
  %c1300 = arith.constant 1300 : index
  %0 = llvm.mlir.constant(0 : i32) : i32
  %c0 = arith.constant 0 : index
  %c100 = arith.constant 100 : index
  %c1 = arith.constant 1 : index
  %c13 = arith.constant 13 : index
  %1 = "disc_ral.dispatch"(%arg0, %c0) {backend_config = "", call_target_name = "ral_recv_input", device = "cpu", has_side_effect = false} : (!disc_ral.context, index) -> memref<110x100x13xf32, "gpu">
  %alloc = memref.alloc() : memref<1300xf32, "gpu">
  "lmhlo.fusion"() ({
    scf.parallel (%arg1) = (%c0) to (%c1300) step (%c1) {
      %reinterpret_cast_0 = memref.reinterpret_cast %alloc to offset: [0], sizes: [1300], strides: [1] : memref<1300xf32, "gpu"> to memref<1300xf32, "gpu">
      memref.store %cst, %reinterpret_cast_0[%arg1] : memref<1300xf32, "gpu">
      scf.yield
    }
    scf.parallel (%arg1, %arg2) = (%c0, %c0) to (%c41, %c256) step (%c1, %c1) {
      %4 = arith.divui %arg1, %c41 : index
      %5 = arith.remui %arg1, %c41 : index
      %6 = arith.divui %arg2, %c32 : index
      %7 = arith.remui %arg2, %c32 : index
      %8 = arith.muli %7, %c8 : index
      %9 = arith.addi %6, %8 : index
      %10 = arith.muli %5, %c32 : index
      %11 = arith.addi %7, %10 : index
      %alloc_0 = memref.alloc() : memref<256xf32, #gpu.address_space<workgroup>>
      %12 = arith.cmpi ult, %11, %c1300 : index
      %13 = scf.if %12 -> (f32) {
        %19 = scf.for %arg3 = %c0 to %c64 step %c1 iter_args(%arg4 = %cst) -> (f32) {
          %20 = arith.muli %4, %c8 : index
          %21 = arith.addi %6, %20 : index
          %22 = arith.muli %21, %c64 : index
          %23 = arith.addi %arg3, %22 : index
          %24 = arith.cmpi slt, %23, %c110 : index
          %25 = scf.if %24 -> (f32) {
            %26 = "disc_shape.linearize"(%23, %11, %c110, %c1300) {operand_segment_sizes = array<i32: 2, 2>} : (index, index, index, index) -> index
            %reinterpret_cast_2 = memref.reinterpret_cast %1 to offset: [0], sizes: [143000], strides: [1] : memref<110x100x13xf32, "gpu"> to memref<143000xf32, "gpu">
            %27 = memref.load %reinterpret_cast_2[%26] : memref<143000xf32, "gpu">
            %28 = arith.cmpf ugt, %arg4, %27 : f32
            %29 = arith.select %28, %arg4, %27 : f32
            %30 = arith.cmpf uno, %27, %27 : f32
            %31 = arith.select %30, %27, %29 : f32
            scf.yield %31 : f32
          } else {
            scf.yield %arg4 : f32
          }
          scf.yield %25 : f32
        }
        scf.yield %19 : f32
      } else {
        scf.yield %cst : f32
      }
      %14 = "disc_shape.linearize"(%9, %c256) {operand_segment_sizes = array<i32: 1, 1>} : (index, index) -> index
      %reinterpret_cast_1 = memref.reinterpret_cast %alloc_0 to offset: [0], sizes: [256], strides: [1] : memref<256xf32, #gpu.address_space<workgroup>> to memref<256xf32, #gpu.address_space<workgroup>>
      memref.store %13, %reinterpret_cast_1[%14] : memref<256xf32, #gpu.address_space<workgroup>>
      gpu.barrier
      %15 = arith.cmpi slt, %6, %c4 : index
      scf.if %15 {
        %19 = arith.addi %9, %c4 : index
        %20 = memref.load %reinterpret_cast_1[%14] : memref<256xf32, #gpu.address_space<workgroup>>
        %21 = "disc_shape.linearize"(%19, %c256) {operand_segment_sizes = array<i32: 1, 1>} : (index, index) -> index
        %22 = memref.load %reinterpret_cast_1[%21] : memref<256xf32, #gpu.address_space<workgroup>>
        %23 = arith.cmpf ugt, %20, %22 : f32
        %24 = arith.select %23, %20, %22 : f32
        %25 = arith.cmpf uno, %22, %22 : f32
        %26 = arith.select %25, %22, %24 : f32
        memref.store %26, %reinterpret_cast_1[%14] : memref<256xf32, #gpu.address_space<workgroup>>
      }
      gpu.barrier
      %16 = arith.cmpi slt, %6, %c2 : index
      scf.if %16 {
        %19 = arith.addi %9, %c2 : index
        %20 = memref.load %reinterpret_cast_1[%14] : memref<256xf32, #gpu.address_space<workgroup>>
        %21 = "disc_shape.linearize"(%19, %c256) {operand_segment_sizes = array<i32: 1, 1>} : (index, index) -> index
        %22 = memref.load %reinterpret_cast_1[%21] : memref<256xf32, #gpu.address_space<workgroup>>
        %23 = arith.cmpf ugt, %20, %22 : f32
        %24 = arith.select %23, %20, %22 : f32
        %25 = arith.cmpf uno, %22, %22 : f32
        %26 = arith.select %25, %22, %24 : f32
        memref.store %26, %reinterpret_cast_1[%14] : memref<256xf32, #gpu.address_space<workgroup>>
      }
      gpu.barrier
      %17 = arith.cmpi eq, %6, %c0 : index
      %18 = arith.andi %17, %12 : i1
      scf.if %18 {
        %19 = arith.addi %9, %c1 : index
        %20 = memref.load %reinterpret_cast_1[%14] : memref<256xf32, #gpu.address_space<workgroup>>
        %21 = "disc_shape.linearize"(%19, %c256) {operand_segment_sizes = array<i32: 1, 1>} : (index, index) -> index
        %22 = memref.load %reinterpret_cast_1[%21] : memref<256xf32, #gpu.address_space<workgroup>>
        %23 = arith.cmpf ugt, %20, %22 : f32
        %24 = arith.select %23, %20, %22 : f32
        %25 = arith.cmpf uno, %22, %22 : f32
        %26 = arith.select %25, %22, %24 : f32
        %27 = memref.generic_atomic_rmw %alloc[%11] : memref<1300xf32, "gpu"> {
        ^bb0(%arg3: f32):
          %28 = arith.cmpf ogt, %arg3, %26 : f32
          %29 = arith.select %28, %arg3, %26 : f32
          memref.atomic_yield %29 : f32
        }
      }
      scf.yield
    }
    "lmhlo.terminator"() : () -> ()
  }) {disc.device = "gpu", disc.fusion.name = "main_kColReduction_reduce__3_1_0", disc.fusion.tag = "8w32h", disc.fusion_type = "kColReduction", disc_col_reduction_schedule_hint = 1 : i32, disc_thread_per_block_hint = 256 : i32} : () -> ()
  %2 = llvm.inttoptr %0 : i32 to !llvm.ptr<i8>
  %alloca = memref.alloca() : memref<2xindex, "cpu">
  memref.store %c100, %alloca[%c0] : memref<2xindex, "cpu">
  memref.store %c13, %alloca[%c1] : memref<2xindex, "cpu">
  %3 = "disc_ral.dispatch"(%arg0, %2, %alloc, %alloca) {backend_config = "", call_target_name = "inc_ref", device = "gpu", has_side_effect = false} : (!disc_ral.context, !llvm.ptr<i8>, memref<1300xf32, "gpu">, memref<2xindex, "cpu">) -> memref<100x13xf32, "gpu">
  %reinterpret_cast = memref.reinterpret_cast %3 to offset: [0], sizes: [100, 13], strides: [13, 1] : memref<100x13xf32, "gpu"> to memref<100x13xf32, "gpu">
  memref.dealloc %alloc : memref<1300xf32, "gpu">
  "disc_ral.dispatch"(%arg0, %c0, %reinterpret_cast) {backend_config = "", call_target_name = "ral_send_output", device = "cpu", has_side_effect = false} : (!disc_ral.context, index, memref<100x13xf32, "gpu">) -> ()
  return
}

// -----// IR Dump After ConvertShapeToStandardPass (disc-convert-shape-to-std) //----- //
func.func @main(%arg0: !disc_ral.context) attributes {tf.entry_function = {input_placements = "gpu", inputs = "input0", output_placements = "gpu", outputs = "output0"}} {
  %cst = arith.constant 0xFF800000 : f32
  %c110 = arith.constant 110 : index
  %c41 = arith.constant 41 : index
  %c2 = arith.constant 2 : index
  %c4 = arith.constant 4 : index
  %c256 = arith.constant 256 : index
  %c64 = arith.constant 64 : index
  %c8 = arith.constant 8 : index
  %c32 = arith.constant 32 : index
  %c1300 = arith.constant 1300 : index
  %0 = llvm.mlir.constant(0 : i32) : i32
  %c0 = arith.constant 0 : index
  %c100 = arith.constant 100 : index
  %c1 = arith.constant 1 : index
  %c13 = arith.constant 13 : index
  %1 = "disc_ral.dispatch"(%arg0, %c0) {backend_config = "", call_target_name = "ral_recv_input", device = "cpu", has_side_effect = false} : (!disc_ral.context, index) -> memref<110x100x13xf32, "gpu">
  %alloc = memref.alloc() : memref<1300xf32, "gpu">
  "lmhlo.fusion"() ({
    scf.parallel (%arg1) = (%c0) to (%c1300) step (%c1) {
      %reinterpret_cast_0 = memref.reinterpret_cast %alloc to offset: [0], sizes: [1300], strides: [1] : memref<1300xf32, "gpu"> to memref<1300xf32, "gpu">
      memref.store %cst, %reinterpret_cast_0[%arg1] : memref<1300xf32, "gpu">
      scf.yield
    }
    scf.parallel (%arg1, %arg2) = (%c0, %c0) to (%c41, %c256) step (%c1, %c1) {
      %4 = arith.divui %arg1, %c41 : index
      %5 = arith.remui %arg1, %c41 : index
      %6 = arith.divui %arg2, %c32 : index
      %7 = arith.remui %arg2, %c32 : index
      %8 = arith.muli %7, %c8 : index
      %9 = arith.addi %6, %8 : index
      %10 = arith.muli %5, %c32 : index
      %11 = arith.addi %7, %10 : index
      %alloc_0 = memref.alloc() : memref<256xf32, #gpu.address_space<workgroup>>
      %12 = arith.cmpi ult, %11, %c1300 : index
      %13 = scf.if %12 -> (f32) {
        %18 = scf.for %arg3 = %c0 to %c64 step %c1 iter_args(%arg4 = %cst) -> (f32) {
          %19 = arith.muli %4, %c8 : index
          %20 = arith.addi %6, %19 : index
          %21 = arith.muli %20, %c64 : index
          %22 = arith.addi %arg3, %21 : index
          %23 = arith.cmpi slt, %22, %c110 : index
          %24 = scf.if %23 -> (f32) {
            %c0_3 = arith.constant 0 : index
            %25 = arith.muli %22, %c1300 : index
            %26 = arith.addi %25, %11 : index
            %reinterpret_cast_4 = memref.reinterpret_cast %1 to offset: [0], sizes: [143000], strides: [1] : memref<110x100x13xf32, "gpu"> to memref<143000xf32, "gpu">
            %27 = memref.load %reinterpret_cast_4[%26] : memref<143000xf32, "gpu">
            %28 = arith.cmpf ugt, %arg4, %27 : f32
            %29 = arith.select %28, %arg4, %27 : f32
            %30 = arith.cmpf uno, %27, %27 : f32
            %31 = arith.select %30, %27, %29 : f32
            scf.yield %31 : f32
          } else {
            scf.yield %arg4 : f32
          }
          scf.yield %24 : f32
        }
        scf.yield %18 : f32
      } else {
        scf.yield %cst : f32
      }
      %c0_1 = arith.constant 0 : index
      %reinterpret_cast_2 = memref.reinterpret_cast %alloc_0 to offset: [0], sizes: [256], strides: [1] : memref<256xf32, #gpu.address_space<workgroup>> to memref<256xf32, #gpu.address_space<workgroup>>
      memref.store %13, %reinterpret_cast_2[%9] : memref<256xf32, #gpu.address_space<workgroup>>
      gpu.barrier
      %14 = arith.cmpi slt, %6, %c4 : index
      scf.if %14 {
        %18 = arith.addi %9, %c4 : index
        %19 = memref.load %reinterpret_cast_2[%9] : memref<256xf32, #gpu.address_space<workgroup>>
        %c0_3 = arith.constant 0 : index
        %20 = memref.load %reinterpret_cast_2[%18] : memref<256xf32, #gpu.address_space<workgroup>>
        %21 = arith.cmpf ugt, %19, %20 : f32
        %22 = arith.select %21, %19, %20 : f32
        %23 = arith.cmpf uno, %20, %20 : f32
        %24 = arith.select %23, %20, %22 : f32
        memref.store %24, %reinterpret_cast_2[%9] : memref<256xf32, #gpu.address_space<workgroup>>
      }
      gpu.barrier
      %15 = arith.cmpi slt, %6, %c2 : index
      scf.if %15 {
        %18 = arith.addi %9, %c2 : index
        %19 = memref.load %reinterpret_cast_2[%9] : memref<256xf32, #gpu.address_space<workgroup>>
        %c0_3 = arith.constant 0 : index
        %20 = memref.load %reinterpret_cast_2[%18] : memref<256xf32, #gpu.address_space<workgroup>>
        %21 = arith.cmpf ugt, %19, %20 : f32
        %22 = arith.select %21, %19, %20 : f32
        %23 = arith.cmpf uno, %20, %20 : f32
        %24 = arith.select %23, %20, %22 : f32
        memref.store %24, %reinterpret_cast_2[%9] : memref<256xf32, #gpu.address_space<workgroup>>
      }
      gpu.barrier
      %16 = arith.cmpi eq, %6, %c0 : index
      %17 = arith.andi %16, %12 : i1
      scf.if %17 {
        %18 = arith.addi %9, %c1 : index
        %19 = memref.load %reinterpret_cast_2[%9] : memref<256xf32, #gpu.address_space<workgroup>>
        %c0_3 = arith.constant 0 : index
        %20 = memref.load %reinterpret_cast_2[%18] : memref<256xf32, #gpu.address_space<workgroup>>
        %21 = arith.cmpf ugt, %19, %20 : f32
        %22 = arith.select %21, %19, %20 : f32
        %23 = arith.cmpf uno, %20, %20 : f32
        %24 = arith.select %23, %20, %22 : f32
        %25 = memref.generic_atomic_rmw %alloc[%11] : memref<1300xf32, "gpu"> {
        ^bb0(%arg3: f32):
          %26 = arith.cmpf ogt, %arg3, %24 : f32
          %27 = arith.select %26, %arg3, %24 : f32
          memref.atomic_yield %27 : f32
        }
      }
      scf.yield
    }
    "lmhlo.terminator"() : () -> ()
  }) {disc.device = "gpu", disc.fusion.name = "main_kColReduction_reduce__3_1_0", disc.fusion.tag = "8w32h", disc.fusion_type = "kColReduction", disc_col_reduction_schedule_hint = 1 : i32, disc_thread_per_block_hint = 256 : i32} : () -> ()
  %2 = llvm.inttoptr %0 : i32 to !llvm.ptr<i8>
  %alloca = memref.alloca() : memref<2xindex, "cpu">
  memref.store %c100, %alloca[%c0] : memref<2xindex, "cpu">
  memref.store %c13, %alloca[%c1] : memref<2xindex, "cpu">
  %3 = "disc_ral.dispatch"(%arg0, %2, %alloc, %alloca) {backend_config = "", call_target_name = "inc_ref", device = "gpu", has_side_effect = false} : (!disc_ral.context, !llvm.ptr<i8>, memref<1300xf32, "gpu">, memref<2xindex, "cpu">) -> memref<100x13xf32, "gpu">
  %reinterpret_cast = memref.reinterpret_cast %3 to offset: [0], sizes: [100, 13], strides: [13, 1] : memref<100x13xf32, "gpu"> to memref<100x13xf32, "gpu">
  memref.dealloc %alloc : memref<1300xf32, "gpu">
  "disc_ral.dispatch"(%arg0, %c0, %reinterpret_cast) {backend_config = "", call_target_name = "ral_send_output", device = "cpu", has_side_effect = false} : (!disc_ral.context, index, memref<100x13xf32, "gpu">) -> ()
  return
}

// -----// IR Dump After Canonicalizer (disc-canonicalize) //----- //
func.func @main(%arg0: !disc_ral.context) attributes {tf.entry_function = {input_placements = "gpu", inputs = "input0", output_placements = "gpu", outputs = "output0"}} {
  %cst = arith.constant 0xFF800000 : f32
  %c110 = arith.constant 110 : index
  %c41 = arith.constant 41 : index
  %c2 = arith.constant 2 : index
  %c4 = arith.constant 4 : index
  %c256 = arith.constant 256 : index
  %c64 = arith.constant 64 : index
  %c8 = arith.constant 8 : index
  %c32 = arith.constant 32 : index
  %c1300 = arith.constant 1300 : index
  %0 = llvm.mlir.constant(0 : i32) : i32
  %c0 = arith.constant 0 : index
  %c100 = arith.constant 100 : index
  %c1 = arith.constant 1 : index
  %c13 = arith.constant 13 : index
  %1 = "disc_ral.dispatch"(%arg0, %c0) {backend_config = "", call_target_name = "ral_recv_input", device = "cpu", has_side_effect = false} : (!disc_ral.context, index) -> memref<110x100x13xf32, "gpu">
  %alloc = memref.alloc() : memref<1300xf32, "gpu">
  "lmhlo.fusion"() ({
    scf.parallel (%arg1) = (%c0) to (%c1300) step (%c1) {
      %reinterpret_cast_0 = memref.reinterpret_cast %alloc to offset: [0], sizes: [1300], strides: [1] : memref<1300xf32, "gpu"> to memref<1300xf32, "gpu">
      memref.store %cst, %reinterpret_cast_0[%arg1] : memref<1300xf32, "gpu">
      scf.yield
    }
    scf.parallel (%arg1, %arg2) = (%c0, %c0) to (%c41, %c256) step (%c1, %c1) {
      %4 = arith.divui %arg1, %c41 : index
      %5 = arith.remui %arg1, %c41 : index
      %6 = arith.divui %arg2, %c32 : index
      %7 = arith.remui %arg2, %c32 : index
      %8 = arith.muli %7, %c8 : index
      %9 = arith.addi %6, %8 : index
      %10 = arith.muli %5, %c32 : index
      %11 = arith.addi %7, %10 : index
      %alloc_0 = memref.alloc() : memref<256xf32, #gpu.address_space<workgroup>>
      %12 = arith.cmpi ult, %11, %c1300 : index
      %13 = scf.if %12 -> (f32) {
        %18 = scf.for %arg3 = %c0 to %c64 step %c1 iter_args(%arg4 = %cst) -> (f32) {
          %19 = arith.muli %4, %c8 : index
          %20 = arith.addi %6, %19 : index
          %21 = arith.muli %20, %c64 : index
          %22 = arith.addi %arg3, %21 : index
          %23 = arith.cmpi slt, %22, %c110 : index
          %24 = scf.if %23 -> (f32) {
            %25 = arith.muli %22, %c1300 : index
            %26 = arith.addi %25, %11 : index
            %reinterpret_cast_2 = memref.reinterpret_cast %1 to offset: [0], sizes: [143000], strides: [1] : memref<110x100x13xf32, "gpu"> to memref<143000xf32, "gpu">
            %27 = memref.load %reinterpret_cast_2[%26] : memref<143000xf32, "gpu">
            %28 = arith.cmpf ugt, %arg4, %27 : f32
            %29 = arith.select %28, %arg4, %27 : f32
            %30 = arith.cmpf uno, %27, %27 : f32
            %31 = arith.select %30, %27, %29 : f32
            scf.yield %31 : f32
          } else {
            scf.yield %arg4 : f32
          }
          scf.yield %24 : f32
        }
        scf.yield %18 : f32
      } else {
        scf.yield %cst : f32
      }
      %reinterpret_cast_1 = memref.reinterpret_cast %alloc_0 to offset: [0], sizes: [256], strides: [1] : memref<256xf32, #gpu.address_space<workgroup>> to memref<256xf32, #gpu.address_space<workgroup>>
      memref.store %13, %reinterpret_cast_1[%9] : memref<256xf32, #gpu.address_space<workgroup>>
      gpu.barrier
      %14 = arith.cmpi slt, %6, %c4 : index
      scf.if %14 {
        %18 = arith.addi %9, %c4 : index
        %19 = memref.load %reinterpret_cast_1[%9] : memref<256xf32, #gpu.address_space<workgroup>>
        %20 = memref.load %reinterpret_cast_1[%18] : memref<256xf32, #gpu.address_space<workgroup>>
        %21 = arith.cmpf ugt, %19, %20 : f32
        %22 = arith.select %21, %19, %20 : f32
        %23 = arith.cmpf uno, %20, %20 : f32
        %24 = arith.select %23, %20, %22 : f32
        memref.store %24, %reinterpret_cast_1[%9] : memref<256xf32, #gpu.address_space<workgroup>>
      }
      gpu.barrier
      %15 = arith.cmpi slt, %6, %c2 : index
      scf.if %15 {
        %18 = arith.addi %9, %c2 : index
        %19 = memref.load %reinterpret_cast_1[%9] : memref<256xf32, #gpu.address_space<workgroup>>
        %20 = memref.load %reinterpret_cast_1[%18] : memref<256xf32, #gpu.address_space<workgroup>>
        %21 = arith.cmpf ugt, %19, %20 : f32
        %22 = arith.select %21, %19, %20 : f32
        %23 = arith.cmpf uno, %20, %20 : f32
        %24 = arith.select %23, %20, %22 : f32
        memref.store %24, %reinterpret_cast_1[%9] : memref<256xf32, #gpu.address_space<workgroup>>
      }
      gpu.barrier
      %16 = arith.cmpi eq, %6, %c0 : index
      %17 = arith.andi %16, %12 : i1
      scf.if %17 {
        %18 = arith.addi %9, %c1 : index
        %19 = memref.load %reinterpret_cast_1[%9] : memref<256xf32, #gpu.address_space<workgroup>>
        %20 = memref.load %reinterpret_cast_1[%18] : memref<256xf32, #gpu.address_space<workgroup>>
        %21 = arith.cmpf ugt, %19, %20 : f32
        %22 = arith.select %21, %19, %20 : f32
        %23 = arith.cmpf uno, %20, %20 : f32
        %24 = arith.select %23, %20, %22 : f32
        %25 = memref.generic_atomic_rmw %alloc[%11] : memref<1300xf32, "gpu"> {
        ^bb0(%arg3: f32):
          %26 = arith.cmpf ogt, %arg3, %24 : f32
          %27 = arith.select %26, %arg3, %24 : f32
          memref.atomic_yield %27 : f32
        }
      }
      scf.yield
    }
    "lmhlo.terminator"() : () -> ()
  }) {disc.device = "gpu", disc.fusion.name = "main_kColReduction_reduce__3_1_0", disc.fusion.tag = "8w32h", disc.fusion_type = "kColReduction", disc_col_reduction_schedule_hint = 1 : i32, disc_thread_per_block_hint = 256 : i32} : () -> ()
  %2 = llvm.inttoptr %0 : i32 to !llvm.ptr<i8>
  %alloca = memref.alloca() : memref<2xindex, "cpu">
  memref.store %c100, %alloca[%c0] : memref<2xindex, "cpu">
  memref.store %c13, %alloca[%c1] : memref<2xindex, "cpu">
  %3 = "disc_ral.dispatch"(%arg0, %2, %alloc, %alloca) {backend_config = "", call_target_name = "inc_ref", device = "gpu", has_side_effect = false} : (!disc_ral.context, !llvm.ptr<i8>, memref<1300xf32, "gpu">, memref<2xindex, "cpu">) -> memref<100x13xf32, "gpu">
  %reinterpret_cast = memref.reinterpret_cast %3 to offset: [0], sizes: [100, 13], strides: [13, 1] : memref<100x13xf32, "gpu"> to memref<100x13xf32, "gpu">
  memref.dealloc %alloc : memref<1300xf32, "gpu">
  "disc_ral.dispatch"(%arg0, %c0, %reinterpret_cast) {backend_config = "", call_target_name = "ral_send_output", device = "cpu", has_side_effect = false} : (!disc_ral.context, index, memref<100x13xf32, "gpu">) -> ()
  return
}

// -----// IR Dump After ParallelLoopCollapsing (disc-parallel-loop-collapsing) //----- //
func.func @main(%arg0: !disc_ral.context) attributes {tf.entry_function = {input_placements = "gpu", inputs = "input0", output_placements = "gpu", outputs = "output0"}} {
  %cst = arith.constant 0xFF800000 : f32
  %c110 = arith.constant 110 : index
  %c41 = arith.constant 41 : index
  %c2 = arith.constant 2 : index
  %c4 = arith.constant 4 : index
  %c256 = arith.constant 256 : index
  %c64 = arith.constant 64 : index
  %c8 = arith.constant 8 : index
  %c32 = arith.constant 32 : index
  %c1300 = arith.constant 1300 : index
  %0 = llvm.mlir.constant(0 : i32) : i32
  %c0 = arith.constant 0 : index
  %c100 = arith.constant 100 : index
  %c1 = arith.constant 1 : index
  %c13 = arith.constant 13 : index
  %1 = "disc_ral.dispatch"(%arg0, %c0) {backend_config = "", call_target_name = "ral_recv_input", device = "cpu", has_side_effect = false} : (!disc_ral.context, index) -> memref<110x100x13xf32, "gpu">
  %alloc = memref.alloc() : memref<1300xf32, "gpu">
  "lmhlo.fusion"() ({
    scf.parallel (%arg1) = (%c0) to (%c1300) step (%c1) {
      %reinterpret_cast_3 = memref.reinterpret_cast %alloc to offset: [0], sizes: [1300], strides: [1] : memref<1300xf32, "gpu"> to memref<1300xf32, "gpu">
      memref.store %cst, %reinterpret_cast_3[%arg1] : memref<1300xf32, "gpu">
      scf.yield
    }
    %c0_0 = arith.constant 0 : index
    %c1_1 = arith.constant 1 : index
    %c1_2 = arith.constant 1 : index
    %4 = arith.muli %c1_2, %c41 : index
    %5 = arith.muli %4, %c256 : index
    scf.parallel (%arg1) = (%c0_0) to (%5) step (%c1_1) {
      %6 = arith.remsi %arg1, %c256 : index
      %7 = arith.divsi %arg1, %c256 : index
      %8 = arith.divui %7, %c41 : index
      %9 = arith.remui %7, %c41 : index
      %10 = arith.divui %6, %c32 : index
      %11 = arith.remui %6, %c32 : index
      %12 = arith.muli %11, %c8 : index
      %13 = arith.addi %10, %12 : index
      %14 = arith.muli %9, %c32 : index
      %15 = arith.addi %11, %14 : index
      %alloc_3 = memref.alloc() : memref<256xf32, #gpu.address_space<workgroup>>
      %16 = arith.cmpi ult, %15, %c1300 : index
      %17 = scf.if %16 -> (f32) {
        %22 = scf.for %arg2 = %c0 to %c64 step %c1 iter_args(%arg3 = %cst) -> (f32) {
          %23 = arith.muli %8, %c8 : index
          %24 = arith.addi %10, %23 : index
          %25 = arith.muli %24, %c64 : index
          %26 = arith.addi %arg2, %25 : index
          %27 = arith.cmpi slt, %26, %c110 : index
          %28 = scf.if %27 -> (f32) {
            %29 = arith.muli %26, %c1300 : index
            %30 = arith.addi %29, %15 : index
            %reinterpret_cast_5 = memref.reinterpret_cast %1 to offset: [0], sizes: [143000], strides: [1] : memref<110x100x13xf32, "gpu"> to memref<143000xf32, "gpu">
            %31 = memref.load %reinterpret_cast_5[%30] : memref<143000xf32, "gpu">
            %32 = arith.cmpf ugt, %arg3, %31 : f32
            %33 = arith.select %32, %arg3, %31 : f32
            %34 = arith.cmpf uno, %31, %31 : f32
            %35 = arith.select %34, %31, %33 : f32
            scf.yield %35 : f32
          } else {
            scf.yield %arg3 : f32
          }
          scf.yield %28 : f32
        }
        scf.yield %22 : f32
      } else {
        scf.yield %cst : f32
      }
      %reinterpret_cast_4 = memref.reinterpret_cast %alloc_3 to offset: [0], sizes: [256], strides: [1] : memref<256xf32, #gpu.address_space<workgroup>> to memref<256xf32, #gpu.address_space<workgroup>>
      memref.store %17, %reinterpret_cast_4[%13] : memref<256xf32, #gpu.address_space<workgroup>>
      gpu.barrier
      %18 = arith.cmpi slt, %10, %c4 : index
      scf.if %18 {
        %22 = arith.addi %13, %c4 : index
        %23 = memref.load %reinterpret_cast_4[%13] : memref<256xf32, #gpu.address_space<workgroup>>
        %24 = memref.load %reinterpret_cast_4[%22] : memref<256xf32, #gpu.address_space<workgroup>>
        %25 = arith.cmpf ugt, %23, %24 : f32
        %26 = arith.select %25, %23, %24 : f32
        %27 = arith.cmpf uno, %24, %24 : f32
        %28 = arith.select %27, %24, %26 : f32
        memref.store %28, %reinterpret_cast_4[%13] : memref<256xf32, #gpu.address_space<workgroup>>
      }
      gpu.barrier
      %19 = arith.cmpi slt, %10, %c2 : index
      scf.if %19 {
        %22 = arith.addi %13, %c2 : index
        %23 = memref.load %reinterpret_cast_4[%13] : memref<256xf32, #gpu.address_space<workgroup>>
        %24 = memref.load %reinterpret_cast_4[%22] : memref<256xf32, #gpu.address_space<workgroup>>
        %25 = arith.cmpf ugt, %23, %24 : f32
        %26 = arith.select %25, %23, %24 : f32
        %27 = arith.cmpf uno, %24, %24 : f32
        %28 = arith.select %27, %24, %26 : f32
        memref.store %28, %reinterpret_cast_4[%13] : memref<256xf32, #gpu.address_space<workgroup>>
      }
      gpu.barrier
      %20 = arith.cmpi eq, %10, %c0 : index
      %21 = arith.andi %20, %16 : i1
      scf.if %21 {
        %22 = arith.addi %13, %c1 : index
        %23 = memref.load %reinterpret_cast_4[%13] : memref<256xf32, #gpu.address_space<workgroup>>
        %24 = memref.load %reinterpret_cast_4[%22] : memref<256xf32, #gpu.address_space<workgroup>>
        %25 = arith.cmpf ugt, %23, %24 : f32
        %26 = arith.select %25, %23, %24 : f32
        %27 = arith.cmpf uno, %24, %24 : f32
        %28 = arith.select %27, %24, %26 : f32
        %29 = memref.generic_atomic_rmw %alloc[%15] : memref<1300xf32, "gpu"> {
        ^bb0(%arg2: f32):
          %30 = arith.cmpf ogt, %arg2, %28 : f32
          %31 = arith.select %30, %arg2, %28 : f32
          memref.atomic_yield %31 : f32
        }
      }
      scf.yield
    }
    "lmhlo.terminator"() : () -> ()
  }) {disc.device = "gpu", disc.fusion.name = "main_kColReduction_reduce__3_1_0", disc.fusion.tag = "8w32h", disc.fusion_type = "kColReduction", disc_col_reduction_schedule_hint = 1 : i32, disc_thread_per_block_hint = 256 : i32} : () -> ()
  %2 = llvm.inttoptr %0 : i32 to !llvm.ptr<i8>
  %alloca = memref.alloca() : memref<2xindex, "cpu">
  memref.store %c100, %alloca[%c0] : memref<2xindex, "cpu">
  memref.store %c13, %alloca[%c1] : memref<2xindex, "cpu">
  %3 = "disc_ral.dispatch"(%arg0, %2, %alloc, %alloca) {backend_config = "", call_target_name = "inc_ref", device = "gpu", has_side_effect = false} : (!disc_ral.context, !llvm.ptr<i8>, memref<1300xf32, "gpu">, memref<2xindex, "cpu">) -> memref<100x13xf32, "gpu">
  %reinterpret_cast = memref.reinterpret_cast %3 to offset: [0], sizes: [100, 13], strides: [13, 1] : memref<100x13xf32, "gpu"> to memref<100x13xf32, "gpu">
  memref.dealloc %alloc : memref<1300xf32, "gpu">
  "disc_ral.dispatch"(%arg0, %c0, %reinterpret_cast) {backend_config = "", call_target_name = "ral_send_output", device = "cpu", has_side_effect = false} : (!disc_ral.context, index, memref<100x13xf32, "gpu">) -> ()
  return
}

// -----// IR Dump After SCFParallelLoopTiling (disc-parallel-loop-tiling) //----- //
func.func @main(%arg0: !disc_ral.context) attributes {tf.entry_function = {input_placements = "gpu", inputs = "input0", output_placements = "gpu", outputs = "output0"}} {
  %cst = arith.constant 0xFF800000 : f32
  %c110 = arith.constant 110 : index
  %c41 = arith.constant 41 : index
  %c2 = arith.constant 2 : index
  %c4 = arith.constant 4 : index
  %c256 = arith.constant 256 : index
  %c64 = arith.constant 64 : index
  %c8 = arith.constant 8 : index
  %c32 = arith.constant 32 : index
  %c1300 = arith.constant 1300 : index
  %0 = llvm.mlir.constant(0 : i32) : i32
  %c0 = arith.constant 0 : index
  %c100 = arith.constant 100 : index
  %c1 = arith.constant 1 : index
  %c13 = arith.constant 13 : index
  %1 = "disc_ral.dispatch"(%arg0, %c0) {backend_config = "", call_target_name = "ral_recv_input", device = "cpu", has_side_effect = false} : (!disc_ral.context, index) -> memref<110x100x13xf32, "gpu">
  %alloc = memref.alloc() : memref<1300xf32, "gpu">
  "lmhlo.fusion"() ({
    %c0_0 = arith.constant 0 : index
    %c256_1 = arith.constant 256 : index
    %4 = arith.muli %c1, %c256_1 : index
    scf.parallel (%arg1) = (%c0) to (%c1300) step (%4) {
      scf.parallel (%arg2) = (%c0_0) to (%4) step (%c1) {
        %8 = arith.addi %arg2, %arg1 : index
        %true = arith.constant true
        %9 = arith.muli %arg2, %c1 : index
        %10 = arith.addi %9, %arg1 : index
        %11 = arith.cmpi ult, %10, %c1300 : index
        %12 = arith.andi %true, %11 : i1
        scf.if %12 {
          %reinterpret_cast_7 = memref.reinterpret_cast %alloc to offset: [0], sizes: [1300], strides: [1] : memref<1300xf32, "gpu"> to memref<1300xf32, "gpu">
          memref.store %cst, %reinterpret_cast_7[%8] : memref<1300xf32, "gpu">
        }
        scf.yield
      }
      scf.yield
    }
    %c0_2 = arith.constant 0 : index
    %c1_3 = arith.constant 1 : index
    %c1_4 = arith.constant 1 : index
    %5 = arith.muli %c1_4, %c41 : index
    %6 = arith.muli %5, %c256 : index
    %c0_5 = arith.constant 0 : index
    %c256_6 = arith.constant 256 : index
    %7 = arith.muli %c1_3, %c256_6 : index
    scf.parallel (%arg1) = (%c0_2) to (%6) step (%7) {
      scf.parallel (%arg2) = (%c0_5) to (%7) step (%c1_3) {
        %8 = arith.addi %arg2, %arg1 : index
        %true = arith.constant true
        %9 = arith.muli %arg2, %c1_3 : index
        %10 = arith.addi %9, %arg1 : index
        %11 = arith.cmpi ult, %10, %6 : index
        %12 = arith.andi %true, %11 : i1
        scf.if %12 {
          %13 = arith.remsi %8, %c256 : index
          %14 = arith.divsi %8, %c256 : index
          %15 = arith.divui %14, %c41 : index
          %16 = arith.remui %14, %c41 : index
          %17 = arith.divui %13, %c32 : index
          %18 = arith.remui %13, %c32 : index
          %19 = arith.muli %18, %c8 : index
          %20 = arith.addi %17, %19 : index
          %21 = arith.muli %16, %c32 : index
          %22 = arith.addi %18, %21 : index
          %alloc_7 = memref.alloc() : memref<256xf32, #gpu.address_space<workgroup>>
          %23 = arith.cmpi ult, %22, %c1300 : index
          %24 = scf.if %23 -> (f32) {
            %29 = scf.for %arg3 = %c0 to %c64 step %c1 iter_args(%arg4 = %cst) -> (f32) {
              %30 = arith.muli %15, %c8 : index
              %31 = arith.addi %17, %30 : index
              %32 = arith.muli %31, %c64 : index
              %33 = arith.addi %arg3, %32 : index
              %34 = arith.cmpi slt, %33, %c110 : index
              %35 = scf.if %34 -> (f32) {
                %36 = arith.muli %33, %c1300 : index
                %37 = arith.addi %36, %22 : index
                %reinterpret_cast_9 = memref.reinterpret_cast %1 to offset: [0], sizes: [143000], strides: [1] : memref<110x100x13xf32, "gpu"> to memref<143000xf32, "gpu">
                %38 = memref.load %reinterpret_cast_9[%37] : memref<143000xf32, "gpu">
                %39 = arith.cmpf ugt, %arg4, %38 : f32
                %40 = arith.select %39, %arg4, %38 : f32
                %41 = arith.cmpf uno, %38, %38 : f32
                %42 = arith.select %41, %38, %40 : f32
                scf.yield %42 : f32
              } else {
                scf.yield %arg4 : f32
              }
              scf.yield %35 : f32
            }
            scf.yield %29 : f32
          } else {
            scf.yield %cst : f32
          }
          %reinterpret_cast_8 = memref.reinterpret_cast %alloc_7 to offset: [0], sizes: [256], strides: [1] : memref<256xf32, #gpu.address_space<workgroup>> to memref<256xf32, #gpu.address_space<workgroup>>
          memref.store %24, %reinterpret_cast_8[%20] : memref<256xf32, #gpu.address_space<workgroup>>
          gpu.barrier
          %25 = arith.cmpi slt, %17, %c4 : index
          scf.if %25 {
            %29 = arith.addi %20, %c4 : index
            %30 = memref.load %reinterpret_cast_8[%20] : memref<256xf32, #gpu.address_space<workgroup>>
            %31 = memref.load %reinterpret_cast_8[%29] : memref<256xf32, #gpu.address_space<workgroup>>
            %32 = arith.cmpf ugt, %30, %31 : f32
            %33 = arith.select %32, %30, %31 : f32
            %34 = arith.cmpf uno, %31, %31 : f32
            %35 = arith.select %34, %31, %33 : f32
            memref.store %35, %reinterpret_cast_8[%20] : memref<256xf32, #gpu.address_space<workgroup>>
          }
          gpu.barrier
          %26 = arith.cmpi slt, %17, %c2 : index
          scf.if %26 {
            %29 = arith.addi %20, %c2 : index
            %30 = memref.load %reinterpret_cast_8[%20] : memref<256xf32, #gpu.address_space<workgroup>>
            %31 = memref.load %reinterpret_cast_8[%29] : memref<256xf32, #gpu.address_space<workgroup>>
            %32 = arith.cmpf ugt, %30, %31 : f32
            %33 = arith.select %32, %30, %31 : f32
            %34 = arith.cmpf uno, %31, %31 : f32
            %35 = arith.select %34, %31, %33 : f32
            memref.store %35, %reinterpret_cast_8[%20] : memref<256xf32, #gpu.address_space<workgroup>>
          }
          gpu.barrier
          %27 = arith.cmpi eq, %17, %c0 : index
          %28 = arith.andi %27, %23 : i1
          scf.if %28 {
            %29 = arith.addi %20, %c1 : index
            %30 = memref.load %reinterpret_cast_8[%20] : memref<256xf32, #gpu.address_space<workgroup>>
            %31 = memref.load %reinterpret_cast_8[%29] : memref<256xf32, #gpu.address_space<workgroup>>
            %32 = arith.cmpf ugt, %30, %31 : f32
            %33 = arith.select %32, %30, %31 : f32
            %34 = arith.cmpf uno, %31, %31 : f32
            %35 = arith.select %34, %31, %33 : f32
            %36 = memref.generic_atomic_rmw %alloc[%22] : memref<1300xf32, "gpu"> {
            ^bb0(%arg3: f32):
              %37 = arith.cmpf ogt, %arg3, %35 : f32
              %38 = arith.select %37, %arg3, %35 : f32
              memref.atomic_yield %38 : f32
            }
          }
        }
        scf.yield
      }
      scf.yield
    }
    "lmhlo.terminator"() : () -> ()
  }) {disc.device = "gpu", disc.fusion.name = "main_kColReduction_reduce__3_1_0", disc.fusion.tag = "8w32h", disc.fusion_type = "kColReduction", disc_col_reduction_schedule_hint = 1 : i32, disc_thread_per_block_hint = 256 : i32} : () -> ()
  %2 = llvm.inttoptr %0 : i32 to !llvm.ptr<i8>
  %alloca = memref.alloca() : memref<2xindex, "cpu">
  memref.store %c100, %alloca[%c0] : memref<2xindex, "cpu">
  memref.store %c13, %alloca[%c1] : memref<2xindex, "cpu">
  %3 = "disc_ral.dispatch"(%arg0, %2, %alloc, %alloca) {backend_config = "", call_target_name = "inc_ref", device = "gpu", has_side_effect = false} : (!disc_ral.context, !llvm.ptr<i8>, memref<1300xf32, "gpu">, memref<2xindex, "cpu">) -> memref<100x13xf32, "gpu">
  %reinterpret_cast = memref.reinterpret_cast %3 to offset: [0], sizes: [100, 13], strides: [13, 1] : memref<100x13xf32, "gpu"> to memref<100x13xf32, "gpu">
  memref.dealloc %alloc : memref<1300xf32, "gpu">
  "disc_ral.dispatch"(%arg0, %c0, %reinterpret_cast) {backend_config = "", call_target_name = "ral_send_output", device = "cpu", has_side_effect = false} : (!disc_ral.context, index, memref<100x13xf32, "gpu">) -> ()
  return
}

// -----// IR Dump After GpuMapParallelLoopsPass (gpu-map-parallel-loops) //----- //
func.func @main(%arg0: !disc_ral.context) attributes {tf.entry_function = {input_placements = "gpu", inputs = "input0", output_placements = "gpu", outputs = "output0"}} {
  %cst = arith.constant 0xFF800000 : f32
  %c110 = arith.constant 110 : index
  %c41 = arith.constant 41 : index
  %c2 = arith.constant 2 : index
  %c4 = arith.constant 4 : index
  %c256 = arith.constant 256 : index
  %c64 = arith.constant 64 : index
  %c8 = arith.constant 8 : index
  %c32 = arith.constant 32 : index
  %c1300 = arith.constant 1300 : index
  %0 = llvm.mlir.constant(0 : i32) : i32
  %c0 = arith.constant 0 : index
  %c100 = arith.constant 100 : index
  %c1 = arith.constant 1 : index
  %c13 = arith.constant 13 : index
  %1 = "disc_ral.dispatch"(%arg0, %c0) {backend_config = "", call_target_name = "ral_recv_input", device = "cpu", has_side_effect = false} : (!disc_ral.context, index) -> memref<110x100x13xf32, "gpu">
  %alloc = memref.alloc() : memref<1300xf32, "gpu">
  "lmhlo.fusion"() ({
    %c0_0 = arith.constant 0 : index
    %c256_1 = arith.constant 256 : index
    %4 = arith.muli %c1, %c256_1 : index
    scf.parallel (%arg1) = (%c0) to (%c1300) step (%4) {
      scf.parallel (%arg2) = (%c0_0) to (%4) step (%c1) {
        %8 = arith.addi %arg2, %arg1 : index
        %true = arith.constant true
        %9 = arith.muli %arg2, %c1 : index
        %10 = arith.addi %9, %arg1 : index
        %11 = arith.cmpi ult, %10, %c1300 : index
        %12 = arith.andi %true, %11 : i1
        scf.if %12 {
          %reinterpret_cast_7 = memref.reinterpret_cast %alloc to offset: [0], sizes: [1300], strides: [1] : memref<1300xf32, "gpu"> to memref<1300xf32, "gpu">
          memref.store %cst, %reinterpret_cast_7[%8] : memref<1300xf32, "gpu">
        }
        scf.yield
      } {mapping = [#gpu.loop_dim_map<processor = thread_x, map = (d0) -> (d0), bound = (d0) -> (d0)>]}
      scf.yield
    } {mapping = [#gpu.loop_dim_map<processor = block_x, map = (d0) -> (d0), bound = (d0) -> (d0)>]}
    %c0_2 = arith.constant 0 : index
    %c1_3 = arith.constant 1 : index
    %c1_4 = arith.constant 1 : index
    %5 = arith.muli %c1_4, %c41 : index
    %6 = arith.muli %5, %c256 : index
    %c0_5 = arith.constant 0 : index
    %c256_6 = arith.constant 256 : index
    %7 = arith.muli %c1_3, %c256_6 : index
    scf.parallel (%arg1) = (%c0_2) to (%6) step (%7) {
      scf.parallel (%arg2) = (%c0_5) to (%7) step (%c1_3) {
        %8 = arith.addi %arg2, %arg1 : index
        %true = arith.constant true
        %9 = arith.muli %arg2, %c1_3 : index
        %10 = arith.addi %9, %arg1 : index
        %11 = arith.cmpi ult, %10, %6 : index
        %12 = arith.andi %true, %11 : i1
        scf.if %12 {
          %13 = arith.remsi %8, %c256 : index
          %14 = arith.divsi %8, %c256 : index
          %15 = arith.divui %14, %c41 : index
          %16 = arith.remui %14, %c41 : index
          %17 = arith.divui %13, %c32 : index
          %18 = arith.remui %13, %c32 : index
          %19 = arith.muli %18, %c8 : index
          %20 = arith.addi %17, %19 : index
          %21 = arith.muli %16, %c32 : index
          %22 = arith.addi %18, %21 : index
          %alloc_7 = memref.alloc() : memref<256xf32, #gpu.address_space<workgroup>>
          %23 = arith.cmpi ult, %22, %c1300 : index
          %24 = scf.if %23 -> (f32) {
            %29 = scf.for %arg3 = %c0 to %c64 step %c1 iter_args(%arg4 = %cst) -> (f32) {
              %30 = arith.muli %15, %c8 : index
              %31 = arith.addi %17, %30 : index
              %32 = arith.muli %31, %c64 : index
              %33 = arith.addi %arg3, %32 : index
              %34 = arith.cmpi slt, %33, %c110 : index
              %35 = scf.if %34 -> (f32) {
                %36 = arith.muli %33, %c1300 : index
                %37 = arith.addi %36, %22 : index
                %reinterpret_cast_9 = memref.reinterpret_cast %1 to offset: [0], sizes: [143000], strides: [1] : memref<110x100x13xf32, "gpu"> to memref<143000xf32, "gpu">
                %38 = memref.load %reinterpret_cast_9[%37] : memref<143000xf32, "gpu">
                %39 = arith.cmpf ugt, %arg4, %38 : f32
                %40 = arith.select %39, %arg4, %38 : f32
                %41 = arith.cmpf uno, %38, %38 : f32
                %42 = arith.select %41, %38, %40 : f32
                scf.yield %42 : f32
              } else {
                scf.yield %arg4 : f32
              }
              scf.yield %35 : f32
            }
            scf.yield %29 : f32
          } else {
            scf.yield %cst : f32
          }
          %reinterpret_cast_8 = memref.reinterpret_cast %alloc_7 to offset: [0], sizes: [256], strides: [1] : memref<256xf32, #gpu.address_space<workgroup>> to memref<256xf32, #gpu.address_space<workgroup>>
          memref.store %24, %reinterpret_cast_8[%20] : memref<256xf32, #gpu.address_space<workgroup>>
          gpu.barrier
          %25 = arith.cmpi slt, %17, %c4 : index
          scf.if %25 {
            %29 = arith.addi %20, %c4 : index
            %30 = memref.load %reinterpret_cast_8[%20] : memref<256xf32, #gpu.address_space<workgroup>>
            %31 = memref.load %reinterpret_cast_8[%29] : memref<256xf32, #gpu.address_space<workgroup>>
            %32 = arith.cmpf ugt, %30, %31 : f32
            %33 = arith.select %32, %30, %31 : f32
            %34 = arith.cmpf uno, %31, %31 : f32
            %35 = arith.select %34, %31, %33 : f32
            memref.store %35, %reinterpret_cast_8[%20] : memref<256xf32, #gpu.address_space<workgroup>>
          }
          gpu.barrier
          %26 = arith.cmpi slt, %17, %c2 : index
          scf.if %26 {
            %29 = arith.addi %20, %c2 : index
            %30 = memref.load %reinterpret_cast_8[%20] : memref<256xf32, #gpu.address_space<workgroup>>
            %31 = memref.load %reinterpret_cast_8[%29] : memref<256xf32, #gpu.address_space<workgroup>>
            %32 = arith.cmpf ugt, %30, %31 : f32
            %33 = arith.select %32, %30, %31 : f32
            %34 = arith.cmpf uno, %31, %31 : f32
            %35 = arith.select %34, %31, %33 : f32
            memref.store %35, %reinterpret_cast_8[%20] : memref<256xf32, #gpu.address_space<workgroup>>
          }
          gpu.barrier
          %27 = arith.cmpi eq, %17, %c0 : index
          %28 = arith.andi %27, %23 : i1
          scf.if %28 {
            %29 = arith.addi %20, %c1 : index
            %30 = memref.load %reinterpret_cast_8[%20] : memref<256xf32, #gpu.address_space<workgroup>>
            %31 = memref.load %reinterpret_cast_8[%29] : memref<256xf32, #gpu.address_space<workgroup>>
            %32 = arith.cmpf ugt, %30, %31 : f32
            %33 = arith.select %32, %30, %31 : f32
            %34 = arith.cmpf uno, %31, %31 : f32
            %35 = arith.select %34, %31, %33 : f32
            %36 = memref.generic_atomic_rmw %alloc[%22] : memref<1300xf32, "gpu"> {
            ^bb0(%arg3: f32):
              %37 = arith.cmpf ogt, %arg3, %35 : f32
              %38 = arith.select %37, %arg3, %35 : f32
              memref.atomic_yield %38 : f32
            }
          }
        }
        scf.yield
      } {mapping = [#gpu.loop_dim_map<processor = thread_x, map = (d0) -> (d0), bound = (d0) -> (d0)>]}
      scf.yield
    } {mapping = [#gpu.loop_dim_map<processor = block_x, map = (d0) -> (d0), bound = (d0) -> (d0)>]}
    "lmhlo.terminator"() : () -> ()
  }) {disc.device = "gpu", disc.fusion.name = "main_kColReduction_reduce__3_1_0", disc.fusion.tag = "8w32h", disc.fusion_type = "kColReduction", disc_col_reduction_schedule_hint = 1 : i32, disc_thread_per_block_hint = 256 : i32} : () -> ()
  %2 = llvm.inttoptr %0 : i32 to !llvm.ptr<i8>
  %alloca = memref.alloca() : memref<2xindex, "cpu">
  memref.store %c100, %alloca[%c0] : memref<2xindex, "cpu">
  memref.store %c13, %alloca[%c1] : memref<2xindex, "cpu">
  %3 = "disc_ral.dispatch"(%arg0, %2, %alloc, %alloca) {backend_config = "", call_target_name = "inc_ref", device = "gpu", has_side_effect = false} : (!disc_ral.context, !llvm.ptr<i8>, memref<1300xf32, "gpu">, memref<2xindex, "cpu">) -> memref<100x13xf32, "gpu">
  %reinterpret_cast = memref.reinterpret_cast %3 to offset: [0], sizes: [100, 13], strides: [13, 1] : memref<100x13xf32, "gpu"> to memref<100x13xf32, "gpu">
  memref.dealloc %alloc : memref<1300xf32, "gpu">
  "disc_ral.dispatch"(%arg0, %c0, %reinterpret_cast) {backend_config = "", call_target_name = "ral_send_output", device = "cpu", has_side_effect = false} : (!disc_ral.context, index, memref<100x13xf32, "gpu">) -> ()
  return
}

// -----// IR Dump After ConvertParallelLoopToGpu (convert-parallel-loops-to-gpu) //----- //
func.func @main(%arg0: !disc_ral.context) attributes {tf.entry_function = {input_placements = "gpu", inputs = "input0", output_placements = "gpu", outputs = "output0"}} {
  %cst = arith.constant 0xFF800000 : f32
  %c110 = arith.constant 110 : index
  %c41 = arith.constant 41 : index
  %c2 = arith.constant 2 : index
  %c4 = arith.constant 4 : index
  %c256 = arith.constant 256 : index
  %c64 = arith.constant 64 : index
  %c8 = arith.constant 8 : index
  %c32 = arith.constant 32 : index
  %c1300 = arith.constant 1300 : index
  %0 = llvm.mlir.constant(0 : i32) : i32
  %c0 = arith.constant 0 : index
  %c100 = arith.constant 100 : index
  %c1 = arith.constant 1 : index
  %c13 = arith.constant 13 : index
  %1 = "disc_ral.dispatch"(%arg0, %c0) {backend_config = "", call_target_name = "ral_recv_input", device = "cpu", has_side_effect = false} : (!disc_ral.context, index) -> memref<110x100x13xf32, "gpu">
  %alloc = memref.alloc() : memref<1300xf32, "gpu">
  "lmhlo.fusion"() ({
    %c0_0 = arith.constant 0 : index
    %c256_1 = arith.constant 256 : index
    %4 = arith.muli %c1, %c256_1 : index
    %c1_2 = arith.constant 1 : index
    %5 = affine.apply affine_map<(d0)[s0, s1] -> ((d0 - s0) ceildiv s1)>(%c1300)[%c0, %4]
    %6 = affine.apply affine_map<(d0)[s0, s1] -> ((d0 - s0) ceildiv s1)>(%4)[%c0_0, %c1]
    gpu.launch blocks(%arg1, %arg2, %arg3) in (%arg7 = %5, %arg8 = %c1_2, %arg9 = %c1_2) threads(%arg4, %arg5, %arg6) in (%arg10 = %6, %arg11 = %c1_2, %arg12 = %c1_2) {
      %12 = affine.apply affine_map<(d0)[s0, s1] -> (d0 * s0 + s1)>(%arg1)[%4, %c0]
      %13 = affine.apply affine_map<(d0)[s0, s1] -> (d0 * s0 + s1)>(%arg4)[%c1, %c0_0]
      %14 = arith.addi %13, %12 : index
      %true = arith.constant true
      %15 = arith.muli %13, %c1 : index
      %16 = arith.addi %15, %12 : index
      %17 = arith.cmpi ult, %16, %c1300 : index
      %18 = arith.andi %true, %17 : i1
      scf.if %18 {
        %reinterpret_cast_9 = memref.reinterpret_cast %alloc to offset: [0], sizes: [1300], strides: [1] : memref<1300xf32, "gpu"> to memref<1300xf32, "gpu">
        memref.store %cst, %reinterpret_cast_9[%14] : memref<1300xf32, "gpu">
      }
      gpu.terminator
    } {SCFToGPU_visited}
    %c0_3 = arith.constant 0 : index
    %c1_4 = arith.constant 1 : index
    %c1_5 = arith.constant 1 : index
    %7 = arith.muli %c1_5, %c41 : index
    %8 = arith.muli %7, %c256 : index
    %c0_6 = arith.constant 0 : index
    %c256_7 = arith.constant 256 : index
    %9 = arith.muli %c1_4, %c256_7 : index
    %c1_8 = arith.constant 1 : index
    %10 = affine.apply affine_map<(d0)[s0, s1] -> ((d0 - s0) ceildiv s1)>(%8)[%c0_3, %9]
    %11 = affine.apply affine_map<(d0)[s0, s1] -> ((d0 - s0) ceildiv s1)>(%9)[%c0_6, %c1_4]
    gpu.launch blocks(%arg1, %arg2, %arg3) in (%arg7 = %10, %arg8 = %c1_8, %arg9 = %c1_8) threads(%arg4, %arg5, %arg6) in (%arg10 = %11, %arg11 = %c1_8, %arg12 = %c1_8) {
      %12 = affine.apply affine_map<(d0)[s0, s1] -> (d0 * s0 + s1)>(%arg1)[%9, %c0_3]
      %13 = affine.apply affine_map<(d0)[s0, s1] -> (d0 * s0 + s1)>(%arg4)[%c1_4, %c0_6]
      %14 = arith.addi %13, %12 : index
      %true = arith.constant true
      %15 = arith.muli %13, %c1_4 : index
      %16 = arith.addi %15, %12 : index
      %17 = arith.cmpi ult, %16, %8 : index
      %18 = arith.andi %true, %17 : i1
      scf.if %18 {
        %19 = arith.remsi %14, %c256 : index
        %20 = arith.divsi %14, %c256 : index
        %21 = arith.divui %20, %c41 : index
        %22 = arith.remui %20, %c41 : index
        %23 = arith.divui %19, %c32 : index
        %24 = arith.remui %19, %c32 : index
        %25 = arith.muli %24, %c8 : index
        %26 = arith.addi %23, %25 : index
        %27 = arith.muli %22, %c32 : index
        %28 = arith.addi %24, %27 : index
        %alloc_9 = memref.alloc() : memref<256xf32, #gpu.address_space<workgroup>>
        %29 = arith.cmpi ult, %28, %c1300 : index
        %30 = scf.if %29 -> (f32) {
          %35 = scf.for %arg13 = %c0 to %c64 step %c1 iter_args(%arg14 = %cst) -> (f32) {
            %36 = arith.muli %21, %c8 : index
            %37 = arith.addi %23, %36 : index
            %38 = arith.muli %37, %c64 : index
            %39 = arith.addi %arg13, %38 : index
            %40 = arith.cmpi slt, %39, %c110 : index
            %41 = scf.if %40 -> (f32) {
              %42 = arith.muli %39, %c1300 : index
              %43 = arith.addi %42, %28 : index
              %reinterpret_cast_11 = memref.reinterpret_cast %1 to offset: [0], sizes: [143000], strides: [1] : memref<110x100x13xf32, "gpu"> to memref<143000xf32, "gpu">
              %44 = memref.load %reinterpret_cast_11[%43] : memref<143000xf32, "gpu">
              %45 = arith.cmpf ugt, %arg14, %44 : f32
              %46 = arith.select %45, %arg14, %44 : f32
              %47 = arith.cmpf uno, %44, %44 : f32
              %48 = arith.select %47, %44, %46 : f32
              scf.yield %48 : f32
            } else {
              scf.yield %arg14 : f32
            }
            scf.yield %41 : f32
          }
          scf.yield %35 : f32
        } else {
          scf.yield %cst : f32
        }
        %reinterpret_cast_10 = memref.reinterpret_cast %alloc_9 to offset: [0], sizes: [256], strides: [1] : memref<256xf32, #gpu.address_space<workgroup>> to memref<256xf32, #gpu.address_space<workgroup>>
        memref.store %30, %reinterpret_cast_10[%26] : memref<256xf32, #gpu.address_space<workgroup>>
        gpu.barrier
        %31 = arith.cmpi slt, %23, %c4 : index
        scf.if %31 {
          %35 = arith.addi %26, %c4 : index
          %36 = memref.load %reinterpret_cast_10[%26] : memref<256xf32, #gpu.address_space<workgroup>>
          %37 = memref.load %reinterpret_cast_10[%35] : memref<256xf32, #gpu.address_space<workgroup>>
          %38 = arith.cmpf ugt, %36, %37 : f32
          %39 = arith.select %38, %36, %37 : f32
          %40 = arith.cmpf uno, %37, %37 : f32
          %41 = arith.select %40, %37, %39 : f32
          memref.store %41, %reinterpret_cast_10[%26] : memref<256xf32, #gpu.address_space<workgroup>>
        }
        gpu.barrier
        %32 = arith.cmpi slt, %23, %c2 : index
        scf.if %32 {
          %35 = arith.addi %26, %c2 : index
          %36 = memref.load %reinterpret_cast_10[%26] : memref<256xf32, #gpu.address_space<workgroup>>
          %37 = memref.load %reinterpret_cast_10[%35] : memref<256xf32, #gpu.address_space<workgroup>>
          %38 = arith.cmpf ugt, %36, %37 : f32
          %39 = arith.select %38, %36, %37 : f32
          %40 = arith.cmpf uno, %37, %37 : f32
          %41 = arith.select %40, %37, %39 : f32
          memref.store %41, %reinterpret_cast_10[%26] : memref<256xf32, #gpu.address_space<workgroup>>
        }
        gpu.barrier
        %33 = arith.cmpi eq, %23, %c0 : index
        %34 = arith.andi %33, %29 : i1
        scf.if %34 {
          %35 = arith.addi %26, %c1 : index
          %36 = memref.load %reinterpret_cast_10[%26] : memref<256xf32, #gpu.address_space<workgroup>>
          %37 = memref.load %reinterpret_cast_10[%35] : memref<256xf32, #gpu.address_space<workgroup>>
          %38 = arith.cmpf ugt, %36, %37 : f32
          %39 = arith.select %38, %36, %37 : f32
          %40 = arith.cmpf uno, %37, %37 : f32
          %41 = arith.select %40, %37, %39 : f32
          %42 = memref.generic_atomic_rmw %alloc[%28] : memref<1300xf32, "gpu"> {
          ^bb0(%arg13: f32):
            %43 = arith.cmpf ogt, %arg13, %41 : f32
            %44 = arith.select %43, %arg13, %41 : f32
            memref.atomic_yield %44 : f32
          }
        }
      }
      gpu.terminator
    } {SCFToGPU_visited}
    "lmhlo.terminator"() : () -> ()
  }) {disc.device = "gpu", disc.fusion.name = "main_kColReduction_reduce__3_1_0", disc.fusion.tag = "8w32h", disc.fusion_type = "kColReduction", disc_col_reduction_schedule_hint = 1 : i32, disc_thread_per_block_hint = 256 : i32} : () -> ()
  %2 = llvm.inttoptr %0 : i32 to !llvm.ptr<i8>
  %alloca = memref.alloca() : memref<2xindex, "cpu">
  memref.store %c100, %alloca[%c0] : memref<2xindex, "cpu">
  memref.store %c13, %alloca[%c1] : memref<2xindex, "cpu">
  %3 = "disc_ral.dispatch"(%arg0, %2, %alloc, %alloca) {backend_config = "", call_target_name = "inc_ref", device = "gpu", has_side_effect = false} : (!disc_ral.context, !llvm.ptr<i8>, memref<1300xf32, "gpu">, memref<2xindex, "cpu">) -> memref<100x13xf32, "gpu">
  %reinterpret_cast = memref.reinterpret_cast %3 to offset: [0], sizes: [100, 13], strides: [13, 1] : memref<100x13xf32, "gpu"> to memref<100x13xf32, "gpu">
  memref.dealloc %alloc : memref<1300xf32, "gpu">
  "disc_ral.dispatch"(%arg0, %c0, %reinterpret_cast) {backend_config = "", call_target_name = "ral_send_output", device = "cpu", has_side_effect = false} : (!disc_ral.context, index, memref<100x13xf32, "gpu">) -> ()
  return
}

// -----// IR Dump After GpuLaunchSinkIndexComputations (gpu-launch-sink-index-computations) //----- //
#map = affine_map<(d0)[s0, s1] -> ((d0 - s0) ceildiv s1)>
#map1 = affine_map<(d0)[s0, s1] -> (d0 * s0 + s1)>
module {
  func.func @main(%arg0: !disc_ral.context) attributes {tf.entry_function = {input_placements = "gpu", inputs = "input0", output_placements = "gpu", outputs = "output0"}} {
    %cst = arith.constant 0xFF800000 : f32
    %c110 = arith.constant 110 : index
    %c41 = arith.constant 41 : index
    %c2 = arith.constant 2 : index
    %c4 = arith.constant 4 : index
    %c256 = arith.constant 256 : index
    %c64 = arith.constant 64 : index
    %c8 = arith.constant 8 : index
    %c32 = arith.constant 32 : index
    %c1300 = arith.constant 1300 : index
    %0 = llvm.mlir.constant(0 : i32) : i32
    %c0 = arith.constant 0 : index
    %c100 = arith.constant 100 : index
    %c1 = arith.constant 1 : index
    %c13 = arith.constant 13 : index
    %1 = "disc_ral.dispatch"(%arg0, %c0) {backend_config = "", call_target_name = "ral_recv_input", device = "cpu", has_side_effect = false} : (!disc_ral.context, index) -> memref<110x100x13xf32, "gpu">
    %alloc = memref.alloc() : memref<1300xf32, "gpu">
    "lmhlo.fusion"() ({
      %c0_0 = arith.constant 0 : index
      %c256_1 = arith.constant 256 : index
      %4 = arith.muli %c1, %c256_1 : index
      %c1_2 = arith.constant 1 : index
      %5 = affine.apply #map(%c1300)[%c0, %4]
      %6 = affine.apply #map(%4)[%c0_0, %c1]
      gpu.launch blocks(%arg1, %arg2, %arg3) in (%arg7 = %5, %arg8 = %c1_2, %arg9 = %c1_2) threads(%arg4, %arg5, %arg6) in (%arg10 = %6, %arg11 = %c1_2, %arg12 = %c1_2) {
        %c0_9 = arith.constant 0 : index
        %c1_10 = arith.constant 1 : index
        %c0_11 = arith.constant 0 : index
        %c1300_12 = arith.constant 1300 : index
        %cst_13 = arith.constant 0xFF800000 : f32
        %12 = affine.apply #map1(%arg1)[%4, %c0_9]
        %13 = affine.apply #map1(%arg4)[%c1_10, %c0_11]
        %14 = arith.addi %13, %12 : index
        %true = arith.constant true
        %15 = arith.muli %13, %c1_10 : index
        %16 = arith.addi %15, %12 : index
        %17 = arith.cmpi ult, %16, %c1300_12 : index
        %18 = arith.andi %true, %17 : i1
        scf.if %18 {
          %reinterpret_cast_14 = memref.reinterpret_cast %alloc to offset: [0], sizes: [1300], strides: [1] : memref<1300xf32, "gpu"> to memref<1300xf32, "gpu">
          memref.store %cst_13, %reinterpret_cast_14[%14] : memref<1300xf32, "gpu">
        }
        gpu.terminator
      } {SCFToGPU_visited}
      %c0_3 = arith.constant 0 : index
      %c1_4 = arith.constant 1 : index
      %c1_5 = arith.constant 1 : index
      %7 = arith.muli %c1_5, %c41 : index
      %8 = arith.muli %7, %c256 : index
      %c0_6 = arith.constant 0 : index
      %c256_7 = arith.constant 256 : index
      %9 = arith.muli %c1_4, %c256_7 : index
      %c1_8 = arith.constant 1 : index
      %10 = affine.apply #map(%8)[%c0_3, %9]
      %11 = affine.apply #map(%9)[%c0_6, %c1_4]
      gpu.launch blocks(%arg1, %arg2, %arg3) in (%arg7 = %10, %arg8 = %c1_8, %arg9 = %c1_8) threads(%arg4, %arg5, %arg6) in (%arg10 = %11, %arg11 = %c1_8, %arg12 = %c1_8) {
        %c0_9 = arith.constant 0 : index
        %c1_10 = arith.constant 1 : index
        %c0_11 = arith.constant 0 : index
        %c256_12 = arith.constant 256 : index
        %c41_13 = arith.constant 41 : index
        %c32_14 = arith.constant 32 : index
        %c8_15 = arith.constant 8 : index
        %c1300_16 = arith.constant 1300 : index
        %c64_17 = arith.constant 64 : index
        %c110_18 = arith.constant 110 : index
        %c0_19 = arith.constant 0 : index
        %c1_20 = arith.constant 1 : index
        %cst_21 = arith.constant 0xFF800000 : f32
        %c4_22 = arith.constant 4 : index
        %c2_23 = arith.constant 2 : index
        %12 = affine.apply #map1(%arg1)[%9, %c0_9]
        %13 = affine.apply #map1(%arg4)[%c1_10, %c0_11]
        %14 = arith.addi %13, %12 : index
        %true = arith.constant true
        %15 = arith.muli %13, %c1_10 : index
        %16 = arith.addi %15, %12 : index
        %17 = arith.cmpi ult, %16, %8 : index
        %18 = arith.andi %true, %17 : i1
        scf.if %18 {
          %19 = arith.remsi %14, %c256_12 : index
          %20 = arith.divsi %14, %c256_12 : index
          %21 = arith.divui %20, %c41_13 : index
          %22 = arith.remui %20, %c41_13 : index
          %23 = arith.divui %19, %c32_14 : index
          %24 = arith.remui %19, %c32_14 : index
          %25 = arith.muli %24, %c8_15 : index
          %26 = arith.addi %23, %25 : index
          %27 = arith.muli %22, %c32_14 : index
          %28 = arith.addi %24, %27 : index
          %alloc_24 = memref.alloc() : memref<256xf32, #gpu.address_space<workgroup>>
          %29 = arith.cmpi ult, %28, %c1300_16 : index
          %30 = scf.if %29 -> (f32) {
            %35 = scf.for %arg13 = %c0_19 to %c64_17 step %c1_20 iter_args(%arg14 = %cst_21) -> (f32) {
              %36 = arith.muli %21, %c8_15 : index
              %37 = arith.addi %23, %36 : index
              %38 = arith.muli %37, %c64_17 : index
              %39 = arith.addi %arg13, %38 : index
              %40 = arith.cmpi slt, %39, %c110_18 : index
              %41 = scf.if %40 -> (f32) {
                %42 = arith.muli %39, %c1300_16 : index
                %43 = arith.addi %42, %28 : index
                %reinterpret_cast_26 = memref.reinterpret_cast %1 to offset: [0], sizes: [143000], strides: [1] : memref<110x100x13xf32, "gpu"> to memref<143000xf32, "gpu">
                %44 = memref.load %reinterpret_cast_26[%43] : memref<143000xf32, "gpu">
                %45 = arith.cmpf ugt, %arg14, %44 : f32
                %46 = arith.select %45, %arg14, %44 : f32
                %47 = arith.cmpf uno, %44, %44 : f32
                %48 = arith.select %47, %44, %46 : f32
                scf.yield %48 : f32
              } else {
                scf.yield %arg14 : f32
              }
              scf.yield %41 : f32
            }
            scf.yield %35 : f32
          } else {
            scf.yield %cst_21 : f32
          }
          %reinterpret_cast_25 = memref.reinterpret_cast %alloc_24 to offset: [0], sizes: [256], strides: [1] : memref<256xf32, #gpu.address_space<workgroup>> to memref<256xf32, #gpu.address_space<workgroup>>
          memref.store %30, %reinterpret_cast_25[%26] : memref<256xf32, #gpu.address_space<workgroup>>
          gpu.barrier
          %31 = arith.cmpi slt, %23, %c4_22 : index
          scf.if %31 {
            %35 = arith.addi %26, %c4_22 : index
            %36 = memref.load %reinterpret_cast_25[%26] : memref<256xf32, #gpu.address_space<workgroup>>
            %37 = memref.load %reinterpret_cast_25[%35] : memref<256xf32, #gpu.address_space<workgroup>>
            %38 = arith.cmpf ugt, %36, %37 : f32
            %39 = arith.select %38, %36, %37 : f32
            %40 = arith.cmpf uno, %37, %37 : f32
            %41 = arith.select %40, %37, %39 : f32
            memref.store %41, %reinterpret_cast_25[%26] : memref<256xf32, #gpu.address_space<workgroup>>
          }
          gpu.barrier
          %32 = arith.cmpi slt, %23, %c2_23 : index
          scf.if %32 {
            %35 = arith.addi %26, %c2_23 : index
            %36 = memref.load %reinterpret_cast_25[%26] : memref<256xf32, #gpu.address_space<workgroup>>
            %37 = memref.load %reinterpret_cast_25[%35] : memref<256xf32, #gpu.address_space<workgroup>>
            %38 = arith.cmpf ugt, %36, %37 : f32
            %39 = arith.select %38, %36, %37 : f32
            %40 = arith.cmpf uno, %37, %37 : f32
            %41 = arith.select %40, %37, %39 : f32
            memref.store %41, %reinterpret_cast_25[%26] : memref<256xf32, #gpu.address_space<workgroup>>
          }
          gpu.barrier
          %33 = arith.cmpi eq, %23, %c0_19 : index
          %34 = arith.andi %33, %29 : i1
          scf.if %34 {
            %35 = arith.addi %26, %c1_20 : index
            %36 = memref.load %reinterpret_cast_25[%26] : memref<256xf32, #gpu.address_space<workgroup>>
            %37 = memref.load %reinterpret_cast_25[%35] : memref<256xf32, #gpu.address_space<workgroup>>
            %38 = arith.cmpf ugt, %36, %37 : f32
            %39 = arith.select %38, %36, %37 : f32
            %40 = arith.cmpf uno, %37, %37 : f32
            %41 = arith.select %40, %37, %39 : f32
            %42 = memref.generic_atomic_rmw %alloc[%28] : memref<1300xf32, "gpu"> {
            ^bb0(%arg13: f32):
              %43 = arith.cmpf ogt, %arg13, %41 : f32
              %44 = arith.select %43, %arg13, %41 : f32
              memref.atomic_yield %44 : f32
            }
          }
        }
        gpu.terminator
      } {SCFToGPU_visited}
      "lmhlo.terminator"() : () -> ()
    }) {disc.device = "gpu", disc.fusion.name = "main_kColReduction_reduce__3_1_0", disc.fusion.tag = "8w32h", disc.fusion_type = "kColReduction", disc_col_reduction_schedule_hint = 1 : i32, disc_thread_per_block_hint = 256 : i32} : () -> ()
    %2 = llvm.inttoptr %0 : i32 to !llvm.ptr<i8>
    %alloca = memref.alloca() : memref<2xindex, "cpu">
    memref.store %c100, %alloca[%c0] : memref<2xindex, "cpu">
    memref.store %c13, %alloca[%c1] : memref<2xindex, "cpu">
    %3 = "disc_ral.dispatch"(%arg0, %2, %alloc, %alloca) {backend_config = "", call_target_name = "inc_ref", device = "gpu", has_side_effect = false} : (!disc_ral.context, !llvm.ptr<i8>, memref<1300xf32, "gpu">, memref<2xindex, "cpu">) -> memref<100x13xf32, "gpu">
    %reinterpret_cast = memref.reinterpret_cast %3 to offset: [0], sizes: [100, 13], strides: [13, 1] : memref<100x13xf32, "gpu"> to memref<100x13xf32, "gpu">
    memref.dealloc %alloc : memref<1300xf32, "gpu">
    "disc_ral.dispatch"(%arg0, %c0, %reinterpret_cast) {backend_config = "", call_target_name = "ral_send_output", device = "cpu", has_side_effect = false} : (!disc_ral.context, index, memref<100x13xf32, "gpu">) -> ()
    return
  }
  func.func @shape_constraint_graph() {
    return
  }
}


// -----// IR Dump After GpuKernelOutlining (gpu-kernel-outlining) //----- //
#map = affine_map<(d0)[s0, s1] -> ((d0 - s0) ceildiv s1)>
#map1 = affine_map<(d0)[s0, s1] -> (d0 * s0 + s1)>
module attributes {gpu.container_module} {
  func.func @main(%arg0: !disc_ral.context) attributes {tf.entry_function = {input_placements = "gpu", inputs = "input0", output_placements = "gpu", outputs = "output0"}} {
    %cst = arith.constant 0xFF800000 : f32
    %c110 = arith.constant 110 : index
    %c41 = arith.constant 41 : index
    %c2 = arith.constant 2 : index
    %c4 = arith.constant 4 : index
    %c256 = arith.constant 256 : index
    %c64 = arith.constant 64 : index
    %c8 = arith.constant 8 : index
    %c32 = arith.constant 32 : index
    %c1300 = arith.constant 1300 : index
    %0 = llvm.mlir.constant(0 : i32) : i32
    %c0 = arith.constant 0 : index
    %c100 = arith.constant 100 : index
    %c1 = arith.constant 1 : index
    %c13 = arith.constant 13 : index
    %1 = "disc_ral.dispatch"(%arg0, %c0) {backend_config = "", call_target_name = "ral_recv_input", device = "cpu", has_side_effect = false} : (!disc_ral.context, index) -> memref<110x100x13xf32, "gpu">
    %alloc = memref.alloc() : memref<1300xf32, "gpu">
    "lmhlo.fusion"() ({
      %c0_0 = arith.constant 0 : index
      %c256_1 = arith.constant 256 : index
      %4 = arith.muli %c1, %c256_1 : index
      %c1_2 = arith.constant 1 : index
      %5 = affine.apply #map(%c1300)[%c0, %4]
      %6 = affine.apply #map(%4)[%c0_0, %c1]
      gpu.launch_func  @main_kernel::@main_kernel blocks in (%5, %c1_2, %c1_2) threads in (%6, %c1_2, %c1_2) args(%4 : index, %alloc : memref<1300xf32, "gpu">)
      %c0_3 = arith.constant 0 : index
      %c1_4 = arith.constant 1 : index
      %c1_5 = arith.constant 1 : index
      %7 = arith.muli %c1_5, %c41 : index
      %8 = arith.muli %7, %c256 : index
      %c0_6 = arith.constant 0 : index
      %c256_7 = arith.constant 256 : index
      %9 = arith.muli %c1_4, %c256_7 : index
      %c1_8 = arith.constant 1 : index
      %10 = affine.apply #map(%8)[%c0_3, %9]
      %11 = affine.apply #map(%9)[%c0_6, %c1_4]
      gpu.launch_func  @main_kernel_0::@main_kernel blocks in (%10, %c1_8, %c1_8) threads in (%11, %c1_8, %c1_8) args(%9 : index, %8 : index, %1 : memref<110x100x13xf32, "gpu">, %alloc : memref<1300xf32, "gpu">)
      "lmhlo.terminator"() : () -> ()
    }) {disc.device = "gpu", disc.fusion.name = "main_kColReduction_reduce__3_1_0", disc.fusion.tag = "8w32h", disc.fusion_type = "kColReduction", disc_col_reduction_schedule_hint = 1 : i32, disc_thread_per_block_hint = 256 : i32} : () -> ()
    %2 = llvm.inttoptr %0 : i32 to !llvm.ptr<i8>
    %alloca = memref.alloca() : memref<2xindex, "cpu">
    memref.store %c100, %alloca[%c0] : memref<2xindex, "cpu">
    memref.store %c13, %alloca[%c1] : memref<2xindex, "cpu">
    %3 = "disc_ral.dispatch"(%arg0, %2, %alloc, %alloca) {backend_config = "", call_target_name = "inc_ref", device = "gpu", has_side_effect = false} : (!disc_ral.context, !llvm.ptr<i8>, memref<1300xf32, "gpu">, memref<2xindex, "cpu">) -> memref<100x13xf32, "gpu">
    %reinterpret_cast = memref.reinterpret_cast %3 to offset: [0], sizes: [100, 13], strides: [13, 1] : memref<100x13xf32, "gpu"> to memref<100x13xf32, "gpu">
    memref.dealloc %alloc : memref<1300xf32, "gpu">
    "disc_ral.dispatch"(%arg0, %c0, %reinterpret_cast) {backend_config = "", call_target_name = "ral_send_output", device = "cpu", has_side_effect = false} : (!disc_ral.context, index, memref<100x13xf32, "gpu">) -> ()
    return
  }
  gpu.module @main_kernel {
    gpu.func @main_kernel(%arg0: index, %arg1: memref<1300xf32, "gpu">) kernel {
      %0 = gpu.block_id  x
      %1 = gpu.block_id  y
      %2 = gpu.block_id  z
      %3 = gpu.thread_id  x
      %4 = gpu.thread_id  y
      %5 = gpu.thread_id  z
      %6 = gpu.grid_dim  x
      %7 = gpu.grid_dim  y
      %8 = gpu.grid_dim  z
      %9 = gpu.block_dim  x
      %10 = gpu.block_dim  y
      %11 = gpu.block_dim  z
      cf.br ^bb1
    ^bb1:  // pred: ^bb0
      %c0 = arith.constant 0 : index
      %c1 = arith.constant 1 : index
      %c0_0 = arith.constant 0 : index
      %c1300 = arith.constant 1300 : index
      %cst = arith.constant 0xFF800000 : f32
      %12 = affine.apply #map1(%0)[%arg0, %c0]
      %13 = affine.apply #map1(%3)[%c1, %c0_0]
      %14 = arith.addi %13, %12 : index
      %true = arith.constant true
      %15 = arith.muli %13, %c1 : index
      %16 = arith.addi %15, %12 : index
      %17 = arith.cmpi ult, %16, %c1300 : index
      %18 = arith.andi %true, %17 : i1
      scf.if %18 {
        %reinterpret_cast = memref.reinterpret_cast %arg1 to offset: [0], sizes: [1300], strides: [1] : memref<1300xf32, "gpu"> to memref<1300xf32, "gpu">
        memref.store %cst, %reinterpret_cast[%14] : memref<1300xf32, "gpu">
      }
      gpu.return
    }
  }
  gpu.module @main_kernel_0 {
    gpu.func @main_kernel(%arg0: index, %arg1: index, %arg2: memref<110x100x13xf32, "gpu">, %arg3: memref<1300xf32, "gpu">) kernel {
      %0 = gpu.block_id  x
      %1 = gpu.block_id  y
      %2 = gpu.block_id  z
      %3 = gpu.thread_id  x
      %4 = gpu.thread_id  y
      %5 = gpu.thread_id  z
      %6 = gpu.grid_dim  x
      %7 = gpu.grid_dim  y
      %8 = gpu.grid_dim  z
      %9 = gpu.block_dim  x
      %10 = gpu.block_dim  y
      %11 = gpu.block_dim  z
      cf.br ^bb1
    ^bb1:  // pred: ^bb0
      %c0 = arith.constant 0 : index
      %c1 = arith.constant 1 : index
      %c0_0 = arith.constant 0 : index
      %c256 = arith.constant 256 : index
      %c41 = arith.constant 41 : index
      %c32 = arith.constant 32 : index
      %c8 = arith.constant 8 : index
      %c1300 = arith.constant 1300 : index
      %c64 = arith.constant 64 : index
      %c110 = arith.constant 110 : index
      %c0_1 = arith.constant 0 : index
      %c1_2 = arith.constant 1 : index
      %cst = arith.constant 0xFF800000 : f32
      %c4 = arith.constant 4 : index
      %c2 = arith.constant 2 : index
      %12 = affine.apply #map1(%0)[%arg0, %c0]
      %13 = affine.apply #map1(%3)[%c1, %c0_0]
      %14 = arith.addi %13, %12 : index
      %true = arith.constant true
      %15 = arith.muli %13, %c1 : index
      %16 = arith.addi %15, %12 : index
      %17 = arith.cmpi ult, %16, %arg1 : index
      %18 = arith.andi %true, %17 : i1
      scf.if %18 {
        %19 = arith.remsi %14, %c256 : index
        %20 = arith.divsi %14, %c256 : index
        %21 = arith.divui %20, %c41 : index
        %22 = arith.remui %20, %c41 : index
        %23 = arith.divui %19, %c32 : index
        %24 = arith.remui %19, %c32 : index
        %25 = arith.muli %24, %c8 : index
        %26 = arith.addi %23, %25 : index
        %27 = arith.muli %22, %c32 : index
        %28 = arith.addi %24, %27 : index
        %alloc = memref.alloc() : memref<256xf32, #gpu.address_space<workgroup>>
        %29 = arith.cmpi ult, %28, %c1300 : index
        %30 = scf.if %29 -> (f32) {
          %35 = scf.for %arg4 = %c0_1 to %c64 step %c1_2 iter_args(%arg5 = %cst) -> (f32) {
            %36 = arith.muli %21, %c8 : index
            %37 = arith.addi %23, %36 : index
            %38 = arith.muli %37, %c64 : index
            %39 = arith.addi %arg4, %38 : index
            %40 = arith.cmpi slt, %39, %c110 : index
            %41 = scf.if %40 -> (f32) {
              %42 = arith.muli %39, %c1300 : index
              %43 = arith.addi %42, %28 : index
              %reinterpret_cast_3 = memref.reinterpret_cast %arg2 to offset: [0], sizes: [143000], strides: [1] : memref<110x100x13xf32, "gpu"> to memref<143000xf32, "gpu">
              %44 = memref.load %reinterpret_cast_3[%43] : memref<143000xf32, "gpu">
              %45 = arith.cmpf ugt, %arg5, %44 : f32
              %46 = arith.select %45, %arg5, %44 : f32
              %47 = arith.cmpf uno, %44, %44 : f32
              %48 = arith.select %47, %44, %46 : f32
              scf.yield %48 : f32
            } else {
              scf.yield %arg5 : f32
            }
            scf.yield %41 : f32
          }
          scf.yield %35 : f32
        } else {
          scf.yield %cst : f32
        }
        %reinterpret_cast = memref.reinterpret_cast %alloc to offset: [0], sizes: [256], strides: [1] : memref<256xf32, #gpu.address_space<workgroup>> to memref<256xf32, #gpu.address_space<workgroup>>
        memref.store %30, %reinterpret_cast[%26] : memref<256xf32, #gpu.address_space<workgroup>>
        gpu.barrier
        %31 = arith.cmpi slt, %23, %c4 : index
        scf.if %31 {
          %35 = arith.addi %26, %c4 : index
          %36 = memref.load %reinterpret_cast[%26] : memref<256xf32, #gpu.address_space<workgroup>>
          %37 = memref.load %reinterpret_cast[%35] : memref<256xf32, #gpu.address_space<workgroup>>
          %38 = arith.cmpf ugt, %36, %37 : f32
          %39 = arith.select %38, %36, %37 : f32
          %40 = arith.cmpf uno, %37, %37 : f32
          %41 = arith.select %40, %37, %39 : f32
          memref.store %41, %reinterpret_cast[%26] : memref<256xf32, #gpu.address_space<workgroup>>
        }
        gpu.barrier
        %32 = arith.cmpi slt, %23, %c2 : index
        scf.if %32 {
          %35 = arith.addi %26, %c2 : index
          %36 = memref.load %reinterpret_cast[%26] : memref<256xf32, #gpu.address_space<workgroup>>
          %37 = memref.load %reinterpret_cast[%35] : memref<256xf32, #gpu.address_space<workgroup>>
          %38 = arith.cmpf ugt, %36, %37 : f32
          %39 = arith.select %38, %36, %37 : f32
          %40 = arith.cmpf uno, %37, %37 : f32
          %41 = arith.select %40, %37, %39 : f32
          memref.store %41, %reinterpret_cast[%26] : memref<256xf32, #gpu.address_space<workgroup>>
        }
        gpu.barrier
        %33 = arith.cmpi eq, %23, %c0_1 : index
        %34 = arith.andi %33, %29 : i1
        scf.if %34 {
          %35 = arith.addi %26, %c1_2 : index
          %36 = memref.load %reinterpret_cast[%26] : memref<256xf32, #gpu.address_space<workgroup>>
          %37 = memref.load %reinterpret_cast[%35] : memref<256xf32, #gpu.address_space<workgroup>>
          %38 = arith.cmpf ugt, %36, %37 : f32
          %39 = arith.select %38, %36, %37 : f32
          %40 = arith.cmpf uno, %37, %37 : f32
          %41 = arith.select %40, %37, %39 : f32
          %42 = memref.generic_atomic_rmw %arg3[%28] : memref<1300xf32, "gpu"> {
          ^bb0(%arg4: f32):
            %43 = arith.cmpf ogt, %arg4, %41 : f32
            %44 = arith.select %43, %arg4, %41 : f32
            memref.atomic_yield %44 : f32
          }
        }
      }
      gpu.return
    }
  }
  func.func @shape_constraint_graph() {
    return
  }
}


// -----// IR Dump After AssignKernelNamePass (disc-assign-kernel-name) //----- //
#map = affine_map<(d0)[s0, s1] -> ((d0 - s0) ceildiv s1)>
#map1 = affine_map<(d0)[s0, s1] -> (d0 * s0 + s1)>
module attributes {gpu.container_module} {
  func.func @main(%arg0: !disc_ral.context) attributes {tf.entry_function = {input_placements = "gpu", inputs = "input0", output_placements = "gpu", outputs = "output0"}} {
    %cst = arith.constant 0xFF800000 : f32
    %c110 = arith.constant 110 : index
    %c41 = arith.constant 41 : index
    %c2 = arith.constant 2 : index
    %c4 = arith.constant 4 : index
    %c256 = arith.constant 256 : index
    %c64 = arith.constant 64 : index
    %c8 = arith.constant 8 : index
    %c32 = arith.constant 32 : index
    %c1300 = arith.constant 1300 : index
    %0 = llvm.mlir.constant(0 : i32) : i32
    %c0 = arith.constant 0 : index
    %c100 = arith.constant 100 : index
    %c1 = arith.constant 1 : index
    %c13 = arith.constant 13 : index
    %1 = "disc_ral.dispatch"(%arg0, %c0) {backend_config = "", call_target_name = "ral_recv_input", device = "cpu", has_side_effect = false} : (!disc_ral.context, index) -> memref<110x100x13xf32, "gpu">
    %alloc = memref.alloc() : memref<1300xf32, "gpu">
    "lmhlo.fusion"() ({
      %c0_0 = arith.constant 0 : index
      %c256_1 = arith.constant 256 : index
      %4 = arith.muli %c1, %c256_1 : index
      %c1_2 = arith.constant 1 : index
      %5 = affine.apply #map(%c1300)[%c0, %4]
      %6 = affine.apply #map(%4)[%c0_0, %c1]
      gpu.launch_func  @main_kernel::@main_kColReduction_reduce__3_1_0___8w32h blocks in (%5, %c1_2, %c1_2) threads in (%6, %c1_2, %c1_2) args(%4 : index, %alloc : memref<1300xf32, "gpu">)
      %c0_3 = arith.constant 0 : index
      %c1_4 = arith.constant 1 : index
      %c1_5 = arith.constant 1 : index
      %7 = arith.muli %c1_5, %c41 : index
      %8 = arith.muli %7, %c256 : index
      %c0_6 = arith.constant 0 : index
      %c256_7 = arith.constant 256 : index
      %9 = arith.muli %c1_4, %c256_7 : index
      %c1_8 = arith.constant 1 : index
      %10 = affine.apply #map(%8)[%c0_3, %9]
      %11 = affine.apply #map(%9)[%c0_6, %c1_4]
      gpu.launch_func  @main_kernel_0::@main_kColReduction_reduce__3_1_0___8w32h_1 blocks in (%10, %c1_8, %c1_8) threads in (%11, %c1_8, %c1_8) args(%9 : index, %8 : index, %1 : memref<110x100x13xf32, "gpu">, %alloc : memref<1300xf32, "gpu">)
      "lmhlo.terminator"() : () -> ()
    }) {disc.device = "gpu", disc.fusion.name = "main_kColReduction_reduce__3_1_0", disc.fusion.tag = "8w32h", disc.fusion_type = "kColReduction", disc_col_reduction_schedule_hint = 1 : i32, disc_thread_per_block_hint = 256 : i32} : () -> ()
    %2 = llvm.inttoptr %0 : i32 to !llvm.ptr<i8>
    %alloca = memref.alloca() : memref<2xindex, "cpu">
    memref.store %c100, %alloca[%c0] : memref<2xindex, "cpu">
    memref.store %c13, %alloca[%c1] : memref<2xindex, "cpu">
    %3 = "disc_ral.dispatch"(%arg0, %2, %alloc, %alloca) {backend_config = "", call_target_name = "inc_ref", device = "gpu", has_side_effect = false} : (!disc_ral.context, !llvm.ptr<i8>, memref<1300xf32, "gpu">, memref<2xindex, "cpu">) -> memref<100x13xf32, "gpu">
    %reinterpret_cast = memref.reinterpret_cast %3 to offset: [0], sizes: [100, 13], strides: [13, 1] : memref<100x13xf32, "gpu"> to memref<100x13xf32, "gpu">
    memref.dealloc %alloc : memref<1300xf32, "gpu">
    "disc_ral.dispatch"(%arg0, %c0, %reinterpret_cast) {backend_config = "", call_target_name = "ral_send_output", device = "cpu", has_side_effect = false} : (!disc_ral.context, index, memref<100x13xf32, "gpu">) -> ()
    return
  }
  gpu.module @main_kernel {
    gpu.func @main_kColReduction_reduce__3_1_0___8w32h(%arg0: index, %arg1: memref<1300xf32, "gpu">) kernel {
      %0 = gpu.block_id  x
      %1 = gpu.block_id  y
      %2 = gpu.block_id  z
      %3 = gpu.thread_id  x
      %4 = gpu.thread_id  y
      %5 = gpu.thread_id  z
      %6 = gpu.grid_dim  x
      %7 = gpu.grid_dim  y
      %8 = gpu.grid_dim  z
      %9 = gpu.block_dim  x
      %10 = gpu.block_dim  y
      %11 = gpu.block_dim  z
      cf.br ^bb1
    ^bb1:  // pred: ^bb0
      %c0 = arith.constant 0 : index
      %c1 = arith.constant 1 : index
      %c0_0 = arith.constant 0 : index
      %c1300 = arith.constant 1300 : index
      %cst = arith.constant 0xFF800000 : f32
      %12 = affine.apply #map1(%0)[%arg0, %c0]
      %13 = affine.apply #map1(%3)[%c1, %c0_0]
      %14 = arith.addi %13, %12 : index
      %true = arith.constant true
      %15 = arith.muli %13, %c1 : index
      %16 = arith.addi %15, %12 : index
      %17 = arith.cmpi ult, %16, %c1300 : index
      %18 = arith.andi %true, %17 : i1
      scf.if %18 {
        %reinterpret_cast = memref.reinterpret_cast %arg1 to offset: [0], sizes: [1300], strides: [1] : memref<1300xf32, "gpu"> to memref<1300xf32, "gpu">
        memref.store %cst, %reinterpret_cast[%14] : memref<1300xf32, "gpu">
      }
      gpu.return
    }
  }
  gpu.module @main_kernel_0 {
    gpu.func @main_kColReduction_reduce__3_1_0___8w32h_1(%arg0: index, %arg1: index, %arg2: memref<110x100x13xf32, "gpu">, %arg3: memref<1300xf32, "gpu">) kernel {
      %0 = gpu.block_id  x
      %1 = gpu.block_id  y
      %2 = gpu.block_id  z
      %3 = gpu.thread_id  x
      %4 = gpu.thread_id  y
      %5 = gpu.thread_id  z
      %6 = gpu.grid_dim  x
      %7 = gpu.grid_dim  y
      %8 = gpu.grid_dim  z
      %9 = gpu.block_dim  x
      %10 = gpu.block_dim  y
      %11 = gpu.block_dim  z
      cf.br ^bb1
    ^bb1:  // pred: ^bb0
      %c0 = arith.constant 0 : index
      %c1 = arith.constant 1 : index
      %c0_0 = arith.constant 0 : index
      %c256 = arith.constant 256 : index
      %c41 = arith.constant 41 : index
      %c32 = arith.constant 32 : index
      %c8 = arith.constant 8 : index
      %c1300 = arith.constant 1300 : index
      %c64 = arith.constant 64 : index
      %c110 = arith.constant 110 : index
      %c0_1 = arith.constant 0 : index
      %c1_2 = arith.constant 1 : index
      %cst = arith.constant 0xFF800000 : f32
      %c4 = arith.constant 4 : index
      %c2 = arith.constant 2 : index
      %12 = affine.apply #map1(%0)[%arg0, %c0]
      %13 = affine.apply #map1(%3)[%c1, %c0_0]
      %14 = arith.addi %13, %12 : index
      %true = arith.constant true
      %15 = arith.muli %13, %c1 : index
      %16 = arith.addi %15, %12 : index
      %17 = arith.cmpi ult, %16, %arg1 : index
      %18 = arith.andi %true, %17 : i1
      scf.if %18 {
        %19 = arith.remsi %14, %c256 : index
        %20 = arith.divsi %14, %c256 : index
        %21 = arith.divui %20, %c41 : index
        %22 = arith.remui %20, %c41 : index
        %23 = arith.divui %19, %c32 : index
        %24 = arith.remui %19, %c32 : index
        %25 = arith.muli %24, %c8 : index
        %26 = arith.addi %23, %25 : index
        %27 = arith.muli %22, %c32 : index
        %28 = arith.addi %24, %27 : index
        %alloc = memref.alloc() : memref<256xf32, #gpu.address_space<workgroup>>
        %29 = arith.cmpi ult, %28, %c1300 : index
        %30 = scf.if %29 -> (f32) {
          %35 = scf.for %arg4 = %c0_1 to %c64 step %c1_2 iter_args(%arg5 = %cst) -> (f32) {
            %36 = arith.muli %21, %c8 : index
            %37 = arith.addi %23, %36 : index
            %38 = arith.muli %37, %c64 : index
            %39 = arith.addi %arg4, %38 : index
            %40 = arith.cmpi slt, %39, %c110 : index
            %41 = scf.if %40 -> (f32) {
              %42 = arith.muli %39, %c1300 : index
              %43 = arith.addi %42, %28 : index
              %reinterpret_cast_3 = memref.reinterpret_cast %arg2 to offset: [0], sizes: [143000], strides: [1] : memref<110x100x13xf32, "gpu"> to memref<143000xf32, "gpu">
              %44 = memref.load %reinterpret_cast_3[%43] : memref<143000xf32, "gpu">
              %45 = arith.cmpf ugt, %arg5, %44 : f32
              %46 = arith.select %45, %arg5, %44 : f32
              %47 = arith.cmpf uno, %44, %44 : f32
              %48 = arith.select %47, %44, %46 : f32
              scf.yield %48 : f32
            } else {
              scf.yield %arg5 : f32
            }
            scf.yield %41 : f32
          }
          scf.yield %35 : f32
        } else {
          scf.yield %cst : f32
        }
        %reinterpret_cast = memref.reinterpret_cast %alloc to offset: [0], sizes: [256], strides: [1] : memref<256xf32, #gpu.address_space<workgroup>> to memref<256xf32, #gpu.address_space<workgroup>>
        memref.store %30, %reinterpret_cast[%26] : memref<256xf32, #gpu.address_space<workgroup>>
        gpu.barrier
        %31 = arith.cmpi slt, %23, %c4 : index
        scf.if %31 {
          %35 = arith.addi %26, %c4 : index
          %36 = memref.load %reinterpret_cast[%26] : memref<256xf32, #gpu.address_space<workgroup>>
          %37 = memref.load %reinterpret_cast[%35] : memref<256xf32, #gpu.address_space<workgroup>>
          %38 = arith.cmpf ugt, %36, %37 : f32
          %39 = arith.select %38, %36, %37 : f32
          %40 = arith.cmpf uno, %37, %37 : f32
          %41 = arith.select %40, %37, %39 : f32
          memref.store %41, %reinterpret_cast[%26] : memref<256xf32, #gpu.address_space<workgroup>>
        }
        gpu.barrier
        %32 = arith.cmpi slt, %23, %c2 : index
        scf.if %32 {
          %35 = arith.addi %26, %c2 : index
          %36 = memref.load %reinterpret_cast[%26] : memref<256xf32, #gpu.address_space<workgroup>>
          %37 = memref.load %reinterpret_cast[%35] : memref<256xf32, #gpu.address_space<workgroup>>
          %38 = arith.cmpf ugt, %36, %37 : f32
          %39 = arith.select %38, %36, %37 : f32
          %40 = arith.cmpf uno, %37, %37 : f32
          %41 = arith.select %40, %37, %39 : f32
          memref.store %41, %reinterpret_cast[%26] : memref<256xf32, #gpu.address_space<workgroup>>
        }
        gpu.barrier
        %33 = arith.cmpi eq, %23, %c0_1 : index
        %34 = arith.andi %33, %29 : i1
        scf.if %34 {
          %35 = arith.addi %26, %c1_2 : index
          %36 = memref.load %reinterpret_cast[%26] : memref<256xf32, #gpu.address_space<workgroup>>
          %37 = memref.load %reinterpret_cast[%35] : memref<256xf32, #gpu.address_space<workgroup>>
          %38 = arith.cmpf ugt, %36, %37 : f32
          %39 = arith.select %38, %36, %37 : f32
          %40 = arith.cmpf uno, %37, %37 : f32
          %41 = arith.select %40, %37, %39 : f32
          %42 = memref.generic_atomic_rmw %arg3[%28] : memref<1300xf32, "gpu"> {
          ^bb0(%arg4: f32):
            %43 = arith.cmpf ogt, %arg4, %41 : f32
            %44 = arith.select %43, %arg4, %41 : f32
            memref.atomic_yield %44 : f32
          }
        }
      }
      gpu.return
    }
  }
  func.func @shape_constraint_graph() {
    return
  }
}


// -----// IR Dump After LhloFusionInlinerPass (lhlo-fusion-inliner) //----- //
func.func @main(%arg0: !disc_ral.context) attributes {tf.entry_function = {input_placements = "gpu", inputs = "input0", output_placements = "gpu", outputs = "output0"}} {
  %cst = arith.constant 0xFF800000 : f32
  %c110 = arith.constant 110 : index
  %c41 = arith.constant 41 : index
  %c2 = arith.constant 2 : index
  %c4 = arith.constant 4 : index
  %c256 = arith.constant 256 : index
  %c64 = arith.constant 64 : index
  %c8 = arith.constant 8 : index
  %c32 = arith.constant 32 : index
  %c1300 = arith.constant 1300 : index
  %0 = llvm.mlir.constant(0 : i32) : i32
  %c0 = arith.constant 0 : index
  %c100 = arith.constant 100 : index
  %c1 = arith.constant 1 : index
  %c13 = arith.constant 13 : index
  %1 = "disc_ral.dispatch"(%arg0, %c0) {backend_config = "", call_target_name = "ral_recv_input", device = "cpu", has_side_effect = false} : (!disc_ral.context, index) -> memref<110x100x13xf32, "gpu">
  %alloc = memref.alloc() : memref<1300xf32, "gpu">
  %c0_0 = arith.constant 0 : index
  %c256_1 = arith.constant 256 : index
  %2 = arith.muli %c1, %c256_1 : index
  %c1_2 = arith.constant 1 : index
  %3 = affine.apply affine_map<(d0)[s0, s1] -> ((d0 - s0) ceildiv s1)>(%c1300)[%c0, %2]
  %4 = affine.apply affine_map<(d0)[s0, s1] -> ((d0 - s0) ceildiv s1)>(%2)[%c0_0, %c1]
  gpu.launch_func  @main_kernel::@main_kColReduction_reduce__3_1_0___8w32h blocks in (%3, %c1_2, %c1_2) threads in (%4, %c1_2, %c1_2) args(%2 : index, %alloc : memref<1300xf32, "gpu">)
  %c0_3 = arith.constant 0 : index
  %c1_4 = arith.constant 1 : index
  %c1_5 = arith.constant 1 : index
  %5 = arith.muli %c1_5, %c41 : index
  %6 = arith.muli %5, %c256 : index
  %c0_6 = arith.constant 0 : index
  %c256_7 = arith.constant 256 : index
  %7 = arith.muli %c1_4, %c256_7 : index
  %c1_8 = arith.constant 1 : index
  %8 = affine.apply affine_map<(d0)[s0, s1] -> ((d0 - s0) ceildiv s1)>(%6)[%c0_3, %7]
  %9 = affine.apply affine_map<(d0)[s0, s1] -> ((d0 - s0) ceildiv s1)>(%7)[%c0_6, %c1_4]
  gpu.launch_func  @main_kernel_0::@main_kColReduction_reduce__3_1_0___8w32h_1 blocks in (%8, %c1_8, %c1_8) threads in (%9, %c1_8, %c1_8) args(%7 : index, %6 : index, %1 : memref<110x100x13xf32, "gpu">, %alloc : memref<1300xf32, "gpu">)
  %10 = llvm.inttoptr %0 : i32 to !llvm.ptr<i8>
  %alloca = memref.alloca() : memref<2xindex, "cpu">
  memref.store %c100, %alloca[%c0] : memref<2xindex, "cpu">
  memref.store %c13, %alloca[%c1] : memref<2xindex, "cpu">
  %11 = "disc_ral.dispatch"(%arg0, %10, %alloc, %alloca) {backend_config = "", call_target_name = "inc_ref", device = "gpu", has_side_effect = false} : (!disc_ral.context, !llvm.ptr<i8>, memref<1300xf32, "gpu">, memref<2xindex, "cpu">) -> memref<100x13xf32, "gpu">
  %reinterpret_cast = memref.reinterpret_cast %11 to offset: [0], sizes: [100, 13], strides: [13, 1] : memref<100x13xf32, "gpu"> to memref<100x13xf32, "gpu">
  memref.dealloc %alloc : memref<1300xf32, "gpu">
  "disc_ral.dispatch"(%arg0, %c0, %reinterpret_cast) {backend_config = "", call_target_name = "ral_send_output", device = "cpu", has_side_effect = false} : (!disc_ral.context, index, memref<100x13xf32, "gpu">) -> ()
  return
}

// -----// IR Dump After ReviseGpuKernelOutliningPass (disc-revise-gpu-kernel-outlining) //----- //
#map = affine_map<(d0)[s0, s1] -> ((d0 - s0) ceildiv s1)>
#map1 = affine_map<(d0)[s0, s1] -> (d0 * s0 + s1)>
module attributes {gpu.container_module} {
  func.func @main(%arg0: !disc_ral.context) attributes {tf.entry_function = {input_placements = "gpu", inputs = "input0", output_placements = "gpu", outputs = "output0"}} {
    %cst = arith.constant 0xFF800000 : f32
    %c110 = arith.constant 110 : index
    %c41 = arith.constant 41 : index
    %c2 = arith.constant 2 : index
    %c4 = arith.constant 4 : index
    %c256 = arith.constant 256 : index
    %c64 = arith.constant 64 : index
    %c8 = arith.constant 8 : index
    %c32 = arith.constant 32 : index
    %c1300 = arith.constant 1300 : index
    %0 = llvm.mlir.constant(0 : i32) : i32
    %c0 = arith.constant 0 : index
    %c100 = arith.constant 100 : index
    %c1 = arith.constant 1 : index
    %c13 = arith.constant 13 : index
    %1 = "disc_ral.dispatch"(%arg0, %c0) {backend_config = "", call_target_name = "ral_recv_input", device = "cpu", has_side_effect = false} : (!disc_ral.context, index) -> memref<110x100x13xf32, "gpu">
    %alloc = memref.alloc() : memref<1300xf32, "gpu">
    %c0_0 = arith.constant 0 : index
    %c256_1 = arith.constant 256 : index
    %2 = arith.muli %c1, %c256_1 : index
    %c1_2 = arith.constant 1 : index
    %3 = affine.apply #map(%c1300)[%c0, %2]
    %4 = affine.apply #map(%2)[%c0_0, %c1]
    gpu.launch_func  @main_kernel::@main_kColReduction_reduce__3_1_0___8w32h blocks in (%3, %c1_2, %c1_2) threads in (%4, %c1_2, %c1_2) args(%2 : index, %alloc : memref<1300xf32, "gpu">)
    %c0_3 = arith.constant 0 : index
    %c1_4 = arith.constant 1 : index
    %c1_5 = arith.constant 1 : index
    %5 = arith.muli %c1_5, %c41 : index
    %6 = arith.muli %5, %c256 : index
    %c0_6 = arith.constant 0 : index
    %c256_7 = arith.constant 256 : index
    %7 = arith.muli %c1_4, %c256_7 : index
    %c1_8 = arith.constant 1 : index
    %8 = affine.apply #map(%6)[%c0_3, %7]
    %9 = affine.apply #map(%7)[%c0_6, %c1_4]
    gpu.launch_func  @main_kernel_0::@main_kColReduction_reduce__3_1_0___8w32h_1 blocks in (%8, %c1_8, %c1_8) threads in (%9, %c1_8, %c1_8) args(%7 : index, %6 : index, %1 : memref<110x100x13xf32, "gpu">, %alloc : memref<1300xf32, "gpu">)
    %10 = llvm.inttoptr %0 : i32 to !llvm.ptr<i8>
    %alloca = memref.alloca() : memref<2xindex, "cpu">
    memref.store %c100, %alloca[%c0] : memref<2xindex, "cpu">
    memref.store %c13, %alloca[%c1] : memref<2xindex, "cpu">
    %11 = "disc_ral.dispatch"(%arg0, %10, %alloc, %alloca) {backend_config = "", call_target_name = "inc_ref", device = "gpu", has_side_effect = false} : (!disc_ral.context, !llvm.ptr<i8>, memref<1300xf32, "gpu">, memref<2xindex, "cpu">) -> memref<100x13xf32, "gpu">
    %reinterpret_cast = memref.reinterpret_cast %11 to offset: [0], sizes: [100, 13], strides: [13, 1] : memref<100x13xf32, "gpu"> to memref<100x13xf32, "gpu">
    memref.dealloc %alloc : memref<1300xf32, "gpu">
    "disc_ral.dispatch"(%arg0, %c0, %reinterpret_cast) {backend_config = "", call_target_name = "ral_send_output", device = "cpu", has_side_effect = false} : (!disc_ral.context, index, memref<100x13xf32, "gpu">) -> ()
    return
  }
  gpu.module @main_kernel {
    gpu.func @main_kColReduction_reduce__3_1_0___8w32h(%arg0: index, %arg1: memref<1300xf32, "gpu">) kernel {
      %0 = gpu.block_id  x
      %1 = gpu.block_id  y
      %2 = gpu.block_id  z
      %3 = gpu.thread_id  x
      %4 = gpu.thread_id  y
      %5 = gpu.thread_id  z
      %6 = gpu.grid_dim  x
      %7 = gpu.grid_dim  y
      %8 = gpu.grid_dim  z
      %9 = gpu.block_dim  x
      %10 = gpu.block_dim  y
      %11 = gpu.block_dim  z
      cf.br ^bb1
    ^bb1:  // pred: ^bb0
      %c0 = arith.constant 0 : index
      %c1 = arith.constant 1 : index
      %c0_0 = arith.constant 0 : index
      %c1300 = arith.constant 1300 : index
      %cst = arith.constant 0xFF800000 : f32
      %12 = affine.apply #map1(%0)[%arg0, %c0]
      %13 = affine.apply #map1(%3)[%c1, %c0_0]
      %14 = arith.addi %13, %12 : index
      %true = arith.constant true
      %15 = arith.muli %13, %c1 : index
      %16 = arith.addi %15, %12 : index
      %17 = arith.cmpi ult, %16, %c1300 : index
      %18 = arith.andi %true, %17 : i1
      scf.if %18 {
        %reinterpret_cast = memref.reinterpret_cast %arg1 to offset: [0], sizes: [1300], strides: [1] : memref<1300xf32, "gpu"> to memref<1300xf32, "gpu">
        memref.store %cst, %reinterpret_cast[%14] : memref<1300xf32, "gpu">
      }
      gpu.return
    }
  }
  gpu.module @main_kernel_0 {
    gpu.func @main_kColReduction_reduce__3_1_0___8w32h_1(%arg0: index, %arg1: index, %arg2: memref<110x100x13xf32, "gpu">, %arg3: memref<1300xf32, "gpu">) workgroup(%arg4 : memref<256xf32, #gpu.address_space<workgroup>>) kernel {
      %0 = gpu.block_id  x
      %1 = gpu.block_id  y
      %2 = gpu.block_id  z
      %3 = gpu.thread_id  x
      %4 = gpu.thread_id  y
      %5 = gpu.thread_id  z
      %6 = gpu.grid_dim  x
      %7 = gpu.grid_dim  y
      %8 = gpu.grid_dim  z
      %9 = gpu.block_dim  x
      %10 = gpu.block_dim  y
      %11 = gpu.block_dim  z
      cf.br ^bb1
    ^bb1:  // pred: ^bb0
      %c0 = arith.constant 0 : index
      %c1 = arith.constant 1 : index
      %c0_0 = arith.constant 0 : index
      %c256 = arith.constant 256 : index
      %c41 = arith.constant 41 : index
      %c32 = arith.constant 32 : index
      %c8 = arith.constant 8 : index
      %c1300 = arith.constant 1300 : index
      %c64 = arith.constant 64 : index
      %c110 = arith.constant 110 : index
      %c0_1 = arith.constant 0 : index
      %c1_2 = arith.constant 1 : index
      %cst = arith.constant 0xFF800000 : f32
      %c4 = arith.constant 4 : index
      %c2 = arith.constant 2 : index
      %12 = affine.apply #map1(%0)[%arg0, %c0]
      %13 = affine.apply #map1(%3)[%c1, %c0_0]
      %14 = arith.addi %13, %12 : index
      %true = arith.constant true
      %15 = arith.muli %13, %c1 : index
      %16 = arith.addi %15, %12 : index
      %17 = arith.cmpi ult, %16, %arg1 : index
      %18 = arith.andi %true, %17 : i1
      scf.if %18 {
        %19 = arith.remsi %14, %c256 : index
        %20 = arith.divsi %14, %c256 : index
        %21 = arith.divui %20, %c41 : index
        %22 = arith.remui %20, %c41 : index
        %23 = arith.divui %19, %c32 : index
        %24 = arith.remui %19, %c32 : index
        %25 = arith.muli %24, %c8 : index
        %26 = arith.addi %23, %25 : index
        %27 = arith.muli %22, %c32 : index
        %28 = arith.addi %24, %27 : index
        %29 = arith.cmpi ult, %28, %c1300 : index
        %30 = scf.if %29 -> (f32) {
          %35 = scf.for %arg5 = %c0_1 to %c64 step %c1_2 iter_args(%arg6 = %cst) -> (f32) {
            %36 = arith.muli %21, %c8 : index
            %37 = arith.addi %23, %36 : index
            %38 = arith.muli %37, %c64 : index
            %39 = arith.addi %arg5, %38 : index
            %40 = arith.cmpi slt, %39, %c110 : index
            %41 = scf.if %40 -> (f32) {
              %42 = arith.muli %39, %c1300 : index
              %43 = arith.addi %42, %28 : index
              %reinterpret_cast_3 = memref.reinterpret_cast %arg2 to offset: [0], sizes: [143000], strides: [1] : memref<110x100x13xf32, "gpu"> to memref<143000xf32, "gpu">
              %44 = memref.load %reinterpret_cast_3[%43] : memref<143000xf32, "gpu">
              %45 = arith.cmpf ugt, %arg6, %44 : f32
              %46 = arith.select %45, %arg6, %44 : f32
              %47 = arith.cmpf uno, %44, %44 : f32
              %48 = arith.select %47, %44, %46 : f32
              scf.yield %48 : f32
            } else {
              scf.yield %arg6 : f32
            }
            scf.yield %41 : f32
          }
          scf.yield %35 : f32
        } else {
          scf.yield %cst : f32
        }
        %reinterpret_cast = memref.reinterpret_cast %arg4 to offset: [0], sizes: [256], strides: [1] : memref<256xf32, #gpu.address_space<workgroup>> to memref<256xf32, #gpu.address_space<workgroup>>
        memref.store %30, %reinterpret_cast[%26] : memref<256xf32, #gpu.address_space<workgroup>>
        gpu.barrier
        %31 = arith.cmpi slt, %23, %c4 : index
        scf.if %31 {
          %35 = arith.addi %26, %c4 : index
          %36 = memref.load %reinterpret_cast[%26] : memref<256xf32, #gpu.address_space<workgroup>>
          %37 = memref.load %reinterpret_cast[%35] : memref<256xf32, #gpu.address_space<workgroup>>
          %38 = arith.cmpf ugt, %36, %37 : f32
          %39 = arith.select %38, %36, %37 : f32
          %40 = arith.cmpf uno, %37, %37 : f32
          %41 = arith.select %40, %37, %39 : f32
          memref.store %41, %reinterpret_cast[%26] : memref<256xf32, #gpu.address_space<workgroup>>
        }
        gpu.barrier
        %32 = arith.cmpi slt, %23, %c2 : index
        scf.if %32 {
          %35 = arith.addi %26, %c2 : index
          %36 = memref.load %reinterpret_cast[%26] : memref<256xf32, #gpu.address_space<workgroup>>
          %37 = memref.load %reinterpret_cast[%35] : memref<256xf32, #gpu.address_space<workgroup>>
          %38 = arith.cmpf ugt, %36, %37 : f32
          %39 = arith.select %38, %36, %37 : f32
          %40 = arith.cmpf uno, %37, %37 : f32
          %41 = arith.select %40, %37, %39 : f32
          memref.store %41, %reinterpret_cast[%26] : memref<256xf32, #gpu.address_space<workgroup>>
        }
        gpu.barrier
        %33 = arith.cmpi eq, %23, %c0_1 : index
        %34 = arith.andi %33, %29 : i1
        scf.if %34 {
          %35 = arith.addi %26, %c1_2 : index
          %36 = memref.load %reinterpret_cast[%26] : memref<256xf32, #gpu.address_space<workgroup>>
          %37 = memref.load %reinterpret_cast[%35] : memref<256xf32, #gpu.address_space<workgroup>>
          %38 = arith.cmpf ugt, %36, %37 : f32
          %39 = arith.select %38, %36, %37 : f32
          %40 = arith.cmpf uno, %37, %37 : f32
          %41 = arith.select %40, %37, %39 : f32
          %42 = memref.generic_atomic_rmw %arg3[%28] : memref<1300xf32, "gpu"> {
          ^bb0(%arg5: f32):
            %43 = arith.cmpf ogt, %arg5, %41 : f32
            %44 = arith.select %43, %arg5, %41 : f32
            memref.atomic_yield %44 : f32
          }
        }
      }
      gpu.return
    }
  }
  func.func @shape_constraint_graph() {
    return
  }
}


// -----// IR Dump After SideEffectLoopInvariantCodeMotionPass (disc-side-effect-loop-invariant-code-motion) //----- //
gpu.func @main_kColReduction_reduce__3_1_0___8w32h(%arg0: index, %arg1: memref<1300xf32, "gpu">) kernel {
  %cst = arith.constant 0xFF800000 : f32
  %c1300 = arith.constant 1300 : index
  %c1 = arith.constant 1 : index
  %c0 = arith.constant 0 : index
  %0 = gpu.block_id  x
  %1 = gpu.thread_id  x
  cf.br ^bb1
^bb1:  // pred: ^bb0
  %2 = affine.apply affine_map<(d0)[s0, s1] -> (d0 * s0 + s1)>(%0)[%arg0, %c0]
  %3 = affine.apply affine_map<(d0)[s0, s1] -> (d0 * s0 + s1)>(%1)[%c1, %c0]
  %4 = arith.addi %3, %2 : index
  %5 = arith.addi %3, %2 : index
  %6 = arith.cmpi ult, %5, %c1300 : index
  scf.if %6 {
    %reinterpret_cast = memref.reinterpret_cast %arg1 to offset: [0], sizes: [1300], strides: [1] : memref<1300xf32, "gpu"> to memref<1300xf32, "gpu">
    memref.store %cst, %reinterpret_cast[%4] : memref<1300xf32, "gpu">
  }
  gpu.return
}

// -----// IR Dump After CSE (cse) //----- //
gpu.module @main_kernel {
  gpu.func @main_kColReduction_reduce__3_1_0___8w32h(%arg0: index, %arg1: memref<1300xf32, "gpu">) kernel {
    %cst = arith.constant 0xFF800000 : f32
    %c1300 = arith.constant 1300 : index
    %c1 = arith.constant 1 : index
    %c0 = arith.constant 0 : index
    %0 = gpu.block_id  x
    %1 = gpu.thread_id  x
    cf.br ^bb1
  ^bb1:  // pred: ^bb0
    %2 = affine.apply affine_map<(d0)[s0, s1] -> (d0 * s0 + s1)>(%0)[%arg0, %c0]
    %3 = affine.apply affine_map<(d0)[s0, s1] -> (d0 * s0 + s1)>(%1)[%c1, %c0]
    %4 = arith.addi %3, %2 : index
    %5 = arith.cmpi ult, %4, %c1300 : index
    scf.if %5 {
      %reinterpret_cast = memref.reinterpret_cast %arg1 to offset: [0], sizes: [1300], strides: [1] : memref<1300xf32, "gpu"> to memref<1300xf32, "gpu">
      memref.store %cst, %reinterpret_cast[%4] : memref<1300xf32, "gpu">
    }
    gpu.return
  }
}

// -----// IR Dump After SCFToControlFlow (convert-scf-to-cf) //----- //
gpu.module @main_kernel {
  gpu.func @main_kColReduction_reduce__3_1_0___8w32h(%arg0: index, %arg1: memref<1300xf32, "gpu">) kernel {
    %cst = arith.constant 0xFF800000 : f32
    %c1300 = arith.constant 1300 : index
    %c1 = arith.constant 1 : index
    %c0 = arith.constant 0 : index
    %0 = gpu.block_id  x
    %1 = gpu.thread_id  x
    cf.br ^bb1
  ^bb1:  // pred: ^bb0
    %2 = affine.apply affine_map<(d0)[s0, s1] -> (d0 * s0 + s1)>(%0)[%arg0, %c0]
    %3 = affine.apply affine_map<(d0)[s0, s1] -> (d0 * s0 + s1)>(%1)[%c1, %c0]
    %4 = arith.addi %3, %2 : index
    %5 = arith.cmpi ult, %4, %c1300 : index
    cf.cond_br %5, ^bb2, ^bb3
  ^bb2:  // pred: ^bb1
    %reinterpret_cast = memref.reinterpret_cast %arg1 to offset: [0], sizes: [1300], strides: [1] : memref<1300xf32, "gpu"> to memref<1300xf32, "gpu">
    memref.store %cst, %reinterpret_cast[%4] : memref<1300xf32, "gpu">
    cf.br ^bb3
  ^bb3:  // 2 preds: ^bb1, ^bb2
    gpu.return
  }
}

// -----// IR Dump After ConvertAffineToStandard (lower-affine) //----- //
gpu.module @main_kernel {
  gpu.func @main_kColReduction_reduce__3_1_0___8w32h(%arg0: index, %arg1: memref<1300xf32, "gpu">) kernel {
    %cst = arith.constant 0xFF800000 : f32
    %c1300 = arith.constant 1300 : index
    %c1 = arith.constant 1 : index
    %c0 = arith.constant 0 : index
    %0 = gpu.block_id  x
    %1 = gpu.thread_id  x
    cf.br ^bb1
  ^bb1:  // pred: ^bb0
    %2 = arith.muli %0, %arg0 : index
    %3 = arith.addi %2, %c0 : index
    %4 = arith.muli %1, %c1 : index
    %5 = arith.addi %4, %c0 : index
    %6 = arith.addi %5, %3 : index
    %7 = arith.cmpi ult, %6, %c1300 : index
    cf.cond_br %7, ^bb2, ^bb3
  ^bb2:  // pred: ^bb1
    %reinterpret_cast = memref.reinterpret_cast %arg1 to offset: [0], sizes: [1300], strides: [1] : memref<1300xf32, "gpu"> to memref<1300xf32, "gpu">
    memref.store %cst, %reinterpret_cast[%6] : memref<1300xf32, "gpu">
    cf.br ^bb3
  ^bb3:  // 2 preds: ^bb1, ^bb2
    gpu.return
  }
}

// -----// IR Dump After StripDebugInfo (strip-debuginfo) //----- //
gpu.module @main_kernel {
  gpu.func @main_kColReduction_reduce__3_1_0___8w32h(%arg0: index, %arg1: memref<1300xf32, "gpu">) kernel {
    %cst = arith.constant 0xFF800000 : f32
    %c1300 = arith.constant 1300 : index
    %c1 = arith.constant 1 : index
    %c0 = arith.constant 0 : index
    %0 = gpu.block_id  x
    %1 = gpu.thread_id  x
    cf.br ^bb1
  ^bb1:  // pred: ^bb0
    %2 = arith.muli %0, %arg0 : index
    %3 = arith.addi %2, %c0 : index
    %4 = arith.muli %1, %c1 : index
    %5 = arith.addi %4, %c0 : index
    %6 = arith.addi %5, %3 : index
    %7 = arith.cmpi ult, %6, %c1300 : index
    cf.cond_br %7, ^bb2, ^bb3
  ^bb2:  // pred: ^bb1
    %reinterpret_cast = memref.reinterpret_cast %arg1 to offset: [0], sizes: [1300], strides: [1] : memref<1300xf32, "gpu"> to memref<1300xf32, "gpu">
    memref.store %cst, %reinterpret_cast[%6] : memref<1300xf32, "gpu">
    cf.br ^bb3
  ^bb3:  // 2 preds: ^bb1, ^bb2
    gpu.return
  }
}

// -----// IR Dump After DiscLowerGpuOpsToNVVMOpsPass (disc-convert-gpu-to-nvvm) //----- //
gpu.module @main_kernel {
  llvm.func @main_kColReduction_reduce__3_1_0___8w32h(%arg0: i32, %arg1: !llvm.ptr<f32>, %arg2: !llvm.ptr<f32>, %arg3: i32, %arg4: i32, %arg5: i32) attributes {gpu.kernel, nvvm.kernel} {
    %0 = llvm.mlir.undef : !llvm.struct<(ptr<f32>, ptr<f32>, i32, array<1 x i32>, array<1 x i32>)>
    %1 = llvm.insertvalue %arg1, %0[0] : !llvm.struct<(ptr<f32>, ptr<f32>, i32, array<1 x i32>, array<1 x i32>)> 
    %2 = llvm.insertvalue %arg2, %1[1] : !llvm.struct<(ptr<f32>, ptr<f32>, i32, array<1 x i32>, array<1 x i32>)> 
    %3 = llvm.insertvalue %arg3, %2[2] : !llvm.struct<(ptr<f32>, ptr<f32>, i32, array<1 x i32>, array<1 x i32>)> 
    %4 = llvm.insertvalue %arg4, %3[3, 0] : !llvm.struct<(ptr<f32>, ptr<f32>, i32, array<1 x i32>, array<1 x i32>)> 
    %5 = llvm.insertvalue %arg5, %4[4, 0] : !llvm.struct<(ptr<f32>, ptr<f32>, i32, array<1 x i32>, array<1 x i32>)> 
    %6 = llvm.mlir.constant(0xFF800000 : f32) : f32
    %7 = llvm.mlir.constant(1300 : index) : i32
    %8 = nvvm.read.ptx.sreg.ctaid.x : i32
    %9 = nvvm.read.ptx.sreg.tid.x : i32
    llvm.br ^bb1
  ^bb1:  // pred: ^bb0
    %10 = llvm.mul %8, %arg0  : i32
    %11 = llvm.add %9, %10  : i32
    %12 = llvm.icmp "ult" %11, %7 : i32
    llvm.cond_br %12, ^bb2, ^bb3
  ^bb2:  // pred: ^bb1
    %13 = llvm.mlir.undef : !llvm.struct<(ptr<f32>, ptr<f32>, i32, array<1 x i32>, array<1 x i32>)>
    %14 = llvm.extractvalue %5[0] : !llvm.struct<(ptr<f32>, ptr<f32>, i32, array<1 x i32>, array<1 x i32>)> 
    %15 = llvm.extractvalue %5[1] : !llvm.struct<(ptr<f32>, ptr<f32>, i32, array<1 x i32>, array<1 x i32>)> 
    %16 = llvm.insertvalue %14, %13[0] : !llvm.struct<(ptr<f32>, ptr<f32>, i32, array<1 x i32>, array<1 x i32>)> 
    %17 = llvm.insertvalue %15, %16[1] : !llvm.struct<(ptr<f32>, ptr<f32>, i32, array<1 x i32>, array<1 x i32>)> 
    %18 = llvm.mlir.constant(0 : index) : i32
    %19 = llvm.insertvalue %18, %17[2] : !llvm.struct<(ptr<f32>, ptr<f32>, i32, array<1 x i32>, array<1 x i32>)> 
    %20 = llvm.mlir.constant(1300 : index) : i32
    %21 = llvm.insertvalue %20, %19[3, 0] : !llvm.struct<(ptr<f32>, ptr<f32>, i32, array<1 x i32>, array<1 x i32>)> 
    %22 = llvm.mlir.constant(1 : index) : i32
    %23 = llvm.insertvalue %22, %21[4, 0] : !llvm.struct<(ptr<f32>, ptr<f32>, i32, array<1 x i32>, array<1 x i32>)> 
    %24 = llvm.extractvalue %23[1] : !llvm.struct<(ptr<f32>, ptr<f32>, i32, array<1 x i32>, array<1 x i32>)> 
    %25 = llvm.getelementptr %24[%11] : (!llvm.ptr<f32>, i32) -> !llvm.ptr<f32>
    llvm.store %6, %25 : !llvm.ptr<f32>
    llvm.br ^bb3
  ^bb3:  // 2 preds: ^bb1, ^bb2
    llvm.return
  }
}

// -----// IR Dump After LLVMInsertValueSimplifierPass (disc-llvm-insert-value-simplifier) //----- //
llvm.func @main_kColReduction_reduce__3_1_0___8w32h(%arg0: i32, %arg1: !llvm.ptr<f32>, %arg2: !llvm.ptr<f32>, %arg3: i32, %arg4: i32, %arg5: i32) attributes {gpu.kernel, nvvm.kernel} {
  %0 = llvm.mlir.constant(1300 : index) : i32
  %1 = llvm.mlir.constant(0xFF800000 : f32) : f32
  %2 = nvvm.read.ptx.sreg.ctaid.x : i32
  %3 = nvvm.read.ptx.sreg.tid.x : i32
  llvm.br ^bb1
^bb1:  // pred: ^bb0
  %4 = llvm.mul %2, %arg0  : i32
  %5 = llvm.add %3, %4  : i32
  %6 = llvm.icmp "ult" %5, %0 : i32
  llvm.cond_br %6, ^bb2, ^bb3
^bb2:  // pred: ^bb1
  %7 = llvm.getelementptr %arg2[%5] : (!llvm.ptr<f32>, i32) -> !llvm.ptr<f32>
  llvm.store %1, %7 : !llvm.ptr<f32>
  llvm.br ^bb3
^bb3:  // 2 preds: ^bb1, ^bb2
  llvm.return
}

// -----// IR Dump After FunctionDeadArgumentEliminationPass (disc-function-dead-argument-elimination) //----- //
gpu.module @main_kernel {
  llvm.func @main_kColReduction_reduce__3_1_0___8w32h(%arg0: i32, %arg1: !llvm.ptr<f32>) attributes {disc.elimargs = [1 : index, 3 : index, 4 : index, 5 : index], gpu.kernel, nvvm.kernel} {
    %0 = llvm.mlir.constant(1300 : index) : i32
    %1 = llvm.mlir.constant(0xFF800000 : f32) : f32
    %2 = nvvm.read.ptx.sreg.ctaid.x : i32
    %3 = nvvm.read.ptx.sreg.tid.x : i32
    llvm.br ^bb1
  ^bb1:  // pred: ^bb0
    %4 = llvm.mul %2, %arg0  : i32
    %5 = llvm.add %3, %4  : i32
    %6 = llvm.icmp "ult" %5, %0 : i32
    llvm.cond_br %6, ^bb2, ^bb3
  ^bb2:  // pred: ^bb1
    %7 = llvm.getelementptr %arg1[%5] : (!llvm.ptr<f32>, i32) -> !llvm.ptr<f32>
    llvm.store %1, %7 : !llvm.ptr<f32>
    llvm.br ^bb3
  ^bb3:  // 2 preds: ^bb1, ^bb2
    llvm.return
  }
}

// -----// IR Dump After GpuKernelToBlobPass (disc-gpu-kernel-to-blob) //----- //
gpu.module @main_kernel attributes {gpu.binary = "P\EDU\BA\01\00\10\008\04\00\00\00\00\00\00\02\00\01\01@\00\00\00\F8\03\00\00\00\00\00\00\F3\03\00\00\00\00\00\00\07\00\01\00P\00\00\00\00\00\00\00\00\00\00\00\11 \00\00\00\00\00\00\00\00\00\00\00\00\00\00\E8\0B\00\00\00\00\00\00\A2\7FELF\02\01\013\07\00\01\00f\02\00\BE\00q\00\01\00!@\0B\07\001\00\80\08\07\00\F5\0E\00P\05P\00@\008\00\03\00@\00\0B\00\01\00\00.shstrtab\00.\08\00'ym\08\00\F0\1B_shndx\00.nv.info\00.text.main_kColReduction_r\0A\00\F5\01e__3_1_0___8w32h8\00\0F2\00\1Boshared4\00\1B\9Fconstant07\00\18\FA\01debug_frame\00.rel\11\00!nv\14\00\11aC\00\0F+\01 \0F\88\00\15\0FT\01\BAo_param[\01\1C\0F\01\00\06\8C[\00\00\00\03\00\0A\00\01\00\11\F0\18\00,\09\00\01\00 .\01\18\00,\04\00\01\00\11L\18\00,\07\00\01\00f2\00\00\00\12\10`\00!\80\01\07\00v\00\FF\FF\FF\FF(\00\0C\00\00\01\00@\03\00\04|\08\00\D1\0F\0C\81\80\80(\00\08\FF\81\80(\08\0B\00\06(\00/0\00\01\00\03\13pX\00\10\04\9B\00R\04\14\00\00\00E\00\01\0B\00\00\13\00p/\08\00\05\00\00\00\A7\03\22\04#\0C\00\00\01\00'\04\12\0C\00\17\11\0C\00!7\04\E8\03\F1\08\015\00\00\04\0A\08\00\02\00\00\00`\01\10\00\03\19\10\00\04\17\0C$\00\10\01N\00%\F0!\10\00\01\01\00\F2\02\F0\11\00\03\1B\FF\00\04\1C\08\00P\00\00\00\B0\00\01\00#K\00\01\00s\02\02\08\10\0A/\22\8B\00\00\07\00\03\08\00\13\10\08\00\13\18\08\00\13 \08\00\13(\08\00\130\08\00\228\08|\00\13\00\08\00\13\08\08\00\13\10\08\00\13\18\08\00\13 \08\00\13(\08\00\130\08\00\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\A2\00\00\00\14,\00\00\00H\00\01\00\00 \01/\05\00\01\00\FF\C0A\02z\01\00\1F\04\B1\0F\00\00\00\C4\0F\00\19y\02\00\01\00\10%\9B\02Q\0E\00\19y\03\0F\00\F5\1A\00!\00\00\00$\0E\00$z\02\02\00X\00\00\03\02\8E\07\00\CA\1F\00\0Cx\00\02\13\05\00\00p@\F0\03\00\DA\0F\00MS\04\A0\80\03\00\EA\0F\005t\03\FF\B3\03\10\FF\C0\03P\E2\0F\00\02xF\02B\80\FF\00\0F\10\00r\B9z\04\00\00F\00\84\00\90\D0\0F\00%v\02\02\00Zl\04\00`\00`\0F\00\86y\00\022\00@\04\19\10\0C0\009My\00`\00PGy\00\00\F09\04\A6\FF\83\03\00\C0\0F\00\18y\00\01\00\0F\10\00\90\0F\01\00-\00W\01.\03\00\01\00\22@\00\01\00=+\01\000\00\08\01\00\1F\0B@\00\04\13k)\00\1F[@\00\0C\13\13\1C\04\0C\01\00\13\C8\15\00&\90\00@\04#\04\00\95\04\00\06\05\12\00\01\00\1F\FET\00\00\00\01\00\13X\95\00/p\00\80\00\0B\1F)'\00\03#\00\C8@\00\04P\06\04\E4\00*\04\00\01\00\1Fa@\00\04\13\F81\00&L\00@\00\1F\0A@\00\00!\1C\01D\01\0D@\00\13H)\00*\D8\00\01\00\1B\08\08\00?\0B\01\00\86\07\00Q\00\00 \05\00\01\00&\10\00\80\00\17\048\00\04\18\00\13\C7\14\01\0C\84\01*0\058\07\1F\00\C0\00\04\132@\00+\06\00\01\00\1A\07\D0\07\12\03\00\06:\08\80\00\01\00\13\06\18\06\04(\0B\0C\01\00*\A8\00\08\00\04\F8\00\13\018\00\04\A8\00\0C\01\009P\03\00\08\00\088\00/\06\00\01\00\17\80\08\00\00\00\00\00\00\00\00\00\00\00\00"} {
  llvm.func @main_kColReduction_reduce__3_1_0___8w32h(%arg0: i32, %arg1: !llvm.ptr<f32>) attributes {disc.elimargs = [1 : index, 3 : index, 4 : index, 5 : index], gpu.kernel, nvvm.kernel} {
    %0 = llvm.mlir.constant(1300 : index) : i32
    %1 = llvm.mlir.constant(0xFF800000 : f32) : f32
    %2 = nvvm.read.ptx.sreg.ctaid.x : i32
    %3 = nvvm.read.ptx.sreg.tid.x : i32
    llvm.br ^bb1
  ^bb1:  // pred: ^bb0
    %4 = llvm.mul %2, %arg0  : i32
    %5 = llvm.add %3, %4  : i32
    %6 = llvm.icmp "ult" %5, %0 : i32
    llvm.cond_br %6, ^bb2, ^bb3
  ^bb2:  // pred: ^bb1
    %7 = llvm.getelementptr %arg1[%5] : (!llvm.ptr<f32>, i32) -> !llvm.ptr<f32>
    llvm.store %1, %7 : !llvm.ptr<f32>
    llvm.br ^bb3
  ^bb3:  // 2 preds: ^bb1, ^bb2
    llvm.return
  }
}

// -----// IR Dump After SideEffectLoopInvariantCodeMotionPass (disc-side-effect-loop-invariant-code-motion) //----- //
gpu.func @main_kColReduction_reduce__3_1_0___8w32h_1(%arg0: index, %arg1: index, %arg2: memref<110x100x13xf32, "gpu">, %arg3: memref<1300xf32, "gpu">) workgroup(%arg4 : memref<256xf32, #gpu.address_space<workgroup>>) kernel {
  %c2 = arith.constant 2 : index
  %c4 = arith.constant 4 : index
  %cst = arith.constant 0xFF800000 : f32
  %c110 = arith.constant 110 : index
  %c64 = arith.constant 64 : index
  %c1300 = arith.constant 1300 : index
  %c8 = arith.constant 8 : index
  %c32 = arith.constant 32 : index
  %c41 = arith.constant 41 : index
  %c256 = arith.constant 256 : index
  %c1 = arith.constant 1 : index
  %c0 = arith.constant 0 : index
  %0 = gpu.block_id  x
  %1 = gpu.thread_id  x
  cf.br ^bb1
^bb1:  // pred: ^bb0
  %2 = affine.apply affine_map<(d0)[s0, s1] -> (d0 * s0 + s1)>(%0)[%arg0, %c0]
  %3 = affine.apply affine_map<(d0)[s0, s1] -> (d0 * s0 + s1)>(%1)[%c1, %c0]
  %4 = arith.addi %3, %2 : index
  %5 = arith.addi %3, %2 : index
  %6 = arith.cmpi ult, %5, %arg1 : index
  scf.if %6 {
    %7 = arith.remsi %4, %c256 : index
    %8 = arith.divsi %4, %c256 : index
    %9 = arith.divui %8, %c41 : index
    %10 = arith.remui %8, %c41 : index
    %11 = arith.divui %7, %c32 : index
    %12 = arith.remui %7, %c32 : index
    %13 = arith.muli %12, %c8 : index
    %14 = arith.addi %11, %13 : index
    %15 = arith.muli %10, %c32 : index
    %16 = arith.addi %12, %15 : index
    %17 = arith.cmpi ult, %16, %c1300 : index
    %18 = scf.if %17 -> (f32) {
      %23 = arith.muli %9, %c8 : index
      %24 = arith.addi %11, %23 : index
      %25 = arith.muli %24, %c64 : index
      %26 = scf.for %arg5 = %c0 to %c64 step %c1 iter_args(%arg6 = %cst) -> (f32) {
        %27 = arith.addi %arg5, %25 : index
        %28 = arith.cmpi slt, %27, %c110 : index
        %29 = scf.if %28 -> (f32) {
          %30 = arith.muli %27, %c1300 : index
          %31 = arith.addi %30, %16 : index
          %reinterpret_cast_0 = memref.reinterpret_cast %arg2 to offset: [0], sizes: [143000], strides: [1] : memref<110x100x13xf32, "gpu"> to memref<143000xf32, "gpu">
          %32 = memref.load %reinterpret_cast_0[%31] : memref<143000xf32, "gpu">
          %33 = arith.cmpf ugt, %arg6, %32 : f32
          %34 = arith.select %33, %arg6, %32 : f32
          %35 = arith.cmpf uno, %32, %32 : f32
          %36 = arith.select %35, %32, %34 : f32
          scf.yield %36 : f32
        } else {
          scf.yield %arg6 : f32
        }
        scf.yield %29 : f32
      }
      scf.yield %26 : f32
    } else {
      scf.yield %cst : f32
    }
    %reinterpret_cast = memref.reinterpret_cast %arg4 to offset: [0], sizes: [256], strides: [1] : memref<256xf32, #gpu.address_space<workgroup>> to memref<256xf32, #gpu.address_space<workgroup>>
    memref.store %18, %reinterpret_cast[%14] : memref<256xf32, #gpu.address_space<workgroup>>
    gpu.barrier
    %19 = arith.cmpi slt, %11, %c4 : index
    scf.if %19 {
      %23 = arith.addi %14, %c4 : index
      %24 = memref.load %reinterpret_cast[%14] : memref<256xf32, #gpu.address_space<workgroup>>
      %25 = memref.load %reinterpret_cast[%23] : memref<256xf32, #gpu.address_space<workgroup>>
      %26 = arith.cmpf ugt, %24, %25 : f32
      %27 = arith.select %26, %24, %25 : f32
      %28 = arith.cmpf uno, %25, %25 : f32
      %29 = arith.select %28, %25, %27 : f32
      memref.store %29, %reinterpret_cast[%14] : memref<256xf32, #gpu.address_space<workgroup>>
    }
    gpu.barrier
    %20 = arith.cmpi slt, %11, %c2 : index
    scf.if %20 {
      %23 = arith.addi %14, %c2 : index
      %24 = memref.load %reinterpret_cast[%14] : memref<256xf32, #gpu.address_space<workgroup>>
      %25 = memref.load %reinterpret_cast[%23] : memref<256xf32, #gpu.address_space<workgroup>>
      %26 = arith.cmpf ugt, %24, %25 : f32
      %27 = arith.select %26, %24, %25 : f32
      %28 = arith.cmpf uno, %25, %25 : f32
      %29 = arith.select %28, %25, %27 : f32
      memref.store %29, %reinterpret_cast[%14] : memref<256xf32, #gpu.address_space<workgroup>>
    }
    gpu.barrier
    %21 = arith.cmpi eq, %11, %c0 : index
    %22 = arith.andi %21, %17 : i1
    scf.if %22 {
      %23 = arith.addi %14, %c1 : index
      %24 = memref.load %reinterpret_cast[%14] : memref<256xf32, #gpu.address_space<workgroup>>
      %25 = memref.load %reinterpret_cast[%23] : memref<256xf32, #gpu.address_space<workgroup>>
      %26 = arith.cmpf ugt, %24, %25 : f32
      %27 = arith.select %26, %24, %25 : f32
      %28 = arith.cmpf uno, %25, %25 : f32
      %29 = arith.select %28, %25, %27 : f32
      %30 = memref.generic_atomic_rmw %arg3[%16] : memref<1300xf32, "gpu"> {
      ^bb0(%arg5: f32):
        %31 = arith.cmpf ogt, %arg5, %29 : f32
        %32 = arith.select %31, %arg5, %29 : f32
        memref.atomic_yield %32 : f32
      }
    }
  }
  gpu.return
}

// -----// IR Dump After CSE (cse) //----- //
gpu.module @main_kernel_0 {
  gpu.func @main_kColReduction_reduce__3_1_0___8w32h_1(%arg0: index, %arg1: index, %arg2: memref<110x100x13xf32, "gpu">, %arg3: memref<1300xf32, "gpu">) workgroup(%arg4 : memref<256xf32, #gpu.address_space<workgroup>>) kernel {
    %c2 = arith.constant 2 : index
    %c4 = arith.constant 4 : index
    %cst = arith.constant 0xFF800000 : f32
    %c110 = arith.constant 110 : index
    %c64 = arith.constant 64 : index
    %c1300 = arith.constant 1300 : index
    %c8 = arith.constant 8 : index
    %c32 = arith.constant 32 : index
    %c41 = arith.constant 41 : index
    %c256 = arith.constant 256 : index
    %c1 = arith.constant 1 : index
    %c0 = arith.constant 0 : index
    %0 = gpu.block_id  x
    %1 = gpu.thread_id  x
    cf.br ^bb1
  ^bb1:  // pred: ^bb0
    %2 = affine.apply affine_map<(d0)[s0, s1] -> (d0 * s0 + s1)>(%0)[%arg0, %c0]
    %3 = affine.apply affine_map<(d0)[s0, s1] -> (d0 * s0 + s1)>(%1)[%c1, %c0]
    %4 = arith.addi %3, %2 : index
    %5 = arith.cmpi ult, %4, %arg1 : index
    scf.if %5 {
      %6 = arith.remsi %4, %c256 : index
      %7 = arith.divsi %4, %c256 : index
      %8 = arith.divui %7, %c41 : index
      %9 = arith.remui %7, %c41 : index
      %10 = arith.divui %6, %c32 : index
      %11 = arith.remui %6, %c32 : index
      %12 = arith.muli %11, %c8 : index
      %13 = arith.addi %10, %12 : index
      %14 = arith.muli %9, %c32 : index
      %15 = arith.addi %11, %14 : index
      %16 = arith.cmpi ult, %15, %c1300 : index
      %17 = scf.if %16 -> (f32) {
        %22 = arith.muli %8, %c8 : index
        %23 = arith.addi %10, %22 : index
        %24 = arith.muli %23, %c64 : index
        %25 = scf.for %arg5 = %c0 to %c64 step %c1 iter_args(%arg6 = %cst) -> (f32) {
          %26 = arith.addi %arg5, %24 : index
          %27 = arith.cmpi slt, %26, %c110 : index
          %28 = scf.if %27 -> (f32) {
            %29 = arith.muli %26, %c1300 : index
            %30 = arith.addi %29, %15 : index
            %reinterpret_cast_0 = memref.reinterpret_cast %arg2 to offset: [0], sizes: [143000], strides: [1] : memref<110x100x13xf32, "gpu"> to memref<143000xf32, "gpu">
            %31 = memref.load %reinterpret_cast_0[%30] : memref<143000xf32, "gpu">
            %32 = arith.cmpf ugt, %arg6, %31 : f32
            %33 = arith.select %32, %arg6, %31 : f32
            %34 = arith.cmpf uno, %31, %31 : f32
            %35 = arith.select %34, %31, %33 : f32
            scf.yield %35 : f32
          } else {
            scf.yield %arg6 : f32
          }
          scf.yield %28 : f32
        }
        scf.yield %25 : f32
      } else {
        scf.yield %cst : f32
      }
      %reinterpret_cast = memref.reinterpret_cast %arg4 to offset: [0], sizes: [256], strides: [1] : memref<256xf32, #gpu.address_space<workgroup>> to memref<256xf32, #gpu.address_space<workgroup>>
      memref.store %17, %reinterpret_cast[%13] : memref<256xf32, #gpu.address_space<workgroup>>
      gpu.barrier
      %18 = arith.cmpi slt, %10, %c4 : index
      scf.if %18 {
        %22 = arith.addi %13, %c4 : index
        %23 = memref.load %reinterpret_cast[%13] : memref<256xf32, #gpu.address_space<workgroup>>
        %24 = memref.load %reinterpret_cast[%22] : memref<256xf32, #gpu.address_space<workgroup>>
        %25 = arith.cmpf ugt, %23, %24 : f32
        %26 = arith.select %25, %23, %24 : f32
        %27 = arith.cmpf uno, %24, %24 : f32
        %28 = arith.select %27, %24, %26 : f32
        memref.store %28, %reinterpret_cast[%13] : memref<256xf32, #gpu.address_space<workgroup>>
      }
      gpu.barrier
      %19 = arith.cmpi slt, %10, %c2 : index
      scf.if %19 {
        %22 = arith.addi %13, %c2 : index
        %23 = memref.load %reinterpret_cast[%13] : memref<256xf32, #gpu.address_space<workgroup>>
        %24 = memref.load %reinterpret_cast[%22] : memref<256xf32, #gpu.address_space<workgroup>>
        %25 = arith.cmpf ugt, %23, %24 : f32
        %26 = arith.select %25, %23, %24 : f32
        %27 = arith.cmpf uno, %24, %24 : f32
        %28 = arith.select %27, %24, %26 : f32
        memref.store %28, %reinterpret_cast[%13] : memref<256xf32, #gpu.address_space<workgroup>>
      }
      gpu.barrier
      %20 = arith.cmpi eq, %10, %c0 : index
      %21 = arith.andi %20, %16 : i1
      scf.if %21 {
        %22 = arith.addi %13, %c1 : index
        %23 = memref.load %reinterpret_cast[%13] : memref<256xf32, #gpu.address_space<workgroup>>
        %24 = memref.load %reinterpret_cast[%22] : memref<256xf32, #gpu.address_space<workgroup>>
        %25 = arith.cmpf ugt, %23, %24 : f32
        %26 = arith.select %25, %23, %24 : f32
        %27 = arith.cmpf uno, %24, %24 : f32
        %28 = arith.select %27, %24, %26 : f32
        %29 = memref.generic_atomic_rmw %arg3[%15] : memref<1300xf32, "gpu"> {
        ^bb0(%arg5: f32):
          %30 = arith.cmpf ogt, %arg5, %28 : f32
          %31 = arith.select %30, %arg5, %28 : f32
          memref.atomic_yield %31 : f32
        }
      }
    }
    gpu.return
  }
}

// -----// IR Dump After SCFToControlFlow (convert-scf-to-cf) //----- //
gpu.module @main_kernel_0 {
  gpu.func @main_kColReduction_reduce__3_1_0___8w32h_1(%arg0: index, %arg1: index, %arg2: memref<110x100x13xf32, "gpu">, %arg3: memref<1300xf32, "gpu">) workgroup(%arg4 : memref<256xf32, #gpu.address_space<workgroup>>) kernel {
    %c2 = arith.constant 2 : index
    %c4 = arith.constant 4 : index
    %cst = arith.constant 0xFF800000 : f32
    %c110 = arith.constant 110 : index
    %c64 = arith.constant 64 : index
    %c1300 = arith.constant 1300 : index
    %c8 = arith.constant 8 : index
    %c32 = arith.constant 32 : index
    %c41 = arith.constant 41 : index
    %c256 = arith.constant 256 : index
    %c1 = arith.constant 1 : index
    %c0 = arith.constant 0 : index
    %0 = gpu.block_id  x
    %1 = gpu.thread_id  x
    cf.br ^bb1
  ^bb1:  // pred: ^bb0
    %2 = affine.apply affine_map<(d0)[s0, s1] -> (d0 * s0 + s1)>(%0)[%arg0, %c0]
    %3 = affine.apply affine_map<(d0)[s0, s1] -> (d0 * s0 + s1)>(%1)[%c1, %c0]
    %4 = arith.addi %3, %2 : index
    %5 = arith.cmpi ult, %4, %arg1 : index
    cf.cond_br %5, ^bb2, ^bb20
  ^bb2:  // pred: ^bb1
    %6 = arith.remsi %4, %c256 : index
    %7 = arith.divsi %4, %c256 : index
    %8 = arith.divui %7, %c41 : index
    %9 = arith.remui %7, %c41 : index
    %10 = arith.divui %6, %c32 : index
    %11 = arith.remui %6, %c32 : index
    %12 = arith.muli %11, %c8 : index
    %13 = arith.addi %10, %12 : index
    %14 = arith.muli %9, %c32 : index
    %15 = arith.addi %11, %14 : index
    %16 = arith.cmpi ult, %15, %c1300 : index
    cf.cond_br %16, ^bb3, ^bb11
  ^bb3:  // pred: ^bb2
    %17 = arith.muli %8, %c8 : index
    %18 = arith.addi %10, %17 : index
    %19 = arith.muli %18, %c64 : index
    cf.br ^bb4(%c0, %cst : index, f32)
  ^bb4(%20: index, %21: f32):  // 2 preds: ^bb3, ^bb9
    %22 = arith.cmpi slt, %20, %c64 : index
    cf.cond_br %22, ^bb5, ^bb10
  ^bb5:  // pred: ^bb4
    %23 = arith.addi %20, %19 : index
    %24 = arith.cmpi slt, %23, %c110 : index
    cf.cond_br %24, ^bb6, ^bb7
  ^bb6:  // pred: ^bb5
    %25 = arith.muli %23, %c1300 : index
    %26 = arith.addi %25, %15 : index
    %reinterpret_cast = memref.reinterpret_cast %arg2 to offset: [0], sizes: [143000], strides: [1] : memref<110x100x13xf32, "gpu"> to memref<143000xf32, "gpu">
    %27 = memref.load %reinterpret_cast[%26] : memref<143000xf32, "gpu">
    %28 = arith.cmpf ugt, %21, %27 : f32
    %29 = arith.select %28, %21, %27 : f32
    %30 = arith.cmpf uno, %27, %27 : f32
    %31 = arith.select %30, %27, %29 : f32
    cf.br ^bb8(%31 : f32)
  ^bb7:  // pred: ^bb5
    cf.br ^bb8(%21 : f32)
  ^bb8(%32: f32):  // 2 preds: ^bb6, ^bb7
    cf.br ^bb9
  ^bb9:  // pred: ^bb8
    %33 = arith.addi %20, %c1 : index
    cf.br ^bb4(%33, %32 : index, f32)
  ^bb10:  // pred: ^bb4
    cf.br ^bb12(%21 : f32)
  ^bb11:  // pred: ^bb2
    cf.br ^bb12(%cst : f32)
  ^bb12(%34: f32):  // 2 preds: ^bb10, ^bb11
    cf.br ^bb13
  ^bb13:  // pred: ^bb12
    %reinterpret_cast_0 = memref.reinterpret_cast %arg4 to offset: [0], sizes: [256], strides: [1] : memref<256xf32, #gpu.address_space<workgroup>> to memref<256xf32, #gpu.address_space<workgroup>>
    memref.store %34, %reinterpret_cast_0[%13] : memref<256xf32, #gpu.address_space<workgroup>>
    gpu.barrier
    %35 = arith.cmpi slt, %10, %c4 : index
    cf.cond_br %35, ^bb14, ^bb15
  ^bb14:  // pred: ^bb13
    %36 = arith.addi %13, %c4 : index
    %37 = memref.load %reinterpret_cast_0[%13] : memref<256xf32, #gpu.address_space<workgroup>>
    %38 = memref.load %reinterpret_cast_0[%36] : memref<256xf32, #gpu.address_space<workgroup>>
    %39 = arith.cmpf ugt, %37, %38 : f32
    %40 = arith.select %39, %37, %38 : f32
    %41 = arith.cmpf uno, %38, %38 : f32
    %42 = arith.select %41, %38, %40 : f32
    memref.store %42, %reinterpret_cast_0[%13] : memref<256xf32, #gpu.address_space<workgroup>>
    cf.br ^bb15
  ^bb15:  // 2 preds: ^bb13, ^bb14
    gpu.barrier
    %43 = arith.cmpi slt, %10, %c2 : index
    cf.cond_br %43, ^bb16, ^bb17
  ^bb16:  // pred: ^bb15
    %44 = arith.addi %13, %c2 : index
    %45 = memref.load %reinterpret_cast_0[%13] : memref<256xf32, #gpu.address_space<workgroup>>
    %46 = memref.load %reinterpret_cast_0[%44] : memref<256xf32, #gpu.address_space<workgroup>>
    %47 = arith.cmpf ugt, %45, %46 : f32
    %48 = arith.select %47, %45, %46 : f32
    %49 = arith.cmpf uno, %46, %46 : f32
    %50 = arith.select %49, %46, %48 : f32
    memref.store %50, %reinterpret_cast_0[%13] : memref<256xf32, #gpu.address_space<workgroup>>
    cf.br ^bb17
  ^bb17:  // 2 preds: ^bb15, ^bb16
    gpu.barrier
    %51 = arith.cmpi eq, %10, %c0 : index
    %52 = arith.andi %51, %16 : i1
    cf.cond_br %52, ^bb18, ^bb19
  ^bb18:  // pred: ^bb17
    %53 = arith.addi %13, %c1 : index
    %54 = memref.load %reinterpret_cast_0[%13] : memref<256xf32, #gpu.address_space<workgroup>>
    %55 = memref.load %reinterpret_cast_0[%53] : memref<256xf32, #gpu.address_space<workgroup>>
    %56 = arith.cmpf ugt, %54, %55 : f32
    %57 = arith.select %56, %54, %55 : f32
    %58 = arith.cmpf uno, %55, %55 : f32
    %59 = arith.select %58, %55, %57 : f32
    %60 = memref.generic_atomic_rmw %arg3[%15] : memref<1300xf32, "gpu"> {
    ^bb0(%arg5: f32):
      %61 = arith.cmpf ogt, %arg5, %59 : f32
      %62 = arith.select %61, %arg5, %59 : f32
      memref.atomic_yield %62 : f32
    }
    cf.br ^bb19
  ^bb19:  // 2 preds: ^bb17, ^bb18
    cf.br ^bb20
  ^bb20:  // 2 preds: ^bb1, ^bb19
    gpu.return
  }
}

// -----// IR Dump After ConvertAffineToStandard (lower-affine) //----- //
gpu.module @main_kernel_0 {
  gpu.func @main_kColReduction_reduce__3_1_0___8w32h_1(%arg0: index, %arg1: index, %arg2: memref<110x100x13xf32, "gpu">, %arg3: memref<1300xf32, "gpu">) workgroup(%arg4 : memref<256xf32, #gpu.address_space<workgroup>>) kernel {
    %c2 = arith.constant 2 : index
    %c4 = arith.constant 4 : index
    %cst = arith.constant 0xFF800000 : f32
    %c110 = arith.constant 110 : index
    %c64 = arith.constant 64 : index
    %c1300 = arith.constant 1300 : index
    %c8 = arith.constant 8 : index
    %c32 = arith.constant 32 : index
    %c41 = arith.constant 41 : index
    %c256 = arith.constant 256 : index
    %c1 = arith.constant 1 : index
    %c0 = arith.constant 0 : index
    %0 = gpu.block_id  x
    %1 = gpu.thread_id  x
    cf.br ^bb1
  ^bb1:  // pred: ^bb0
    %2 = arith.muli %0, %arg0 : index
    %3 = arith.addi %2, %c0 : index
    %4 = arith.muli %1, %c1 : index
    %5 = arith.addi %4, %c0 : index
    %6 = arith.addi %5, %3 : index
    %7 = arith.cmpi ult, %6, %arg1 : index
    cf.cond_br %7, ^bb2, ^bb20
  ^bb2:  // pred: ^bb1
    %8 = arith.remsi %6, %c256 : index
    %9 = arith.divsi %6, %c256 : index
    %10 = arith.divui %9, %c41 : index
    %11 = arith.remui %9, %c41 : index
    %12 = arith.divui %8, %c32 : index
    %13 = arith.remui %8, %c32 : index
    %14 = arith.muli %13, %c8 : index
    %15 = arith.addi %12, %14 : index
    %16 = arith.muli %11, %c32 : index
    %17 = arith.addi %13, %16 : index
    %18 = arith.cmpi ult, %17, %c1300 : index
    cf.cond_br %18, ^bb3, ^bb11
  ^bb3:  // pred: ^bb2
    %19 = arith.muli %10, %c8 : index
    %20 = arith.addi %12, %19 : index
    %21 = arith.muli %20, %c64 : index
    cf.br ^bb4(%c0, %cst : index, f32)
  ^bb4(%22: index, %23: f32):  // 2 preds: ^bb3, ^bb9
    %24 = arith.cmpi slt, %22, %c64 : index
    cf.cond_br %24, ^bb5, ^bb10
  ^bb5:  // pred: ^bb4
    %25 = arith.addi %22, %21 : index
    %26 = arith.cmpi slt, %25, %c110 : index
    cf.cond_br %26, ^bb6, ^bb7
  ^bb6:  // pred: ^bb5
    %27 = arith.muli %25, %c1300 : index
    %28 = arith.addi %27, %17 : index
    %reinterpret_cast = memref.reinterpret_cast %arg2 to offset: [0], sizes: [143000], strides: [1] : memref<110x100x13xf32, "gpu"> to memref<143000xf32, "gpu">
    %29 = memref.load %reinterpret_cast[%28] : memref<143000xf32, "gpu">
    %30 = arith.cmpf ugt, %23, %29 : f32
    %31 = arith.select %30, %23, %29 : f32
    %32 = arith.cmpf uno, %29, %29 : f32
    %33 = arith.select %32, %29, %31 : f32
    cf.br ^bb8(%33 : f32)
  ^bb7:  // pred: ^bb5
    cf.br ^bb8(%23 : f32)
  ^bb8(%34: f32):  // 2 preds: ^bb6, ^bb7
    cf.br ^bb9
  ^bb9:  // pred: ^bb8
    %35 = arith.addi %22, %c1 : index
    cf.br ^bb4(%35, %34 : index, f32)
  ^bb10:  // pred: ^bb4
    cf.br ^bb12(%23 : f32)
  ^bb11:  // pred: ^bb2
    cf.br ^bb12(%cst : f32)
  ^bb12(%36: f32):  // 2 preds: ^bb10, ^bb11
    cf.br ^bb13
  ^bb13:  // pred: ^bb12
    %reinterpret_cast_0 = memref.reinterpret_cast %arg4 to offset: [0], sizes: [256], strides: [1] : memref<256xf32, #gpu.address_space<workgroup>> to memref<256xf32, #gpu.address_space<workgroup>>
    memref.store %36, %reinterpret_cast_0[%15] : memref<256xf32, #gpu.address_space<workgroup>>
    gpu.barrier
    %37 = arith.cmpi slt, %12, %c4 : index
    cf.cond_br %37, ^bb14, ^bb15
  ^bb14:  // pred: ^bb13
    %38 = arith.addi %15, %c4 : index
    %39 = memref.load %reinterpret_cast_0[%15] : memref<256xf32, #gpu.address_space<workgroup>>
    %40 = memref.load %reinterpret_cast_0[%38] : memref<256xf32, #gpu.address_space<workgroup>>
    %41 = arith.cmpf ugt, %39, %40 : f32
    %42 = arith.select %41, %39, %40 : f32
    %43 = arith.cmpf uno, %40, %40 : f32
    %44 = arith.select %43, %40, %42 : f32
    memref.store %44, %reinterpret_cast_0[%15] : memref<256xf32, #gpu.address_space<workgroup>>
    cf.br ^bb15
  ^bb15:  // 2 preds: ^bb13, ^bb14
    gpu.barrier
    %45 = arith.cmpi slt, %12, %c2 : index
    cf.cond_br %45, ^bb16, ^bb17
  ^bb16:  // pred: ^bb15
    %46 = arith.addi %15, %c2 : index
    %47 = memref.load %reinterpret_cast_0[%15] : memref<256xf32, #gpu.address_space<workgroup>>
    %48 = memref.load %reinterpret_cast_0[%46] : memref<256xf32, #gpu.address_space<workgroup>>
    %49 = arith.cmpf ugt, %47, %48 : f32
    %50 = arith.select %49, %47, %48 : f32
    %51 = arith.cmpf uno, %48, %48 : f32
    %52 = arith.select %51, %48, %50 : f32
    memref.store %52, %reinterpret_cast_0[%15] : memref<256xf32, #gpu.address_space<workgroup>>
    cf.br ^bb17
  ^bb17:  // 2 preds: ^bb15, ^bb16
    gpu.barrier
    %53 = arith.cmpi eq, %12, %c0 : index
    %54 = arith.andi %53, %18 : i1
    cf.cond_br %54, ^bb18, ^bb19
  ^bb18:  // pred: ^bb17
    %55 = arith.addi %15, %c1 : index
    %56 = memref.load %reinterpret_cast_0[%15] : memref<256xf32, #gpu.address_space<workgroup>>
    %57 = memref.load %reinterpret_cast_0[%55] : memref<256xf32, #gpu.address_space<workgroup>>
    %58 = arith.cmpf ugt, %56, %57 : f32
    %59 = arith.select %58, %56, %57 : f32
    %60 = arith.cmpf uno, %57, %57 : f32
    %61 = arith.select %60, %57, %59 : f32
    %62 = memref.generic_atomic_rmw %arg3[%17] : memref<1300xf32, "gpu"> {
    ^bb0(%arg5: f32):
      %63 = arith.cmpf ogt, %arg5, %61 : f32
      %64 = arith.select %63, %arg5, %61 : f32
      memref.atomic_yield %64 : f32
    }
    cf.br ^bb19
  ^bb19:  // 2 preds: ^bb17, ^bb18
    cf.br ^bb20
  ^bb20:  // 2 preds: ^bb1, ^bb19
    gpu.return
  }
}

// -----// IR Dump After StripDebugInfo (strip-debuginfo) //----- //
gpu.module @main_kernel_0 {
  gpu.func @main_kColReduction_reduce__3_1_0___8w32h_1(%arg0: index, %arg1: index, %arg2: memref<110x100x13xf32, "gpu">, %arg3: memref<1300xf32, "gpu">) workgroup(%arg4 : memref<256xf32, #gpu.address_space<workgroup>>) kernel {
    %c2 = arith.constant 2 : index
    %c4 = arith.constant 4 : index
    %cst = arith.constant 0xFF800000 : f32
    %c110 = arith.constant 110 : index
    %c64 = arith.constant 64 : index
    %c1300 = arith.constant 1300 : index
    %c8 = arith.constant 8 : index
    %c32 = arith.constant 32 : index
    %c41 = arith.constant 41 : index
    %c256 = arith.constant 256 : index
    %c1 = arith.constant 1 : index
    %c0 = arith.constant 0 : index
    %0 = gpu.block_id  x
    %1 = gpu.thread_id  x
    cf.br ^bb1
  ^bb1:  // pred: ^bb0
    %2 = arith.muli %0, %arg0 : index
    %3 = arith.addi %2, %c0 : index
    %4 = arith.muli %1, %c1 : index
    %5 = arith.addi %4, %c0 : index
    %6 = arith.addi %5, %3 : index
    %7 = arith.cmpi ult, %6, %arg1 : index
    cf.cond_br %7, ^bb2, ^bb20
  ^bb2:  // pred: ^bb1
    %8 = arith.remsi %6, %c256 : index
    %9 = arith.divsi %6, %c256 : index
    %10 = arith.divui %9, %c41 : index
    %11 = arith.remui %9, %c41 : index
    %12 = arith.divui %8, %c32 : index
    %13 = arith.remui %8, %c32 : index
    %14 = arith.muli %13, %c8 : index
    %15 = arith.addi %12, %14 : index
    %16 = arith.muli %11, %c32 : index
    %17 = arith.addi %13, %16 : index
    %18 = arith.cmpi ult, %17, %c1300 : index
    cf.cond_br %18, ^bb3, ^bb11
  ^bb3:  // pred: ^bb2
    %19 = arith.muli %10, %c8 : index
    %20 = arith.addi %12, %19 : index
    %21 = arith.muli %20, %c64 : index
    cf.br ^bb4(%c0, %cst : index, f32)
  ^bb4(%22: index, %23: f32):  // 2 preds: ^bb3, ^bb9
    %24 = arith.cmpi slt, %22, %c64 : index
    cf.cond_br %24, ^bb5, ^bb10
  ^bb5:  // pred: ^bb4
    %25 = arith.addi %22, %21 : index
    %26 = arith.cmpi slt, %25, %c110 : index
    cf.cond_br %26, ^bb6, ^bb7
  ^bb6:  // pred: ^bb5
    %27 = arith.muli %25, %c1300 : index
    %28 = arith.addi %27, %17 : index
    %reinterpret_cast = memref.reinterpret_cast %arg2 to offset: [0], sizes: [143000], strides: [1] : memref<110x100x13xf32, "gpu"> to memref<143000xf32, "gpu">
    %29 = memref.load %reinterpret_cast[%28] : memref<143000xf32, "gpu">
    %30 = arith.cmpf ugt, %23, %29 : f32
    %31 = arith.select %30, %23, %29 : f32
    %32 = arith.cmpf uno, %29, %29 : f32
    %33 = arith.select %32, %29, %31 : f32
    cf.br ^bb8(%33 : f32)
  ^bb7:  // pred: ^bb5
    cf.br ^bb8(%23 : f32)
  ^bb8(%34: f32):  // 2 preds: ^bb6, ^bb7
    cf.br ^bb9
  ^bb9:  // pred: ^bb8
    %35 = arith.addi %22, %c1 : index
    cf.br ^bb4(%35, %34 : index, f32)
  ^bb10:  // pred: ^bb4
    cf.br ^bb12(%23 : f32)
  ^bb11:  // pred: ^bb2
    cf.br ^bb12(%cst : f32)
  ^bb12(%36: f32):  // 2 preds: ^bb10, ^bb11
    cf.br ^bb13
  ^bb13:  // pred: ^bb12
    %reinterpret_cast_0 = memref.reinterpret_cast %arg4 to offset: [0], sizes: [256], strides: [1] : memref<256xf32, #gpu.address_space<workgroup>> to memref<256xf32, #gpu.address_space<workgroup>>
    memref.store %36, %reinterpret_cast_0[%15] : memref<256xf32, #gpu.address_space<workgroup>>
    gpu.barrier
    %37 = arith.cmpi slt, %12, %c4 : index
    cf.cond_br %37, ^bb14, ^bb15
  ^bb14:  // pred: ^bb13
    %38 = arith.addi %15, %c4 : index
    %39 = memref.load %reinterpret_cast_0[%15] : memref<256xf32, #gpu.address_space<workgroup>>
    %40 = memref.load %reinterpret_cast_0[%38] : memref<256xf32, #gpu.address_space<workgroup>>
    %41 = arith.cmpf ugt, %39, %40 : f32
    %42 = arith.select %41, %39, %40 : f32
    %43 = arith.cmpf uno, %40, %40 : f32
    %44 = arith.select %43, %40, %42 : f32
    memref.store %44, %reinterpret_cast_0[%15] : memref<256xf32, #gpu.address_space<workgroup>>
    cf.br ^bb15
  ^bb15:  // 2 preds: ^bb13, ^bb14
    gpu.barrier
    %45 = arith.cmpi slt, %12, %c2 : index
    cf.cond_br %45, ^bb16, ^bb17
  ^bb16:  // pred: ^bb15
    %46 = arith.addi %15, %c2 : index
    %47 = memref.load %reinterpret_cast_0[%15] : memref<256xf32, #gpu.address_space<workgroup>>
    %48 = memref.load %reinterpret_cast_0[%46] : memref<256xf32, #gpu.address_space<workgroup>>
    %49 = arith.cmpf ugt, %47, %48 : f32
    %50 = arith.select %49, %47, %48 : f32
    %51 = arith.cmpf uno, %48, %48 : f32
    %52 = arith.select %51, %48, %50 : f32
    memref.store %52, %reinterpret_cast_0[%15] : memref<256xf32, #gpu.address_space<workgroup>>
    cf.br ^bb17
  ^bb17:  // 2 preds: ^bb15, ^bb16
    gpu.barrier
    %53 = arith.cmpi eq, %12, %c0 : index
    %54 = arith.andi %53, %18 : i1
    cf.cond_br %54, ^bb18, ^bb19
  ^bb18:  // pred: ^bb17
    %55 = arith.addi %15, %c1 : index
    %56 = memref.load %reinterpret_cast_0[%15] : memref<256xf32, #gpu.address_space<workgroup>>
    %57 = memref.load %reinterpret_cast_0[%55] : memref<256xf32, #gpu.address_space<workgroup>>
    %58 = arith.cmpf ugt, %56, %57 : f32
    %59 = arith.select %58, %56, %57 : f32
    %60 = arith.cmpf uno, %57, %57 : f32
    %61 = arith.select %60, %57, %59 : f32
    %62 = memref.generic_atomic_rmw %arg3[%17] : memref<1300xf32, "gpu"> {
    ^bb0(%arg5: f32):
      %63 = arith.cmpf ogt, %arg5, %61 : f32
      %64 = arith.select %63, %arg5, %61 : f32
      memref.atomic_yield %64 : f32
    }
    cf.br ^bb19
  ^bb19:  // 2 preds: ^bb17, ^bb18
    cf.br ^bb20
  ^bb20:  // 2 preds: ^bb1, ^bb19
    gpu.return
  }
}

// -----// IR Dump After DiscLowerGpuOpsToNVVMOpsPass (disc-convert-gpu-to-nvvm) //----- //
gpu.module @main_kernel_0 {
  llvm.mlir.global internal @__wg_main_kColReduction_reduce__3_1_0___8w32h_1_0() {addr_space = 3 : i32} : !llvm.array<256 x f32>
  llvm.func @main_kColReduction_reduce__3_1_0___8w32h_1(%arg0: i32, %arg1: i32, %arg2: !llvm.ptr<f32>, %arg3: !llvm.ptr<f32>, %arg4: i32, %arg5: i32, %arg6: i32, %arg7: i32, %arg8: i32, %arg9: i32, %arg10: i32, %arg11: !llvm.ptr<f32>, %arg12: !llvm.ptr<f32>, %arg13: i32, %arg14: i32, %arg15: i32) attributes {gpu.kernel, nvvm.kernel} {
    %0 = llvm.mlir.undef : !llvm.struct<(ptr<f32>, ptr<f32>, i32, array<3 x i32>, array<3 x i32>)>
    %1 = llvm.insertvalue %arg2, %0[0] : !llvm.struct<(ptr<f32>, ptr<f32>, i32, array<3 x i32>, array<3 x i32>)> 
    %2 = llvm.insertvalue %arg3, %1[1] : !llvm.struct<(ptr<f32>, ptr<f32>, i32, array<3 x i32>, array<3 x i32>)> 
    %3 = llvm.insertvalue %arg4, %2[2] : !llvm.struct<(ptr<f32>, ptr<f32>, i32, array<3 x i32>, array<3 x i32>)> 
    %4 = llvm.insertvalue %arg5, %3[3, 0] : !llvm.struct<(ptr<f32>, ptr<f32>, i32, array<3 x i32>, array<3 x i32>)> 
    %5 = llvm.insertvalue %arg8, %4[4, 0] : !llvm.struct<(ptr<f32>, ptr<f32>, i32, array<3 x i32>, array<3 x i32>)> 
    %6 = llvm.insertvalue %arg6, %5[3, 1] : !llvm.struct<(ptr<f32>, ptr<f32>, i32, array<3 x i32>, array<3 x i32>)> 
    %7 = llvm.insertvalue %arg9, %6[4, 1] : !llvm.struct<(ptr<f32>, ptr<f32>, i32, array<3 x i32>, array<3 x i32>)> 
    %8 = llvm.insertvalue %arg7, %7[3, 2] : !llvm.struct<(ptr<f32>, ptr<f32>, i32, array<3 x i32>, array<3 x i32>)> 
    %9 = llvm.insertvalue %arg10, %8[4, 2] : !llvm.struct<(ptr<f32>, ptr<f32>, i32, array<3 x i32>, array<3 x i32>)> 
    %10 = llvm.mlir.undef : !llvm.struct<(ptr<f32>, ptr<f32>, i32, array<1 x i32>, array<1 x i32>)>
    %11 = llvm.insertvalue %arg11, %10[0] : !llvm.struct<(ptr<f32>, ptr<f32>, i32, array<1 x i32>, array<1 x i32>)> 
    %12 = llvm.insertvalue %arg12, %11[1] : !llvm.struct<(ptr<f32>, ptr<f32>, i32, array<1 x i32>, array<1 x i32>)> 
    %13 = llvm.insertvalue %arg13, %12[2] : !llvm.struct<(ptr<f32>, ptr<f32>, i32, array<1 x i32>, array<1 x i32>)> 
    %14 = llvm.insertvalue %arg14, %13[3, 0] : !llvm.struct<(ptr<f32>, ptr<f32>, i32, array<1 x i32>, array<1 x i32>)> 
    %15 = llvm.insertvalue %arg15, %14[4, 0] : !llvm.struct<(ptr<f32>, ptr<f32>, i32, array<1 x i32>, array<1 x i32>)> 
    %16 = llvm.mlir.addressof @__wg_main_kColReduction_reduce__3_1_0___8w32h_1_0 : !llvm.ptr<array<256 x f32>, 3>
    %17 = llvm.getelementptr %16[0, 0] : (!llvm.ptr<array<256 x f32>, 3>) -> !llvm.ptr<f32, 3>
    %18 = llvm.mlir.undef : !llvm.struct<(ptr<f32, 3>, ptr<f32, 3>, i32, array<1 x i32>, array<1 x i32>)>
    %19 = llvm.insertvalue %17, %18[0] : !llvm.struct<(ptr<f32, 3>, ptr<f32, 3>, i32, array<1 x i32>, array<1 x i32>)> 
    %20 = llvm.insertvalue %17, %19[1] : !llvm.struct<(ptr<f32, 3>, ptr<f32, 3>, i32, array<1 x i32>, array<1 x i32>)> 
    %21 = llvm.mlir.constant(0 : index) : i32
    %22 = llvm.insertvalue %21, %20[2] : !llvm.struct<(ptr<f32, 3>, ptr<f32, 3>, i32, array<1 x i32>, array<1 x i32>)> 
    %23 = llvm.mlir.constant(256 : index) : i32
    %24 = llvm.insertvalue %23, %22[3, 0] : !llvm.struct<(ptr<f32, 3>, ptr<f32, 3>, i32, array<1 x i32>, array<1 x i32>)> 
    %25 = llvm.mlir.constant(1 : index) : i32
    %26 = llvm.insertvalue %25, %24[4, 0] : !llvm.struct<(ptr<f32, 3>, ptr<f32, 3>, i32, array<1 x i32>, array<1 x i32>)> 
    %27 = llvm.mlir.constant(2 : index) : i32
    %28 = llvm.mlir.constant(4 : index) : i32
    %29 = llvm.mlir.constant(0xFF800000 : f32) : f32
    %30 = llvm.mlir.constant(110 : index) : i32
    %31 = llvm.mlir.constant(64 : index) : i32
    %32 = llvm.mlir.constant(1300 : index) : i32
    %33 = llvm.mlir.constant(8 : index) : i32
    %34 = llvm.mlir.constant(32 : index) : i32
    %35 = llvm.mlir.constant(41 : index) : i32
    %36 = llvm.mlir.constant(256 : index) : i32
    %37 = llvm.mlir.constant(1 : index) : i32
    %38 = llvm.mlir.constant(0 : index) : i32
    %39 = nvvm.read.ptx.sreg.ctaid.x : i32
    %40 = nvvm.read.ptx.sreg.tid.x : i32
    llvm.br ^bb1
  ^bb1:  // pred: ^bb0
    %41 = llvm.mul %39, %arg0  : i32
    %42 = llvm.add %40, %41  : i32
    %43 = llvm.icmp "ult" %42, %arg1 : i32
    llvm.cond_br %43, ^bb2, ^bb21
  ^bb2:  // pred: ^bb1
    %44 = llvm.srem %42, %36  : i32
    %45 = llvm.sdiv %42, %36  : i32
    %46 = llvm.udiv %45, %35  : i32
    %47 = llvm.urem %45, %35  : i32
    %48 = llvm.udiv %44, %34  : i32
    %49 = llvm.urem %44, %34  : i32
    %50 = llvm.mul %49, %33  : i32
    %51 = llvm.add %48, %50  : i32
    %52 = llvm.mul %47, %34  : i32
    %53 = llvm.add %49, %52  : i32
    %54 = llvm.icmp "ult" %53, %32 : i32
    llvm.cond_br %54, ^bb3, ^bb10(%29 : f32)
  ^bb3:  // pred: ^bb2
    %55 = llvm.mul %46, %33  : i32
    %56 = llvm.add %48, %55  : i32
    %57 = llvm.mul %56, %31  : i32
    llvm.br ^bb4(%38, %29 : i32, f32)
  ^bb4(%58: i32, %59: f32):  // 2 preds: ^bb3, ^bb9
    %60 = llvm.icmp "slt" %58, %31 : i32
    llvm.cond_br %60, ^bb5, ^bb10(%59 : f32)
  ^bb5:  // pred: ^bb4
    %61 = llvm.add %58, %57  : i32
    %62 = llvm.icmp "slt" %61, %30 : i32
    llvm.cond_br %62, ^bb6, ^bb7
  ^bb6:  // pred: ^bb5
    %63 = llvm.mul %61, %32  : i32
    %64 = llvm.add %63, %53  : i32
    %65 = llvm.mlir.undef : !llvm.struct<(ptr<f32>, ptr<f32>, i32, array<1 x i32>, array<1 x i32>)>
    %66 = llvm.extractvalue %9[0] : !llvm.struct<(ptr<f32>, ptr<f32>, i32, array<3 x i32>, array<3 x i32>)> 
    %67 = llvm.extractvalue %9[1] : !llvm.struct<(ptr<f32>, ptr<f32>, i32, array<3 x i32>, array<3 x i32>)> 
    %68 = llvm.insertvalue %66, %65[0] : !llvm.struct<(ptr<f32>, ptr<f32>, i32, array<1 x i32>, array<1 x i32>)> 
    %69 = llvm.insertvalue %67, %68[1] : !llvm.struct<(ptr<f32>, ptr<f32>, i32, array<1 x i32>, array<1 x i32>)> 
    %70 = llvm.mlir.constant(0 : index) : i32
    %71 = llvm.insertvalue %70, %69[2] : !llvm.struct<(ptr<f32>, ptr<f32>, i32, array<1 x i32>, array<1 x i32>)> 
    %72 = llvm.mlir.constant(143000 : index) : i32
    %73 = llvm.insertvalue %72, %71[3, 0] : !llvm.struct<(ptr<f32>, ptr<f32>, i32, array<1 x i32>, array<1 x i32>)> 
    %74 = llvm.mlir.constant(1 : index) : i32
    %75 = llvm.insertvalue %74, %73[4, 0] : !llvm.struct<(ptr<f32>, ptr<f32>, i32, array<1 x i32>, array<1 x i32>)> 
    %76 = llvm.extractvalue %75[1] : !llvm.struct<(ptr<f32>, ptr<f32>, i32, array<1 x i32>, array<1 x i32>)> 
    %77 = llvm.getelementptr %76[%64] : (!llvm.ptr<f32>, i32) -> !llvm.ptr<f32>
    %78 = llvm.load %77 : !llvm.ptr<f32>
    %79 = llvm.fcmp "ugt" %59, %78 : f32
    %80 = llvm.select %79, %59, %78 : i1, f32
    %81 = llvm.fcmp "uno" %78, %78 : f32
    %82 = llvm.select %81, %78, %80 : i1, f32
    llvm.br ^bb8(%82 : f32)
  ^bb7:  // pred: ^bb5
    llvm.br ^bb8(%59 : f32)
  ^bb8(%83: f32):  // 2 preds: ^bb6, ^bb7
    llvm.br ^bb9
  ^bb9:  // pred: ^bb8
    %84 = llvm.add %58, %37  : i32
    llvm.br ^bb4(%84, %83 : i32, f32)
  ^bb10(%85: f32):  // 2 preds: ^bb2, ^bb4
    llvm.br ^bb11(%85 : f32)
  ^bb11(%86: f32):  // pred: ^bb10
    llvm.br ^bb12
  ^bb12:  // pred: ^bb11
    %87 = llvm.mlir.undef : !llvm.struct<(ptr<f32, 3>, ptr<f32, 3>, i32, array<1 x i32>, array<1 x i32>)>
    %88 = llvm.extractvalue %26[0] : !llvm.struct<(ptr<f32, 3>, ptr<f32, 3>, i32, array<1 x i32>, array<1 x i32>)> 
    %89 = llvm.extractvalue %26[1] : !llvm.struct<(ptr<f32, 3>, ptr<f32, 3>, i32, array<1 x i32>, array<1 x i32>)> 
    %90 = llvm.insertvalue %88, %87[0] : !llvm.struct<(ptr<f32, 3>, ptr<f32, 3>, i32, array<1 x i32>, array<1 x i32>)> 
    %91 = llvm.insertvalue %89, %90[1] : !llvm.struct<(ptr<f32, 3>, ptr<f32, 3>, i32, array<1 x i32>, array<1 x i32>)> 
    %92 = llvm.mlir.constant(0 : index) : i32
    %93 = llvm.insertvalue %92, %91[2] : !llvm.struct<(ptr<f32, 3>, ptr<f32, 3>, i32, array<1 x i32>, array<1 x i32>)> 
    %94 = llvm.mlir.constant(256 : index) : i32
    %95 = llvm.insertvalue %94, %93[3, 0] : !llvm.struct<(ptr<f32, 3>, ptr<f32, 3>, i32, array<1 x i32>, array<1 x i32>)> 
    %96 = llvm.mlir.constant(1 : index) : i32
    %97 = llvm.insertvalue %96, %95[4, 0] : !llvm.struct<(ptr<f32, 3>, ptr<f32, 3>, i32, array<1 x i32>, array<1 x i32>)> 
    %98 = llvm.extractvalue %97[1] : !llvm.struct<(ptr<f32, 3>, ptr<f32, 3>, i32, array<1 x i32>, array<1 x i32>)> 
    %99 = llvm.getelementptr %98[%51] : (!llvm.ptr<f32, 3>, i32) -> !llvm.ptr<f32, 3>
    llvm.store %86, %99 : !llvm.ptr<f32, 3>
    nvvm.barrier0
    %100 = llvm.icmp "slt" %48, %28 : i32
    llvm.cond_br %100, ^bb13, ^bb14
  ^bb13:  // pred: ^bb12
    %101 = llvm.add %51, %28  : i32
    %102 = llvm.extractvalue %97[1] : !llvm.struct<(ptr<f32, 3>, ptr<f32, 3>, i32, array<1 x i32>, array<1 x i32>)> 
    %103 = llvm.getelementptr %102[%51] : (!llvm.ptr<f32, 3>, i32) -> !llvm.ptr<f32, 3>
    %104 = llvm.load %103 : !llvm.ptr<f32, 3>
    %105 = llvm.extractvalue %97[1] : !llvm.struct<(ptr<f32, 3>, ptr<f32, 3>, i32, array<1 x i32>, array<1 x i32>)> 
    %106 = llvm.getelementptr %105[%101] : (!llvm.ptr<f32, 3>, i32) -> !llvm.ptr<f32, 3>
    %107 = llvm.load %106 : !llvm.ptr<f32, 3>
    %108 = llvm.fcmp "ugt" %104, %107 : f32
    %109 = llvm.select %108, %104, %107 : i1, f32
    %110 = llvm.fcmp "uno" %107, %107 : f32
    %111 = llvm.select %110, %107, %109 : i1, f32
    %112 = llvm.extractvalue %97[1] : !llvm.struct<(ptr<f32, 3>, ptr<f32, 3>, i32, array<1 x i32>, array<1 x i32>)> 
    %113 = llvm.getelementptr %112[%51] : (!llvm.ptr<f32, 3>, i32) -> !llvm.ptr<f32, 3>
    llvm.store %111, %113 : !llvm.ptr<f32, 3>
    llvm.br ^bb14
  ^bb14:  // 2 preds: ^bb12, ^bb13
    nvvm.barrier0
    %114 = llvm.icmp "slt" %48, %27 : i32
    llvm.cond_br %114, ^bb15, ^bb16
  ^bb15:  // pred: ^bb14
    %115 = llvm.add %51, %27  : i32
    %116 = llvm.extractvalue %97[1] : !llvm.struct<(ptr<f32, 3>, ptr<f32, 3>, i32, array<1 x i32>, array<1 x i32>)> 
    %117 = llvm.getelementptr %116[%51] : (!llvm.ptr<f32, 3>, i32) -> !llvm.ptr<f32, 3>
    %118 = llvm.load %117 : !llvm.ptr<f32, 3>
    %119 = llvm.extractvalue %97[1] : !llvm.struct<(ptr<f32, 3>, ptr<f32, 3>, i32, array<1 x i32>, array<1 x i32>)> 
    %120 = llvm.getelementptr %119[%115] : (!llvm.ptr<f32, 3>, i32) -> !llvm.ptr<f32, 3>
    %121 = llvm.load %120 : !llvm.ptr<f32, 3>
    %122 = llvm.fcmp "ugt" %118, %121 : f32
    %123 = llvm.select %122, %118, %121 : i1, f32
    %124 = llvm.fcmp "uno" %121, %121 : f32
    %125 = llvm.select %124, %121, %123 : i1, f32
    %126 = llvm.extractvalue %97[1] : !llvm.struct<(ptr<f32, 3>, ptr<f32, 3>, i32, array<1 x i32>, array<1 x i32>)> 
    %127 = llvm.getelementptr %126[%51] : (!llvm.ptr<f32, 3>, i32) -> !llvm.ptr<f32, 3>
    llvm.store %125, %127 : !llvm.ptr<f32, 3>
    llvm.br ^bb16
  ^bb16:  // 2 preds: ^bb14, ^bb15
    nvvm.barrier0
    %128 = llvm.icmp "eq" %48, %38 : i32
    %129 = llvm.and %128, %54  : i1
    llvm.cond_br %129, ^bb17, ^bb20
  ^bb17:  // pred: ^bb16
    %130 = llvm.add %51, %37  : i32
    %131 = llvm.extractvalue %97[1] : !llvm.struct<(ptr<f32, 3>, ptr<f32, 3>, i32, array<1 x i32>, array<1 x i32>)> 
    %132 = llvm.getelementptr %131[%51] : (!llvm.ptr<f32, 3>, i32) -> !llvm.ptr<f32, 3>
    %133 = llvm.load %132 : !llvm.ptr<f32, 3>
    %134 = llvm.extractvalue %97[1] : !llvm.struct<(ptr<f32, 3>, ptr<f32, 3>, i32, array<1 x i32>, array<1 x i32>)> 
    %135 = llvm.getelementptr %134[%130] : (!llvm.ptr<f32, 3>, i32) -> !llvm.ptr<f32, 3>
    %136 = llvm.load %135 : !llvm.ptr<f32, 3>
    %137 = llvm.fcmp "ugt" %133, %136 : f32
    %138 = llvm.select %137, %133, %136 : i1, f32
    %139 = llvm.fcmp "uno" %136, %136 : f32
    %140 = llvm.select %139, %136, %138 : i1, f32
    %141 = llvm.extractvalue %15[1] : !llvm.struct<(ptr<f32>, ptr<f32>, i32, array<1 x i32>, array<1 x i32>)> 
    %142 = llvm.getelementptr %141[%53] : (!llvm.ptr<f32>, i32) -> !llvm.ptr<f32>
    %143 = llvm.load %142 : !llvm.ptr<f32>
    llvm.br ^bb18(%143 : f32)
  ^bb18(%144: f32):  // 2 preds: ^bb17, ^bb18
    %145 = llvm.fcmp "ogt" %144, %140 : f32
    %146 = llvm.select %145, %144, %140 : i1, f32
    %147 = llvm.bitcast %142 : !llvm.ptr<f32> to !llvm.ptr<i32>
    %148 = llvm.bitcast %144 : f32 to i32
    %149 = llvm.bitcast %146 : f32 to i32
    %150 = llvm.cmpxchg %147, %148, %149 acq_rel monotonic : !llvm.ptr<i32>, i32
    %151 = llvm.extractvalue %150[0] : !llvm.struct<(i32, i1)> 
    %152 = llvm.bitcast %151 : i32 to f32
    %153 = llvm.extractvalue %150[1] : !llvm.struct<(i32, i1)> 
    llvm.cond_br %153, ^bb19, ^bb18(%152 : f32)
  ^bb19:  // pred: ^bb18
    llvm.br ^bb20
  ^bb20:  // 2 preds: ^bb16, ^bb19
    llvm.br ^bb21
  ^bb21:  // 2 preds: ^bb1, ^bb20
    llvm.return
  }
}

// -----// IR Dump After LLVMInsertValueSimplifierPass (disc-llvm-insert-value-simplifier) //----- //
llvm.func @main_kColReduction_reduce__3_1_0___8w32h_1(%arg0: i32, %arg1: i32, %arg2: !llvm.ptr<f32>, %arg3: !llvm.ptr<f32>, %arg4: i32, %arg5: i32, %arg6: i32, %arg7: i32, %arg8: i32, %arg9: i32, %arg10: i32, %arg11: !llvm.ptr<f32>, %arg12: !llvm.ptr<f32>, %arg13: i32, %arg14: i32, %arg15: i32) attributes {gpu.kernel, nvvm.kernel} {
  %0 = llvm.mlir.constant(41 : index) : i32
  %1 = llvm.mlir.constant(32 : index) : i32
  %2 = llvm.mlir.constant(8 : index) : i32
  %3 = llvm.mlir.constant(1300 : index) : i32
  %4 = llvm.mlir.constant(64 : index) : i32
  %5 = llvm.mlir.constant(110 : index) : i32
  %6 = llvm.mlir.constant(0xFF800000 : f32) : f32
  %7 = llvm.mlir.constant(4 : index) : i32
  %8 = llvm.mlir.constant(2 : index) : i32
  %9 = llvm.mlir.constant(1 : index) : i32
  %10 = llvm.mlir.constant(256 : index) : i32
  %11 = llvm.mlir.constant(0 : index) : i32
  %12 = llvm.mlir.addressof @__wg_main_kColReduction_reduce__3_1_0___8w32h_1_0 : !llvm.ptr<array<256 x f32>, 3>
  %13 = llvm.getelementptr %12[0, 0] : (!llvm.ptr<array<256 x f32>, 3>) -> !llvm.ptr<f32, 3>
  %14 = nvvm.read.ptx.sreg.ctaid.x : i32
  %15 = nvvm.read.ptx.sreg.tid.x : i32
  llvm.br ^bb1
^bb1:  // pred: ^bb0
  %16 = llvm.mul %14, %arg0  : i32
  %17 = llvm.add %15, %16  : i32
  %18 = llvm.icmp "ult" %17, %arg1 : i32
  llvm.cond_br %18, ^bb2, ^bb21
^bb2:  // pred: ^bb1
  %19 = llvm.srem %17, %10  : i32
  %20 = llvm.sdiv %17, %10  : i32
  %21 = llvm.udiv %20, %0  : i32
  %22 = llvm.urem %20, %0  : i32
  %23 = llvm.udiv %19, %1  : i32
  %24 = llvm.urem %19, %1  : i32
  %25 = llvm.mul %24, %2  : i32
  %26 = llvm.add %23, %25  : i32
  %27 = llvm.mul %22, %1  : i32
  %28 = llvm.add %24, %27  : i32
  %29 = llvm.icmp "ult" %28, %3 : i32
  llvm.cond_br %29, ^bb3, ^bb10(%6 : f32)
^bb3:  // pred: ^bb2
  %30 = llvm.mul %21, %2  : i32
  %31 = llvm.add %23, %30  : i32
  %32 = llvm.mul %31, %4  : i32
  llvm.br ^bb4(%11, %6 : i32, f32)
^bb4(%33: i32, %34: f32):  // 2 preds: ^bb3, ^bb9
  %35 = llvm.icmp "slt" %33, %4 : i32
  llvm.cond_br %35, ^bb5, ^bb10(%34 : f32)
^bb5:  // pred: ^bb4
  %36 = llvm.add %33, %32  : i32
  %37 = llvm.icmp "slt" %36, %5 : i32
  llvm.cond_br %37, ^bb6, ^bb7
^bb6:  // pred: ^bb5
  %38 = llvm.mul %36, %3  : i32
  %39 = llvm.add %38, %28  : i32
  %40 = llvm.getelementptr %arg3[%39] : (!llvm.ptr<f32>, i32) -> !llvm.ptr<f32>
  %41 = llvm.load %40 : !llvm.ptr<f32>
  %42 = llvm.fcmp "ugt" %34, %41 : f32
  %43 = llvm.select %42, %34, %41 : i1, f32
  %44 = llvm.fcmp "uno" %41, %41 : f32
  %45 = llvm.select %44, %41, %43 : i1, f32
  llvm.br ^bb8(%45 : f32)
^bb7:  // pred: ^bb5
  llvm.br ^bb8(%34 : f32)
^bb8(%46: f32):  // 2 preds: ^bb6, ^bb7
  llvm.br ^bb9
^bb9:  // pred: ^bb8
  %47 = llvm.add %33, %9  : i32
  llvm.br ^bb4(%47, %46 : i32, f32)
^bb10(%48: f32):  // 2 preds: ^bb2, ^bb4
  llvm.br ^bb11(%48 : f32)
^bb11(%49: f32):  // pred: ^bb10
  llvm.br ^bb12
^bb12:  // pred: ^bb11
  %50 = llvm.getelementptr %13[%26] : (!llvm.ptr<f32, 3>, i32) -> !llvm.ptr<f32, 3>
  llvm.store %49, %50 : !llvm.ptr<f32, 3>
  nvvm.barrier0
  %51 = llvm.icmp "slt" %23, %7 : i32
  llvm.cond_br %51, ^bb13, ^bb14
^bb13:  // pred: ^bb12
  %52 = llvm.add %26, %7  : i32
  %53 = llvm.getelementptr %13[%26] : (!llvm.ptr<f32, 3>, i32) -> !llvm.ptr<f32, 3>
  %54 = llvm.load %53 : !llvm.ptr<f32, 3>
  %55 = llvm.getelementptr %13[%52] : (!llvm.ptr<f32, 3>, i32) -> !llvm.ptr<f32, 3>
  %56 = llvm.load %55 : !llvm.ptr<f32, 3>
  %57 = llvm.fcmp "ugt" %54, %56 : f32
  %58 = llvm.select %57, %54, %56 : i1, f32
  %59 = llvm.fcmp "uno" %56, %56 : f32
  %60 = llvm.select %59, %56, %58 : i1, f32
  %61 = llvm.getelementptr %13[%26] : (!llvm.ptr<f32, 3>, i32) -> !llvm.ptr<f32, 3>
  llvm.store %60, %61 : !llvm.ptr<f32, 3>
  llvm.br ^bb14
^bb14:  // 2 preds: ^bb12, ^bb13
  nvvm.barrier0
  %62 = llvm.icmp "slt" %23, %8 : i32
  llvm.cond_br %62, ^bb15, ^bb16
^bb15:  // pred: ^bb14
  %63 = llvm.add %26, %8  : i32
  %64 = llvm.getelementptr %13[%26] : (!llvm.ptr<f32, 3>, i32) -> !llvm.ptr<f32, 3>
  %65 = llvm.load %64 : !llvm.ptr<f32, 3>
  %66 = llvm.getelementptr %13[%63] : (!llvm.ptr<f32, 3>, i32) -> !llvm.ptr<f32, 3>
  %67 = llvm.load %66 : !llvm.ptr<f32, 3>
  %68 = llvm.fcmp "ugt" %65, %67 : f32
  %69 = llvm.select %68, %65, %67 : i1, f32
  %70 = llvm.fcmp "uno" %67, %67 : f32
  %71 = llvm.select %70, %67, %69 : i1, f32
  %72 = llvm.getelementptr %13[%26] : (!llvm.ptr<f32, 3>, i32) -> !llvm.ptr<f32, 3>
  llvm.store %71, %72 : !llvm.ptr<f32, 3>
  llvm.br ^bb16
^bb16:  // 2 preds: ^bb14, ^bb15
  nvvm.barrier0
  %73 = llvm.icmp "eq" %23, %11 : i32
  %74 = llvm.and %73, %29  : i1
  llvm.cond_br %74, ^bb17, ^bb20
^bb17:  // pred: ^bb16
  %75 = llvm.add %26, %9  : i32
  %76 = llvm.getelementptr %13[%26] : (!llvm.ptr<f32, 3>, i32) -> !llvm.ptr<f32, 3>
  %77 = llvm.load %76 : !llvm.ptr<f32, 3>
  %78 = llvm.getelementptr %13[%75] : (!llvm.ptr<f32, 3>, i32) -> !llvm.ptr<f32, 3>
  %79 = llvm.load %78 : !llvm.ptr<f32, 3>
  %80 = llvm.fcmp "ugt" %77, %79 : f32
  %81 = llvm.select %80, %77, %79 : i1, f32
  %82 = llvm.fcmp "uno" %79, %79 : f32
  %83 = llvm.select %82, %79, %81 : i1, f32
  %84 = llvm.getelementptr %arg12[%28] : (!llvm.ptr<f32>, i32) -> !llvm.ptr<f32>
  %85 = llvm.load %84 : !llvm.ptr<f32>
  llvm.br ^bb18(%85 : f32)
^bb18(%86: f32):  // 2 preds: ^bb17, ^bb18
  %87 = llvm.fcmp "ogt" %86, %83 : f32
  %88 = llvm.select %87, %86, %83 : i1, f32
  %89 = llvm.bitcast %84 : !llvm.ptr<f32> to !llvm.ptr<i32>
  %90 = llvm.bitcast %86 : f32 to i32
  %91 = llvm.bitcast %88 : f32 to i32
  %92 = llvm.cmpxchg %89, %90, %91 acq_rel monotonic : !llvm.ptr<i32>, i32
  %93 = llvm.extractvalue %92[0] : !llvm.struct<(i32, i1)> 
  %94 = llvm.bitcast %93 : i32 to f32
  %95 = llvm.extractvalue %92[1] : !llvm.struct<(i32, i1)> 
  llvm.cond_br %95, ^bb19, ^bb18(%94 : f32)
^bb19:  // pred: ^bb18
  llvm.br ^bb20
^bb20:  // 2 preds: ^bb16, ^bb19
  llvm.br ^bb21
^bb21:  // 2 preds: ^bb1, ^bb20
  llvm.return
}

// -----// IR Dump After FunctionDeadArgumentEliminationPass (disc-function-dead-argument-elimination) //----- //
gpu.module @main_kernel_0 {
  llvm.mlir.global internal @__wg_main_kColReduction_reduce__3_1_0___8w32h_1_0() {addr_space = 3 : i32} : !llvm.array<256 x f32>
  llvm.func @main_kColReduction_reduce__3_1_0___8w32h_1(%arg0: i32, %arg1: i32, %arg2: !llvm.ptr<f32>, %arg3: !llvm.ptr<f32>) attributes {disc.elimargs = [2 : index, 4 : index, 5 : index, 6 : index, 7 : index, 8 : index, 9 : index, 10 : index, 11 : index, 13 : index, 14 : index, 15 : index], gpu.kernel, nvvm.kernel} {
    %0 = llvm.mlir.constant(41 : index) : i32
    %1 = llvm.mlir.constant(32 : index) : i32
    %2 = llvm.mlir.constant(8 : index) : i32
    %3 = llvm.mlir.constant(1300 : index) : i32
    %4 = llvm.mlir.constant(64 : index) : i32
    %5 = llvm.mlir.constant(110 : index) : i32
    %6 = llvm.mlir.constant(0xFF800000 : f32) : f32
    %7 = llvm.mlir.constant(4 : index) : i32
    %8 = llvm.mlir.constant(2 : index) : i32
    %9 = llvm.mlir.constant(1 : index) : i32
    %10 = llvm.mlir.constant(256 : index) : i32
    %11 = llvm.mlir.constant(0 : index) : i32
    %12 = llvm.mlir.addressof @__wg_main_kColReduction_reduce__3_1_0___8w32h_1_0 : !llvm.ptr<array<256 x f32>, 3>
    %13 = llvm.getelementptr %12[0, 0] : (!llvm.ptr<array<256 x f32>, 3>) -> !llvm.ptr<f32, 3>
    %14 = nvvm.read.ptx.sreg.ctaid.x : i32
    %15 = nvvm.read.ptx.sreg.tid.x : i32
    llvm.br ^bb1
  ^bb1:  // pred: ^bb0
    %16 = llvm.mul %14, %arg0  : i32
    %17 = llvm.add %15, %16  : i32
    %18 = llvm.icmp "ult" %17, %arg1 : i32
    llvm.cond_br %18, ^bb2, ^bb21
  ^bb2:  // pred: ^bb1
    %19 = llvm.srem %17, %10  : i32
    %20 = llvm.sdiv %17, %10  : i32
    %21 = llvm.udiv %20, %0  : i32
    %22 = llvm.urem %20, %0  : i32
    %23 = llvm.udiv %19, %1  : i32
    %24 = llvm.urem %19, %1  : i32
    %25 = llvm.mul %24, %2  : i32
    %26 = llvm.add %23, %25  : i32
    %27 = llvm.mul %22, %1  : i32
    %28 = llvm.add %24, %27  : i32
    %29 = llvm.icmp "ult" %28, %3 : i32
    llvm.cond_br %29, ^bb3, ^bb10(%6 : f32)
  ^bb3:  // pred: ^bb2
    %30 = llvm.mul %21, %2  : i32
    %31 = llvm.add %23, %30  : i32
    %32 = llvm.mul %31, %4  : i32
    llvm.br ^bb4(%11, %6 : i32, f32)
  ^bb4(%33: i32, %34: f32):  // 2 preds: ^bb3, ^bb9
    %35 = llvm.icmp "slt" %33, %4 : i32
    llvm.cond_br %35, ^bb5, ^bb10(%34 : f32)
  ^bb5:  // pred: ^bb4
    %36 = llvm.add %33, %32  : i32
    %37 = llvm.icmp "slt" %36, %5 : i32
    llvm.cond_br %37, ^bb6, ^bb7
  ^bb6:  // pred: ^bb5
    %38 = llvm.mul %36, %3  : i32
    %39 = llvm.add %38, %28  : i32
    %40 = llvm.getelementptr %arg2[%39] : (!llvm.ptr<f32>, i32) -> !llvm.ptr<f32>
    %41 = llvm.load %40 : !llvm.ptr<f32>
    %42 = llvm.fcmp "ugt" %34, %41 : f32
    %43 = llvm.select %42, %34, %41 : i1, f32
    %44 = llvm.fcmp "uno" %41, %41 : f32
    %45 = llvm.select %44, %41, %43 : i1, f32
    llvm.br ^bb8(%45 : f32)
  ^bb7:  // pred: ^bb5
    llvm.br ^bb8(%34 : f32)
  ^bb8(%46: f32):  // 2 preds: ^bb6, ^bb7
    llvm.br ^bb9
  ^bb9:  // pred: ^bb8
    %47 = llvm.add %33, %9  : i32
    llvm.br ^bb4(%47, %46 : i32, f32)
  ^bb10(%48: f32):  // 2 preds: ^bb2, ^bb4
    llvm.br ^bb11(%48 : f32)
  ^bb11(%49: f32):  // pred: ^bb10
    llvm.br ^bb12
  ^bb12:  // pred: ^bb11
    %50 = llvm.getelementptr %13[%26] : (!llvm.ptr<f32, 3>, i32) -> !llvm.ptr<f32, 3>
    llvm.store %49, %50 : !llvm.ptr<f32, 3>
    nvvm.barrier0
    %51 = llvm.icmp "slt" %23, %7 : i32
    llvm.cond_br %51, ^bb13, ^bb14
  ^bb13:  // pred: ^bb12
    %52 = llvm.add %26, %7  : i32
    %53 = llvm.getelementptr %13[%26] : (!llvm.ptr<f32, 3>, i32) -> !llvm.ptr<f32, 3>
    %54 = llvm.load %53 : !llvm.ptr<f32, 3>
    %55 = llvm.getelementptr %13[%52] : (!llvm.ptr<f32, 3>, i32) -> !llvm.ptr<f32, 3>
    %56 = llvm.load %55 : !llvm.ptr<f32, 3>
    %57 = llvm.fcmp "ugt" %54, %56 : f32
    %58 = llvm.select %57, %54, %56 : i1, f32
    %59 = llvm.fcmp "uno" %56, %56 : f32
    %60 = llvm.select %59, %56, %58 : i1, f32
    %61 = llvm.getelementptr %13[%26] : (!llvm.ptr<f32, 3>, i32) -> !llvm.ptr<f32, 3>
    llvm.store %60, %61 : !llvm.ptr<f32, 3>
    llvm.br ^bb14
  ^bb14:  // 2 preds: ^bb12, ^bb13
    nvvm.barrier0
    %62 = llvm.icmp "slt" %23, %8 : i32
    llvm.cond_br %62, ^bb15, ^bb16
  ^bb15:  // pred: ^bb14
    %63 = llvm.add %26, %8  : i32
    %64 = llvm.getelementptr %13[%26] : (!llvm.ptr<f32, 3>, i32) -> !llvm.ptr<f32, 3>
    %65 = llvm.load %64 : !llvm.ptr<f32, 3>
    %66 = llvm.getelementptr %13[%63] : (!llvm.ptr<f32, 3>, i32) -> !llvm.ptr<f32, 3>
    %67 = llvm.load %66 : !llvm.ptr<f32, 3>
    %68 = llvm.fcmp "ugt" %65, %67 : f32
    %69 = llvm.select %68, %65, %67 : i1, f32
    %70 = llvm.fcmp "uno" %67, %67 : f32
    %71 = llvm.select %70, %67, %69 : i1, f32
    %72 = llvm.getelementptr %13[%26] : (!llvm.ptr<f32, 3>, i32) -> !llvm.ptr<f32, 3>
    llvm.store %71, %72 : !llvm.ptr<f32, 3>
    llvm.br ^bb16
  ^bb16:  // 2 preds: ^bb14, ^bb15
    nvvm.barrier0
    %73 = llvm.icmp "eq" %23, %11 : i32
    %74 = llvm.and %73, %29  : i1
    llvm.cond_br %74, ^bb17, ^bb20
  ^bb17:  // pred: ^bb16
    %75 = llvm.add %26, %9  : i32
    %76 = llvm.getelementptr %13[%26] : (!llvm.ptr<f32, 3>, i32) -> !llvm.ptr<f32, 3>
    %77 = llvm.load %76 : !llvm.ptr<f32, 3>
    %78 = llvm.getelementptr %13[%75] : (!llvm.ptr<f32, 3>, i32) -> !llvm.ptr<f32, 3>
    %79 = llvm.load %78 : !llvm.ptr<f32, 3>
    %80 = llvm.fcmp "ugt" %77, %79 : f32
    %81 = llvm.select %80, %77, %79 : i1, f32
    %82 = llvm.fcmp "uno" %79, %79 : f32
    %83 = llvm.select %82, %79, %81 : i1, f32
    %84 = llvm.getelementptr %arg3[%28] : (!llvm.ptr<f32>, i32) -> !llvm.ptr<f32>
    %85 = llvm.load %84 : !llvm.ptr<f32>
    llvm.br ^bb18(%85 : f32)
  ^bb18(%86: f32):  // 2 preds: ^bb17, ^bb18
    %87 = llvm.fcmp "ogt" %86, %83 : f32
    %88 = llvm.select %87, %86, %83 : i1, f32
    %89 = llvm.bitcast %84 : !llvm.ptr<f32> to !llvm.ptr<i32>
    %90 = llvm.bitcast %86 : f32 to i32
    %91 = llvm.bitcast %88 : f32 to i32
    %92 = llvm.cmpxchg %89, %90, %91 acq_rel monotonic : !llvm.ptr<i32>, i32
    %93 = llvm.extractvalue %92[0] : !llvm.struct<(i32, i1)> 
    %94 = llvm.bitcast %93 : i32 to f32
    %95 = llvm.extractvalue %92[1] : !llvm.struct<(i32, i1)> 
    llvm.cond_br %95, ^bb19, ^bb18(%94 : f32)
  ^bb19:  // pred: ^bb18
    llvm.br ^bb20
  ^bb20:  // 2 preds: ^bb16, ^bb19
    llvm.br ^bb21
  ^bb21:  // 2 preds: ^bb1, ^bb20
    llvm.return
  }
}

// -----// IR Dump After GpuKernelToBlobPass (disc-gpu-kernel-to-blob) //----- //
gpu.module @main_kernel_0 attributes {gpu.binary = "P\EDU\BA\01\00\10\00\D8\08\00\00\00\00\00\00\02\00\01\01@\00\00\00\98\08\00\00\00\00\00\00\91\08\00\00\00\00\00\00\07\00\01\00P\00\00\00\00\00\00\00\00\00\00\00\11 \00\00\00\00\00\00\00\00\00\00\00\00\00\00\A8\14\00\00\00\00\00\00\A2\7FELF\02\01\013\07\00\01\00g\02\00\BE\00q\00\01\00\22\14\00\01\00\11\11\06\00\F5\0E\00P\05P\00@\008\00\03\00@\00\0C\00\01\00\00.shstrtab\00.\08\00'ym\08\00\F0\1B_shndx\00.nv.info\00.text.main_kColReduction_r\0A\00\F5\03e__3_1_0___8w32h_1:\00\0F4\00\1Doshared6\00\1AOrela\A0\00\1F?rel\D5\00\22\9Fconstant09\00\1A\B2debug_frame{\00\09\11\00!nv\14\00\11aE\00\0F\9E\01 \0F\8A\00\17\0F\C9\01\F4\8F$____wg_3\00\17\00\0C\00/24\02\02'o_param\09\02\1C\0F\01\00\05\8C]\00\00\00\03\00\0A\00\01\00\11\C2\18\00,\0B\00\01\00 \9C\01\18\00,\09\00\01\00\11\DC\18\00,\04\00\01\00\11\FA\18\00,\07\00\01\00f2\00\00\00\12\10x\00!\80\08\07\00v\00\FF\FF\FF\FF(\00\0C\00\00\01\00@\03\00\04|\08\00\D1\0F\0C\81\80\80(\00\08\FF\81\80(\08\0B\00\06(\00/0\00\01\00\03\13pX\00\10\04\9B\00\10\04\CD\04\02E\002\04\D4\01\18\00C/\08\00\06\DF\04\22\04#\0C\00\00\01\00'\04\12\0C\00\17\11\0C\00!7\04 \05\F1\08\015\00\00\04\0A\08\00\03\00\00\00`\01\18\00\03\19\18\00\04\17\0C$\00u\03\00\10\00\00\F0!\10\009\02\00\08\10\00\10\01(\01%\F0\11\10\00\01\01\00\F2\0A\F0\11\00\03\1B\FF\00\04\1C\0C\00P\00\00\00\B0\06\00\00\B0\07\00\00\04\1ET\01#K\00\01\00\B1\02\02\08\10\0A/\22\00\00\00\08\10\00#\00\08\08\00\13\10\08\00\13\18\08\00\13 \08\00\13(\08\00\130\08\00\228\08\84\00\13\00\08\00\13\08\08\00\13\10\08\00\13\18\08\00\13 \08\00\13(\08\00\130\08\00\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\00Z\06b,\00\00\00H\00\01\005\02\00\00d\01\0F\01\00\FF\DA@$v\01\FF\7F\04\B2\FF\00\8E\07\00\C4\0F\00\19y\00\01\00\10%\BB\02a\0E\00\19y\03\00\01\00\10!-\00\F0\04\0E\00$z\00\00\00X\00\00\03\02\8E\07\00\CA\1F\00\0C\10\00\C5Y\00\00p`\F0\03\00\DA\0F\00M\9B\04\B0\80\03\00\EA\0F\00\19x\03\FF\1F\0F\00\F1\00\14\01\00\00\E2\0F\00\B9z\04\00\00F\00\00\F3\04\93\E2\0F\00Ey\00\00\C0\040\00\B1\E2\0F\00$t\05\FF\00\00\80\FF\90\00a\E2\0F\00\11r\03\81\00`\FF@\8F\07\00\C8P\00 \04\FF\C1\02\10\03P\00q\E4\0F\00\12x\03\03\81\04\F1\00\C0\8E\07\00\C6\0F\00'x\02\04}\0C\CE\C7@\00P\C8\0F\00$x4\03B\00\00\03\0AP\00P\19x\07\FF\05\F0\020\16\01\00 \00@\12x\08\00\A0\00\01@\00\D4\E2\0F\04$x\03\07\D7\FF\FF\FF\04\020\00\11\060\00\22\00\16p\00P\0Cx\00\00\7F\D8\04\22@\F4\B0\00@x\03\03 \06\04\000\00\10\E4\10\00\11\02\91\03\22\06\02\90\00`\0Cx\00\03\13\050\00\13\F20\00C\02\02\04\00p\01\96\D8\0F\00G\19\00\00\B0\030\01\95$x\0A\06\00E\01\00\FFP\00%\06\07P\00\10\E4\A0\00t\05\07\E0\22\0A\00\0A \00Dr\07\FF\FFP\00\010\00 \09\04\90\00\14\05 \00\08P\01\10\C4 \00 \04\08\10\01\16\09`\00$\06@@\00\11\CA\B0\00 \06m\E0\00!B\FC\D0\01@$\E8\08\040\00\14\070\005\E4\09\FF\C0\00\93\C8\0F\00%\E6\08\08\00ZP\00p\CA\0F\00\81\E9\0C\08 \00\C0\00\19\1E\0C\00\A2\00\00\10x\0A\06@\000\FF\E0\FF@\00f\04\10x\0D\06\02\10\00\00p\00\12\0Ap\00#\F8\03\10\00\12\0D\10\00\11\F6\10\001\1Cx\00\01\00\F1\04p\F0\F0\03\00\D2\0F\00\10\C8\0A\04\14\05\00\00\07\E0\FF\A0\01:$\C4\0B\90\000\C6\0A\0A\90\00\14\0B\90\005\C9\0E\0A\90\00\94\E2\02\00\10\B8\08\04(\0A@\00G\1F\00$\B4\D0\000\E2\0F\00\A0\00\14\03\A0\00W\C6\0F\00%\B6\E0\00f\CC\0F\00\81\B9\08\E0\00q\22\0F\00\0B\E2\00\0CF\06\A4\80\FA\03\00\C8O\00\1C\E8\00\B0\001\02\00\E4 \00\11\05 \00!\C0\FAP\01A\08\82\0C\0CP\02:\00\80\06\F0\00\02 \00g\10\D8\0D\04<\0F\E0\00'\D4\0A\E0\00`/\00%\D6\0A\0D\90\00\14\0A\90\00&\D9\0A\E0\00a\A2\0E\00$\E2\05\10\02\13\0C\D0\00Q\0B\C2\00\0E\0Eo\00i\FC\03\00\E4\8F\08P\01\00`\01\14\C8\10\00\11p\10\00A\0B\C2\00\050\00#\C0\FC\B0\00\22\0E\0E\B0\00\11\00\B0\01#\0B\B2q\06\00P\00R\C6\0F\01$\C2p\00\13\0Ep\00\0D`\00\1B\B8`\001\B2\00\05@\00\12\C0\F0\01c\10x\07\07P\14p\01\10\D6p\00#\08\08p\00\10\06\C0\01\22$\B2`\00\1F\08`\00\04W\10x\06\06\04P\02A\0B\D2\00\0A\D6\08\10\80`\00aO\08\0B\D2\00\05\10\00\13\C0P\02$\D8\00\90\00\13\02\80\01\10\07p\03BpR\F8\03\80\00\22\0A\0A\80\00!\80\05\80\00\12\D2\80\00\13\0A\80\00`GI\00\00\D0\FCq\04\11\83\B0\03\14A0\05\03\C0\03A\88s\00\02@\00\00\9E\07f\E8\0F\00\1D{\00\01\00\93\EC\0F\00\84\A9\07\02\00\10 \00*\22\0E\D0\00\11\C4\90\00\01\90\041pD\F2 \05T\84\A9\04\02\00P\00qb\0E\00\0B\A2\00\07\CE\09\10\80\D0\00u\1F\08\0B\A2\00\04\07P\01T/\00\1C\A8\00P\00\13\01\E0\00 \00?\10\00#@\F6\E0\00!\07\070\02\00`\01\1B\E4P\01\8F\C6\0F\00\88\A3\00\02\07\C0\00\09!\B9\05O\09\22\00\08 \06+\84\B9\A0\001\B2\00\05\00\01`\80\F4\03\00\C4\1F\10\00(\04\05\A0\00\06\10\02\12\01`\027\05\05\04\F0\01/\88\B3@\01\0B9M\19\00p\019$t\04\C0\03!\84y\CB\08\02@\00\C3\E6\0F\00%v\04\03\00\\\00\00\04\00\05f\84y\03\02\00\04\C0\00E\81y\06\04@\03bb\03\00\0Br\00q\06\22\80\F0`\011r\00\00\10\00#\C0\F2@\01\02\1F\00@\00\00\80\04\90\031\0Br\000\040\00@\F0\E0\05(\0EF\10\02b\E6\0F\00\08r\07 \00\00\01\00p\CC\0F\00\A9s\07\04h\09\C0\07\E1\1E\00\00\A4\0E\00\0Cr\00\07\10\00 pR@\00QO\00$r\06p\02\14\07 \06V\09\00\00\90\FFp\02\1BMp\02TGy\00\00\F0 \00f\C0\0F\00\18y\00\01\00\0F\10\00\90\0F\01\00-#\01\00\A0\01\0B\01\00\22@\00\01\00=\9E\01\000\00\08\01\00\1F\0B@\00\04\13\DE)\00?\09\02\00@\00\0A\13\13<\0B\0C\01\00\13\E8U\00\03\8F\03\01$\00\13\05w\02\00\01\00\22\18\00\01\00.q\01T\00\00\01\00\11\90\95\02O\00\00p\00\80\00\0B\1F)'\00\03\02m\00\00v\0C\01\06\00\06\E4\00*\04\00\01\00\1Fc@\00\04\130@\00\17x@\00\1F\0A@\00\00!\8F\01D\01\0D@\00\13\A8@\00*\D8\00\01\00\1B\08\08\00?~\01\00\CE\0E\00\01\1E\07\01\01\00&\10\00\80\00\17\048\00\04\18\00\138@\01\0C\84\01\13\90@\00\17x1\01\0F\C0\00\01\132T\01\15\06R\00\0A\10\0F&\80\08\80\00j\06\00\00\11\80\00\01\00\13\97\94\00+\03\00\01\00\03\B0\13/\00\04\80\00\0B\14\06\AB\01/\14\00\01\00\02\1B\A8\08\00\17\08\08\02\17\05\E8\00\0C\01\009p\0A\00\08\00\088\00\18\06\A0\00\0F\01\00\05\03\A9\00\80\08\00\00\00\00\00\00\00\00\00\00\00\00\00\00"} {
  llvm.mlir.global internal @__wg_main_kColReduction_reduce__3_1_0___8w32h_1_0() {addr_space = 3 : i32} : !llvm.array<256 x f32>
  llvm.func @main_kColReduction_reduce__3_1_0___8w32h_1(%arg0: i32, %arg1: i32, %arg2: !llvm.ptr<f32>, %arg3: !llvm.ptr<f32>) attributes {disc.elimargs = [2 : index, 4 : index, 5 : index, 6 : index, 7 : index, 8 : index, 9 : index, 10 : index, 11 : index, 13 : index, 14 : index, 15 : index], gpu.kernel, nvvm.kernel} {
    %0 = llvm.mlir.constant(41 : index) : i32
    %1 = llvm.mlir.constant(32 : index) : i32
    %2 = llvm.mlir.constant(8 : index) : i32
    %3 = llvm.mlir.constant(1300 : index) : i32
    %4 = llvm.mlir.constant(64 : index) : i32
    %5 = llvm.mlir.constant(110 : index) : i32
    %6 = llvm.mlir.constant(0xFF800000 : f32) : f32
    %7 = llvm.mlir.constant(4 : index) : i32
    %8 = llvm.mlir.constant(2 : index) : i32
    %9 = llvm.mlir.constant(1 : index) : i32
    %10 = llvm.mlir.constant(256 : index) : i32
    %11 = llvm.mlir.constant(0 : index) : i32
    %12 = llvm.mlir.addressof @__wg_main_kColReduction_reduce__3_1_0___8w32h_1_0 : !llvm.ptr<array<256 x f32>, 3>
    %13 = llvm.getelementptr %12[0, 0] : (!llvm.ptr<array<256 x f32>, 3>) -> !llvm.ptr<f32, 3>
    %14 = nvvm.read.ptx.sreg.ctaid.x : i32
    %15 = nvvm.read.ptx.sreg.tid.x : i32
    llvm.br ^bb1
  ^bb1:  // pred: ^bb0
    %16 = llvm.mul %14, %arg0  : i32
    %17 = llvm.add %15, %16  : i32
    %18 = llvm.icmp "ult" %17, %arg1 : i32
    llvm.cond_br %18, ^bb2, ^bb21
  ^bb2:  // pred: ^bb1
    %19 = llvm.srem %17, %10  : i32
    %20 = llvm.sdiv %17, %10  : i32
    %21 = llvm.udiv %20, %0  : i32
    %22 = llvm.urem %20, %0  : i32
    %23 = llvm.udiv %19, %1  : i32
    %24 = llvm.urem %19, %1  : i32
    %25 = llvm.mul %24, %2  : i32
    %26 = llvm.add %23, %25  : i32
    %27 = llvm.mul %22, %1  : i32
    %28 = llvm.add %24, %27  : i32
    %29 = llvm.icmp "ult" %28, %3 : i32
    llvm.cond_br %29, ^bb3, ^bb10(%6 : f32)
  ^bb3:  // pred: ^bb2
    %30 = llvm.mul %21, %2  : i32
    %31 = llvm.add %23, %30  : i32
    %32 = llvm.mul %31, %4  : i32
    llvm.br ^bb4(%11, %6 : i32, f32)
  ^bb4(%33: i32, %34: f32):  // 2 preds: ^bb3, ^bb9
    %35 = llvm.icmp "slt" %33, %4 : i32
    llvm.cond_br %35, ^bb5, ^bb10(%34 : f32)
  ^bb5:  // pred: ^bb4
    %36 = llvm.add %33, %32  : i32
    %37 = llvm.icmp "slt" %36, %5 : i32
    llvm.cond_br %37, ^bb6, ^bb7
  ^bb6:  // pred: ^bb5
    %38 = llvm.mul %36, %3  : i32
    %39 = llvm.add %38, %28  : i32
    %40 = llvm.getelementptr %arg2[%39] : (!llvm.ptr<f32>, i32) -> !llvm.ptr<f32>
    %41 = llvm.load %40 : !llvm.ptr<f32>
    %42 = llvm.fcmp "ugt" %34, %41 : f32
    %43 = llvm.select %42, %34, %41 : i1, f32
    %44 = llvm.fcmp "uno" %41, %41 : f32
    %45 = llvm.select %44, %41, %43 : i1, f32
    llvm.br ^bb8(%45 : f32)
  ^bb7:  // pred: ^bb5
    llvm.br ^bb8(%34 : f32)
  ^bb8(%46: f32):  // 2 preds: ^bb6, ^bb7
    llvm.br ^bb9
  ^bb9:  // pred: ^bb8
    %47 = llvm.add %33, %9  : i32
    llvm.br ^bb4(%47, %46 : i32, f32)
  ^bb10(%48: f32):  // 2 preds: ^bb2, ^bb4
    llvm.br ^bb11(%48 : f32)
  ^bb11(%49: f32):  // pred: ^bb10
    llvm.br ^bb12
  ^bb12:  // pred: ^bb11
    %50 = llvm.getelementptr %13[%26] : (!llvm.ptr<f32, 3>, i32) -> !llvm.ptr<f32, 3>
    llvm.store %49, %50 : !llvm.ptr<f32, 3>
    nvvm.barrier0
    %51 = llvm.icmp "slt" %23, %7 : i32
    llvm.cond_br %51, ^bb13, ^bb14
  ^bb13:  // pred: ^bb12
    %52 = llvm.add %26, %7  : i32
    %53 = llvm.getelementptr %13[%26] : (!llvm.ptr<f32, 3>, i32) -> !llvm.ptr<f32, 3>
    %54 = llvm.load %53 : !llvm.ptr<f32, 3>
    %55 = llvm.getelementptr %13[%52] : (!llvm.ptr<f32, 3>, i32) -> !llvm.ptr<f32, 3>
    %56 = llvm.load %55 : !llvm.ptr<f32, 3>
    %57 = llvm.fcmp "ugt" %54, %56 : f32
    %58 = llvm.select %57, %54, %56 : i1, f32
    %59 = llvm.fcmp "uno" %56, %56 : f32
    %60 = llvm.select %59, %56, %58 : i1, f32
    %61 = llvm.getelementptr %13[%26] : (!llvm.ptr<f32, 3>, i32) -> !llvm.ptr<f32, 3>
    llvm.store %60, %61 : !llvm.ptr<f32, 3>
    llvm.br ^bb14
  ^bb14:  // 2 preds: ^bb12, ^bb13
    nvvm.barrier0
    %62 = llvm.icmp "slt" %23, %8 : i32
    llvm.cond_br %62, ^bb15, ^bb16
  ^bb15:  // pred: ^bb14
    %63 = llvm.add %26, %8  : i32
    %64 = llvm.getelementptr %13[%26] : (!llvm.ptr<f32, 3>, i32) -> !llvm.ptr<f32, 3>
    %65 = llvm.load %64 : !llvm.ptr<f32, 3>
    %66 = llvm.getelementptr %13[%63] : (!llvm.ptr<f32, 3>, i32) -> !llvm.ptr<f32, 3>
    %67 = llvm.load %66 : !llvm.ptr<f32, 3>
    %68 = llvm.fcmp "ugt" %65, %67 : f32
    %69 = llvm.select %68, %65, %67 : i1, f32
    %70 = llvm.fcmp "uno" %67, %67 : f32
    %71 = llvm.select %70, %67, %69 : i1, f32
    %72 = llvm.getelementptr %13[%26] : (!llvm.ptr<f32, 3>, i32) -> !llvm.ptr<f32, 3>
    llvm.store %71, %72 : !llvm.ptr<f32, 3>
    llvm.br ^bb16
  ^bb16:  // 2 preds: ^bb14, ^bb15
    nvvm.barrier0
    %73 = llvm.icmp "eq" %23, %11 : i32
    %74 = llvm.and %73, %29  : i1
    llvm.cond_br %74, ^bb17, ^bb20
  ^bb17:  // pred: ^bb16
    %75 = llvm.add %26, %9  : i32
    %76 = llvm.getelementptr %13[%26] : (!llvm.ptr<f32, 3>, i32) -> !llvm.ptr<f32, 3>
    %77 = llvm.load %76 : !llvm.ptr<f32, 3>
    %78 = llvm.getelementptr %13[%75] : (!llvm.ptr<f32, 3>, i32) -> !llvm.ptr<f32, 3>
    %79 = llvm.load %78 : !llvm.ptr<f32, 3>
    %80 = llvm.fcmp "ugt" %77, %79 : f32
    %81 = llvm.select %80, %77, %79 : i1, f32
    %82 = llvm.fcmp "uno" %79, %79 : f32
    %83 = llvm.select %82, %79, %81 : i1, f32
    %84 = llvm.getelementptr %arg3[%28] : (!llvm.ptr<f32>, i32) -> !llvm.ptr<f32>
    %85 = llvm.load %84 : !llvm.ptr<f32>
    llvm.br ^bb18(%85 : f32)
  ^bb18(%86: f32):  // 2 preds: ^bb17, ^bb18
    %87 = llvm.fcmp "ogt" %86, %83 : f32
    %88 = llvm.select %87, %86, %83 : i1, f32
    %89 = llvm.bitcast %84 : !llvm.ptr<f32> to !llvm.ptr<i32>
    %90 = llvm.bitcast %86 : f32 to i32
    %91 = llvm.bitcast %88 : f32 to i32
    %92 = llvm.cmpxchg %89, %90, %91 acq_rel monotonic : !llvm.ptr<i32>, i32
    %93 = llvm.extractvalue %92[0] : !llvm.struct<(i32, i1)> 
    %94 = llvm.bitcast %93 : i32 to f32
    %95 = llvm.extractvalue %92[1] : !llvm.struct<(i32, i1)> 
    llvm.cond_br %95, ^bb19, ^bb18(%94 : f32)
  ^bb19:  // pred: ^bb18
    llvm.br ^bb20
  ^bb20:  // 2 preds: ^bb16, ^bb19
    llvm.br ^bb21
  ^bb21:  // 2 preds: ^bb1, ^bb20
    llvm.return
  }
}

// -----// IR Dump After Canonicalizer (canonicalize) //----- //
func.func @main(%arg0: !disc_ral.context) attributes {tf.entry_function = {input_placements = "gpu", inputs = "input0", output_placements = "gpu", outputs = "output0"}} {
  %c256 = arith.constant 256 : index
  %c41 = arith.constant 41 : index
  %c10496 = arith.constant 10496 : index
  %c6 = arith.constant 6 : index
  %0 = llvm.mlir.constant(0 : i32) : i32
  %c0 = arith.constant 0 : index
  %c100 = arith.constant 100 : index
  %c1 = arith.constant 1 : index
  %c13 = arith.constant 13 : index
  %1 = "disc_ral.dispatch"(%arg0, %c0) {backend_config = "", call_target_name = "ral_recv_input", device = "cpu", has_side_effect = false} : (!disc_ral.context, index) -> memref<110x100x13xf32, "gpu">
  %alloc = memref.alloc() : memref<1300xf32, "gpu">
  gpu.launch_func  @main_kernel::@main_kColReduction_reduce__3_1_0___8w32h blocks in (%c6, %c1, %c1) threads in (%c256, %c1, %c1) args(%c256 : index, %alloc : memref<1300xf32, "gpu">)
  gpu.launch_func  @main_kernel_0::@main_kColReduction_reduce__3_1_0___8w32h_1 blocks in (%c41, %c1, %c1) threads in (%c256, %c1, %c1) args(%c256 : index, %c10496 : index, %1 : memref<110x100x13xf32, "gpu">, %alloc : memref<1300xf32, "gpu">)
  %2 = llvm.inttoptr %0 : i32 to !llvm.ptr<i8>
  %alloca = memref.alloca() : memref<2xindex, "cpu">
  memref.store %c100, %alloca[%c0] : memref<2xindex, "cpu">
  memref.store %c13, %alloca[%c1] : memref<2xindex, "cpu">
  %3 = "disc_ral.dispatch"(%arg0, %2, %alloc, %alloca) {backend_config = "", call_target_name = "inc_ref", device = "gpu", has_side_effect = false} : (!disc_ral.context, !llvm.ptr<i8>, memref<1300xf32, "gpu">, memref<2xindex, "cpu">) -> memref<100x13xf32, "gpu">
  %reinterpret_cast = memref.reinterpret_cast %3 to offset: [0], sizes: [100, 13], strides: [13, 1] : memref<100x13xf32, "gpu"> to memref<100x13xf32, "gpu">
  memref.dealloc %alloc : memref<1300xf32, "gpu">
  "disc_ral.dispatch"(%arg0, %c0, %reinterpret_cast) {backend_config = "", call_target_name = "ral_send_output", device = "cpu", has_side_effect = false} : (!disc_ral.context, index, memref<100x13xf32, "gpu">) -> ()
  return
}

// -----// IR Dump After StripDebugInfo (strip-debuginfo) //----- //
module attributes {gpu.container_module} {
  func.func @main(%arg0: !disc_ral.context) attributes {tf.entry_function = {input_placements = "gpu", inputs = "input0", output_placements = "gpu", outputs = "output0"}} {
    %c256 = arith.constant 256 : index
    %c41 = arith.constant 41 : index
    %c10496 = arith.constant 10496 : index
    %c6 = arith.constant 6 : index
    %0 = llvm.mlir.constant(0 : i32) : i32
    %c0 = arith.constant 0 : index
    %c100 = arith.constant 100 : index
    %c1 = arith.constant 1 : index
    %c13 = arith.constant 13 : index
    %1 = "disc_ral.dispatch"(%arg0, %c0) {backend_config = "", call_target_name = "ral_recv_input", device = "cpu", has_side_effect = false} : (!disc_ral.context, index) -> memref<110x100x13xf32, "gpu">
    %alloc = memref.alloc() : memref<1300xf32, "gpu">
    gpu.launch_func  @main_kernel::@main_kColReduction_reduce__3_1_0___8w32h blocks in (%c6, %c1, %c1) threads in (%c256, %c1, %c1) args(%c256 : index, %alloc : memref<1300xf32, "gpu">)
    gpu.launch_func  @main_kernel_0::@main_kColReduction_reduce__3_1_0___8w32h_1 blocks in (%c41, %c1, %c1) threads in (%c256, %c1, %c1) args(%c256 : index, %c10496 : index, %1 : memref<110x100x13xf32, "gpu">, %alloc : memref<1300xf32, "gpu">)
    %2 = llvm.inttoptr %0 : i32 to !llvm.ptr<i8>
    %alloca = memref.alloca() : memref<2xindex, "cpu">
    memref.store %c100, %alloca[%c0] : memref<2xindex, "cpu">
    memref.store %c13, %alloca[%c1] : memref<2xindex, "cpu">
    %3 = "disc_ral.dispatch"(%arg0, %2, %alloc, %alloca) {backend_config = "", call_target_name = "inc_ref", device = "gpu", has_side_effect = false} : (!disc_ral.context, !llvm.ptr<i8>, memref<1300xf32, "gpu">, memref<2xindex, "cpu">) -> memref<100x13xf32, "gpu">
    %reinterpret_cast = memref.reinterpret_cast %3 to offset: [0], sizes: [100, 13], strides: [13, 1] : memref<100x13xf32, "gpu"> to memref<100x13xf32, "gpu">
    memref.dealloc %alloc : memref<1300xf32, "gpu">
    "disc_ral.dispatch"(%arg0, %c0, %reinterpret_cast) {backend_config = "", call_target_name = "ral_send_output", device = "cpu", has_side_effect = false} : (!disc_ral.context, index, memref<100x13xf32, "gpu">) -> ()
    return
  }
  gpu.module @main_kernel attributes {gpu.binary = "P\EDU\BA\01\00\10\008\04\00\00\00\00\00\00\02\00\01\01@\00\00\00\F8\03\00\00\00\00\00\00\F3\03\00\00\00\00\00\00\07\00\01\00P\00\00\00\00\00\00\00\00\00\00\00\11 \00\00\00\00\00\00\00\00\00\00\00\00\00\00\E8\0B\00\00\00\00\00\00\A2\7FELF\02\01\013\07\00\01\00f\02\00\BE\00q\00\01\00!@\0B\07\001\00\80\08\07\00\F5\0E\00P\05P\00@\008\00\03\00@\00\0B\00\01\00\00.shstrtab\00.\08\00'ym\08\00\F0\1B_shndx\00.nv.info\00.text.main_kColReduction_r\0A\00\F5\01e__3_1_0___8w32h8\00\0F2\00\1Boshared4\00\1B\9Fconstant07\00\18\FA\01debug_frame\00.rel\11\00!nv\14\00\11aC\00\0F+\01 \0F\88\00\15\0FT\01\BAo_param[\01\1C\0F\01\00\06\8C[\00\00\00\03\00\0A\00\01\00\11\F0\18\00,\09\00\01\00 .\01\18\00,\04\00\01\00\11L\18\00,\07\00\01\00f2\00\00\00\12\10`\00!\80\01\07\00v\00\FF\FF\FF\FF(\00\0C\00\00\01\00@\03\00\04|\08\00\D1\0F\0C\81\80\80(\00\08\FF\81\80(\08\0B\00\06(\00/0\00\01\00\03\13pX\00\10\04\9B\00R\04\14\00\00\00E\00\01\0B\00\00\13\00p/\08\00\05\00\00\00\A7\03\22\04#\0C\00\00\01\00'\04\12\0C\00\17\11\0C\00!7\04\E8\03\F1\08\015\00\00\04\0A\08\00\02\00\00\00`\01\10\00\03\19\10\00\04\17\0C$\00\10\01N\00%\F0!\10\00\01\01\00\F2\02\F0\11\00\03\1B\FF\00\04\1C\08\00P\00\00\00\B0\00\01\00#K\00\01\00s\02\02\08\10\0A/\22\8B\00\00\07\00\03\08\00\13\10\08\00\13\18\08\00\13 \08\00\13(\08\00\130\08\00\228\08|\00\13\00\08\00\13\08\08\00\13\10\08\00\13\18\08\00\13 \08\00\13(\08\00\130\08\00\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\A2\00\00\00\14,\00\00\00H\00\01\00\00 \01/\05\00\01\00\FF\C0A\02z\01\00\1F\04\B1\0F\00\00\00\C4\0F\00\19y\02\00\01\00\10%\9B\02Q\0E\00\19y\03\0F\00\F5\1A\00!\00\00\00$\0E\00$z\02\02\00X\00\00\03\02\8E\07\00\CA\1F\00\0Cx\00\02\13\05\00\00p@\F0\03\00\DA\0F\00MS\04\A0\80\03\00\EA\0F\005t\03\FF\B3\03\10\FF\C0\03P\E2\0F\00\02xF\02B\80\FF\00\0F\10\00r\B9z\04\00\00F\00\84\00\90\D0\0F\00%v\02\02\00Zl\04\00`\00`\0F\00\86y\00\022\00@\04\19\10\0C0\009My\00`\00PGy\00\00\F09\04\A6\FF\83\03\00\C0\0F\00\18y\00\01\00\0F\10\00\90\0F\01\00-\00W\01.\03\00\01\00\22@\00\01\00=+\01\000\00\08\01\00\1F\0B@\00\04\13k)\00\1F[@\00\0C\13\13\1C\04\0C\01\00\13\C8\15\00&\90\00@\04#\04\00\95\04\00\06\05\12\00\01\00\1F\FET\00\00\00\01\00\13X\95\00/p\00\80\00\0B\1F)'\00\03#\00\C8@\00\04P\06\04\E4\00*\04\00\01\00\1Fa@\00\04\13\F81\00&L\00@\00\1F\0A@\00\00!\1C\01D\01\0D@\00\13H)\00*\D8\00\01\00\1B\08\08\00?\0B\01\00\86\07\00Q\00\00 \05\00\01\00&\10\00\80\00\17\048\00\04\18\00\13\C7\14\01\0C\84\01*0\058\07\1F\00\C0\00\04\132@\00+\06\00\01\00\1A\07\D0\07\12\03\00\06:\08\80\00\01\00\13\06\18\06\04(\0B\0C\01\00*\A8\00\08\00\04\F8\00\13\018\00\04\A8\00\0C\01\009P\03\00\08\00\088\00/\06\00\01\00\17\80\08\00\00\00\00\00\00\00\00\00\00\00\00"} {
    llvm.func @main_kColReduction_reduce__3_1_0___8w32h(%arg0: i32, %arg1: !llvm.ptr<f32>) attributes {disc.elimargs = [1 : index, 3 : index, 4 : index, 5 : index], gpu.kernel, nvvm.kernel} {
      %0 = llvm.mlir.constant(1300 : index) : i32
      %1 = llvm.mlir.constant(0xFF800000 : f32) : f32
      %2 = nvvm.read.ptx.sreg.ctaid.x : i32
      %3 = nvvm.read.ptx.sreg.tid.x : i32
      llvm.br ^bb1
    ^bb1:  // pred: ^bb0
      %4 = llvm.mul %2, %arg0  : i32
      %5 = llvm.add %3, %4  : i32
      %6 = llvm.icmp "ult" %5, %0 : i32
      llvm.cond_br %6, ^bb2, ^bb3
    ^bb2:  // pred: ^bb1
      %7 = llvm.getelementptr %arg1[%5] : (!llvm.ptr<f32>, i32) -> !llvm.ptr<f32>
      llvm.store %1, %7 : !llvm.ptr<f32>
      llvm.br ^bb3
    ^bb3:  // 2 preds: ^bb1, ^bb2
      llvm.return
    }
  }
  gpu.module @main_kernel_0 attributes {gpu.binary = "P\EDU\BA\01\00\10\00\D8\08\00\00\00\00\00\00\02\00\01\01@\00\00\00\98\08\00\00\00\00\00\00\91\08\00\00\00\00\00\00\07\00\01\00P\00\00\00\00\00\00\00\00\00\00\00\11 \00\00\00\00\00\00\00\00\00\00\00\00\00\00\A8\14\00\00\00\00\00\00\A2\7FELF\02\01\013\07\00\01\00g\02\00\BE\00q\00\01\00\22\14\00\01\00\11\11\06\00\F5\0E\00P\05P\00@\008\00\03\00@\00\0C\00\01\00\00.shstrtab\00.\08\00'ym\08\00\F0\1B_shndx\00.nv.info\00.text.main_kColReduction_r\0A\00\F5\03e__3_1_0___8w32h_1:\00\0F4\00\1Doshared6\00\1AOrela\A0\00\1F?rel\D5\00\22\9Fconstant09\00\1A\B2debug_frame{\00\09\11\00!nv\14\00\11aE\00\0F\9E\01 \0F\8A\00\17\0F\C9\01\F4\8F$____wg_3\00\17\00\0C\00/24\02\02'o_param\09\02\1C\0F\01\00\05\8C]\00\00\00\03\00\0A\00\01\00\11\C2\18\00,\0B\00\01\00 \9C\01\18\00,\09\00\01\00\11\DC\18\00,\04\00\01\00\11\FA\18\00,\07\00\01\00f2\00\00\00\12\10x\00!\80\08\07\00v\00\FF\FF\FF\FF(\00\0C\00\00\01\00@\03\00\04|\08\00\D1\0F\0C\81\80\80(\00\08\FF\81\80(\08\0B\00\06(\00/0\00\01\00\03\13pX\00\10\04\9B\00\10\04\CD\04\02E\002\04\D4\01\18\00C/\08\00\06\DF\04\22\04#\0C\00\00\01\00'\04\12\0C\00\17\11\0C\00!7\04 \05\F1\08\015\00\00\04\0A\08\00\03\00\00\00`\01\18\00\03\19\18\00\04\17\0C$\00u\03\00\10\00\00\F0!\10\009\02\00\08\10\00\10\01(\01%\F0\11\10\00\01\01\00\F2\0A\F0\11\00\03\1B\FF\00\04\1C\0C\00P\00\00\00\B0\06\00\00\B0\07\00\00\04\1ET\01#K\00\01\00\B1\02\02\08\10\0A/\22\00\00\00\08\10\00#\00\08\08\00\13\10\08\00\13\18\08\00\13 \08\00\13(\08\00\130\08\00\228\08\84\00\13\00\08\00\13\08\08\00\13\10\08\00\13\18\08\00\13 \08\00\13(\08\00\130\08\00\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\00Z\06b,\00\00\00H\00\01\005\02\00\00d\01\0F\01\00\FF\DA@$v\01\FF\7F\04\B2\FF\00\8E\07\00\C4\0F\00\19y\00\01\00\10%\BB\02a\0E\00\19y\03\00\01\00\10!-\00\F0\04\0E\00$z\00\00\00X\00\00\03\02\8E\07\00\CA\1F\00\0C\10\00\C5Y\00\00p`\F0\03\00\DA\0F\00M\9B\04\B0\80\03\00\EA\0F\00\19x\03\FF\1F\0F\00\F1\00\14\01\00\00\E2\0F\00\B9z\04\00\00F\00\00\F3\04\93\E2\0F\00Ey\00\00\C0\040\00\B1\E2\0F\00$t\05\FF\00\00\80\FF\90\00a\E2\0F\00\11r\03\81\00`\FF@\8F\07\00\C8P\00 \04\FF\C1\02\10\03P\00q\E4\0F\00\12x\03\03\81\04\F1\00\C0\8E\07\00\C6\0F\00'x\02\04}\0C\CE\C7@\00P\C8\0F\00$x4\03B\00\00\03\0AP\00P\19x\07\FF\05\F0\020\16\01\00 \00@\12x\08\00\A0\00\01@\00\D4\E2\0F\04$x\03\07\D7\FF\FF\FF\04\020\00\11\060\00\22\00\16p\00P\0Cx\00\00\7F\D8\04\22@\F4\B0\00@x\03\03 \06\04\000\00\10\E4\10\00\11\02\91\03\22\06\02\90\00`\0Cx\00\03\13\050\00\13\F20\00C\02\02\04\00p\01\96\D8\0F\00G\19\00\00\B0\030\01\95$x\0A\06\00E\01\00\FFP\00%\06\07P\00\10\E4\A0\00t\05\07\E0\22\0A\00\0A \00Dr\07\FF\FFP\00\010\00 \09\04\90\00\14\05 \00\08P\01\10\C4 \00 \04\08\10\01\16\09`\00$\06@@\00\11\CA\B0\00 \06m\E0\00!B\FC\D0\01@$\E8\08\040\00\14\070\005\E4\09\FF\C0\00\93\C8\0F\00%\E6\08\08\00ZP\00p\CA\0F\00\81\E9\0C\08 \00\C0\00\19\1E\0C\00\A2\00\00\10x\0A\06@\000\FF\E0\FF@\00f\04\10x\0D\06\02\10\00\00p\00\12\0Ap\00#\F8\03\10\00\12\0D\10\00\11\F6\10\001\1Cx\00\01\00\F1\04p\F0\F0\03\00\D2\0F\00\10\C8\0A\04\14\05\00\00\07\E0\FF\A0\01:$\C4\0B\90\000\C6\0A\0A\90\00\14\0B\90\005\C9\0E\0A\90\00\94\E2\02\00\10\B8\08\04(\0A@\00G\1F\00$\B4\D0\000\E2\0F\00\A0\00\14\03\A0\00W\C6\0F\00%\B6\E0\00f\CC\0F\00\81\B9\08\E0\00q\22\0F\00\0B\E2\00\0CF\06\A4\80\FA\03\00\C8O\00\1C\E8\00\B0\001\02\00\E4 \00\11\05 \00!\C0\FAP\01A\08\82\0C\0CP\02:\00\80\06\F0\00\02 \00g\10\D8\0D\04<\0F\E0\00'\D4\0A\E0\00`/\00%\D6\0A\0D\90\00\14\0A\90\00&\D9\0A\E0\00a\A2\0E\00$\E2\05\10\02\13\0C\D0\00Q\0B\C2\00\0E\0Eo\00i\FC\03\00\E4\8F\08P\01\00`\01\14\C8\10\00\11p\10\00A\0B\C2\00\050\00#\C0\FC\B0\00\22\0E\0E\B0\00\11\00\B0\01#\0B\B2q\06\00P\00R\C6\0F\01$\C2p\00\13\0Ep\00\0D`\00\1B\B8`\001\B2\00\05@\00\12\C0\F0\01c\10x\07\07P\14p\01\10\D6p\00#\08\08p\00\10\06\C0\01\22$\B2`\00\1F\08`\00\04W\10x\06\06\04P\02A\0B\D2\00\0A\D6\08\10\80`\00aO\08\0B\D2\00\05\10\00\13\C0P\02$\D8\00\90\00\13\02\80\01\10\07p\03BpR\F8\03\80\00\22\0A\0A\80\00!\80\05\80\00\12\D2\80\00\13\0A\80\00`GI\00\00\D0\FCq\04\11\83\B0\03\14A0\05\03\C0\03A\88s\00\02@\00\00\9E\07f\E8\0F\00\1D{\00\01\00\93\EC\0F\00\84\A9\07\02\00\10 \00*\22\0E\D0\00\11\C4\90\00\01\90\041pD\F2 \05T\84\A9\04\02\00P\00qb\0E\00\0B\A2\00\07\CE\09\10\80\D0\00u\1F\08\0B\A2\00\04\07P\01T/\00\1C\A8\00P\00\13\01\E0\00 \00?\10\00#@\F6\E0\00!\07\070\02\00`\01\1B\E4P\01\8F\C6\0F\00\88\A3\00\02\07\C0\00\09!\B9\05O\09\22\00\08 \06+\84\B9\A0\001\B2\00\05\00\01`\80\F4\03\00\C4\1F\10\00(\04\05\A0\00\06\10\02\12\01`\027\05\05\04\F0\01/\88\B3@\01\0B9M\19\00p\019$t\04\C0\03!\84y\CB\08\02@\00\C3\E6\0F\00%v\04\03\00\\\00\00\04\00\05f\84y\03\02\00\04\C0\00E\81y\06\04@\03bb\03\00\0Br\00q\06\22\80\F0`\011r\00\00\10\00#\C0\F2@\01\02\1F\00@\00\00\80\04\90\031\0Br\000\040\00@\F0\E0\05(\0EF\10\02b\E6\0F\00\08r\07 \00\00\01\00p\CC\0F\00\A9s\07\04h\09\C0\07\E1\1E\00\00\A4\0E\00\0Cr\00\07\10\00 pR@\00QO\00$r\06p\02\14\07 \06V\09\00\00\90\FFp\02\1BMp\02TGy\00\00\F0 \00f\C0\0F\00\18y\00\01\00\0F\10\00\90\0F\01\00-#\01\00\A0\01\0B\01\00\22@\00\01\00=\9E\01\000\00\08\01\00\1F\0B@\00\04\13\DE)\00?\09\02\00@\00\0A\13\13<\0B\0C\01\00\13\E8U\00\03\8F\03\01$\00\13\05w\02\00\01\00\22\18\00\01\00.q\01T\00\00\01\00\11\90\95\02O\00\00p\00\80\00\0B\1F)'\00\03\02m\00\00v\0C\01\06\00\06\E4\00*\04\00\01\00\1Fc@\00\04\130@\00\17x@\00\1F\0A@\00\00!\8F\01D\01\0D@\00\13\A8@\00*\D8\00\01\00\1B\08\08\00?~\01\00\CE\0E\00\01\1E\07\01\01\00&\10\00\80\00\17\048\00\04\18\00\138@\01\0C\84\01\13\90@\00\17x1\01\0F\C0\00\01\132T\01\15\06R\00\0A\10\0F&\80\08\80\00j\06\00\00\11\80\00\01\00\13\97\94\00+\03\00\01\00\03\B0\13/\00\04\80\00\0B\14\06\AB\01/\14\00\01\00\02\1B\A8\08\00\17\08\08\02\17\05\E8\00\0C\01\009p\0A\00\08\00\088\00\18\06\A0\00\0F\01\00\05\03\A9\00\80\08\00\00\00\00\00\00\00\00\00\00\00\00\00\00"} {
    llvm.mlir.global internal @__wg_main_kColReduction_reduce__3_1_0___8w32h_1_0() {addr_space = 3 : i32} : !llvm.array<256 x f32>
    llvm.func @main_kColReduction_reduce__3_1_0___8w32h_1(%arg0: i32, %arg1: i32, %arg2: !llvm.ptr<f32>, %arg3: !llvm.ptr<f32>) attributes {disc.elimargs = [2 : index, 4 : index, 5 : index, 6 : index, 7 : index, 8 : index, 9 : index, 10 : index, 11 : index, 13 : index, 14 : index, 15 : index], gpu.kernel, nvvm.kernel} {
      %0 = llvm.mlir.constant(41 : index) : i32
      %1 = llvm.mlir.constant(32 : index) : i32
      %2 = llvm.mlir.constant(8 : index) : i32
      %3 = llvm.mlir.constant(1300 : index) : i32
      %4 = llvm.mlir.constant(64 : index) : i32
      %5 = llvm.mlir.constant(110 : index) : i32
      %6 = llvm.mlir.constant(0xFF800000 : f32) : f32
      %7 = llvm.mlir.constant(4 : index) : i32
      %8 = llvm.mlir.constant(2 : index) : i32
      %9 = llvm.mlir.constant(1 : index) : i32
      %10 = llvm.mlir.constant(256 : index) : i32
      %11 = llvm.mlir.constant(0 : index) : i32
      %12 = llvm.mlir.addressof @__wg_main_kColReduction_reduce__3_1_0___8w32h_1_0 : !llvm.ptr<array<256 x f32>, 3>
      %13 = llvm.getelementptr %12[0, 0] : (!llvm.ptr<array<256 x f32>, 3>) -> !llvm.ptr<f32, 3>
      %14 = nvvm.read.ptx.sreg.ctaid.x : i32
      %15 = nvvm.read.ptx.sreg.tid.x : i32
      llvm.br ^bb1
    ^bb1:  // pred: ^bb0
      %16 = llvm.mul %14, %arg0  : i32
      %17 = llvm.add %15, %16  : i32
      %18 = llvm.icmp "ult" %17, %arg1 : i32
      llvm.cond_br %18, ^bb2, ^bb21
    ^bb2:  // pred: ^bb1
      %19 = llvm.srem %17, %10  : i32
      %20 = llvm.sdiv %17, %10  : i32
      %21 = llvm.udiv %20, %0  : i32
      %22 = llvm.urem %20, %0  : i32
      %23 = llvm.udiv %19, %1  : i32
      %24 = llvm.urem %19, %1  : i32
      %25 = llvm.mul %24, %2  : i32
      %26 = llvm.add %23, %25  : i32
      %27 = llvm.mul %22, %1  : i32
      %28 = llvm.add %24, %27  : i32
      %29 = llvm.icmp "ult" %28, %3 : i32
      llvm.cond_br %29, ^bb3, ^bb10(%6 : f32)
    ^bb3:  // pred: ^bb2
      %30 = llvm.mul %21, %2  : i32
      %31 = llvm.add %23, %30  : i32
      %32 = llvm.mul %31, %4  : i32
      llvm.br ^bb4(%11, %6 : i32, f32)
    ^bb4(%33: i32, %34: f32):  // 2 preds: ^bb3, ^bb9
      %35 = llvm.icmp "slt" %33, %4 : i32
      llvm.cond_br %35, ^bb5, ^bb10(%34 : f32)
    ^bb5:  // pred: ^bb4
      %36 = llvm.add %33, %32  : i32
      %37 = llvm.icmp "slt" %36, %5 : i32
      llvm.cond_br %37, ^bb6, ^bb7
    ^bb6:  // pred: ^bb5
      %38 = llvm.mul %36, %3  : i32
      %39 = llvm.add %38, %28  : i32
      %40 = llvm.getelementptr %arg2[%39] : (!llvm.ptr<f32>, i32) -> !llvm.ptr<f32>
      %41 = llvm.load %40 : !llvm.ptr<f32>
      %42 = llvm.fcmp "ugt" %34, %41 : f32
      %43 = llvm.select %42, %34, %41 : i1, f32
      %44 = llvm.fcmp "uno" %41, %41 : f32
      %45 = llvm.select %44, %41, %43 : i1, f32
      llvm.br ^bb8(%45 : f32)
    ^bb7:  // pred: ^bb5
      llvm.br ^bb8(%34 : f32)
    ^bb8(%46: f32):  // 2 preds: ^bb6, ^bb7
      llvm.br ^bb9
    ^bb9:  // pred: ^bb8
      %47 = llvm.add %33, %9  : i32
      llvm.br ^bb4(%47, %46 : i32, f32)
    ^bb10(%48: f32):  // 2 preds: ^bb2, ^bb4
      llvm.br ^bb11(%48 : f32)
    ^bb11(%49: f32):  // pred: ^bb10
      llvm.br ^bb12
    ^bb12:  // pred: ^bb11
      %50 = llvm.getelementptr %13[%26] : (!llvm.ptr<f32, 3>, i32) -> !llvm.ptr<f32, 3>
      llvm.store %49, %50 : !llvm.ptr<f32, 3>
      nvvm.barrier0
      %51 = llvm.icmp "slt" %23, %7 : i32
      llvm.cond_br %51, ^bb13, ^bb14
    ^bb13:  // pred: ^bb12
      %52 = llvm.add %26, %7  : i32
      %53 = llvm.getelementptr %13[%26] : (!llvm.ptr<f32, 3>, i32) -> !llvm.ptr<f32, 3>
      %54 = llvm.load %53 : !llvm.ptr<f32, 3>
      %55 = llvm.getelementptr %13[%52] : (!llvm.ptr<f32, 3>, i32) -> !llvm.ptr<f32, 3>
      %56 = llvm.load %55 : !llvm.ptr<f32, 3>
      %57 = llvm.fcmp "ugt" %54, %56 : f32
      %58 = llvm.select %57, %54, %56 : i1, f32
      %59 = llvm.fcmp "uno" %56, %56 : f32
      %60 = llvm.select %59, %56, %58 : i1, f32
      %61 = llvm.getelementptr %13[%26] : (!llvm.ptr<f32, 3>, i32) -> !llvm.ptr<f32, 3>
      llvm.store %60, %61 : !llvm.ptr<f32, 3>
      llvm.br ^bb14
    ^bb14:  // 2 preds: ^bb12, ^bb13
      nvvm.barrier0
      %62 = llvm.icmp "slt" %23, %8 : i32
      llvm.cond_br %62, ^bb15, ^bb16
    ^bb15:  // pred: ^bb14
      %63 = llvm.add %26, %8  : i32
      %64 = llvm.getelementptr %13[%26] : (!llvm.ptr<f32, 3>, i32) -> !llvm.ptr<f32, 3>
      %65 = llvm.load %64 : !llvm.ptr<f32, 3>
      %66 = llvm.getelementptr %13[%63] : (!llvm.ptr<f32, 3>, i32) -> !llvm.ptr<f32, 3>
      %67 = llvm.load %66 : !llvm.ptr<f32, 3>
      %68 = llvm.fcmp "ugt" %65, %67 : f32
      %69 = llvm.select %68, %65, %67 : i1, f32
      %70 = llvm.fcmp "uno" %67, %67 : f32
      %71 = llvm.select %70, %67, %69 : i1, f32
      %72 = llvm.getelementptr %13[%26] : (!llvm.ptr<f32, 3>, i32) -> !llvm.ptr<f32, 3>
      llvm.store %71, %72 : !llvm.ptr<f32, 3>
      llvm.br ^bb16
    ^bb16:  // 2 preds: ^bb14, ^bb15
      nvvm.barrier0
      %73 = llvm.icmp "eq" %23, %11 : i32
      %74 = llvm.and %73, %29  : i1
      llvm.cond_br %74, ^bb17, ^bb20
    ^bb17:  // pred: ^bb16
      %75 = llvm.add %26, %9  : i32
      %76 = llvm.getelementptr %13[%26] : (!llvm.ptr<f32, 3>, i32) -> !llvm.ptr<f32, 3>
      %77 = llvm.load %76 : !llvm.ptr<f32, 3>
      %78 = llvm.getelementptr %13[%75] : (!llvm.ptr<f32, 3>, i32) -> !llvm.ptr<f32, 3>
      %79 = llvm.load %78 : !llvm.ptr<f32, 3>
      %80 = llvm.fcmp "ugt" %77, %79 : f32
      %81 = llvm.select %80, %77, %79 : i1, f32
      %82 = llvm.fcmp "uno" %79, %79 : f32
      %83 = llvm.select %82, %79, %81 : i1, f32
      %84 = llvm.getelementptr %arg3[%28] : (!llvm.ptr<f32>, i32) -> !llvm.ptr<f32>
      %85 = llvm.load %84 : !llvm.ptr<f32>
      llvm.br ^bb18(%85 : f32)
    ^bb18(%86: f32):  // 2 preds: ^bb17, ^bb18
      %87 = llvm.fcmp "ogt" %86, %83 : f32
      %88 = llvm.select %87, %86, %83 : i1, f32
      %89 = llvm.bitcast %84 : !llvm.ptr<f32> to !llvm.ptr<i32>
      %90 = llvm.bitcast %86 : f32 to i32
      %91 = llvm.bitcast %88 : f32 to i32
      %92 = llvm.cmpxchg %89, %90, %91 acq_rel monotonic : !llvm.ptr<i32>, i32
      %93 = llvm.extractvalue %92[0] : !llvm.struct<(i32, i1)> 
      %94 = llvm.bitcast %93 : i32 to f32
      %95 = llvm.extractvalue %92[1] : !llvm.struct<(i32, i1)> 
      llvm.cond_br %95, ^bb19, ^bb18(%94 : f32)
    ^bb19:  // pred: ^bb18
      llvm.br ^bb20
    ^bb20:  // 2 preds: ^bb16, ^bb19
      llvm.br ^bb21
    ^bb21:  // 2 preds: ^bb1, ^bb20
      llvm.return
    }
  }
  func.func @shape_constraint_graph() {
    return
  }
}


// -----// IR Dump After DiscStripShapeConstraintOpsPass (disc-strip-shape-constraint-ops) //----- //
module attributes {gpu.container_module} {
  func.func @main(%arg0: !disc_ral.context) attributes {tf.entry_function = {input_placements = "gpu", inputs = "input0", output_placements = "gpu", outputs = "output0"}} {
    %c256 = arith.constant 256 : index
    %c41 = arith.constant 41 : index
    %c10496 = arith.constant 10496 : index
    %c6 = arith.constant 6 : index
    %0 = llvm.mlir.constant(0 : i32) : i32
    %c0 = arith.constant 0 : index
    %c100 = arith.constant 100 : index
    %c1 = arith.constant 1 : index
    %c13 = arith.constant 13 : index
    %1 = "disc_ral.dispatch"(%arg0, %c0) {backend_config = "", call_target_name = "ral_recv_input", device = "cpu", has_side_effect = false} : (!disc_ral.context, index) -> memref<110x100x13xf32, "gpu">
    %alloc = memref.alloc() : memref<1300xf32, "gpu">
    gpu.launch_func  @main_kernel::@main_kColReduction_reduce__3_1_0___8w32h blocks in (%c6, %c1, %c1) threads in (%c256, %c1, %c1) args(%c256 : index, %alloc : memref<1300xf32, "gpu">)
    gpu.launch_func  @main_kernel_0::@main_kColReduction_reduce__3_1_0___8w32h_1 blocks in (%c41, %c1, %c1) threads in (%c256, %c1, %c1) args(%c256 : index, %c10496 : index, %1 : memref<110x100x13xf32, "gpu">, %alloc : memref<1300xf32, "gpu">)
    %2 = llvm.inttoptr %0 : i32 to !llvm.ptr<i8>
    %alloca = memref.alloca() : memref<2xindex, "cpu">
    memref.store %c100, %alloca[%c0] : memref<2xindex, "cpu">
    memref.store %c13, %alloca[%c1] : memref<2xindex, "cpu">
    %3 = "disc_ral.dispatch"(%arg0, %2, %alloc, %alloca) {backend_config = "", call_target_name = "inc_ref", device = "gpu", has_side_effect = false} : (!disc_ral.context, !llvm.ptr<i8>, memref<1300xf32, "gpu">, memref<2xindex, "cpu">) -> memref<100x13xf32, "gpu">
    %reinterpret_cast = memref.reinterpret_cast %3 to offset: [0], sizes: [100, 13], strides: [13, 1] : memref<100x13xf32, "gpu"> to memref<100x13xf32, "gpu">
    memref.dealloc %alloc : memref<1300xf32, "gpu">
    "disc_ral.dispatch"(%arg0, %c0, %reinterpret_cast) {backend_config = "", call_target_name = "ral_send_output", device = "cpu", has_side_effect = false} : (!disc_ral.context, index, memref<100x13xf32, "gpu">) -> ()
    return
  }
  gpu.module @main_kernel attributes {gpu.binary = "P\EDU\BA\01\00\10\008\04\00\00\00\00\00\00\02\00\01\01@\00\00\00\F8\03\00\00\00\00\00\00\F3\03\00\00\00\00\00\00\07\00\01\00P\00\00\00\00\00\00\00\00\00\00\00\11 \00\00\00\00\00\00\00\00\00\00\00\00\00\00\E8\0B\00\00\00\00\00\00\A2\7FELF\02\01\013\07\00\01\00f\02\00\BE\00q\00\01\00!@\0B\07\001\00\80\08\07\00\F5\0E\00P\05P\00@\008\00\03\00@\00\0B\00\01\00\00.shstrtab\00.\08\00'ym\08\00\F0\1B_shndx\00.nv.info\00.text.main_kColReduction_r\0A\00\F5\01e__3_1_0___8w32h8\00\0F2\00\1Boshared4\00\1B\9Fconstant07\00\18\FA\01debug_frame\00.rel\11\00!nv\14\00\11aC\00\0F+\01 \0F\88\00\15\0FT\01\BAo_param[\01\1C\0F\01\00\06\8C[\00\00\00\03\00\0A\00\01\00\11\F0\18\00,\09\00\01\00 .\01\18\00,\04\00\01\00\11L\18\00,\07\00\01\00f2\00\00\00\12\10`\00!\80\01\07\00v\00\FF\FF\FF\FF(\00\0C\00\00\01\00@\03\00\04|\08\00\D1\0F\0C\81\80\80(\00\08\FF\81\80(\08\0B\00\06(\00/0\00\01\00\03\13pX\00\10\04\9B\00R\04\14\00\00\00E\00\01\0B\00\00\13\00p/\08\00\05\00\00\00\A7\03\22\04#\0C\00\00\01\00'\04\12\0C\00\17\11\0C\00!7\04\E8\03\F1\08\015\00\00\04\0A\08\00\02\00\00\00`\01\10\00\03\19\10\00\04\17\0C$\00\10\01N\00%\F0!\10\00\01\01\00\F2\02\F0\11\00\03\1B\FF\00\04\1C\08\00P\00\00\00\B0\00\01\00#K\00\01\00s\02\02\08\10\0A/\22\8B\00\00\07\00\03\08\00\13\10\08\00\13\18\08\00\13 \08\00\13(\08\00\130\08\00\228\08|\00\13\00\08\00\13\08\08\00\13\10\08\00\13\18\08\00\13 \08\00\13(\08\00\130\08\00\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\A2\00\00\00\14,\00\00\00H\00\01\00\00 \01/\05\00\01\00\FF\C0A\02z\01\00\1F\04\B1\0F\00\00\00\C4\0F\00\19y\02\00\01\00\10%\9B\02Q\0E\00\19y\03\0F\00\F5\1A\00!\00\00\00$\0E\00$z\02\02\00X\00\00\03\02\8E\07\00\CA\1F\00\0Cx\00\02\13\05\00\00p@\F0\03\00\DA\0F\00MS\04\A0\80\03\00\EA\0F\005t\03\FF\B3\03\10\FF\C0\03P\E2\0F\00\02xF\02B\80\FF\00\0F\10\00r\B9z\04\00\00F\00\84\00\90\D0\0F\00%v\02\02\00Zl\04\00`\00`\0F\00\86y\00\022\00@\04\19\10\0C0\009My\00`\00PGy\00\00\F09\04\A6\FF\83\03\00\C0\0F\00\18y\00\01\00\0F\10\00\90\0F\01\00-\00W\01.\03\00\01\00\22@\00\01\00=+\01\000\00\08\01\00\1F\0B@\00\04\13k)\00\1F[@\00\0C\13\13\1C\04\0C\01\00\13\C8\15\00&\90\00@\04#\04\00\95\04\00\06\05\12\00\01\00\1F\FET\00\00\00\01\00\13X\95\00/p\00\80\00\0B\1F)'\00\03#\00\C8@\00\04P\06\04\E4\00*\04\00\01\00\1Fa@\00\04\13\F81\00&L\00@\00\1F\0A@\00\00!\1C\01D\01\0D@\00\13H)\00*\D8\00\01\00\1B\08\08\00?\0B\01\00\86\07\00Q\00\00 \05\00\01\00&\10\00\80\00\17\048\00\04\18\00\13\C7\14\01\0C\84\01*0\058\07\1F\00\C0\00\04\132@\00+\06\00\01\00\1A\07\D0\07\12\03\00\06:\08\80\00\01\00\13\06\18\06\04(\0B\0C\01\00*\A8\00\08\00\04\F8\00\13\018\00\04\A8\00\0C\01\009P\03\00\08\00\088\00/\06\00\01\00\17\80\08\00\00\00\00\00\00\00\00\00\00\00\00"} {
    llvm.func @main_kColReduction_reduce__3_1_0___8w32h(%arg0: i32, %arg1: !llvm.ptr<f32>) attributes {disc.elimargs = [1 : index, 3 : index, 4 : index, 5 : index], gpu.kernel, nvvm.kernel} {
      %0 = llvm.mlir.constant(1300 : index) : i32
      %1 = llvm.mlir.constant(0xFF800000 : f32) : f32
      %2 = nvvm.read.ptx.sreg.ctaid.x : i32
      %3 = nvvm.read.ptx.sreg.tid.x : i32
      llvm.br ^bb1
    ^bb1:  // pred: ^bb0
      %4 = llvm.mul %2, %arg0  : i32
      %5 = llvm.add %3, %4  : i32
      %6 = llvm.icmp "ult" %5, %0 : i32
      llvm.cond_br %6, ^bb2, ^bb3
    ^bb2:  // pred: ^bb1
      %7 = llvm.getelementptr %arg1[%5] : (!llvm.ptr<f32>, i32) -> !llvm.ptr<f32>
      llvm.store %1, %7 : !llvm.ptr<f32>
      llvm.br ^bb3
    ^bb3:  // 2 preds: ^bb1, ^bb2
      llvm.return
    }
  }
  gpu.module @main_kernel_0 attributes {gpu.binary = "P\EDU\BA\01\00\10\00\D8\08\00\00\00\00\00\00\02\00\01\01@\00\00\00\98\08\00\00\00\00\00\00\91\08\00\00\00\00\00\00\07\00\01\00P\00\00\00\00\00\00\00\00\00\00\00\11 \00\00\00\00\00\00\00\00\00\00\00\00\00\00\A8\14\00\00\00\00\00\00\A2\7FELF\02\01\013\07\00\01\00g\02\00\BE\00q\00\01\00\22\14\00\01\00\11\11\06\00\F5\0E\00P\05P\00@\008\00\03\00@\00\0C\00\01\00\00.shstrtab\00.\08\00'ym\08\00\F0\1B_shndx\00.nv.info\00.text.main_kColReduction_r\0A\00\F5\03e__3_1_0___8w32h_1:\00\0F4\00\1Doshared6\00\1AOrela\A0\00\1F?rel\D5\00\22\9Fconstant09\00\1A\B2debug_frame{\00\09\11\00!nv\14\00\11aE\00\0F\9E\01 \0F\8A\00\17\0F\C9\01\F4\8F$____wg_3\00\17\00\0C\00/24\02\02'o_param\09\02\1C\0F\01\00\05\8C]\00\00\00\03\00\0A\00\01\00\11\C2\18\00,\0B\00\01\00 \9C\01\18\00,\09\00\01\00\11\DC\18\00,\04\00\01\00\11\FA\18\00,\07\00\01\00f2\00\00\00\12\10x\00!\80\08\07\00v\00\FF\FF\FF\FF(\00\0C\00\00\01\00@\03\00\04|\08\00\D1\0F\0C\81\80\80(\00\08\FF\81\80(\08\0B\00\06(\00/0\00\01\00\03\13pX\00\10\04\9B\00\10\04\CD\04\02E\002\04\D4\01\18\00C/\08\00\06\DF\04\22\04#\0C\00\00\01\00'\04\12\0C\00\17\11\0C\00!7\04 \05\F1\08\015\00\00\04\0A\08\00\03\00\00\00`\01\18\00\03\19\18\00\04\17\0C$\00u\03\00\10\00\00\F0!\10\009\02\00\08\10\00\10\01(\01%\F0\11\10\00\01\01\00\F2\0A\F0\11\00\03\1B\FF\00\04\1C\0C\00P\00\00\00\B0\06\00\00\B0\07\00\00\04\1ET\01#K\00\01\00\B1\02\02\08\10\0A/\22\00\00\00\08\10\00#\00\08\08\00\13\10\08\00\13\18\08\00\13 \08\00\13(\08\00\130\08\00\228\08\84\00\13\00\08\00\13\08\08\00\13\10\08\00\13\18\08\00\13 \08\00\13(\08\00\130\08\00\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\00Z\06b,\00\00\00H\00\01\005\02\00\00d\01\0F\01\00\FF\DA@$v\01\FF\7F\04\B2\FF\00\8E\07\00\C4\0F\00\19y\00\01\00\10%\BB\02a\0E\00\19y\03\00\01\00\10!-\00\F0\04\0E\00$z\00\00\00X\00\00\03\02\8E\07\00\CA\1F\00\0C\10\00\C5Y\00\00p`\F0\03\00\DA\0F\00M\9B\04\B0\80\03\00\EA\0F\00\19x\03\FF\1F\0F\00\F1\00\14\01\00\00\E2\0F\00\B9z\04\00\00F\00\00\F3\04\93\E2\0F\00Ey\00\00\C0\040\00\B1\E2\0F\00$t\05\FF\00\00\80\FF\90\00a\E2\0F\00\11r\03\81\00`\FF@\8F\07\00\C8P\00 \04\FF\C1\02\10\03P\00q\E4\0F\00\12x\03\03\81\04\F1\00\C0\8E\07\00\C6\0F\00'x\02\04}\0C\CE\C7@\00P\C8\0F\00$x4\03B\00\00\03\0AP\00P\19x\07\FF\05\F0\020\16\01\00 \00@\12x\08\00\A0\00\01@\00\D4\E2\0F\04$x\03\07\D7\FF\FF\FF\04\020\00\11\060\00\22\00\16p\00P\0Cx\00\00\7F\D8\04\22@\F4\B0\00@x\03\03 \06\04\000\00\10\E4\10\00\11\02\91\03\22\06\02\90\00`\0Cx\00\03\13\050\00\13\F20\00C\02\02\04\00p\01\96\D8\0F\00G\19\00\00\B0\030\01\95$x\0A\06\00E\01\00\FFP\00%\06\07P\00\10\E4\A0\00t\05\07\E0\22\0A\00\0A \00Dr\07\FF\FFP\00\010\00 \09\04\90\00\14\05 \00\08P\01\10\C4 \00 \04\08\10\01\16\09`\00$\06@@\00\11\CA\B0\00 \06m\E0\00!B\FC\D0\01@$\E8\08\040\00\14\070\005\E4\09\FF\C0\00\93\C8\0F\00%\E6\08\08\00ZP\00p\CA\0F\00\81\E9\0C\08 \00\C0\00\19\1E\0C\00\A2\00\00\10x\0A\06@\000\FF\E0\FF@\00f\04\10x\0D\06\02\10\00\00p\00\12\0Ap\00#\F8\03\10\00\12\0D\10\00\11\F6\10\001\1Cx\00\01\00\F1\04p\F0\F0\03\00\D2\0F\00\10\C8\0A\04\14\05\00\00\07\E0\FF\A0\01:$\C4\0B\90\000\C6\0A\0A\90\00\14\0B\90\005\C9\0E\0A\90\00\94\E2\02\00\10\B8\08\04(\0A@\00G\1F\00$\B4\D0\000\E2\0F\00\A0\00\14\03\A0\00W\C6\0F\00%\B6\E0\00f\CC\0F\00\81\B9\08\E0\00q\22\0F\00\0B\E2\00\0CF\06\A4\80\FA\03\00\C8O\00\1C\E8\00\B0\001\02\00\E4 \00\11\05 \00!\C0\FAP\01A\08\82\0C\0CP\02:\00\80\06\F0\00\02 \00g\10\D8\0D\04<\0F\E0\00'\D4\0A\E0\00`/\00%\D6\0A\0D\90\00\14\0A\90\00&\D9\0A\E0\00a\A2\0E\00$\E2\05\10\02\13\0C\D0\00Q\0B\C2\00\0E\0Eo\00i\FC\03\00\E4\8F\08P\01\00`\01\14\C8\10\00\11p\10\00A\0B\C2\00\050\00#\C0\FC\B0\00\22\0E\0E\B0\00\11\00\B0\01#\0B\B2q\06\00P\00R\C6\0F\01$\C2p\00\13\0Ep\00\0D`\00\1B\B8`\001\B2\00\05@\00\12\C0\F0\01c\10x\07\07P\14p\01\10\D6p\00#\08\08p\00\10\06\C0\01\22$\B2`\00\1F\08`\00\04W\10x\06\06\04P\02A\0B\D2\00\0A\D6\08\10\80`\00aO\08\0B\D2\00\05\10\00\13\C0P\02$\D8\00\90\00\13\02\80\01\10\07p\03BpR\F8\03\80\00\22\0A\0A\80\00!\80\05\80\00\12\D2\80\00\13\0A\80\00`GI\00\00\D0\FCq\04\11\83\B0\03\14A0\05\03\C0\03A\88s\00\02@\00\00\9E\07f\E8\0F\00\1D{\00\01\00\93\EC\0F\00\84\A9\07\02\00\10 \00*\22\0E\D0\00\11\C4\90\00\01\90\041pD\F2 \05T\84\A9\04\02\00P\00qb\0E\00\0B\A2\00\07\CE\09\10\80\D0\00u\1F\08\0B\A2\00\04\07P\01T/\00\1C\A8\00P\00\13\01\E0\00 \00?\10\00#@\F6\E0\00!\07\070\02\00`\01\1B\E4P\01\8F\C6\0F\00\88\A3\00\02\07\C0\00\09!\B9\05O\09\22\00\08 \06+\84\B9\A0\001\B2\00\05\00\01`\80\F4\03\00\C4\1F\10\00(\04\05\A0\00\06\10\02\12\01`\027\05\05\04\F0\01/\88\B3@\01\0B9M\19\00p\019$t\04\C0\03!\84y\CB\08\02@\00\C3\E6\0F\00%v\04\03\00\\\00\00\04\00\05f\84y\03\02\00\04\C0\00E\81y\06\04@\03bb\03\00\0Br\00q\06\22\80\F0`\011r\00\00\10\00#\C0\F2@\01\02\1F\00@\00\00\80\04\90\031\0Br\000\040\00@\F0\E0\05(\0EF\10\02b\E6\0F\00\08r\07 \00\00\01\00p\CC\0F\00\A9s\07\04h\09\C0\07\E1\1E\00\00\A4\0E\00\0Cr\00\07\10\00 pR@\00QO\00$r\06p\02\14\07 \06V\09\00\00\90\FFp\02\1BMp\02TGy\00\00\F0 \00f\C0\0F\00\18y\00\01\00\0F\10\00\90\0F\01\00-#\01\00\A0\01\0B\01\00\22@\00\01\00=\9E\01\000\00\08\01\00\1F\0B@\00\04\13\DE)\00?\09\02\00@\00\0A\13\13<\0B\0C\01\00\13\E8U\00\03\8F\03\01$\00\13\05w\02\00\01\00\22\18\00\01\00.q\01T\00\00\01\00\11\90\95\02O\00\00p\00\80\00\0B\1F)'\00\03\02m\00\00v\0C\01\06\00\06\E4\00*\04\00\01\00\1Fc@\00\04\130@\00\17x@\00\1F\0A@\00\00!\8F\01D\01\0D@\00\13\A8@\00*\D8\00\01\00\1B\08\08\00?~\01\00\CE\0E\00\01\1E\07\01\01\00&\10\00\80\00\17\048\00\04\18\00\138@\01\0C\84\01\13\90@\00\17x1\01\0F\C0\00\01\132T\01\15\06R\00\0A\10\0F&\80\08\80\00j\06\00\00\11\80\00\01\00\13\97\94\00+\03\00\01\00\03\B0\13/\00\04\80\00\0B\14\06\AB\01/\14\00\01\00\02\1B\A8\08\00\17\08\08\02\17\05\E8\00\0C\01\009p\0A\00\08\00\088\00\18\06\A0\00\0F\01\00\05\03\A9\00\80\08\00\00\00\00\00\00\00\00\00\00\00\00\00\00"} {
    llvm.mlir.global internal @__wg_main_kColReduction_reduce__3_1_0___8w32h_1_0() {addr_space = 3 : i32} : !llvm.array<256 x f32>
    llvm.func @main_kColReduction_reduce__3_1_0___8w32h_1(%arg0: i32, %arg1: i32, %arg2: !llvm.ptr<f32>, %arg3: !llvm.ptr<f32>) attributes {disc.elimargs = [2 : index, 4 : index, 5 : index, 6 : index, 7 : index, 8 : index, 9 : index, 10 : index, 11 : index, 13 : index, 14 : index, 15 : index], gpu.kernel, nvvm.kernel} {
      %0 = llvm.mlir.constant(41 : index) : i32
      %1 = llvm.mlir.constant(32 : index) : i32
      %2 = llvm.mlir.constant(8 : index) : i32
      %3 = llvm.mlir.constant(1300 : index) : i32
      %4 = llvm.mlir.constant(64 : index) : i32
      %5 = llvm.mlir.constant(110 : index) : i32
      %6 = llvm.mlir.constant(0xFF800000 : f32) : f32
      %7 = llvm.mlir.constant(4 : index) : i32
      %8 = llvm.mlir.constant(2 : index) : i32
      %9 = llvm.mlir.constant(1 : index) : i32
      %10 = llvm.mlir.constant(256 : index) : i32
      %11 = llvm.mlir.constant(0 : index) : i32
      %12 = llvm.mlir.addressof @__wg_main_kColReduction_reduce__3_1_0___8w32h_1_0 : !llvm.ptr<array<256 x f32>, 3>
      %13 = llvm.getelementptr %12[0, 0] : (!llvm.ptr<array<256 x f32>, 3>) -> !llvm.ptr<f32, 3>
      %14 = nvvm.read.ptx.sreg.ctaid.x : i32
      %15 = nvvm.read.ptx.sreg.tid.x : i32
      llvm.br ^bb1
    ^bb1:  // pred: ^bb0
      %16 = llvm.mul %14, %arg0  : i32
      %17 = llvm.add %15, %16  : i32
      %18 = llvm.icmp "ult" %17, %arg1 : i32
      llvm.cond_br %18, ^bb2, ^bb21
    ^bb2:  // pred: ^bb1
      %19 = llvm.srem %17, %10  : i32
      %20 = llvm.sdiv %17, %10  : i32
      %21 = llvm.udiv %20, %0  : i32
      %22 = llvm.urem %20, %0  : i32
      %23 = llvm.udiv %19, %1  : i32
      %24 = llvm.urem %19, %1  : i32
      %25 = llvm.mul %24, %2  : i32
      %26 = llvm.add %23, %25  : i32
      %27 = llvm.mul %22, %1  : i32
      %28 = llvm.add %24, %27  : i32
      %29 = llvm.icmp "ult" %28, %3 : i32
      llvm.cond_br %29, ^bb3, ^bb10(%6 : f32)
    ^bb3:  // pred: ^bb2
      %30 = llvm.mul %21, %2  : i32
      %31 = llvm.add %23, %30  : i32
      %32 = llvm.mul %31, %4  : i32
      llvm.br ^bb4(%11, %6 : i32, f32)
    ^bb4(%33: i32, %34: f32):  // 2 preds: ^bb3, ^bb9
      %35 = llvm.icmp "slt" %33, %4 : i32
      llvm.cond_br %35, ^bb5, ^bb10(%34 : f32)
    ^bb5:  // pred: ^bb4
      %36 = llvm.add %33, %32  : i32
      %37 = llvm.icmp "slt" %36, %5 : i32
      llvm.cond_br %37, ^bb6, ^bb7
    ^bb6:  // pred: ^bb5
      %38 = llvm.mul %36, %3  : i32
      %39 = llvm.add %38, %28  : i32
      %40 = llvm.getelementptr %arg2[%39] : (!llvm.ptr<f32>, i32) -> !llvm.ptr<f32>
      %41 = llvm.load %40 : !llvm.ptr<f32>
      %42 = llvm.fcmp "ugt" %34, %41 : f32
      %43 = llvm.select %42, %34, %41 : i1, f32
      %44 = llvm.fcmp "uno" %41, %41 : f32
      %45 = llvm.select %44, %41, %43 : i1, f32
      llvm.br ^bb8(%45 : f32)
    ^bb7:  // pred: ^bb5
      llvm.br ^bb8(%34 : f32)
    ^bb8(%46: f32):  // 2 preds: ^bb6, ^bb7
      llvm.br ^bb9
    ^bb9:  // pred: ^bb8
      %47 = llvm.add %33, %9  : i32
      llvm.br ^bb4(%47, %46 : i32, f32)
    ^bb10(%48: f32):  // 2 preds: ^bb2, ^bb4
      llvm.br ^bb11(%48 : f32)
    ^bb11(%49: f32):  // pred: ^bb10
      llvm.br ^bb12
    ^bb12:  // pred: ^bb11
      %50 = llvm.getelementptr %13[%26] : (!llvm.ptr<f32, 3>, i32) -> !llvm.ptr<f32, 3>
      llvm.store %49, %50 : !llvm.ptr<f32, 3>
      nvvm.barrier0
      %51 = llvm.icmp "slt" %23, %7 : i32
      llvm.cond_br %51, ^bb13, ^bb14
    ^bb13:  // pred: ^bb12
      %52 = llvm.add %26, %7  : i32
      %53 = llvm.getelementptr %13[%26] : (!llvm.ptr<f32, 3>, i32) -> !llvm.ptr<f32, 3>
      %54 = llvm.load %53 : !llvm.ptr<f32, 3>
      %55 = llvm.getelementptr %13[%52] : (!llvm.ptr<f32, 3>, i32) -> !llvm.ptr<f32, 3>
      %56 = llvm.load %55 : !llvm.ptr<f32, 3>
      %57 = llvm.fcmp "ugt" %54, %56 : f32
      %58 = llvm.select %57, %54, %56 : i1, f32
      %59 = llvm.fcmp "uno" %56, %56 : f32
      %60 = llvm.select %59, %56, %58 : i1, f32
      %61 = llvm.getelementptr %13[%26] : (!llvm.ptr<f32, 3>, i32) -> !llvm.ptr<f32, 3>
      llvm.store %60, %61 : !llvm.ptr<f32, 3>
      llvm.br ^bb14
    ^bb14:  // 2 preds: ^bb12, ^bb13
      nvvm.barrier0
      %62 = llvm.icmp "slt" %23, %8 : i32
      llvm.cond_br %62, ^bb15, ^bb16
    ^bb15:  // pred: ^bb14
      %63 = llvm.add %26, %8  : i32
      %64 = llvm.getelementptr %13[%26] : (!llvm.ptr<f32, 3>, i32) -> !llvm.ptr<f32, 3>
      %65 = llvm.load %64 : !llvm.ptr<f32, 3>
      %66 = llvm.getelementptr %13[%63] : (!llvm.ptr<f32, 3>, i32) -> !llvm.ptr<f32, 3>
      %67 = llvm.load %66 : !llvm.ptr<f32, 3>
      %68 = llvm.fcmp "ugt" %65, %67 : f32
      %69 = llvm.select %68, %65, %67 : i1, f32
      %70 = llvm.fcmp "uno" %67, %67 : f32
      %71 = llvm.select %70, %67, %69 : i1, f32
      %72 = llvm.getelementptr %13[%26] : (!llvm.ptr<f32, 3>, i32) -> !llvm.ptr<f32, 3>
      llvm.store %71, %72 : !llvm.ptr<f32, 3>
      llvm.br ^bb16
    ^bb16:  // 2 preds: ^bb14, ^bb15
      nvvm.barrier0
      %73 = llvm.icmp "eq" %23, %11 : i32
      %74 = llvm.and %73, %29  : i1
      llvm.cond_br %74, ^bb17, ^bb20
    ^bb17:  // pred: ^bb16
      %75 = llvm.add %26, %9  : i32
      %76 = llvm.getelementptr %13[%26] : (!llvm.ptr<f32, 3>, i32) -> !llvm.ptr<f32, 3>
      %77 = llvm.load %76 : !llvm.ptr<f32, 3>
      %78 = llvm.getelementptr %13[%75] : (!llvm.ptr<f32, 3>, i32) -> !llvm.ptr<f32, 3>
      %79 = llvm.load %78 : !llvm.ptr<f32, 3>
      %80 = llvm.fcmp "ugt" %77, %79 : f32
      %81 = llvm.select %80, %77, %79 : i1, f32
      %82 = llvm.fcmp "uno" %79, %79 : f32
      %83 = llvm.select %82, %79, %81 : i1, f32
      %84 = llvm.getelementptr %arg3[%28] : (!llvm.ptr<f32>, i32) -> !llvm.ptr<f32>
      %85 = llvm.load %84 : !llvm.ptr<f32>
      llvm.br ^bb18(%85 : f32)
    ^bb18(%86: f32):  // 2 preds: ^bb17, ^bb18
      %87 = llvm.fcmp "ogt" %86, %83 : f32
      %88 = llvm.select %87, %86, %83 : i1, f32
      %89 = llvm.bitcast %84 : !llvm.ptr<f32> to !llvm.ptr<i32>
      %90 = llvm.bitcast %86 : f32 to i32
      %91 = llvm.bitcast %88 : f32 to i32
      %92 = llvm.cmpxchg %89, %90, %91 acq_rel monotonic : !llvm.ptr<i32>, i32
      %93 = llvm.extractvalue %92[0] : !llvm.struct<(i32, i1)> 
      %94 = llvm.bitcast %93 : i32 to f32
      %95 = llvm.extractvalue %92[1] : !llvm.struct<(i32, i1)> 
      llvm.cond_br %95, ^bb19, ^bb18(%94 : f32)
    ^bb19:  // pred: ^bb18
      llvm.br ^bb20
    ^bb20:  // 2 preds: ^bb16, ^bb19
      llvm.br ^bb21
    ^bb21:  // 2 preds: ^bb1, ^bb20
      llvm.return
    }
  }
}


// -----// IR Dump After DiscToLLVMPass (disc-to-llvm) //----- //
module attributes {gpu.container_module} {
  llvm.mlir.global internal constant @ral_send_output___cpu___pvoid_i64_m2df32___void("ral_send_output___cpu___pvoid_i64_m2df32___void\00") {addr_space = 0 : i32}
  llvm.mlir.global internal constant @dealloc___gpu___pvoid_pvoid___void("dealloc___gpu___pvoid_pvoid___void\00") {addr_space = 0 : i32}
  llvm.mlir.global internal constant @inc_ref___gpu___pvoid_pvoid_m1df32_m1di64___m2df32("inc_ref___gpu___pvoid_pvoid_m1df32_m1di64___m2df32\00") {addr_space = 0 : i32}
  llvm.mlir.global internal constant @main_kernel_0_main_kColReduction_reduce__3_1_0___8w32h_1_kernel_name("main_kColReduction_reduce__3_1_0___8w32h_1\00") {addr_space = 0 : i32}
  llvm.mlir.global internal constant @main_kernel_0_blob_gpu.binary("P\EDU\BA\01\00\10\00\D8\08\00\00\00\00\00\00\02\00\01\01@\00\00\00\98\08\00\00\00\00\00\00\91\08\00\00\00\00\00\00\07\00\01\00P\00\00\00\00\00\00\00\00\00\00\00\11 \00\00\00\00\00\00\00\00\00\00\00\00\00\00\A8\14\00\00\00\00\00\00\A2\7FELF\02\01\013\07\00\01\00g\02\00\BE\00q\00\01\00\22\14\00\01\00\11\11\06\00\F5\0E\00P\05P\00@\008\00\03\00@\00\0C\00\01\00\00.shstrtab\00.\08\00'ym\08\00\F0\1B_shndx\00.nv.info\00.text.main_kColReduction_r\0A\00\F5\03e__3_1_0___8w32h_1:\00\0F4\00\1Doshared6\00\1AOrela\A0\00\1F?rel\D5\00\22\9Fconstant09\00\1A\B2debug_frame{\00\09\11\00!nv\14\00\11aE\00\0F\9E\01 \0F\8A\00\17\0F\C9\01\F4\8F$____wg_3\00\17\00\0C\00/24\02\02'o_param\09\02\1C\0F\01\00\05\8C]\00\00\00\03\00\0A\00\01\00\11\C2\18\00,\0B\00\01\00 \9C\01\18\00,\09\00\01\00\11\DC\18\00,\04\00\01\00\11\FA\18\00,\07\00\01\00f2\00\00\00\12\10x\00!\80\08\07\00v\00\FF\FF\FF\FF(\00\0C\00\00\01\00@\03\00\04|\08\00\D1\0F\0C\81\80\80(\00\08\FF\81\80(\08\0B\00\06(\00/0\00\01\00\03\13pX\00\10\04\9B\00\10\04\CD\04\02E\002\04\D4\01\18\00C/\08\00\06\DF\04\22\04#\0C\00\00\01\00'\04\12\0C\00\17\11\0C\00!7\04 \05\F1\08\015\00\00\04\0A\08\00\03\00\00\00`\01\18\00\03\19\18\00\04\17\0C$\00u\03\00\10\00\00\F0!\10\009\02\00\08\10\00\10\01(\01%\F0\11\10\00\01\01\00\F2\0A\F0\11\00\03\1B\FF\00\04\1C\0C\00P\00\00\00\B0\06\00\00\B0\07\00\00\04\1ET\01#K\00\01\00\B1\02\02\08\10\0A/\22\00\00\00\08\10\00#\00\08\08\00\13\10\08\00\13\18\08\00\13 \08\00\13(\08\00\130\08\00\228\08\84\00\13\00\08\00\13\08\08\00\13\10\08\00\13\18\08\00\13 \08\00\13(\08\00\130\08\00\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\00Z\06b,\00\00\00H\00\01\005\02\00\00d\01\0F\01\00\FF\DA@$v\01\FF\7F\04\B2\FF\00\8E\07\00\C4\0F\00\19y\00\01\00\10%\BB\02a\0E\00\19y\03\00\01\00\10!-\00\F0\04\0E\00$z\00\00\00X\00\00\03\02\8E\07\00\CA\1F\00\0C\10\00\C5Y\00\00p`\F0\03\00\DA\0F\00M\9B\04\B0\80\03\00\EA\0F\00\19x\03\FF\1F\0F\00\F1\00\14\01\00\00\E2\0F\00\B9z\04\00\00F\00\00\F3\04\93\E2\0F\00Ey\00\00\C0\040\00\B1\E2\0F\00$t\05\FF\00\00\80\FF\90\00a\E2\0F\00\11r\03\81\00`\FF@\8F\07\00\C8P\00 \04\FF\C1\02\10\03P\00q\E4\0F\00\12x\03\03\81\04\F1\00\C0\8E\07\00\C6\0F\00'x\02\04}\0C\CE\C7@\00P\C8\0F\00$x4\03B\00\00\03\0AP\00P\19x\07\FF\05\F0\020\16\01\00 \00@\12x\08\00\A0\00\01@\00\D4\E2\0F\04$x\03\07\D7\FF\FF\FF\04\020\00\11\060\00\22\00\16p\00P\0Cx\00\00\7F\D8\04\22@\F4\B0\00@x\03\03 \06\04\000\00\10\E4\10\00\11\02\91\03\22\06\02\90\00`\0Cx\00\03\13\050\00\13\F20\00C\02\02\04\00p\01\96\D8\0F\00G\19\00\00\B0\030\01\95$x\0A\06\00E\01\00\FFP\00%\06\07P\00\10\E4\A0\00t\05\07\E0\22\0A\00\0A \00Dr\07\FF\FFP\00\010\00 \09\04\90\00\14\05 \00\08P\01\10\C4 \00 \04\08\10\01\16\09`\00$\06@@\00\11\CA\B0\00 \06m\E0\00!B\FC\D0\01@$\E8\08\040\00\14\070\005\E4\09\FF\C0\00\93\C8\0F\00%\E6\08\08\00ZP\00p\CA\0F\00\81\E9\0C\08 \00\C0\00\19\1E\0C\00\A2\00\00\10x\0A\06@\000\FF\E0\FF@\00f\04\10x\0D\06\02\10\00\00p\00\12\0Ap\00#\F8\03\10\00\12\0D\10\00\11\F6\10\001\1Cx\00\01\00\F1\04p\F0\F0\03\00\D2\0F\00\10\C8\0A\04\14\05\00\00\07\E0\FF\A0\01:$\C4\0B\90\000\C6\0A\0A\90\00\14\0B\90\005\C9\0E\0A\90\00\94\E2\02\00\10\B8\08\04(\0A@\00G\1F\00$\B4\D0\000\E2\0F\00\A0\00\14\03\A0\00W\C6\0F\00%\B6\E0\00f\CC\0F\00\81\B9\08\E0\00q\22\0F\00\0B\E2\00\0CF\06\A4\80\FA\03\00\C8O\00\1C\E8\00\B0\001\02\00\E4 \00\11\05 \00!\C0\FAP\01A\08\82\0C\0CP\02:\00\80\06\F0\00\02 \00g\10\D8\0D\04<\0F\E0\00'\D4\0A\E0\00`/\00%\D6\0A\0D\90\00\14\0A\90\00&\D9\0A\E0\00a\A2\0E\00$\E2\05\10\02\13\0C\D0\00Q\0B\C2\00\0E\0Eo\00i\FC\03\00\E4\8F\08P\01\00`\01\14\C8\10\00\11p\10\00A\0B\C2\00\050\00#\C0\FC\B0\00\22\0E\0E\B0\00\11\00\B0\01#\0B\B2q\06\00P\00R\C6\0F\01$\C2p\00\13\0Ep\00\0D`\00\1B\B8`\001\B2\00\05@\00\12\C0\F0\01c\10x\07\07P\14p\01\10\D6p\00#\08\08p\00\10\06\C0\01\22$\B2`\00\1F\08`\00\04W\10x\06\06\04P\02A\0B\D2\00\0A\D6\08\10\80`\00aO\08\0B\D2\00\05\10\00\13\C0P\02$\D8\00\90\00\13\02\80\01\10\07p\03BpR\F8\03\80\00\22\0A\0A\80\00!\80\05\80\00\12\D2\80\00\13\0A\80\00`GI\00\00\D0\FCq\04\11\83\B0\03\14A0\05\03\C0\03A\88s\00\02@\00\00\9E\07f\E8\0F\00\1D{\00\01\00\93\EC\0F\00\84\A9\07\02\00\10 \00*\22\0E\D0\00\11\C4\90\00\01\90\041pD\F2 \05T\84\A9\04\02\00P\00qb\0E\00\0B\A2\00\07\CE\09\10\80\D0\00u\1F\08\0B\A2\00\04\07P\01T/\00\1C\A8\00P\00\13\01\E0\00 \00?\10\00#@\F6\E0\00!\07\070\02\00`\01\1B\E4P\01\8F\C6\0F\00\88\A3\00\02\07\C0\00\09!\B9\05O\09\22\00\08 \06+\84\B9\A0\001\B2\00\05\00\01`\80\F4\03\00\C4\1F\10\00(\04\05\A0\00\06\10\02\12\01`\027\05\05\04\F0\01/\88\B3@\01\0B9M\19\00p\019$t\04\C0\03!\84y\CB\08\02@\00\C3\E6\0F\00%v\04\03\00\\\00\00\04\00\05f\84y\03\02\00\04\C0\00E\81y\06\04@\03bb\03\00\0Br\00q\06\22\80\F0`\011r\00\00\10\00#\C0\F2@\01\02\1F\00@\00\00\80\04\90\031\0Br\000\040\00@\F0\E0\05(\0EF\10\02b\E6\0F\00\08r\07 \00\00\01\00p\CC\0F\00\A9s\07\04h\09\C0\07\E1\1E\00\00\A4\0E\00\0Cr\00\07\10\00 pR@\00QO\00$r\06p\02\14\07 \06V\09\00\00\90\FFp\02\1BMp\02TGy\00\00\F0 \00f\C0\0F\00\18y\00\01\00\0F\10\00\90\0F\01\00-#\01\00\A0\01\0B\01\00\22@\00\01\00=\9E\01\000\00\08\01\00\1F\0B@\00\04\13\DE)\00?\09\02\00@\00\0A\13\13<\0B\0C\01\00\13\E8U\00\03\8F\03\01$\00\13\05w\02\00\01\00\22\18\00\01\00.q\01T\00\00\01\00\11\90\95\02O\00\00p\00\80\00\0B\1F)'\00\03\02m\00\00v\0C\01\06\00\06\E4\00*\04\00\01\00\1Fc@\00\04\130@\00\17x@\00\1F\0A@\00\00!\8F\01D\01\0D@\00\13\A8@\00*\D8\00\01\00\1B\08\08\00?~\01\00\CE\0E\00\01\1E\07\01\01\00&\10\00\80\00\17\048\00\04\18\00\138@\01\0C\84\01\13\90@\00\17x1\01\0F\C0\00\01\132T\01\15\06R\00\0A\10\0F&\80\08\80\00j\06\00\00\11\80\00\01\00\13\97\94\00+\03\00\01\00\03\B0\13/\00\04\80\00\0B\14\06\AB\01/\14\00\01\00\02\1B\A8\08\00\17\08\08\02\17\05\E8\00\0C\01\009p\0A\00\08\00\088\00\18\06\A0\00\0F\01\00\05\03\A9\00\80\08\00\00\00\00\00\00\00\00\00\00\00\00\00\00") {addr_space = 0 : i32}
  llvm.mlir.global internal constant @ral_kernel_launch___gpu___pvoid_ppvoid_i64_pvoid_i64_i64_i64_i64_i64_i64_i32_pvoid_i32_ppvoid___void("ral_kernel_launch___gpu___pvoid_ppvoid_i64_pvoid_i64_i64_i64_i64_i64_i64_i32_pvoid_i32_ppvoid___void\00") {addr_space = 0 : i32}
  llvm.mlir.global internal constant @main_kernel_main_kColReduction_reduce__3_1_0___8w32h_kernel_name("main_kColReduction_reduce__3_1_0___8w32h\00") {addr_space = 0 : i32}
  llvm.mlir.global internal constant @main_kernel_blob_gpu.binary("P\EDU\BA\01\00\10\008\04\00\00\00\00\00\00\02\00\01\01@\00\00\00\F8\03\00\00\00\00\00\00\F3\03\00\00\00\00\00\00\07\00\01\00P\00\00\00\00\00\00\00\00\00\00\00\11 \00\00\00\00\00\00\00\00\00\00\00\00\00\00\E8\0B\00\00\00\00\00\00\A2\7FELF\02\01\013\07\00\01\00f\02\00\BE\00q\00\01\00!@\0B\07\001\00\80\08\07\00\F5\0E\00P\05P\00@\008\00\03\00@\00\0B\00\01\00\00.shstrtab\00.\08\00'ym\08\00\F0\1B_shndx\00.nv.info\00.text.main_kColReduction_r\0A\00\F5\01e__3_1_0___8w32h8\00\0F2\00\1Boshared4\00\1B\9Fconstant07\00\18\FA\01debug_frame\00.rel\11\00!nv\14\00\11aC\00\0F+\01 \0F\88\00\15\0FT\01\BAo_param[\01\1C\0F\01\00\06\8C[\00\00\00\03\00\0A\00\01\00\11\F0\18\00,\09\00\01\00 .\01\18\00,\04\00\01\00\11L\18\00,\07\00\01\00f2\00\00\00\12\10`\00!\80\01\07\00v\00\FF\FF\FF\FF(\00\0C\00\00\01\00@\03\00\04|\08\00\D1\0F\0C\81\80\80(\00\08\FF\81\80(\08\0B\00\06(\00/0\00\01\00\03\13pX\00\10\04\9B\00R\04\14\00\00\00E\00\01\0B\00\00\13\00p/\08\00\05\00\00\00\A7\03\22\04#\0C\00\00\01\00'\04\12\0C\00\17\11\0C\00!7\04\E8\03\F1\08\015\00\00\04\0A\08\00\02\00\00\00`\01\10\00\03\19\10\00\04\17\0C$\00\10\01N\00%\F0!\10\00\01\01\00\F2\02\F0\11\00\03\1B\FF\00\04\1C\08\00P\00\00\00\B0\00\01\00#K\00\01\00s\02\02\08\10\0A/\22\8B\00\00\07\00\03\08\00\13\10\08\00\13\18\08\00\13 \08\00\13(\08\00\130\08\00\228\08|\00\13\00\08\00\13\08\08\00\13\10\08\00\13\18\08\00\13 \08\00\13(\08\00\130\08\00\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\A2\00\00\00\14,\00\00\00H\00\01\00\00 \01/\05\00\01\00\FF\C0A\02z\01\00\1F\04\B1\0F\00\00\00\C4\0F\00\19y\02\00\01\00\10%\9B\02Q\0E\00\19y\03\0F\00\F5\1A\00!\00\00\00$\0E\00$z\02\02\00X\00\00\03\02\8E\07\00\CA\1F\00\0Cx\00\02\13\05\00\00p@\F0\03\00\DA\0F\00MS\04\A0\80\03\00\EA\0F\005t\03\FF\B3\03\10\FF\C0\03P\E2\0F\00\02xF\02B\80\FF\00\0F\10\00r\B9z\04\00\00F\00\84\00\90\D0\0F\00%v\02\02\00Zl\04\00`\00`\0F\00\86y\00\022\00@\04\19\10\0C0\009My\00`\00PGy\00\00\F09\04\A6\FF\83\03\00\C0\0F\00\18y\00\01\00\0F\10\00\90\0F\01\00-\00W\01.\03\00\01\00\22@\00\01\00=+\01\000\00\08\01\00\1F\0B@\00\04\13k)\00\1F[@\00\0C\13\13\1C\04\0C\01\00\13\C8\15\00&\90\00@\04#\04\00\95\04\00\06\05\12\00\01\00\1F\FET\00\00\00\01\00\13X\95\00/p\00\80\00\0B\1F)'\00\03#\00\C8@\00\04P\06\04\E4\00*\04\00\01\00\1Fa@\00\04\13\F81\00&L\00@\00\1F\0A@\00\00!\1C\01D\01\0D@\00\13H)\00*\D8\00\01\00\1B\08\08\00?\0B\01\00\86\07\00Q\00\00 \05\00\01\00&\10\00\80\00\17\048\00\04\18\00\13\C7\14\01\0C\84\01*0\058\07\1F\00\C0\00\04\132@\00+\06\00\01\00\1A\07\D0\07\12\03\00\06:\08\80\00\01\00\13\06\18\06\04(\0B\0C\01\00*\A8\00\08\00\04\F8\00\13\018\00\04\A8\00\0C\01\009P\03\00\08\00\088\00/\06\00\01\00\17\80\08\00\00\00\00\00\00\00\00\00\00\00\00") {addr_space = 0 : i32}
  llvm.mlir.global internal constant @alloc___gpu___pvoid_i64___pvoid("alloc___gpu___pvoid_i64___pvoid\00") {addr_space = 0 : i32}
  llvm.mlir.global internal constant @ral_recv_input___cpu___pvoid_i64___m3df32("ral_recv_input___cpu___pvoid_i64___m3df32\00") {addr_space = 0 : i32}
  llvm.func @disc_ral_call(!llvm.ptr<i8>, !llvm.ptr<i8>, !llvm.ptr<ptr<i8>>)
  llvm.func @main(%arg0: !llvm.ptr<i8>) attributes {tf.entry_function = {input_placements = "gpu", inputs = "input0", output_placements = "gpu", outputs = "output0"}} {
    %0 = llvm.mlir.constant(256 : index) : i64
    %1 = llvm.mlir.constant(41 : index) : i64
    %2 = llvm.mlir.constant(10496 : index) : i64
    %3 = llvm.mlir.constant(6 : index) : i64
    %4 = llvm.mlir.constant(0 : i32) : i32
    %5 = llvm.mlir.constant(0 : index) : i64
    %6 = llvm.mlir.constant(100 : index) : i64
    %7 = llvm.mlir.constant(1 : index) : i64
    %8 = llvm.mlir.constant(13 : index) : i64
    %9 = llvm.mlir.constant(0 : i32) : i32
    %10 = llvm.mlir.constant(1 : i32) : i32
    %11 = llvm.alloca %10 x !llvm.struct<"", (ptr<i8>, i64, struct<(ptr<f32>, ptr<f32>, i64, array<3 x i64>, array<3 x i64>)>)> : (i32) -> !llvm.ptr<struct<"", (ptr<i8>, i64, struct<(ptr<f32>, ptr<f32>, i64, array<3 x i64>, array<3 x i64>)>)>>
    %12 = llvm.mlir.constant(3 : i32) : i32
    %13 = llvm.alloca %12 x !llvm.ptr<i8> : (i32) -> !llvm.ptr<ptr<i8>>
    %14 = llvm.mlir.constant(0 : i32) : i32
    %15 = llvm.getelementptr %11[%9, 0] : (!llvm.ptr<struct<"", (ptr<i8>, i64, struct<(ptr<f32>, ptr<f32>, i64, array<3 x i64>, array<3 x i64>)>)>>, i32) -> !llvm.ptr<ptr<i8>>
    llvm.store %arg0, %15 : !llvm.ptr<ptr<i8>>
    %16 = llvm.getelementptr %13[%14] : (!llvm.ptr<ptr<i8>>, i32) -> !llvm.ptr<ptr<i8>>
    %17 = llvm.bitcast %15 : !llvm.ptr<ptr<i8>> to !llvm.ptr<i8>
    llvm.store %17, %16 : !llvm.ptr<ptr<i8>>
    %18 = llvm.mlir.constant(1 : i32) : i32
    %19 = llvm.getelementptr %11[%9, 1] : (!llvm.ptr<struct<"", (ptr<i8>, i64, struct<(ptr<f32>, ptr<f32>, i64, array<3 x i64>, array<3 x i64>)>)>>, i32) -> !llvm.ptr<i64>
    llvm.store %5, %19 : !llvm.ptr<i64>
    %20 = llvm.getelementptr %13[%18] : (!llvm.ptr<ptr<i8>>, i32) -> !llvm.ptr<ptr<i8>>
    %21 = llvm.bitcast %19 : !llvm.ptr<i64> to !llvm.ptr<i8>
    llvm.store %21, %20 : !llvm.ptr<ptr<i8>>
    %22 = llvm.mlir.constant(2 : i32) : i32
    %23 = llvm.getelementptr %11[%9, 2] : (!llvm.ptr<struct<"", (ptr<i8>, i64, struct<(ptr<f32>, ptr<f32>, i64, array<3 x i64>, array<3 x i64>)>)>>, i32) -> !llvm.ptr<struct<(ptr<f32>, ptr<f32>, i64, array<3 x i64>, array<3 x i64>)>>
    %24 = llvm.getelementptr %13[%22] : (!llvm.ptr<ptr<i8>>, i32) -> !llvm.ptr<ptr<i8>>
    %25 = llvm.bitcast %23 : !llvm.ptr<struct<(ptr<f32>, ptr<f32>, i64, array<3 x i64>, array<3 x i64>)>> to !llvm.ptr<i8>
    llvm.store %25, %24 : !llvm.ptr<ptr<i8>>
    %26 = llvm.mlir.addressof @ral_recv_input___cpu___pvoid_i64___m3df32 : !llvm.ptr<array<42 x i8>>
    %27 = llvm.mlir.constant(0 : index) : i64
    %28 = llvm.getelementptr %26[%27, %27] : (!llvm.ptr<array<42 x i8>>, i64, i64) -> !llvm.ptr<i8>
    llvm.call @disc_ral_call(%arg0, %28, %13) : (!llvm.ptr<i8>, !llvm.ptr<i8>, !llvm.ptr<ptr<i8>>) -> ()
    %29 = llvm.load %23 : !llvm.ptr<struct<(ptr<f32>, ptr<f32>, i64, array<3 x i64>, array<3 x i64>)>>
    %30 = llvm.mlir.constant(1300 : index) : i64
    %31 = llvm.mlir.constant(1 : index) : i64
    %32 = llvm.mlir.null : !llvm.ptr<f32>
    %33 = llvm.getelementptr %32[%30] : (!llvm.ptr<f32>, i64) -> !llvm.ptr<f32>
    %34 = llvm.ptrtoint %33 : !llvm.ptr<f32> to i64
    %35 = llvm.mlir.constant(0 : i32) : i32
    %36 = llvm.mlir.constant(1 : i32) : i32
    %37 = llvm.alloca %36 x !llvm.struct<".1", (ptr<i8>, i64, ptr<i8>)> : (i32) -> !llvm.ptr<struct<".1", (ptr<i8>, i64, ptr<i8>)>>
    %38 = llvm.mlir.constant(3 : i32) : i32
    %39 = llvm.alloca %38 x !llvm.ptr<i8> : (i32) -> !llvm.ptr<ptr<i8>>
    %40 = llvm.mlir.constant(0 : i32) : i32
    %41 = llvm.getelementptr %37[%35, 0] : (!llvm.ptr<struct<".1", (ptr<i8>, i64, ptr<i8>)>>, i32) -> !llvm.ptr<ptr<i8>>
    llvm.store %arg0, %41 : !llvm.ptr<ptr<i8>>
    %42 = llvm.getelementptr %39[%40] : (!llvm.ptr<ptr<i8>>, i32) -> !llvm.ptr<ptr<i8>>
    %43 = llvm.bitcast %41 : !llvm.ptr<ptr<i8>> to !llvm.ptr<i8>
    llvm.store %43, %42 : !llvm.ptr<ptr<i8>>
    %44 = llvm.mlir.constant(1 : i32) : i32
    %45 = llvm.getelementptr %37[%35, 1] : (!llvm.ptr<struct<".1", (ptr<i8>, i64, ptr<i8>)>>, i32) -> !llvm.ptr<i64>
    llvm.store %34, %45 : !llvm.ptr<i64>
    %46 = llvm.getelementptr %39[%44] : (!llvm.ptr<ptr<i8>>, i32) -> !llvm.ptr<ptr<i8>>
    %47 = llvm.bitcast %45 : !llvm.ptr<i64> to !llvm.ptr<i8>
    llvm.store %47, %46 : !llvm.ptr<ptr<i8>>
    %48 = llvm.mlir.constant(2 : i32) : i32
    %49 = llvm.getelementptr %37[%35, 2] : (!llvm.ptr<struct<".1", (ptr<i8>, i64, ptr<i8>)>>, i32) -> !llvm.ptr<ptr<i8>>
    %50 = llvm.getelementptr %39[%48] : (!llvm.ptr<ptr<i8>>, i32) -> !llvm.ptr<ptr<i8>>
    %51 = llvm.bitcast %49 : !llvm.ptr<ptr<i8>> to !llvm.ptr<i8>
    llvm.store %51, %50 : !llvm.ptr<ptr<i8>>
    %52 = llvm.mlir.addressof @alloc___gpu___pvoid_i64___pvoid : !llvm.ptr<array<32 x i8>>
    %53 = llvm.mlir.constant(0 : index) : i64
    %54 = llvm.getelementptr %52[%53, %53] : (!llvm.ptr<array<32 x i8>>, i64, i64) -> !llvm.ptr<i8>
    llvm.call @disc_ral_call(%arg0, %54, %39) : (!llvm.ptr<i8>, !llvm.ptr<i8>, !llvm.ptr<ptr<i8>>) -> ()
    %55 = llvm.load %49 : !llvm.ptr<ptr<i8>>
    %56 = llvm.mlir.undef : !llvm.struct<(ptr<f32>, ptr<f32>, i64, array<1 x i64>, array<1 x i64>)>
    %57 = llvm.bitcast %55 : !llvm.ptr<i8> to !llvm.ptr<f32>
    %58 = llvm.insertvalue %57, %56[0] : !llvm.struct<(ptr<f32>, ptr<f32>, i64, array<1 x i64>, array<1 x i64>)> 
    %59 = llvm.insertvalue %57, %58[1] : !llvm.struct<(ptr<f32>, ptr<f32>, i64, array<1 x i64>, array<1 x i64>)> 
    %60 = llvm.mlir.constant(0 : index) : i64
    %61 = llvm.insertvalue %60, %59[2] : !llvm.struct<(ptr<f32>, ptr<f32>, i64, array<1 x i64>, array<1 x i64>)> 
    %62 = llvm.mlir.constant(1 : index) : i64
    %63 = llvm.insertvalue %30, %61[3, 0] : !llvm.struct<(ptr<f32>, ptr<f32>, i64, array<1 x i64>, array<1 x i64>)> 
    %64 = llvm.insertvalue %62, %63[4, 0] : !llvm.struct<(ptr<f32>, ptr<f32>, i64, array<1 x i64>, array<1 x i64>)> 
    %65 = llvm.mlir.addressof @main_kernel_blob_gpu.binary : !llvm.ptr<array<1096 x i8>>
    %66 = llvm.mlir.constant(0 : index) : i64
    %67 = llvm.getelementptr %65[%66, %66] : (!llvm.ptr<array<1096 x i8>>, i64, i64) -> !llvm.ptr<i8>
    %68 = llvm.mlir.constant(1 : i32) : i32
    %69 = llvm.alloca %68 x !llvm.ptr<i8> : (i32) -> !llvm.ptr<ptr<i8>>
    %70 = llvm.mlir.constant(0 : i32) : i32
    %71 = llvm.getelementptr %69[%70] : (!llvm.ptr<ptr<i8>>, i32) -> !llvm.ptr<ptr<i8>>
    llvm.store %67, %71 : !llvm.ptr<ptr<i8>>
    %72 = llvm.mlir.constant(1 : i64) : i64
    %73 = llvm.mlir.addressof @main_kernel_main_kColReduction_reduce__3_1_0___8w32h_kernel_name : !llvm.ptr<array<41 x i8>>
    %74 = llvm.mlir.constant(0 : index) : i64
    %75 = llvm.getelementptr %73[%74, %74] : (!llvm.ptr<array<41 x i8>>, i64, i64) -> !llvm.ptr<i8>
    %76 = llvm.extractvalue %64[0] : !llvm.struct<(ptr<f32>, ptr<f32>, i64, array<1 x i64>, array<1 x i64>)> 
    %77 = llvm.extractvalue %64[1] : !llvm.struct<(ptr<f32>, ptr<f32>, i64, array<1 x i64>, array<1 x i64>)> 
    %78 = llvm.extractvalue %64[2] : !llvm.struct<(ptr<f32>, ptr<f32>, i64, array<1 x i64>, array<1 x i64>)> 
    %79 = llvm.extractvalue %64[3, 0] : !llvm.struct<(ptr<f32>, ptr<f32>, i64, array<1 x i64>, array<1 x i64>)> 
    %80 = llvm.extractvalue %64[4, 0] : !llvm.struct<(ptr<f32>, ptr<f32>, i64, array<1 x i64>, array<1 x i64>)> 
    %81 = llvm.mlir.constant(1 : i32) : i32
    %82 = llvm.alloca %81 x !llvm.struct<".2", (i64, ptr<f32>)> : (i32) -> !llvm.ptr<struct<".2", (i64, ptr<f32>)>>
    %83 = llvm.mlir.constant(2 : i32) : i32
    %84 = llvm.alloca %83 x !llvm.ptr<i8> : (i32) -> !llvm.ptr<ptr<i8>>
    %85 = llvm.mlir.constant(0 : i32) : i32
    %86 = llvm.mlir.constant(0 : i32) : i32
    %87 = llvm.getelementptr %82[%85, 0] : (!llvm.ptr<struct<".2", (i64, ptr<f32>)>>, i32) -> !llvm.ptr<i64>
    llvm.store %0, %87 : !llvm.ptr<i64>
    %88 = llvm.getelementptr %84[%86] : (!llvm.ptr<ptr<i8>>, i32) -> !llvm.ptr<ptr<i8>>
    %89 = llvm.bitcast %87 : !llvm.ptr<i64> to !llvm.ptr<i8>
    llvm.store %89, %88 : !llvm.ptr<ptr<i8>>
    %90 = llvm.mlir.constant(1 : i32) : i32
    %91 = llvm.getelementptr %82[%85, 1] : (!llvm.ptr<struct<".2", (i64, ptr<f32>)>>, i32) -> !llvm.ptr<ptr<f32>>
    llvm.store %77, %91 : !llvm.ptr<ptr<f32>>
    %92 = llvm.getelementptr %84[%90] : (!llvm.ptr<ptr<i8>>, i32) -> !llvm.ptr<ptr<i8>>
    %93 = llvm.bitcast %91 : !llvm.ptr<ptr<f32>> to !llvm.ptr<i8>
    llvm.store %93, %92 : !llvm.ptr<ptr<i8>>
    %94 = llvm.mlir.constant(0 : i32) : i32
    %95 = llvm.mlir.constant(2 : i32) : i32
    %96 = llvm.inttoptr %94 : i32 to !llvm.ptr<i8>
    %97 = llvm.mlir.constant(0 : i32) : i32
    %98 = llvm.mlir.constant(1 : i32) : i32
    %99 = llvm.alloca %98 x !llvm.struct<".3", (ptr<i8>, ptr<ptr<i8>>, i64, ptr<i8>, i64, i64, i64, i64, i64, i64, i32, ptr<i8>, i32, ptr<ptr<i8>>)> : (i32) -> !llvm.ptr<struct<".3", (ptr<i8>, ptr<ptr<i8>>, i64, ptr<i8>, i64, i64, i64, i64, i64, i64, i32, ptr<i8>, i32, ptr<ptr<i8>>)>>
    %100 = llvm.mlir.constant(14 : i32) : i32
    %101 = llvm.alloca %100 x !llvm.ptr<i8> : (i32) -> !llvm.ptr<ptr<i8>>
    %102 = llvm.mlir.constant(0 : i32) : i32
    %103 = llvm.getelementptr %99[%97, 0] : (!llvm.ptr<struct<".3", (ptr<i8>, ptr<ptr<i8>>, i64, ptr<i8>, i64, i64, i64, i64, i64, i64, i32, ptr<i8>, i32, ptr<ptr<i8>>)>>, i32) -> !llvm.ptr<ptr<i8>>
    llvm.store %arg0, %103 : !llvm.ptr<ptr<i8>>
    %104 = llvm.getelementptr %101[%102] : (!llvm.ptr<ptr<i8>>, i32) -> !llvm.ptr<ptr<i8>>
    %105 = llvm.bitcast %103 : !llvm.ptr<ptr<i8>> to !llvm.ptr<i8>
    llvm.store %105, %104 : !llvm.ptr<ptr<i8>>
    %106 = llvm.mlir.constant(1 : i32) : i32
    %107 = llvm.getelementptr %99[%97, 1] : (!llvm.ptr<struct<".3", (ptr<i8>, ptr<ptr<i8>>, i64, ptr<i8>, i64, i64, i64, i64, i64, i64, i32, ptr<i8>, i32, ptr<ptr<i8>>)>>, i32) -> !llvm.ptr<ptr<ptr<i8>>>
    llvm.store %69, %107 : !llvm.ptr<ptr<ptr<i8>>>
    %108 = llvm.getelementptr %101[%106] : (!llvm.ptr<ptr<i8>>, i32) -> !llvm.ptr<ptr<i8>>
    %109 = llvm.bitcast %107 : !llvm.ptr<ptr<ptr<i8>>> to !llvm.ptr<i8>
    llvm.store %109, %108 : !llvm.ptr<ptr<i8>>
    %110 = llvm.mlir.constant(2 : i32) : i32
    %111 = llvm.getelementptr %99[%97, 2] : (!llvm.ptr<struct<".3", (ptr<i8>, ptr<ptr<i8>>, i64, ptr<i8>, i64, i64, i64, i64, i64, i64, i32, ptr<i8>, i32, ptr<ptr<i8>>)>>, i32) -> !llvm.ptr<i64>
    llvm.store %72, %111 : !llvm.ptr<i64>
    %112 = llvm.getelementptr %101[%110] : (!llvm.ptr<ptr<i8>>, i32) -> !llvm.ptr<ptr<i8>>
    %113 = llvm.bitcast %111 : !llvm.ptr<i64> to !llvm.ptr<i8>
    llvm.store %113, %112 : !llvm.ptr<ptr<i8>>
    %114 = llvm.mlir.constant(3 : i32) : i32
    %115 = llvm.getelementptr %99[%97, 3] : (!llvm.ptr<struct<".3", (ptr<i8>, ptr<ptr<i8>>, i64, ptr<i8>, i64, i64, i64, i64, i64, i64, i32, ptr<i8>, i32, ptr<ptr<i8>>)>>, i32) -> !llvm.ptr<ptr<i8>>
    llvm.store %75, %115 : !llvm.ptr<ptr<i8>>
    %116 = llvm.getelementptr %101[%114] : (!llvm.ptr<ptr<i8>>, i32) -> !llvm.ptr<ptr<i8>>
    %117 = llvm.bitcast %115 : !llvm.ptr<ptr<i8>> to !llvm.ptr<i8>
    llvm.store %117, %116 : !llvm.ptr<ptr<i8>>
    %118 = llvm.mlir.constant(4 : i32) : i32
    %119 = llvm.getelementptr %99[%97, 4] : (!llvm.ptr<struct<".3", (ptr<i8>, ptr<ptr<i8>>, i64, ptr<i8>, i64, i64, i64, i64, i64, i64, i32, ptr<i8>, i32, ptr<ptr<i8>>)>>, i32) -> !llvm.ptr<i64>
    llvm.store %3, %119 : !llvm.ptr<i64>
    %120 = llvm.getelementptr %101[%118] : (!llvm.ptr<ptr<i8>>, i32) -> !llvm.ptr<ptr<i8>>
    %121 = llvm.bitcast %119 : !llvm.ptr<i64> to !llvm.ptr<i8>
    llvm.store %121, %120 : !llvm.ptr<ptr<i8>>
    %122 = llvm.mlir.constant(5 : i32) : i32
    %123 = llvm.getelementptr %99[%97, 5] : (!llvm.ptr<struct<".3", (ptr<i8>, ptr<ptr<i8>>, i64, ptr<i8>, i64, i64, i64, i64, i64, i64, i32, ptr<i8>, i32, ptr<ptr<i8>>)>>, i32) -> !llvm.ptr<i64>
    llvm.store %7, %123 : !llvm.ptr<i64>
    %124 = llvm.getelementptr %101[%122] : (!llvm.ptr<ptr<i8>>, i32) -> !llvm.ptr<ptr<i8>>
    %125 = llvm.bitcast %123 : !llvm.ptr<i64> to !llvm.ptr<i8>
    llvm.store %125, %124 : !llvm.ptr<ptr<i8>>
    %126 = llvm.mlir.constant(6 : i32) : i32
    %127 = llvm.getelementptr %99[%97, 6] : (!llvm.ptr<struct<".3", (ptr<i8>, ptr<ptr<i8>>, i64, ptr<i8>, i64, i64, i64, i64, i64, i64, i32, ptr<i8>, i32, ptr<ptr<i8>>)>>, i32) -> !llvm.ptr<i64>
    llvm.store %7, %127 : !llvm.ptr<i64>
    %128 = llvm.getelementptr %101[%126] : (!llvm.ptr<ptr<i8>>, i32) -> !llvm.ptr<ptr<i8>>
    %129 = llvm.bitcast %127 : !llvm.ptr<i64> to !llvm.ptr<i8>
    llvm.store %129, %128 : !llvm.ptr<ptr<i8>>
    %130 = llvm.mlir.constant(7 : i32) : i32
    %131 = llvm.getelementptr %99[%97, 7] : (!llvm.ptr<struct<".3", (ptr<i8>, ptr<ptr<i8>>, i64, ptr<i8>, i64, i64, i64, i64, i64, i64, i32, ptr<i8>, i32, ptr<ptr<i8>>)>>, i32) -> !llvm.ptr<i64>
    llvm.store %0, %131 : !llvm.ptr<i64>
    %132 = llvm.getelementptr %101[%130] : (!llvm.ptr<ptr<i8>>, i32) -> !llvm.ptr<ptr<i8>>
    %133 = llvm.bitcast %131 : !llvm.ptr<i64> to !llvm.ptr<i8>
    llvm.store %133, %132 : !llvm.ptr<ptr<i8>>
    %134 = llvm.mlir.constant(8 : i32) : i32
    %135 = llvm.getelementptr %99[%97, 8] : (!llvm.ptr<struct<".3", (ptr<i8>, ptr<ptr<i8>>, i64, ptr<i8>, i64, i64, i64, i64, i64, i64, i32, ptr<i8>, i32, ptr<ptr<i8>>)>>, i32) -> !llvm.ptr<i64>
    llvm.store %7, %135 : !llvm.ptr<i64>
    %136 = llvm.getelementptr %101[%134] : (!llvm.ptr<ptr<i8>>, i32) -> !llvm.ptr<ptr<i8>>
    %137 = llvm.bitcast %135 : !llvm.ptr<i64> to !llvm.ptr<i8>
    llvm.store %137, %136 : !llvm.ptr<ptr<i8>>
    %138 = llvm.mlir.constant(9 : i32) : i32
    %139 = llvm.getelementptr %99[%97, 9] : (!llvm.ptr<struct<".3", (ptr<i8>, ptr<ptr<i8>>, i64, ptr<i8>, i64, i64, i64, i64, i64, i64, i32, ptr<i8>, i32, ptr<ptr<i8>>)>>, i32) -> !llvm.ptr<i64>
    llvm.store %7, %139 : !llvm.ptr<i64>
    %140 = llvm.getelementptr %101[%138] : (!llvm.ptr<ptr<i8>>, i32) -> !llvm.ptr<ptr<i8>>
    %141 = llvm.bitcast %139 : !llvm.ptr<i64> to !llvm.ptr<i8>
    llvm.store %141, %140 : !llvm.ptr<ptr<i8>>
    %142 = llvm.mlir.constant(10 : i32) : i32
    %143 = llvm.getelementptr %99[%97, 10] : (!llvm.ptr<struct<".3", (ptr<i8>, ptr<ptr<i8>>, i64, ptr<i8>, i64, i64, i64, i64, i64, i64, i32, ptr<i8>, i32, ptr<ptr<i8>>)>>, i32) -> !llvm.ptr<i32>
    llvm.store %94, %143 : !llvm.ptr<i32>
    %144 = llvm.getelementptr %101[%142] : (!llvm.ptr<ptr<i8>>, i32) -> !llvm.ptr<ptr<i8>>
    %145 = llvm.bitcast %143 : !llvm.ptr<i32> to !llvm.ptr<i8>
    llvm.store %145, %144 : !llvm.ptr<ptr<i8>>
    %146 = llvm.mlir.constant(11 : i32) : i32
    %147 = llvm.getelementptr %99[%97, 11] : (!llvm.ptr<struct<".3", (ptr<i8>, ptr<ptr<i8>>, i64, ptr<i8>, i64, i64, i64, i64, i64, i64, i32, ptr<i8>, i32, ptr<ptr<i8>>)>>, i32) -> !llvm.ptr<ptr<i8>>
    llvm.store %96, %147 : !llvm.ptr<ptr<i8>>
    %148 = llvm.getelementptr %101[%146] : (!llvm.ptr<ptr<i8>>, i32) -> !llvm.ptr<ptr<i8>>
    %149 = llvm.bitcast %147 : !llvm.ptr<ptr<i8>> to !llvm.ptr<i8>
    llvm.store %149, %148 : !llvm.ptr<ptr<i8>>
    %150 = llvm.mlir.constant(12 : i32) : i32
    %151 = llvm.getelementptr %99[%97, 12] : (!llvm.ptr<struct<".3", (ptr<i8>, ptr<ptr<i8>>, i64, ptr<i8>, i64, i64, i64, i64, i64, i64, i32, ptr<i8>, i32, ptr<ptr<i8>>)>>, i32) -> !llvm.ptr<i32>
    llvm.store %95, %151 : !llvm.ptr<i32>
    %152 = llvm.getelementptr %101[%150] : (!llvm.ptr<ptr<i8>>, i32) -> !llvm.ptr<ptr<i8>>
    %153 = llvm.bitcast %151 : !llvm.ptr<i32> to !llvm.ptr<i8>
    llvm.store %153, %152 : !llvm.ptr<ptr<i8>>
    %154 = llvm.mlir.constant(13 : i32) : i32
    %155 = llvm.getelementptr %99[%97, 13] : (!llvm.ptr<struct<".3", (ptr<i8>, ptr<ptr<i8>>, i64, ptr<i8>, i64, i64, i64, i64, i64, i64, i32, ptr<i8>, i32, ptr<ptr<i8>>)>>, i32) -> !llvm.ptr<ptr<ptr<i8>>>
    llvm.store %84, %155 : !llvm.ptr<ptr<ptr<i8>>>
    %156 = llvm.getelementptr %101[%154] : (!llvm.ptr<ptr<i8>>, i32) -> !llvm.ptr<ptr<i8>>
    %157 = llvm.bitcast %155 : !llvm.ptr<ptr<ptr<i8>>> to !llvm.ptr<i8>
    llvm.store %157, %156 : !llvm.ptr<ptr<i8>>
    %158 = llvm.mlir.addressof @ral_kernel_launch___gpu___pvoid_ppvoid_i64_pvoid_i64_i64_i64_i64_i64_i64_i32_pvoid_i32_ppvoid___void : !llvm.ptr<array<101 x i8>>
    %159 = llvm.mlir.constant(0 : index) : i64
    %160 = llvm.getelementptr %158[%159, %159] : (!llvm.ptr<array<101 x i8>>, i64, i64) -> !llvm.ptr<i8>
    llvm.call @disc_ral_call(%arg0, %160, %101) : (!llvm.ptr<i8>, !llvm.ptr<i8>, !llvm.ptr<ptr<i8>>) -> ()
    %161 = llvm.mlir.addressof @main_kernel_0_blob_gpu.binary : !llvm.ptr<array<2280 x i8>>
    %162 = llvm.mlir.constant(0 : index) : i64
    %163 = llvm.getelementptr %161[%162, %162] : (!llvm.ptr<array<2280 x i8>>, i64, i64) -> !llvm.ptr<i8>
    %164 = llvm.mlir.constant(1 : i32) : i32
    %165 = llvm.alloca %164 x !llvm.ptr<i8> : (i32) -> !llvm.ptr<ptr<i8>>
    %166 = llvm.mlir.constant(0 : i32) : i32
    %167 = llvm.getelementptr %165[%166] : (!llvm.ptr<ptr<i8>>, i32) -> !llvm.ptr<ptr<i8>>
    llvm.store %163, %167 : !llvm.ptr<ptr<i8>>
    %168 = llvm.mlir.constant(1 : i64) : i64
    %169 = llvm.mlir.addressof @main_kernel_0_main_kColReduction_reduce__3_1_0___8w32h_1_kernel_name : !llvm.ptr<array<43 x i8>>
    %170 = llvm.mlir.constant(0 : index) : i64
    %171 = llvm.getelementptr %169[%170, %170] : (!llvm.ptr<array<43 x i8>>, i64, i64) -> !llvm.ptr<i8>
    %172 = llvm.extractvalue %29[0] : !llvm.struct<(ptr<f32>, ptr<f32>, i64, array<3 x i64>, array<3 x i64>)> 
    %173 = llvm.extractvalue %29[1] : !llvm.struct<(ptr<f32>, ptr<f32>, i64, array<3 x i64>, array<3 x i64>)> 
    %174 = llvm.extractvalue %29[2] : !llvm.struct<(ptr<f32>, ptr<f32>, i64, array<3 x i64>, array<3 x i64>)> 
    %175 = llvm.extractvalue %29[3, 0] : !llvm.struct<(ptr<f32>, ptr<f32>, i64, array<3 x i64>, array<3 x i64>)> 
    %176 = llvm.extractvalue %29[3, 1] : !llvm.struct<(ptr<f32>, ptr<f32>, i64, array<3 x i64>, array<3 x i64>)> 
    %177 = llvm.extractvalue %29[3, 2] : !llvm.struct<(ptr<f32>, ptr<f32>, i64, array<3 x i64>, array<3 x i64>)> 
    %178 = llvm.extractvalue %29[4, 0] : !llvm.struct<(ptr<f32>, ptr<f32>, i64, array<3 x i64>, array<3 x i64>)> 
    %179 = llvm.extractvalue %29[4, 1] : !llvm.struct<(ptr<f32>, ptr<f32>, i64, array<3 x i64>, array<3 x i64>)> 
    %180 = llvm.extractvalue %29[4, 2] : !llvm.struct<(ptr<f32>, ptr<f32>, i64, array<3 x i64>, array<3 x i64>)> 
    %181 = llvm.extractvalue %64[0] : !llvm.struct<(ptr<f32>, ptr<f32>, i64, array<1 x i64>, array<1 x i64>)> 
    %182 = llvm.extractvalue %64[1] : !llvm.struct<(ptr<f32>, ptr<f32>, i64, array<1 x i64>, array<1 x i64>)> 
    %183 = llvm.extractvalue %64[2] : !llvm.struct<(ptr<f32>, ptr<f32>, i64, array<1 x i64>, array<1 x i64>)> 
    %184 = llvm.extractvalue %64[3, 0] : !llvm.struct<(ptr<f32>, ptr<f32>, i64, array<1 x i64>, array<1 x i64>)> 
    %185 = llvm.extractvalue %64[4, 0] : !llvm.struct<(ptr<f32>, ptr<f32>, i64, array<1 x i64>, array<1 x i64>)> 
    %186 = llvm.mlir.constant(1 : i32) : i32
    %187 = llvm.alloca %186 x !llvm.struct<".4", (i64, i64, ptr<f32>, ptr<f32>)> : (i32) -> !llvm.ptr<struct<".4", (i64, i64, ptr<f32>, ptr<f32>)>>
    %188 = llvm.mlir.constant(4 : i32) : i32
    %189 = llvm.alloca %188 x !llvm.ptr<i8> : (i32) -> !llvm.ptr<ptr<i8>>
    %190 = llvm.mlir.constant(0 : i32) : i32
    %191 = llvm.mlir.constant(0 : i32) : i32
    %192 = llvm.getelementptr %187[%190, 0] : (!llvm.ptr<struct<".4", (i64, i64, ptr<f32>, ptr<f32>)>>, i32) -> !llvm.ptr<i64>
    llvm.store %0, %192 : !llvm.ptr<i64>
    %193 = llvm.getelementptr %189[%191] : (!llvm.ptr<ptr<i8>>, i32) -> !llvm.ptr<ptr<i8>>
    %194 = llvm.bitcast %192 : !llvm.ptr<i64> to !llvm.ptr<i8>
    llvm.store %194, %193 : !llvm.ptr<ptr<i8>>
    %195 = llvm.mlir.constant(1 : i32) : i32
    %196 = llvm.getelementptr %187[%190, 1] : (!llvm.ptr<struct<".4", (i64, i64, ptr<f32>, ptr<f32>)>>, i32) -> !llvm.ptr<i64>
    llvm.store %2, %196 : !llvm.ptr<i64>
    %197 = llvm.getelementptr %189[%195] : (!llvm.ptr<ptr<i8>>, i32) -> !llvm.ptr<ptr<i8>>
    %198 = llvm.bitcast %196 : !llvm.ptr<i64> to !llvm.ptr<i8>
    llvm.store %198, %197 : !llvm.ptr<ptr<i8>>
    %199 = llvm.mlir.constant(2 : i32) : i32
    %200 = llvm.getelementptr %187[%190, 2] : (!llvm.ptr<struct<".4", (i64, i64, ptr<f32>, ptr<f32>)>>, i32) -> !llvm.ptr<ptr<f32>>
    llvm.store %173, %200 : !llvm.ptr<ptr<f32>>
    %201 = llvm.getelementptr %189[%199] : (!llvm.ptr<ptr<i8>>, i32) -> !llvm.ptr<ptr<i8>>
    %202 = llvm.bitcast %200 : !llvm.ptr<ptr<f32>> to !llvm.ptr<i8>
    llvm.store %202, %201 : !llvm.ptr<ptr<i8>>
    %203 = llvm.mlir.constant(3 : i32) : i32
    %204 = llvm.getelementptr %187[%190, 3] : (!llvm.ptr<struct<".4", (i64, i64, ptr<f32>, ptr<f32>)>>, i32) -> !llvm.ptr<ptr<f32>>
    llvm.store %182, %204 : !llvm.ptr<ptr<f32>>
    %205 = llvm.getelementptr %189[%203] : (!llvm.ptr<ptr<i8>>, i32) -> !llvm.ptr<ptr<i8>>
    %206 = llvm.bitcast %204 : !llvm.ptr<ptr<f32>> to !llvm.ptr<i8>
    llvm.store %206, %205 : !llvm.ptr<ptr<i8>>
    %207 = llvm.mlir.constant(0 : i32) : i32
    %208 = llvm.mlir.constant(4 : i32) : i32
    %209 = llvm.inttoptr %207 : i32 to !llvm.ptr<i8>
    %210 = llvm.mlir.constant(0 : i32) : i32
    %211 = llvm.mlir.constant(1 : i32) : i32
    %212 = llvm.alloca %211 x !llvm.struct<".5", (ptr<i8>, ptr<ptr<i8>>, i64, ptr<i8>, i64, i64, i64, i64, i64, i64, i32, ptr<i8>, i32, ptr<ptr<i8>>)> : (i32) -> !llvm.ptr<struct<".5", (ptr<i8>, ptr<ptr<i8>>, i64, ptr<i8>, i64, i64, i64, i64, i64, i64, i32, ptr<i8>, i32, ptr<ptr<i8>>)>>
    %213 = llvm.mlir.constant(14 : i32) : i32
    %214 = llvm.alloca %213 x !llvm.ptr<i8> : (i32) -> !llvm.ptr<ptr<i8>>
    %215 = llvm.mlir.constant(0 : i32) : i32
    %216 = llvm.getelementptr %212[%210, 0] : (!llvm.ptr<struct<".5", (ptr<i8>, ptr<ptr<i8>>, i64, ptr<i8>, i64, i64, i64, i64, i64, i64, i32, ptr<i8>, i32, ptr<ptr<i8>>)>>, i32) -> !llvm.ptr<ptr<i8>>
    llvm.store %arg0, %216 : !llvm.ptr<ptr<i8>>
    %217 = llvm.getelementptr %214[%215] : (!llvm.ptr<ptr<i8>>, i32) -> !llvm.ptr<ptr<i8>>
    %218 = llvm.bitcast %216 : !llvm.ptr<ptr<i8>> to !llvm.ptr<i8>
    llvm.store %218, %217 : !llvm.ptr<ptr<i8>>
    %219 = llvm.mlir.constant(1 : i32) : i32
    %220 = llvm.getelementptr %212[%210, 1] : (!llvm.ptr<struct<".5", (ptr<i8>, ptr<ptr<i8>>, i64, ptr<i8>, i64, i64, i64, i64, i64, i64, i32, ptr<i8>, i32, ptr<ptr<i8>>)>>, i32) -> !llvm.ptr<ptr<ptr<i8>>>
    llvm.store %165, %220 : !llvm.ptr<ptr<ptr<i8>>>
    %221 = llvm.getelementptr %214[%219] : (!llvm.ptr<ptr<i8>>, i32) -> !llvm.ptr<ptr<i8>>
    %222 = llvm.bitcast %220 : !llvm.ptr<ptr<ptr<i8>>> to !llvm.ptr<i8>
    llvm.store %222, %221 : !llvm.ptr<ptr<i8>>
    %223 = llvm.mlir.constant(2 : i32) : i32
    %224 = llvm.getelementptr %212[%210, 2] : (!llvm.ptr<struct<".5", (ptr<i8>, ptr<ptr<i8>>, i64, ptr<i8>, i64, i64, i64, i64, i64, i64, i32, ptr<i8>, i32, ptr<ptr<i8>>)>>, i32) -> !llvm.ptr<i64>
    llvm.store %168, %224 : !llvm.ptr<i64>
    %225 = llvm.getelementptr %214[%223] : (!llvm.ptr<ptr<i8>>, i32) -> !llvm.ptr<ptr<i8>>
    %226 = llvm.bitcast %224 : !llvm.ptr<i64> to !llvm.ptr<i8>
    llvm.store %226, %225 : !llvm.ptr<ptr<i8>>
    %227 = llvm.mlir.constant(3 : i32) : i32
    %228 = llvm.getelementptr %212[%210, 3] : (!llvm.ptr<struct<".5", (ptr<i8>, ptr<ptr<i8>>, i64, ptr<i8>, i64, i64, i64, i64, i64, i64, i32, ptr<i8>, i32, ptr<ptr<i8>>)>>, i32) -> !llvm.ptr<ptr<i8>>
    llvm.store %171, %228 : !llvm.ptr<ptr<i8>>
    %229 = llvm.getelementptr %214[%227] : (!llvm.ptr<ptr<i8>>, i32) -> !llvm.ptr<ptr<i8>>
    %230 = llvm.bitcast %228 : !llvm.ptr<ptr<i8>> to !llvm.ptr<i8>
    llvm.store %230, %229 : !llvm.ptr<ptr<i8>>
    %231 = llvm.mlir.constant(4 : i32) : i32
    %232 = llvm.getelementptr %212[%210, 4] : (!llvm.ptr<struct<".5", (ptr<i8>, ptr<ptr<i8>>, i64, ptr<i8>, i64, i64, i64, i64, i64, i64, i32, ptr<i8>, i32, ptr<ptr<i8>>)>>, i32) -> !llvm.ptr<i64>
    llvm.store %1, %232 : !llvm.ptr<i64>
    %233 = llvm.getelementptr %214[%231] : (!llvm.ptr<ptr<i8>>, i32) -> !llvm.ptr<ptr<i8>>
    %234 = llvm.bitcast %232 : !llvm.ptr<i64> to !llvm.ptr<i8>
    llvm.store %234, %233 : !llvm.ptr<ptr<i8>>
    %235 = llvm.mlir.constant(5 : i32) : i32
    %236 = llvm.getelementptr %212[%210, 5] : (!llvm.ptr<struct<".5", (ptr<i8>, ptr<ptr<i8>>, i64, ptr<i8>, i64, i64, i64, i64, i64, i64, i32, ptr<i8>, i32, ptr<ptr<i8>>)>>, i32) -> !llvm.ptr<i64>
    llvm.store %7, %236 : !llvm.ptr<i64>
    %237 = llvm.getelementptr %214[%235] : (!llvm.ptr<ptr<i8>>, i32) -> !llvm.ptr<ptr<i8>>
    %238 = llvm.bitcast %236 : !llvm.ptr<i64> to !llvm.ptr<i8>
    llvm.store %238, %237 : !llvm.ptr<ptr<i8>>
    %239 = llvm.mlir.constant(6 : i32) : i32
    %240 = llvm.getelementptr %212[%210, 6] : (!llvm.ptr<struct<".5", (ptr<i8>, ptr<ptr<i8>>, i64, ptr<i8>, i64, i64, i64, i64, i64, i64, i32, ptr<i8>, i32, ptr<ptr<i8>>)>>, i32) -> !llvm.ptr<i64>
    llvm.store %7, %240 : !llvm.ptr<i64>
    %241 = llvm.getelementptr %214[%239] : (!llvm.ptr<ptr<i8>>, i32) -> !llvm.ptr<ptr<i8>>
    %242 = llvm.bitcast %240 : !llvm.ptr<i64> to !llvm.ptr<i8>
    llvm.store %242, %241 : !llvm.ptr<ptr<i8>>
    %243 = llvm.mlir.constant(7 : i32) : i32
    %244 = llvm.getelementptr %212[%210, 7] : (!llvm.ptr<struct<".5", (ptr<i8>, ptr<ptr<i8>>, i64, ptr<i8>, i64, i64, i64, i64, i64, i64, i32, ptr<i8>, i32, ptr<ptr<i8>>)>>, i32) -> !llvm.ptr<i64>
    llvm.store %0, %244 : !llvm.ptr<i64>
    %245 = llvm.getelementptr %214[%243] : (!llvm.ptr<ptr<i8>>, i32) -> !llvm.ptr<ptr<i8>>
    %246 = llvm.bitcast %244 : !llvm.ptr<i64> to !llvm.ptr<i8>
    llvm.store %246, %245 : !llvm.ptr<ptr<i8>>
    %247 = llvm.mlir.constant(8 : i32) : i32
    %248 = llvm.getelementptr %212[%210, 8] : (!llvm.ptr<struct<".5", (ptr<i8>, ptr<ptr<i8>>, i64, ptr<i8>, i64, i64, i64, i64, i64, i64, i32, ptr<i8>, i32, ptr<ptr<i8>>)>>, i32) -> !llvm.ptr<i64>
    llvm.store %7, %248 : !llvm.ptr<i64>
    %249 = llvm.getelementptr %214[%247] : (!llvm.ptr<ptr<i8>>, i32) -> !llvm.ptr<ptr<i8>>
    %250 = llvm.bitcast %248 : !llvm.ptr<i64> to !llvm.ptr<i8>
    llvm.store %250, %249 : !llvm.ptr<ptr<i8>>
    %251 = llvm.mlir.constant(9 : i32) : i32
    %252 = llvm.getelementptr %212[%210, 9] : (!llvm.ptr<struct<".5", (ptr<i8>, ptr<ptr<i8>>, i64, ptr<i8>, i64, i64, i64, i64, i64, i64, i32, ptr<i8>, i32, ptr<ptr<i8>>)>>, i32) -> !llvm.ptr<i64>
    llvm.store %7, %252 : !llvm.ptr<i64>
    %253 = llvm.getelementptr %214[%251] : (!llvm.ptr<ptr<i8>>, i32) -> !llvm.ptr<ptr<i8>>
    %254 = llvm.bitcast %252 : !llvm.ptr<i64> to !llvm.ptr<i8>
    llvm.store %254, %253 : !llvm.ptr<ptr<i8>>
    %255 = llvm.mlir.constant(10 : i32) : i32
    %256 = llvm.getelementptr %212[%210, 10] : (!llvm.ptr<struct<".5", (ptr<i8>, ptr<ptr<i8>>, i64, ptr<i8>, i64, i64, i64, i64, i64, i64, i32, ptr<i8>, i32, ptr<ptr<i8>>)>>, i32) -> !llvm.ptr<i32>
    llvm.store %207, %256 : !llvm.ptr<i32>
    %257 = llvm.getelementptr %214[%255] : (!llvm.ptr<ptr<i8>>, i32) -> !llvm.ptr<ptr<i8>>
    %258 = llvm.bitcast %256 : !llvm.ptr<i32> to !llvm.ptr<i8>
    llvm.store %258, %257 : !llvm.ptr<ptr<i8>>
    %259 = llvm.mlir.constant(11 : i32) : i32
    %260 = llvm.getelementptr %212[%210, 11] : (!llvm.ptr<struct<".5", (ptr<i8>, ptr<ptr<i8>>, i64, ptr<i8>, i64, i64, i64, i64, i64, i64, i32, ptr<i8>, i32, ptr<ptr<i8>>)>>, i32) -> !llvm.ptr<ptr<i8>>
    llvm.store %209, %260 : !llvm.ptr<ptr<i8>>
    %261 = llvm.getelementptr %214[%259] : (!llvm.ptr<ptr<i8>>, i32) -> !llvm.ptr<ptr<i8>>
    %262 = llvm.bitcast %260 : !llvm.ptr<ptr<i8>> to !llvm.ptr<i8>
    llvm.store %262, %261 : !llvm.ptr<ptr<i8>>
    %263 = llvm.mlir.constant(12 : i32) : i32
    %264 = llvm.getelementptr %212[%210, 12] : (!llvm.ptr<struct<".5", (ptr<i8>, ptr<ptr<i8>>, i64, ptr<i8>, i64, i64, i64, i64, i64, i64, i32, ptr<i8>, i32, ptr<ptr<i8>>)>>, i32) -> !llvm.ptr<i32>
    llvm.store %208, %264 : !llvm.ptr<i32>
    %265 = llvm.getelementptr %214[%263] : (!llvm.ptr<ptr<i8>>, i32) -> !llvm.ptr<ptr<i8>>
    %266 = llvm.bitcast %264 : !llvm.ptr<i32> to !llvm.ptr<i8>
    llvm.store %266, %265 : !llvm.ptr<ptr<i8>>
    %267 = llvm.mlir.constant(13 : i32) : i32
    %268 = llvm.getelementptr %212[%210, 13] : (!llvm.ptr<struct<".5", (ptr<i8>, ptr<ptr<i8>>, i64, ptr<i8>, i64, i64, i64, i64, i64, i64, i32, ptr<i8>, i32, ptr<ptr<i8>>)>>, i32) -> !llvm.ptr<ptr<ptr<i8>>>
    llvm.store %189, %268 : !llvm.ptr<ptr<ptr<i8>>>
    %269 = llvm.getelementptr %214[%267] : (!llvm.ptr<ptr<i8>>, i32) -> !llvm.ptr<ptr<i8>>
    %270 = llvm.bitcast %268 : !llvm.ptr<ptr<ptr<i8>>> to !llvm.ptr<i8>
    llvm.store %270, %269 : !llvm.ptr<ptr<i8>>
    %271 = llvm.mlir.addressof @ral_kernel_launch___gpu___pvoid_ppvoid_i64_pvoid_i64_i64_i64_i64_i64_i64_i32_pvoid_i32_ppvoid___void : !llvm.ptr<array<101 x i8>>
    %272 = llvm.mlir.constant(0 : index) : i64
    %273 = llvm.getelementptr %271[%272, %272] : (!llvm.ptr<array<101 x i8>>, i64, i64) -> !llvm.ptr<i8>
    llvm.call @disc_ral_call(%arg0, %273, %214) : (!llvm.ptr<i8>, !llvm.ptr<i8>, !llvm.ptr<ptr<i8>>) -> ()
    %274 = llvm.inttoptr %4 : i32 to !llvm.ptr<i8>
    %275 = llvm.mlir.constant(2 : index) : i64
    %276 = llvm.mlir.constant(1 : index) : i64
    %277 = llvm.mlir.null : !llvm.ptr<i64>
    %278 = llvm.getelementptr %277[%275] : (!llvm.ptr<i64>, i64) -> !llvm.ptr<i64>
    %279 = llvm.ptrtoint %278 : !llvm.ptr<i64> to i64
    %280 = llvm.alloca %279 x i64 : (i64) -> !llvm.ptr<i64>
    %281 = llvm.mlir.undef : !llvm.struct<(ptr<i64>, ptr<i64>, i64, array<1 x i64>, array<1 x i64>)>
    %282 = llvm.insertvalue %280, %281[0] : !llvm.struct<(ptr<i64>, ptr<i64>, i64, array<1 x i64>, array<1 x i64>)> 
    %283 = llvm.insertvalue %280, %282[1] : !llvm.struct<(ptr<i64>, ptr<i64>, i64, array<1 x i64>, array<1 x i64>)> 
    %284 = llvm.mlir.constant(0 : index) : i64
    %285 = llvm.insertvalue %284, %283[2] : !llvm.struct<(ptr<i64>, ptr<i64>, i64, array<1 x i64>, array<1 x i64>)> 
    %286 = llvm.insertvalue %275, %285[3, 0] : !llvm.struct<(ptr<i64>, ptr<i64>, i64, array<1 x i64>, array<1 x i64>)> 
    %287 = llvm.insertvalue %276, %286[4, 0] : !llvm.struct<(ptr<i64>, ptr<i64>, i64, array<1 x i64>, array<1 x i64>)> 
    %288 = llvm.extractvalue %287[1] : !llvm.struct<(ptr<i64>, ptr<i64>, i64, array<1 x i64>, array<1 x i64>)> 
    %289 = llvm.getelementptr %288[%5] : (!llvm.ptr<i64>, i64) -> !llvm.ptr<i64>
    llvm.store %6, %289 : !llvm.ptr<i64>
    %290 = llvm.extractvalue %287[1] : !llvm.struct<(ptr<i64>, ptr<i64>, i64, array<1 x i64>, array<1 x i64>)> 
    %291 = llvm.getelementptr %290[%7] : (!llvm.ptr<i64>, i64) -> !llvm.ptr<i64>
    llvm.store %8, %291 : !llvm.ptr<i64>
    %292 = llvm.mlir.constant(0 : i32) : i32
    %293 = llvm.mlir.constant(1 : i32) : i32
    %294 = llvm.extractvalue %64[0] : !llvm.struct<(ptr<f32>, ptr<f32>, i64, array<1 x i64>, array<1 x i64>)> 
    %295 = llvm.extractvalue %64[1] : !llvm.struct<(ptr<f32>, ptr<f32>, i64, array<1 x i64>, array<1 x i64>)> 
    %296 = llvm.extractvalue %64[2] : !llvm.struct<(ptr<f32>, ptr<f32>, i64, array<1 x i64>, array<1 x i64>)> 
    %297 = llvm.extractvalue %64[3, 0] : !llvm.struct<(ptr<f32>, ptr<f32>, i64, array<1 x i64>, array<1 x i64>)> 
    %298 = llvm.extractvalue %64[4, 0] : !llvm.struct<(ptr<f32>, ptr<f32>, i64, array<1 x i64>, array<1 x i64>)> 
    %299 = llvm.extractvalue %287[0] : !llvm.struct<(ptr<i64>, ptr<i64>, i64, array<1 x i64>, array<1 x i64>)> 
    %300 = llvm.extractvalue %287[1] : !llvm.struct<(ptr<i64>, ptr<i64>, i64, array<1 x i64>, array<1 x i64>)> 
    %301 = llvm.extractvalue %287[2] : !llvm.struct<(ptr<i64>, ptr<i64>, i64, array<1 x i64>, array<1 x i64>)> 
    %302 = llvm.extractvalue %287[3, 0] : !llvm.struct<(ptr<i64>, ptr<i64>, i64, array<1 x i64>, array<1 x i64>)> 
    %303 = llvm.extractvalue %287[4, 0] : !llvm.struct<(ptr<i64>, ptr<i64>, i64, array<1 x i64>, array<1 x i64>)> 
    %304 = llvm.alloca %293 x !llvm.struct<".6", (ptr<i8>, ptr<i8>, ptr<f32>, ptr<f32>, i64, i64, i64, ptr<i64>, ptr<i64>, i64, i64, i64, struct<(ptr<f32>, ptr<f32>, i64, array<2 x i64>, array<2 x i64>)>)> : (i32) -> !llvm.ptr<struct<".6", (ptr<i8>, ptr<i8>, ptr<f32>, ptr<f32>, i64, i64, i64, ptr<i64>, ptr<i64>, i64, i64, i64, struct<(ptr<f32>, ptr<f32>, i64, array<2 x i64>, array<2 x i64>)>)>>
    %305 = llvm.mlir.constant(13 : i32) : i32
    %306 = llvm.alloca %305 x !llvm.ptr<i8> : (i32) -> !llvm.ptr<ptr<i8>>
    %307 = llvm.mlir.constant(0 : i32) : i32
    %308 = llvm.getelementptr %304[%292, 0] : (!llvm.ptr<struct<".6", (ptr<i8>, ptr<i8>, ptr<f32>, ptr<f32>, i64, i64, i64, ptr<i64>, ptr<i64>, i64, i64, i64, struct<(ptr<f32>, ptr<f32>, i64, array<2 x i64>, array<2 x i64>)>)>>, i32) -> !llvm.ptr<ptr<i8>>
    llvm.store %arg0, %308 : !llvm.ptr<ptr<i8>>
    %309 = llvm.getelementptr %306[%307] : (!llvm.ptr<ptr<i8>>, i32) -> !llvm.ptr<ptr<i8>>
    %310 = llvm.bitcast %308 : !llvm.ptr<ptr<i8>> to !llvm.ptr<i8>
    llvm.store %310, %309 : !llvm.ptr<ptr<i8>>
    %311 = llvm.mlir.constant(1 : i32) : i32
    %312 = llvm.getelementptr %304[%292, 1] : (!llvm.ptr<struct<".6", (ptr<i8>, ptr<i8>, ptr<f32>, ptr<f32>, i64, i64, i64, ptr<i64>, ptr<i64>, i64, i64, i64, struct<(ptr<f32>, ptr<f32>, i64, array<2 x i64>, array<2 x i64>)>)>>, i32) -> !llvm.ptr<ptr<i8>>
    llvm.store %274, %312 : !llvm.ptr<ptr<i8>>
    %313 = llvm.getelementptr %306[%311] : (!llvm.ptr<ptr<i8>>, i32) -> !llvm.ptr<ptr<i8>>
    %314 = llvm.bitcast %312 : !llvm.ptr<ptr<i8>> to !llvm.ptr<i8>
    llvm.store %314, %313 : !llvm.ptr<ptr<i8>>
    %315 = llvm.mlir.constant(2 : i32) : i32
    %316 = llvm.getelementptr %304[%292, 2] : (!llvm.ptr<struct<".6", (ptr<i8>, ptr<i8>, ptr<f32>, ptr<f32>, i64, i64, i64, ptr<i64>, ptr<i64>, i64, i64, i64, struct<(ptr<f32>, ptr<f32>, i64, array<2 x i64>, array<2 x i64>)>)>>, i32) -> !llvm.ptr<ptr<f32>>
    llvm.store %294, %316 : !llvm.ptr<ptr<f32>>
    %317 = llvm.getelementptr %306[%315] : (!llvm.ptr<ptr<i8>>, i32) -> !llvm.ptr<ptr<i8>>
    %318 = llvm.bitcast %316 : !llvm.ptr<ptr<f32>> to !llvm.ptr<i8>
    llvm.store %318, %317 : !llvm.ptr<ptr<i8>>
    %319 = llvm.mlir.constant(3 : i32) : i32
    %320 = llvm.getelementptr %304[%292, 3] : (!llvm.ptr<struct<".6", (ptr<i8>, ptr<i8>, ptr<f32>, ptr<f32>, i64, i64, i64, ptr<i64>, ptr<i64>, i64, i64, i64, struct<(ptr<f32>, ptr<f32>, i64, array<2 x i64>, array<2 x i64>)>)>>, i32) -> !llvm.ptr<ptr<f32>>
    llvm.store %295, %320 : !llvm.ptr<ptr<f32>>
    %321 = llvm.getelementptr %306[%319] : (!llvm.ptr<ptr<i8>>, i32) -> !llvm.ptr<ptr<i8>>
    %322 = llvm.bitcast %320 : !llvm.ptr<ptr<f32>> to !llvm.ptr<i8>
    llvm.store %322, %321 : !llvm.ptr<ptr<i8>>
    %323 = llvm.mlir.constant(4 : i32) : i32
    %324 = llvm.getelementptr %304[%292, 4] : (!llvm.ptr<struct<".6", (ptr<i8>, ptr<i8>, ptr<f32>, ptr<f32>, i64, i64, i64, ptr<i64>, ptr<i64>, i64, i64, i64, struct<(ptr<f32>, ptr<f32>, i64, array<2 x i64>, array<2 x i64>)>)>>, i32) -> !llvm.ptr<i64>
    llvm.store %296, %324 : !llvm.ptr<i64>
    %325 = llvm.getelementptr %306[%323] : (!llvm.ptr<ptr<i8>>, i32) -> !llvm.ptr<ptr<i8>>
    %326 = llvm.bitcast %324 : !llvm.ptr<i64> to !llvm.ptr<i8>
    llvm.store %326, %325 : !llvm.ptr<ptr<i8>>
    %327 = llvm.mlir.constant(5 : i32) : i32
    %328 = llvm.getelementptr %304[%292, 5] : (!llvm.ptr<struct<".6", (ptr<i8>, ptr<i8>, ptr<f32>, ptr<f32>, i64, i64, i64, ptr<i64>, ptr<i64>, i64, i64, i64, struct<(ptr<f32>, ptr<f32>, i64, array<2 x i64>, array<2 x i64>)>)>>, i32) -> !llvm.ptr<i64>
    llvm.store %297, %328 : !llvm.ptr<i64>
    %329 = llvm.getelementptr %306[%327] : (!llvm.ptr<ptr<i8>>, i32) -> !llvm.ptr<ptr<i8>>
    %330 = llvm.bitcast %328 : !llvm.ptr<i64> to !llvm.ptr<i8>
    llvm.store %330, %329 : !llvm.ptr<ptr<i8>>
    %331 = llvm.mlir.constant(6 : i32) : i32
    %332 = llvm.getelementptr %304[%292, 6] : (!llvm.ptr<struct<".6", (ptr<i8>, ptr<i8>, ptr<f32>, ptr<f32>, i64, i64, i64, ptr<i64>, ptr<i64>, i64, i64, i64, struct<(ptr<f32>, ptr<f32>, i64, array<2 x i64>, array<2 x i64>)>)>>, i32) -> !llvm.ptr<i64>
    llvm.store %298, %332 : !llvm.ptr<i64>
    %333 = llvm.getelementptr %306[%331] : (!llvm.ptr<ptr<i8>>, i32) -> !llvm.ptr<ptr<i8>>
    %334 = llvm.bitcast %332 : !llvm.ptr<i64> to !llvm.ptr<i8>
    llvm.store %334, %333 : !llvm.ptr<ptr<i8>>
    %335 = llvm.mlir.constant(7 : i32) : i32
    %336 = llvm.getelementptr %304[%292, 7] : (!llvm.ptr<struct<".6", (ptr<i8>, ptr<i8>, ptr<f32>, ptr<f32>, i64, i64, i64, ptr<i64>, ptr<i64>, i64, i64, i64, struct<(ptr<f32>, ptr<f32>, i64, array<2 x i64>, array<2 x i64>)>)>>, i32) -> !llvm.ptr<ptr<i64>>
    llvm.store %299, %336 : !llvm.ptr<ptr<i64>>
    %337 = llvm.getelementptr %306[%335] : (!llvm.ptr<ptr<i8>>, i32) -> !llvm.ptr<ptr<i8>>
    %338 = llvm.bitcast %336 : !llvm.ptr<ptr<i64>> to !llvm.ptr<i8>
    llvm.store %338, %337 : !llvm.ptr<ptr<i8>>
    %339 = llvm.mlir.constant(8 : i32) : i32
    %340 = llvm.getelementptr %304[%292, 8] : (!llvm.ptr<struct<".6", (ptr<i8>, ptr<i8>, ptr<f32>, ptr<f32>, i64, i64, i64, ptr<i64>, ptr<i64>, i64, i64, i64, struct<(ptr<f32>, ptr<f32>, i64, array<2 x i64>, array<2 x i64>)>)>>, i32) -> !llvm.ptr<ptr<i64>>
    llvm.store %300, %340 : !llvm.ptr<ptr<i64>>
    %341 = llvm.getelementptr %306[%339] : (!llvm.ptr<ptr<i8>>, i32) -> !llvm.ptr<ptr<i8>>
    %342 = llvm.bitcast %340 : !llvm.ptr<ptr<i64>> to !llvm.ptr<i8>
    llvm.store %342, %341 : !llvm.ptr<ptr<i8>>
    %343 = llvm.mlir.constant(9 : i32) : i32
    %344 = llvm.getelementptr %304[%292, 9] : (!llvm.ptr<struct<".6", (ptr<i8>, ptr<i8>, ptr<f32>, ptr<f32>, i64, i64, i64, ptr<i64>, ptr<i64>, i64, i64, i64, struct<(ptr<f32>, ptr<f32>, i64, array<2 x i64>, array<2 x i64>)>)>>, i32) -> !llvm.ptr<i64>
    llvm.store %301, %344 : !llvm.ptr<i64>
    %345 = llvm.getelementptr %306[%343] : (!llvm.ptr<ptr<i8>>, i32) -> !llvm.ptr<ptr<i8>>
    %346 = llvm.bitcast %344 : !llvm.ptr<i64> to !llvm.ptr<i8>
    llvm.store %346, %345 : !llvm.ptr<ptr<i8>>
    %347 = llvm.mlir.constant(10 : i32) : i32
    %348 = llvm.getelementptr %304[%292, 10] : (!llvm.ptr<struct<".6", (ptr<i8>, ptr<i8>, ptr<f32>, ptr<f32>, i64, i64, i64, ptr<i64>, ptr<i64>, i64, i64, i64, struct<(ptr<f32>, ptr<f32>, i64, array<2 x i64>, array<2 x i64>)>)>>, i32) -> !llvm.ptr<i64>
    llvm.store %302, %348 : !llvm.ptr<i64>
    %349 = llvm.getelementptr %306[%347] : (!llvm.ptr<ptr<i8>>, i32) -> !llvm.ptr<ptr<i8>>
    %350 = llvm.bitcast %348 : !llvm.ptr<i64> to !llvm.ptr<i8>
    llvm.store %350, %349 : !llvm.ptr<ptr<i8>>
    %351 = llvm.mlir.constant(11 : i32) : i32
    %352 = llvm.getelementptr %304[%292, 11] : (!llvm.ptr<struct<".6", (ptr<i8>, ptr<i8>, ptr<f32>, ptr<f32>, i64, i64, i64, ptr<i64>, ptr<i64>, i64, i64, i64, struct<(ptr<f32>, ptr<f32>, i64, array<2 x i64>, array<2 x i64>)>)>>, i32) -> !llvm.ptr<i64>
    llvm.store %303, %352 : !llvm.ptr<i64>
    %353 = llvm.getelementptr %306[%351] : (!llvm.ptr<ptr<i8>>, i32) -> !llvm.ptr<ptr<i8>>
    %354 = llvm.bitcast %352 : !llvm.ptr<i64> to !llvm.ptr<i8>
    llvm.store %354, %353 : !llvm.ptr<ptr<i8>>
    %355 = llvm.mlir.constant(12 : i32) : i32
    %356 = llvm.getelementptr %304[%292, 12] : (!llvm.ptr<struct<".6", (ptr<i8>, ptr<i8>, ptr<f32>, ptr<f32>, i64, i64, i64, ptr<i64>, ptr<i64>, i64, i64, i64, struct<(ptr<f32>, ptr<f32>, i64, array<2 x i64>, array<2 x i64>)>)>>, i32) -> !llvm.ptr<struct<(ptr<f32>, ptr<f32>, i64, array<2 x i64>, array<2 x i64>)>>
    %357 = llvm.getelementptr %306[%355] : (!llvm.ptr<ptr<i8>>, i32) -> !llvm.ptr<ptr<i8>>
    %358 = llvm.bitcast %356 : !llvm.ptr<struct<(ptr<f32>, ptr<f32>, i64, array<2 x i64>, array<2 x i64>)>> to !llvm.ptr<i8>
    llvm.store %358, %357 : !llvm.ptr<ptr<i8>>
    %359 = llvm.mlir.addressof @inc_ref___gpu___pvoid_pvoid_m1df32_m1di64___m2df32 : !llvm.ptr<array<51 x i8>>
    %360 = llvm.mlir.constant(0 : index) : i64
    %361 = llvm.getelementptr %359[%360, %360] : (!llvm.ptr<array<51 x i8>>, i64, i64) -> !llvm.ptr<i8>
    llvm.call @disc_ral_call(%arg0, %361, %306) : (!llvm.ptr<i8>, !llvm.ptr<i8>, !llvm.ptr<ptr<i8>>) -> ()
    %362 = llvm.load %356 : !llvm.ptr<struct<(ptr<f32>, ptr<f32>, i64, array<2 x i64>, array<2 x i64>)>>
    %363 = llvm.mlir.undef : !llvm.struct<(ptr<f32>, ptr<f32>, i64, array<2 x i64>, array<2 x i64>)>
    %364 = llvm.extractvalue %362[0] : !llvm.struct<(ptr<f32>, ptr<f32>, i64, array<2 x i64>, array<2 x i64>)> 
    %365 = llvm.extractvalue %362[1] : !llvm.struct<(ptr<f32>, ptr<f32>, i64, array<2 x i64>, array<2 x i64>)> 
    %366 = llvm.insertvalue %364, %363[0] : !llvm.struct<(ptr<f32>, ptr<f32>, i64, array<2 x i64>, array<2 x i64>)> 
    %367 = llvm.insertvalue %365, %366[1] : !llvm.struct<(ptr<f32>, ptr<f32>, i64, array<2 x i64>, array<2 x i64>)> 
    %368 = llvm.mlir.constant(0 : index) : i64
    %369 = llvm.insertvalue %368, %367[2] : !llvm.struct<(ptr<f32>, ptr<f32>, i64, array<2 x i64>, array<2 x i64>)> 
    %370 = llvm.mlir.constant(100 : index) : i64
    %371 = llvm.insertvalue %370, %369[3, 0] : !llvm.struct<(ptr<f32>, ptr<f32>, i64, array<2 x i64>, array<2 x i64>)> 
    %372 = llvm.mlir.constant(13 : index) : i64
    %373 = llvm.insertvalue %372, %371[4, 0] : !llvm.struct<(ptr<f32>, ptr<f32>, i64, array<2 x i64>, array<2 x i64>)> 
    %374 = llvm.mlir.constant(13 : index) : i64
    %375 = llvm.insertvalue %374, %373[3, 1] : !llvm.struct<(ptr<f32>, ptr<f32>, i64, array<2 x i64>, array<2 x i64>)> 
    %376 = llvm.mlir.constant(1 : index) : i64
    %377 = llvm.insertvalue %376, %375[4, 1] : !llvm.struct<(ptr<f32>, ptr<f32>, i64, array<2 x i64>, array<2 x i64>)> 
    %378 = llvm.extractvalue %64[0] : !llvm.struct<(ptr<f32>, ptr<f32>, i64, array<1 x i64>, array<1 x i64>)> 
    %379 = llvm.bitcast %378 : !llvm.ptr<f32> to !llvm.ptr<i8>
    %380 = llvm.mlir.constant(0 : i32) : i32
    %381 = llvm.mlir.constant(1 : i32) : i32
    %382 = llvm.alloca %381 x !llvm.struct<".7", (ptr<i8>, ptr<i8>)> : (i32) -> !llvm.ptr<struct<".7", (ptr<i8>, ptr<i8>)>>
    %383 = llvm.mlir.constant(2 : i32) : i32
    %384 = llvm.alloca %383 x !llvm.ptr<i8> : (i32) -> !llvm.ptr<ptr<i8>>
    %385 = llvm.mlir.constant(0 : i32) : i32
    %386 = llvm.getelementptr %382[%380, 0] : (!llvm.ptr<struct<".7", (ptr<i8>, ptr<i8>)>>, i32) -> !llvm.ptr<ptr<i8>>
    llvm.store %arg0, %386 : !llvm.ptr<ptr<i8>>
    %387 = llvm.getelementptr %384[%385] : (!llvm.ptr<ptr<i8>>, i32) -> !llvm.ptr<ptr<i8>>
    %388 = llvm.bitcast %386 : !llvm.ptr<ptr<i8>> to !llvm.ptr<i8>
    llvm.store %388, %387 : !llvm.ptr<ptr<i8>>
    %389 = llvm.mlir.constant(1 : i32) : i32
    %390 = llvm.getelementptr %382[%380, 1] : (!llvm.ptr<struct<".7", (ptr<i8>, ptr<i8>)>>, i32) -> !llvm.ptr<ptr<i8>>
    llvm.store %379, %390 : !llvm.ptr<ptr<i8>>
    %391 = llvm.getelementptr %384[%389] : (!llvm.ptr<ptr<i8>>, i32) -> !llvm.ptr<ptr<i8>>
    %392 = llvm.bitcast %390 : !llvm.ptr<ptr<i8>> to !llvm.ptr<i8>
    llvm.store %392, %391 : !llvm.ptr<ptr<i8>>
    %393 = llvm.mlir.addressof @dealloc___gpu___pvoid_pvoid___void : !llvm.ptr<array<35 x i8>>
    %394 = llvm.mlir.constant(0 : index) : i64
    %395 = llvm.getelementptr %393[%394, %394] : (!llvm.ptr<array<35 x i8>>, i64, i64) -> !llvm.ptr<i8>
    llvm.call @disc_ral_call(%arg0, %395, %384) : (!llvm.ptr<i8>, !llvm.ptr<i8>, !llvm.ptr<ptr<i8>>) -> ()
    %396 = llvm.mlir.constant(0 : i32) : i32
    %397 = llvm.mlir.constant(1 : i32) : i32
    %398 = llvm.extractvalue %377[0] : !llvm.struct<(ptr<f32>, ptr<f32>, i64, array<2 x i64>, array<2 x i64>)> 
    %399 = llvm.extractvalue %377[1] : !llvm.struct<(ptr<f32>, ptr<f32>, i64, array<2 x i64>, array<2 x i64>)> 
    %400 = llvm.extractvalue %377[2] : !llvm.struct<(ptr<f32>, ptr<f32>, i64, array<2 x i64>, array<2 x i64>)> 
    %401 = llvm.extractvalue %377[3, 0] : !llvm.struct<(ptr<f32>, ptr<f32>, i64, array<2 x i64>, array<2 x i64>)> 
    %402 = llvm.extractvalue %377[3, 1] : !llvm.struct<(ptr<f32>, ptr<f32>, i64, array<2 x i64>, array<2 x i64>)> 
    %403 = llvm.extractvalue %377[4, 0] : !llvm.struct<(ptr<f32>, ptr<f32>, i64, array<2 x i64>, array<2 x i64>)> 
    %404 = llvm.extractvalue %377[4, 1] : !llvm.struct<(ptr<f32>, ptr<f32>, i64, array<2 x i64>, array<2 x i64>)> 
    %405 = llvm.alloca %397 x !llvm.struct<".8", (ptr<i8>, i64, ptr<f32>, ptr<f32>, i64, i64, i64, i64, i64)> : (i32) -> !llvm.ptr<struct<".8", (ptr<i8>, i64, ptr<f32>, ptr<f32>, i64, i64, i64, i64, i64)>>
    %406 = llvm.mlir.constant(9 : i32) : i32
    %407 = llvm.alloca %406 x !llvm.ptr<i8> : (i32) -> !llvm.ptr<ptr<i8>>
    %408 = llvm.mlir.constant(0 : i32) : i32
    %409 = llvm.getelementptr %405[%396, 0] : (!llvm.ptr<struct<".8", (ptr<i8>, i64, ptr<f32>, ptr<f32>, i64, i64, i64, i64, i64)>>, i32) -> !llvm.ptr<ptr<i8>>
    llvm.store %arg0, %409 : !llvm.ptr<ptr<i8>>
    %410 = llvm.getelementptr %407[%408] : (!llvm.ptr<ptr<i8>>, i32) -> !llvm.ptr<ptr<i8>>
    %411 = llvm.bitcast %409 : !llvm.ptr<ptr<i8>> to !llvm.ptr<i8>
    llvm.store %411, %410 : !llvm.ptr<ptr<i8>>
    %412 = llvm.mlir.constant(1 : i32) : i32
    %413 = llvm.getelementptr %405[%396, 1] : (!llvm.ptr<struct<".8", (ptr<i8>, i64, ptr<f32>, ptr<f32>, i64, i64, i64, i64, i64)>>, i32) -> !llvm.ptr<i64>
    llvm.store %5, %413 : !llvm.ptr<i64>
    %414 = llvm.getelementptr %407[%412] : (!llvm.ptr<ptr<i8>>, i32) -> !llvm.ptr<ptr<i8>>
    %415 = llvm.bitcast %413 : !llvm.ptr<i64> to !llvm.ptr<i8>
    llvm.store %415, %414 : !llvm.ptr<ptr<i8>>
    %416 = llvm.mlir.constant(2 : i32) : i32
    %417 = llvm.getelementptr %405[%396, 2] : (!llvm.ptr<struct<".8", (ptr<i8>, i64, ptr<f32>, ptr<f32>, i64, i64, i64, i64, i64)>>, i32) -> !llvm.ptr<ptr<f32>>
    llvm.store %398, %417 : !llvm.ptr<ptr<f32>>
    %418 = llvm.getelementptr %407[%416] : (!llvm.ptr<ptr<i8>>, i32) -> !llvm.ptr<ptr<i8>>
    %419 = llvm.bitcast %417 : !llvm.ptr<ptr<f32>> to !llvm.ptr<i8>
    llvm.store %419, %418 : !llvm.ptr<ptr<i8>>
    %420 = llvm.mlir.constant(3 : i32) : i32
    %421 = llvm.getelementptr %405[%396, 3] : (!llvm.ptr<struct<".8", (ptr<i8>, i64, ptr<f32>, ptr<f32>, i64, i64, i64, i64, i64)>>, i32) -> !llvm.ptr<ptr<f32>>
    llvm.store %399, %421 : !llvm.ptr<ptr<f32>>
    %422 = llvm.getelementptr %407[%420] : (!llvm.ptr<ptr<i8>>, i32) -> !llvm.ptr<ptr<i8>>
    %423 = llvm.bitcast %421 : !llvm.ptr<ptr<f32>> to !llvm.ptr<i8>
    llvm.store %423, %422 : !llvm.ptr<ptr<i8>>
    %424 = llvm.mlir.constant(4 : i32) : i32
    %425 = llvm.getelementptr %405[%396, 4] : (!llvm.ptr<struct<".8", (ptr<i8>, i64, ptr<f32>, ptr<f32>, i64, i64, i64, i64, i64)>>, i32) -> !llvm.ptr<i64>
    llvm.store %400, %425 : !llvm.ptr<i64>
    %426 = llvm.getelementptr %407[%424] : (!llvm.ptr<ptr<i8>>, i32) -> !llvm.ptr<ptr<i8>>
    %427 = llvm.bitcast %425 : !llvm.ptr<i64> to !llvm.ptr<i8>
    llvm.store %427, %426 : !llvm.ptr<ptr<i8>>
    %428 = llvm.mlir.constant(5 : i32) : i32
    %429 = llvm.getelementptr %405[%396, 5] : (!llvm.ptr<struct<".8", (ptr<i8>, i64, ptr<f32>, ptr<f32>, i64, i64, i64, i64, i64)>>, i32) -> !llvm.ptr<i64>
    llvm.store %401, %429 : !llvm.ptr<i64>
    %430 = llvm.getelementptr %407[%428] : (!llvm.ptr<ptr<i8>>, i32) -> !llvm.ptr<ptr<i8>>
    %431 = llvm.bitcast %429 : !llvm.ptr<i64> to !llvm.ptr<i8>
    llvm.store %431, %430 : !llvm.ptr<ptr<i8>>
    %432 = llvm.mlir.constant(6 : i32) : i32
    %433 = llvm.getelementptr %405[%396, 6] : (!llvm.ptr<struct<".8", (ptr<i8>, i64, ptr<f32>, ptr<f32>, i64, i64, i64, i64, i64)>>, i32) -> !llvm.ptr<i64>
    llvm.store %402, %433 : !llvm.ptr<i64>
    %434 = llvm.getelementptr %407[%432] : (!llvm.ptr<ptr<i8>>, i32) -> !llvm.ptr<ptr<i8>>
    %435 = llvm.bitcast %433 : !llvm.ptr<i64> to !llvm.ptr<i8>
    llvm.store %435, %434 : !llvm.ptr<ptr<i8>>
    %436 = llvm.mlir.constant(7 : i32) : i32
    %437 = llvm.getelementptr %405[%396, 7] : (!llvm.ptr<struct<".8", (ptr<i8>, i64, ptr<f32>, ptr<f32>, i64, i64, i64, i64, i64)>>, i32) -> !llvm.ptr<i64>
    llvm.store %403, %437 : !llvm.ptr<i64>
    %438 = llvm.getelementptr %407[%436] : (!llvm.ptr<ptr<i8>>, i32) -> !llvm.ptr<ptr<i8>>
    %439 = llvm.bitcast %437 : !llvm.ptr<i64> to !llvm.ptr<i8>
    llvm.store %439, %438 : !llvm.ptr<ptr<i8>>
    %440 = llvm.mlir.constant(8 : i32) : i32
    %441 = llvm.getelementptr %405[%396, 8] : (!llvm.ptr<struct<".8", (ptr<i8>, i64, ptr<f32>, ptr<f32>, i64, i64, i64, i64, i64)>>, i32) -> !llvm.ptr<i64>
    llvm.store %404, %441 : !llvm.ptr<i64>
    %442 = llvm.getelementptr %407[%440] : (!llvm.ptr<ptr<i8>>, i32) -> !llvm.ptr<ptr<i8>>
    %443 = llvm.bitcast %441 : !llvm.ptr<i64> to !llvm.ptr<i8>
    llvm.store %443, %442 : !llvm.ptr<ptr<i8>>
    %444 = llvm.mlir.addressof @ral_send_output___cpu___pvoid_i64_m2df32___void : !llvm.ptr<array<48 x i8>>
    %445 = llvm.mlir.constant(0 : index) : i64
    %446 = llvm.getelementptr %444[%445, %445] : (!llvm.ptr<array<48 x i8>>, i64, i64) -> !llvm.ptr<i8>
    llvm.call @disc_ral_call(%arg0, %446, %407) : (!llvm.ptr<i8>, !llvm.ptr<i8>, !llvm.ptr<ptr<i8>>) -> ()
    llvm.return
  }
}


===-------------------------------------------------------------------------===
                         ... Execution time report ...
===-------------------------------------------------------------------------===
  Total Execution Time: 0.2219 seconds

  ----Wall Time----  ----Name----
    0.0004 (  0.2%)  Inliner
    0.0000 (  0.0%)    (A) CallGraph
    0.0001 (  0.1%)  'func.func' Pipeline
    0.0001 (  0.1%)    Canonicalizer
    0.0004 (  0.2%)  'func.func' Pipeline
    0.0000 (  0.0%)    MhloDecompositionRewriterPass
    0.0000 (  0.0%)    RemoveShapeConstraintsPass
    0.0001 (  0.0%)    Canonicalizer
    0.0000 (  0.0%)    CSE
    0.0000 (  0.0%)      (A) DominanceInfo
    0.0001 (  0.0%)    Canonicalizer
    0.0000 (  0.0%)    DiscTranformWeightDataLayoutForWeightOnlyQuantPass
    0.0001 (  0.0%)    Canonicalizer
    0.0000 (  0.0%)    DiscCustomCallRewriterPass
    0.0000 (  0.0%)    DiscConvertFakeQuantOpPass
    0.0000 (  0.0%)    DiscLowerGpuQuantizeAndDequantizePass
    0.0000 (  0.0%)    ConvertShapeToStandardPass
    0.0010 (  0.4%)  DiscShapeOptimizationPass
    0.0004 (  0.2%)  'builtin.func' Pipeline
    0.0003 (  0.1%)    CSE
    0.0000 (  0.0%)      (A) DominanceInfo
    0.0004 (  0.2%)  'func.func' Pipeline
    0.0000 (  0.0%)    ConvertTensorToStandardPass
    0.0000 (  0.0%)    ConvertHloToStandardPass
    0.0001 (  0.1%)    Canonicalizer
    0.0000 (  0.0%)    CSE
    0.0000 (  0.0%)      (A) DominanceInfo
    0.0001 (  0.0%)    Canonicalizer
    0.0000 (  0.0%)    DiscAlgebraicSimplifierPass
    0.0000 (  0.0%)    SplitLargeOpsPass
    0.0000 (  0.0%)    DotRewriterPass
    0.0008 (  0.4%)  DiscShapeOptimizationPass
    0.0001 (  0.0%)  'func.func' Pipeline
    0.0000 (  0.0%)    DiscDotMergePass
    0.0008 (  0.3%)  DiscShapeOptimizationPass
    0.0004 (  0.2%)  'func.func' Pipeline
    0.0004 (  0.2%)    HloCanonicalizeReductionPass
    0.0014 (  0.6%)  DiscShapeOptimizationPass
    0.0000 (  0.0%)  DiscMarkShapeCalculationPass
    0.0002 (  0.1%)  PlaceOpsPass
    0.0003 (  0.1%)  'func.func' Pipeline
    0.0001 (  0.1%)    Canonicalizer
    0.0000 (  0.0%)    CSE
    0.0000 (  0.0%)      (A) DominanceInfo
    0.0001 (  0.0%)    Canonicalizer
    0.0000 (  0.0%)    ElementTypeConverterPass
    0.0009 (  0.4%)  DiscShapeOptimizationPass
    0.0001 (  0.0%)  'func.func' Pipeline
    0.0000 (  0.0%)    ReductionRewriterPass
    0.0000 (  0.0%)    ConvRewriterPass
    0.0000 (  0.0%)    ConvRewriterPass
    0.0000 (  0.0%)    QuantizedDotRewriterPass
    0.0008 (  0.4%)  DiscShapeOptimizationPass
    0.0015 (  0.7%)  'func.func' Pipeline
    0.0001 (  0.1%)    Canonicalizer
    0.0000 (  0.0%)    CSE
    0.0000 (  0.0%)      (A) DominanceInfo
    0.0001 (  0.0%)    Canonicalizer
    0.0012 (  0.5%)    TransposeSimplifierPass
    0.0000 (  0.0%)    GpuConvPaddingLegalizationPass
    0.0008 (  0.4%)  DiscShapeOptimizationPass
    0.0000 (  0.0%)  'func.func' Pipeline
    0.0000 (  0.0%)    DiscAlgebraicSimplifierPass
    0.0009 (  0.4%)  DiscShapeOptimizationPass
    0.0004 (  0.2%)  'func.func' Pipeline
    0.0003 (  0.1%)    Canonicalizer
    0.0000 (  0.0%)    CSE
    0.0000 (  0.0%)      (A) DominanceInfo
    0.0001 (  0.0%)    Canonicalizer
    0.0003 (  0.1%)  FuncBufferize
    0.0000 (  0.0%)  DiscHloLegalizeToLhloPass
    0.0006 (  0.3%)  HloLegalizeToLhloPass
    0.0004 (  0.2%)  'func.func' Pipeline
    0.0004 (  0.2%)    Canonicalizer
    0.0000 (  0.0%)  DiscLhloRewriterPass
    0.0005 (  0.2%)  'func.func' Pipeline
    0.0001 (  0.0%)    Canonicalizer
    0.0000 (  0.0%)    ConvertShapeToStandardPass
    0.0001 (  0.0%)    Canonicalizer
    0.0000 (  0.0%)    CSE
    0.0000 (  0.0%)      (A) DominanceInfo
    0.0001 (  0.0%)    Canonicalizer
    0.0000 (  0.0%)    LegalizeToTensorOpPass
    0.0001 (  0.0%)    Canonicalizer
    0.0000 (  0.0%)    StdBufferizePass
    0.0000 (  0.0%)  ArithBufferize
    0.0003 (  0.1%)  'func.func' Pipeline
    0.0000 (  0.0%)    TensorBufferize
    0.0000 (  0.0%)    FinalizingBufferize
    0.0001 (  0.0%)    Canonicalizer
    0.0000 (  0.0%)    CSE
    0.0000 (  0.0%)      (A) DominanceInfo
    0.0001 (  0.0%)    Canonicalizer
    0.0000 (  0.0%)    DiscMemrefCanonicalizer
    0.0004 (  0.2%)  DiscAssignMemorySpacePass
    0.0028 (  1.3%)  'func.func' Pipeline
    0.0000 (  0.0%)    DiscDuplicateComputationForFusionPass
    0.0000 (  0.0%)    PromoteBuffersToStack
    0.0000 (  0.0%)    DiscMemRefLoadStoreSimplifierPass
    0.0005 (  0.2%)    DiscFusionPass
    0.0000 (  0.0%)    DiscFuseSplatConstPass
    0.0008 (  0.4%)    DiscSpecializeFusionWithSpeculationPass
    0.0006 (  0.2%)    Canonicalizer
    0.0000 (  0.0%)    CSE
    0.0000 (  0.0%)      (A) DominanceInfo
    0.0001 (  0.0%)    Canonicalizer
    0.0001 (  0.0%)    Canonicalizer
    0.0000 (  0.0%)    CSE
    0.0000 (  0.0%)      (A) DominanceInfo
    0.0001 (  0.0%)    Canonicalizer
    0.0004 (  0.2%)    BufferDeallocation
    0.0000 (  0.0%)    DiscBufferDeallocationPass
    0.0005 (  0.2%)  RalInjectExecutionContextPass
    0.0008 (  0.4%)  'func.func' Pipeline
    0.0008 (  0.4%)    DiscLowerToLibraryCallPass
    0.0002 (  0.1%)  DiscConstToRALPass
    0.0226 ( 10.2%)  'func.func' Pipeline
    0.0001 (  0.0%)    DiscMemRefLoadStoreSimplifierPass
    0.0016 (  0.7%)    DiscLhloLegalizeRootsToParallelLoopsPass
    0.0013 (  0.6%)    ExpandOps
    0.0002 (  0.1%)    UnhandledAtomicRMWConverterPass
    0.0015 (  0.7%)    InputInlineFusionPass
    0.0001 (  0.0%)    ForLoopUnrollInterleave
    0.0014 (  0.6%)    ArithExpandOps
    0.0002 (  0.1%)    DiscBF16ExpansionPass
    0.0002 (  0.1%)    FoldMemRefAliasOps
    0.0021 (  0.9%)    DiscFlattenMemrefAccessPass
    0.0022 (  1.0%)    Canonicalizer
    0.0015 (  0.7%)    CSE
    0.0000 (  0.0%)      (A) DominanceInfo
    0.0003 (  0.1%)    Canonicalizer
    0.0003 (  0.1%)    DiscMemRefCSEPass
    0.0014 (  0.6%)    ConvertShapeToStandardPass
    0.0015 (  0.7%)    Canonicalizer
    0.0001 (  0.0%)    CSE
    0.0000 (  0.0%)      (A) DominanceInfo
    0.0003 (  0.1%)    Canonicalizer
    0.0014 (  0.6%)    ParallelLoopCollapsing
    0.0016 (  0.7%)    SCFParallelLoopTiling
    0.0016 (  0.7%)    GpuMapParallelLoopsPass
    0.0020 (  0.9%)    ConvertParallelLoopToGpu
    0.0002 (  0.1%)  'func' Pipeline
    0.0002 (  0.1%)    CSE
    0.0000 (  0.0%)      (A) DominanceInfo
    0.0020 (  0.9%)  GpuLaunchSinkIndexComputations
    0.0024 (  1.1%)  GpuKernelOutlining
    0.0021 (  1.0%)  AssignKernelNamePass
    0.0007 (  0.3%)  'func.func' Pipeline
    0.0007 (  0.3%)    LhloFusionInlinerPass
    0.0002 (  0.1%)  DiscCompIntensFusionToCUDASourcePass
    0.0020 (  0.9%)  ReviseGpuKernelOutliningPass
    0.1383 ( 62.3%)  'gpu.module' Pipeline
    0.0002 (  0.1%)    LoopInvariantCodeMotion
    0.0015 (  0.7%)    'gpu.func' Pipeline
    0.0015 (  0.7%)      SideEffectLoopInvariantCodeMotionPass
    0.0001 (  0.0%)    LoopInvariantCodeMotion
    0.0013 (  0.6%)    CSE
    0.0000 (  0.0%)      (A) DominanceInfo
    0.0014 (  0.6%)    SCFToControlFlow
    0.0014 (  0.6%)    ConvertAffineToStandard
    0.0013 (  0.6%)    StripDebugInfo
    0.0037 (  1.7%)    DiscLowerGpuOpsToNVVMOpsPass
    0.0017 (  0.8%)    'llvm.func' Pipeline
    0.0017 (  0.8%)      LLVMInsertValueSimplifierPass
    0.0017 (  0.8%)    FunctionDeadArgumentEliminationPass
    0.1240 ( 55.9%)    GpuKernelToBlobPass
    0.0002 (  0.1%)  DiscGPUSourceToLibPass
    0.0009 (  0.4%)  'func.func' Pipeline
    0.0007 (  0.3%)    Canonicalizer
    0.0000 (  0.0%)    CSE
    0.0000 (  0.0%)      (A) DominanceInfo
    0.0001 (  0.1%)    Canonicalizer
    0.0000 (  0.0%)    RemoveDeadBufferPass
    0.0001 (  0.0%)    LinalgLowerToLoops
    0.0002 (  0.1%)  SCFToControlFlow
    0.0003 (  0.1%)  'func.func' Pipeline
    0.0001 (  0.0%)    ExpandStridedMetadata
    0.0001 (  0.1%)    Canonicalizer
    0.0000 (  0.0%)    CSE
    0.0000 (  0.0%)      (A) DominanceInfo
    0.0001 (  0.1%)    Canonicalizer
    0.0002 (  0.1%)  ConvertAffineToStandard
    0.0060 (  2.7%)  StripDebugInfo
    0.0059 (  2.7%)  DiscStripShapeConstraintOpsPass
    0.0121 (  5.4%)  DiscToLLVMPass
    0.0068 (  3.1%)  Rest
    0.2219 (100.0%)  Total
[DISC] LowerHLOToLLVM takes: 2.224150e-01 s.
before optimize llvm module:
; ModuleID = 'LLVMDialectModule'
source_filename = "LLVMDialectModule"

%0 = type { ptr, i64, { ptr, ptr, i64, [3 x i64], [3 x i64] } }
%.1 = type { ptr, i64, ptr }
%.2 = type { i64, ptr }
%.3 = type { ptr, ptr, i64, ptr, i64, i64, i64, i64, i64, i64, i32, ptr, i32, ptr }
%.4 = type { i64, i64, ptr, ptr }
%.5 = type { ptr, ptr, i64, ptr, i64, i64, i64, i64, i64, i64, i32, ptr, i32, ptr }
%.6 = type { ptr, ptr, ptr, ptr, i64, i64, i64, ptr, ptr, i64, i64, i64, { ptr, ptr, i64, [2 x i64], [2 x i64] } }
%.7 = type { ptr, ptr }
%.8 = type { ptr, i64, ptr, ptr, i64, i64, i64, i64, i64 }

@ral_send_output___cpu___pvoid_i64_m2df32___void = internal constant [48 x i8] c"ral_send_output___cpu___pvoid_i64_m2df32___void\00"
@dealloc___gpu___pvoid_pvoid___void = internal constant [35 x i8] c"dealloc___gpu___pvoid_pvoid___void\00"
@inc_ref___gpu___pvoid_pvoid_m1df32_m1di64___m2df32 = internal constant [51 x i8] c"inc_ref___gpu___pvoid_pvoid_m1df32_m1di64___m2df32\00"
@main_kernel_0_main_kColReduction_reduce__3_1_0___8w32h_1_kernel_name = internal constant [43 x i8] c"main_kColReduction_reduce__3_1_0___8w32h_1\00"
@main_kernel_0_blob_gpu.binary = internal constant [2280 x i8] c"P\EDU\BA\01\00\10\00\D8\08\00\00\00\00\00\00\02\00\01\01@\00\00\00\98\08\00\00\00\00\00\00\91\08\00\00\00\00\00\00\07\00\01\00P\00\00\00\00\00\00\00\00\00\00\00\11 \00\00\00\00\00\00\00\00\00\00\00\00\00\00\A8\14\00\00\00\00\00\00\A2\7FELF\02\01\013\07\00\01\00g\02\00\BE\00q\00\01\00\22\14\00\01\00\11\11\06\00\F5\0E\00P\05P\00@\008\00\03\00@\00\0C\00\01\00\00.shstrtab\00.\08\00'ym\08\00\F0\1B_shndx\00.nv.info\00.text.main_kColReduction_r\0A\00\F5\03e__3_1_0___8w32h_1:\00\0F4\00\1Doshared6\00\1AOrela\A0\00\1F?rel\D5\00\22\9Fconstant09\00\1A\B2debug_frame{\00\09\11\00!nv\14\00\11aE\00\0F\9E\01 \0F\8A\00\17\0F\C9\01\F4\8F$____wg_3\00\17\00\0C\00/24\02\02'o_param\09\02\1C\0F\01\00\05\8C]\00\00\00\03\00\0A\00\01\00\11\C2\18\00,\0B\00\01\00 \9C\01\18\00,\09\00\01\00\11\DC\18\00,\04\00\01\00\11\FA\18\00,\07\00\01\00f2\00\00\00\12\10x\00!\80\08\07\00v\00\FF\FF\FF\FF(\00\0C\00\00\01\00@\03\00\04|\08\00\D1\0F\0C\81\80\80(\00\08\FF\81\80(\08\0B\00\06(\00/0\00\01\00\03\13pX\00\10\04\9B\00\10\04\CD\04\02E\002\04\D4\01\18\00C/\08\00\06\DF\04\22\04#\0C\00\00\01\00'\04\12\0C\00\17\11\0C\00!7\04 \05\F1\08\015\00\00\04\0A\08\00\03\00\00\00`\01\18\00\03\19\18\00\04\17\0C$\00u\03\00\10\00\00\F0!\10\009\02\00\08\10\00\10\01(\01%\F0\11\10\00\01\01\00\F2\0A\F0\11\00\03\1B\FF\00\04\1C\0C\00P\00\00\00\B0\06\00\00\B0\07\00\00\04\1ET\01#K\00\01\00\B1\02\02\08\10\0A/\22\00\00\00\08\10\00#\00\08\08\00\13\10\08\00\13\18\08\00\13 \08\00\13(\08\00\130\08\00\228\08\84\00\13\00\08\00\13\08\08\00\13\10\08\00\13\18\08\00\13 \08\00\13(\08\00\130\08\00\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\00Z\06b,\00\00\00H\00\01\005\02\00\00d\01\0F\01\00\FF\DA@$v\01\FF\7F\04\B2\FF\00\8E\07\00\C4\0F\00\19y\00\01\00\10%\BB\02a\0E\00\19y\03\00\01\00\10!-\00\F0\04\0E\00$z\00\00\00X\00\00\03\02\8E\07\00\CA\1F\00\0C\10\00\C5Y\00\00p`\F0\03\00\DA\0F\00M\9B\04\B0\80\03\00\EA\0F\00\19x\03\FF\1F\0F\00\F1\00\14\01\00\00\E2\0F\00\B9z\04\00\00F\00\00\F3\04\93\E2\0F\00Ey\00\00\C0\040\00\B1\E2\0F\00$t\05\FF\00\00\80\FF\90\00a\E2\0F\00\11r\03\81\00`\FF@\8F\07\00\C8P\00 \04\FF\C1\02\10\03P\00q\E4\0F\00\12x\03\03\81\04\F1\00\C0\8E\07\00\C6\0F\00'x\02\04}\0C\CE\C7@\00P\C8\0F\00$x4\03B\00\00\03\0AP\00P\19x\07\FF\05\F0\020\16\01\00 \00@\12x\08\00\A0\00\01@\00\D4\E2\0F\04$x\03\07\D7\FF\FF\FF\04\020\00\11\060\00\22\00\16p\00P\0Cx\00\00\7F\D8\04\22@\F4\B0\00@x\03\03 \06\04\000\00\10\E4\10\00\11\02\91\03\22\06\02\90\00`\0Cx\00\03\13\050\00\13\F20\00C\02\02\04\00p\01\96\D8\0F\00G\19\00\00\B0\030\01\95$x\0A\06\00E\01\00\FFP\00%\06\07P\00\10\E4\A0\00t\05\07\E0\22\0A\00\0A \00Dr\07\FF\FFP\00\010\00 \09\04\90\00\14\05 \00\08P\01\10\C4 \00 \04\08\10\01\16\09`\00$\06@@\00\11\CA\B0\00 \06m\E0\00!B\FC\D0\01@$\E8\08\040\00\14\070\005\E4\09\FF\C0\00\93\C8\0F\00%\E6\08\08\00ZP\00p\CA\0F\00\81\E9\0C\08 \00\C0\00\19\1E\0C\00\A2\00\00\10x\0A\06@\000\FF\E0\FF@\00f\04\10x\0D\06\02\10\00\00p\00\12\0Ap\00#\F8\03\10\00\12\0D\10\00\11\F6\10\001\1Cx\00\01\00\F1\04p\F0\F0\03\00\D2\0F\00\10\C8\0A\04\14\05\00\00\07\E0\FF\A0\01:$\C4\0B\90\000\C6\0A\0A\90\00\14\0B\90\005\C9\0E\0A\90\00\94\E2\02\00\10\B8\08\04(\0A@\00G\1F\00$\B4\D0\000\E2\0F\00\A0\00\14\03\A0\00W\C6\0F\00%\B6\E0\00f\CC\0F\00\81\B9\08\E0\00q\22\0F\00\0B\E2\00\0CF\06\A4\80\FA\03\00\C8O\00\1C\E8\00\B0\001\02\00\E4 \00\11\05 \00!\C0\FAP\01A\08\82\0C\0CP\02:\00\80\06\F0\00\02 \00g\10\D8\0D\04<\0F\E0\00'\D4\0A\E0\00`/\00%\D6\0A\0D\90\00\14\0A\90\00&\D9\0A\E0\00a\A2\0E\00$\E2\05\10\02\13\0C\D0\00Q\0B\C2\00\0E\0Eo\00i\FC\03\00\E4\8F\08P\01\00`\01\14\C8\10\00\11p\10\00A\0B\C2\00\050\00#\C0\FC\B0\00\22\0E\0E\B0\00\11\00\B0\01#\0B\B2q\06\00P\00R\C6\0F\01$\C2p\00\13\0Ep\00\0D`\00\1B\B8`\001\B2\00\05@\00\12\C0\F0\01c\10x\07\07P\14p\01\10\D6p\00#\08\08p\00\10\06\C0\01\22$\B2`\00\1F\08`\00\04W\10x\06\06\04P\02A\0B\D2\00\0A\D6\08\10\80`\00aO\08\0B\D2\00\05\10\00\13\C0P\02$\D8\00\90\00\13\02\80\01\10\07p\03BpR\F8\03\80\00\22\0A\0A\80\00!\80\05\80\00\12\D2\80\00\13\0A\80\00`GI\00\00\D0\FCq\04\11\83\B0\03\14A0\05\03\C0\03A\88s\00\02@\00\00\9E\07f\E8\0F\00\1D{\00\01\00\93\EC\0F\00\84\A9\07\02\00\10 \00*\22\0E\D0\00\11\C4\90\00\01\90\041pD\F2 \05T\84\A9\04\02\00P\00qb\0E\00\0B\A2\00\07\CE\09\10\80\D0\00u\1F\08\0B\A2\00\04\07P\01T/\00\1C\A8\00P\00\13\01\E0\00 \00?\10\00#@\F6\E0\00!\07\070\02\00`\01\1B\E4P\01\8F\C6\0F\00\88\A3\00\02\07\C0\00\09!\B9\05O\09\22\00\08 \06+\84\B9\A0\001\B2\00\05\00\01`\80\F4\03\00\C4\1F\10\00(\04\05\A0\00\06\10\02\12\01`\027\05\05\04\F0\01/\88\B3@\01\0B9M\19\00p\019$t\04\C0\03!\84y\CB\08\02@\00\C3\E6\0F\00%v\04\03\00\\\00\00\04\00\05f\84y\03\02\00\04\C0\00E\81y\06\04@\03bb\03\00\0Br\00q\06\22\80\F0`\011r\00\00\10\00#\C0\F2@\01\02\1F\00@\00\00\80\04\90\031\0Br\000\040\00@\F0\E0\05(\0EF\10\02b\E6\0F\00\08r\07 \00\00\01\00p\CC\0F\00\A9s\07\04h\09\C0\07\E1\1E\00\00\A4\0E\00\0Cr\00\07\10\00 pR@\00QO\00$r\06p\02\14\07 \06V\09\00\00\90\FFp\02\1BMp\02TGy\00\00\F0 \00f\C0\0F\00\18y\00\01\00\0F\10\00\90\0F\01\00-#\01\00\A0\01\0B\01\00\22@\00\01\00=\9E\01\000\00\08\01\00\1F\0B@\00\04\13\DE)\00?\09\02\00@\00\0A\13\13<\0B\0C\01\00\13\E8U\00\03\8F\03\01$\00\13\05w\02\00\01\00\22\18\00\01\00.q\01T\00\00\01\00\11\90\95\02O\00\00p\00\80\00\0B\1F)'\00\03\02m\00\00v\0C\01\06\00\06\E4\00*\04\00\01\00\1Fc@\00\04\130@\00\17x@\00\1F\0A@\00\00!\8F\01D\01\0D@\00\13\A8@\00*\D8\00\01\00\1B\08\08\00?~\01\00\CE\0E\00\01\1E\07\01\01\00&\10\00\80\00\17\048\00\04\18\00\138@\01\0C\84\01\13\90@\00\17x1\01\0F\C0\00\01\132T\01\15\06R\00\0A\10\0F&\80\08\80\00j\06\00\00\11\80\00\01\00\13\97\94\00+\03\00\01\00\03\B0\13/\00\04\80\00\0B\14\06\AB\01/\14\00\01\00\02\1B\A8\08\00\17\08\08\02\17\05\E8\00\0C\01\009p\0A\00\08\00\088\00\18\06\A0\00\0F\01\00\05\03\A9\00\80\08\00\00\00\00\00\00\00\00\00\00\00\00\00\00"
@ral_kernel_launch___gpu___pvoid_ppvoid_i64_pvoid_i64_i64_i64_i64_i64_i64_i32_pvoid_i32_ppvoid___void = internal constant [101 x i8] c"ral_kernel_launch___gpu___pvoid_ppvoid_i64_pvoid_i64_i64_i64_i64_i64_i64_i32_pvoid_i32_ppvoid___void\00"
@main_kernel_main_kColReduction_reduce__3_1_0___8w32h_kernel_name = internal constant [41 x i8] c"main_kColReduction_reduce__3_1_0___8w32h\00"
@main_kernel_blob_gpu.binary = internal constant [1096 x i8] c"P\EDU\BA\01\00\10\008\04\00\00\00\00\00\00\02\00\01\01@\00\00\00\F8\03\00\00\00\00\00\00\F3\03\00\00\00\00\00\00\07\00\01\00P\00\00\00\00\00\00\00\00\00\00\00\11 \00\00\00\00\00\00\00\00\00\00\00\00\00\00\E8\0B\00\00\00\00\00\00\A2\7FELF\02\01\013\07\00\01\00f\02\00\BE\00q\00\01\00!@\0B\07\001\00\80\08\07\00\F5\0E\00P\05P\00@\008\00\03\00@\00\0B\00\01\00\00.shstrtab\00.\08\00'ym\08\00\F0\1B_shndx\00.nv.info\00.text.main_kColReduction_r\0A\00\F5\01e__3_1_0___8w32h8\00\0F2\00\1Boshared4\00\1B\9Fconstant07\00\18\FA\01debug_frame\00.rel\11\00!nv\14\00\11aC\00\0F+\01 \0F\88\00\15\0FT\01\BAo_param[\01\1C\0F\01\00\06\8C[\00\00\00\03\00\0A\00\01\00\11\F0\18\00,\09\00\01\00 .\01\18\00,\04\00\01\00\11L\18\00,\07\00\01\00f2\00\00\00\12\10`\00!\80\01\07\00v\00\FF\FF\FF\FF(\00\0C\00\00\01\00@\03\00\04|\08\00\D1\0F\0C\81\80\80(\00\08\FF\81\80(\08\0B\00\06(\00/0\00\01\00\03\13pX\00\10\04\9B\00R\04\14\00\00\00E\00\01\0B\00\00\13\00p/\08\00\05\00\00\00\A7\03\22\04#\0C\00\00\01\00'\04\12\0C\00\17\11\0C\00!7\04\E8\03\F1\08\015\00\00\04\0A\08\00\02\00\00\00`\01\10\00\03\19\10\00\04\17\0C$\00\10\01N\00%\F0!\10\00\01\01\00\F2\02\F0\11\00\03\1B\FF\00\04\1C\08\00P\00\00\00\B0\00\01\00#K\00\01\00s\02\02\08\10\0A/\22\8B\00\00\07\00\03\08\00\13\10\08\00\13\18\08\00\13 \08\00\13(\08\00\130\08\00\228\08|\00\13\00\08\00\13\08\08\00\13\10\08\00\13\18\08\00\13 \08\00\13(\08\00\130\08\00\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\A2\00\00\00\14,\00\00\00H\00\01\00\00 \01/\05\00\01\00\FF\C0A\02z\01\00\1F\04\B1\0F\00\00\00\C4\0F\00\19y\02\00\01\00\10%\9B\02Q\0E\00\19y\03\0F\00\F5\1A\00!\00\00\00$\0E\00$z\02\02\00X\00\00\03\02\8E\07\00\CA\1F\00\0Cx\00\02\13\05\00\00p@\F0\03\00\DA\0F\00MS\04\A0\80\03\00\EA\0F\005t\03\FF\B3\03\10\FF\C0\03P\E2\0F\00\02xF\02B\80\FF\00\0F\10\00r\B9z\04\00\00F\00\84\00\90\D0\0F\00%v\02\02\00Zl\04\00`\00`\0F\00\86y\00\022\00@\04\19\10\0C0\009My\00`\00PGy\00\00\F09\04\A6\FF\83\03\00\C0\0F\00\18y\00\01\00\0F\10\00\90\0F\01\00-\00W\01.\03\00\01\00\22@\00\01\00=+\01\000\00\08\01\00\1F\0B@\00\04\13k)\00\1F[@\00\0C\13\13\1C\04\0C\01\00\13\C8\15\00&\90\00@\04#\04\00\95\04\00\06\05\12\00\01\00\1F\FET\00\00\00\01\00\13X\95\00/p\00\80\00\0B\1F)'\00\03#\00\C8@\00\04P\06\04\E4\00*\04\00\01\00\1Fa@\00\04\13\F81\00&L\00@\00\1F\0A@\00\00!\1C\01D\01\0D@\00\13H)\00*\D8\00\01\00\1B\08\08\00?\0B\01\00\86\07\00Q\00\00 \05\00\01\00&\10\00\80\00\17\048\00\04\18\00\13\C7\14\01\0C\84\01*0\058\07\1F\00\C0\00\04\132@\00+\06\00\01\00\1A\07\D0\07\12\03\00\06:\08\80\00\01\00\13\06\18\06\04(\0B\0C\01\00*\A8\00\08\00\04\F8\00\13\018\00\04\A8\00\0C\01\009P\03\00\08\00\088\00/\06\00\01\00\17\80\08\00\00\00\00\00\00\00\00\00\00\00\00"
@alloc___gpu___pvoid_i64___pvoid = internal constant [32 x i8] c"alloc___gpu___pvoid_i64___pvoid\00"
@ral_recv_input___cpu___pvoid_i64___m3df32 = internal constant [42 x i8] c"ral_recv_input___cpu___pvoid_i64___m3df32\00"

declare ptr @malloc(i64)

declare void @free(ptr)

declare void @disc_ral_call(ptr, ptr, ptr)

define void @main(ptr %0) {
  %2 = alloca %0, align 8
  %3 = alloca ptr, i32 3, align 8
  %4 = getelementptr %0, ptr %2, i32 0, i32 0
  store ptr %0, ptr %4, align 8
  %5 = getelementptr ptr, ptr %3, i32 0
  store ptr %4, ptr %5, align 8
  %6 = getelementptr %0, ptr %2, i32 0, i32 1
  store i64 0, ptr %6, align 4
  %7 = getelementptr ptr, ptr %3, i32 1
  store ptr %6, ptr %7, align 8
  %8 = getelementptr %0, ptr %2, i32 0, i32 2
  %9 = getelementptr ptr, ptr %3, i32 2
  store ptr %8, ptr %9, align 8
  call void @disc_ral_call(ptr %0, ptr @ral_recv_input___cpu___pvoid_i64___m3df32, ptr %3)
  %10 = load { ptr, ptr, i64, [3 x i64], [3 x i64] }, ptr %8, align 8
  %11 = alloca %.1, align 8
  %12 = alloca ptr, i32 3, align 8
  %13 = getelementptr %.1, ptr %11, i32 0, i32 0
  store ptr %0, ptr %13, align 8
  %14 = getelementptr ptr, ptr %12, i32 0
  store ptr %13, ptr %14, align 8
  %15 = getelementptr %.1, ptr %11, i32 0, i32 1
  store i64 ptrtoint (ptr getelementptr (float, ptr null, i64 1300) to i64), ptr %15, align 4
  %16 = getelementptr ptr, ptr %12, i32 1
  store ptr %15, ptr %16, align 8
  %17 = getelementptr %.1, ptr %11, i32 0, i32 2
  %18 = getelementptr ptr, ptr %12, i32 2
  store ptr %17, ptr %18, align 8
  call void @disc_ral_call(ptr %0, ptr @alloc___gpu___pvoid_i64___pvoid, ptr %12)
  %19 = load ptr, ptr %17, align 8
  %20 = insertvalue { ptr, ptr, i64, [1 x i64], [1 x i64] } undef, ptr %19, 0
  %21 = insertvalue { ptr, ptr, i64, [1 x i64], [1 x i64] } %20, ptr %19, 1
  %22 = insertvalue { ptr, ptr, i64, [1 x i64], [1 x i64] } %21, i64 0, 2
  %23 = insertvalue { ptr, ptr, i64, [1 x i64], [1 x i64] } %22, i64 1300, 3, 0
  %24 = insertvalue { ptr, ptr, i64, [1 x i64], [1 x i64] } %23, i64 1, 4, 0
  %25 = alloca ptr, align 8
  %26 = getelementptr ptr, ptr %25, i32 0
  store ptr @main_kernel_blob_gpu.binary, ptr %26, align 8
  %27 = extractvalue { ptr, ptr, i64, [1 x i64], [1 x i64] } %24, 0
  %28 = extractvalue { ptr, ptr, i64, [1 x i64], [1 x i64] } %24, 1
  %29 = extractvalue { ptr, ptr, i64, [1 x i64], [1 x i64] } %24, 2
  %30 = extractvalue { ptr, ptr, i64, [1 x i64], [1 x i64] } %24, 3, 0
  %31 = extractvalue { ptr, ptr, i64, [1 x i64], [1 x i64] } %24, 4, 0
  %32 = alloca %.2, align 8
  %33 = alloca ptr, i32 2, align 8
  %34 = getelementptr %.2, ptr %32, i32 0, i32 0
  store i64 256, ptr %34, align 4
  %35 = getelementptr ptr, ptr %33, i32 0
  store ptr %34, ptr %35, align 8
  %36 = getelementptr %.2, ptr %32, i32 0, i32 1
  store ptr %28, ptr %36, align 8
  %37 = getelementptr ptr, ptr %33, i32 1
  store ptr %36, ptr %37, align 8
  %38 = alloca %.3, align 8
  %39 = alloca ptr, i32 14, align 8
  %40 = getelementptr %.3, ptr %38, i32 0, i32 0
  store ptr %0, ptr %40, align 8
  %41 = getelementptr ptr, ptr %39, i32 0
  store ptr %40, ptr %41, align 8
  %42 = getelementptr %.3, ptr %38, i32 0, i32 1
  store ptr %25, ptr %42, align 8
  %43 = getelementptr ptr, ptr %39, i32 1
  store ptr %42, ptr %43, align 8
  %44 = getelementptr %.3, ptr %38, i32 0, i32 2
  store i64 1, ptr %44, align 4
  %45 = getelementptr ptr, ptr %39, i32 2
  store ptr %44, ptr %45, align 8
  %46 = getelementptr %.3, ptr %38, i32 0, i32 3
  store ptr @main_kernel_main_kColReduction_reduce__3_1_0___8w32h_kernel_name, ptr %46, align 8
  %47 = getelementptr ptr, ptr %39, i32 3
  store ptr %46, ptr %47, align 8
  %48 = getelementptr %.3, ptr %38, i32 0, i32 4
  store i64 6, ptr %48, align 4
  %49 = getelementptr ptr, ptr %39, i32 4
  store ptr %48, ptr %49, align 8
  %50 = getelementptr %.3, ptr %38, i32 0, i32 5
  store i64 1, ptr %50, align 4
  %51 = getelementptr ptr, ptr %39, i32 5
  store ptr %50, ptr %51, align 8
  %52 = getelementptr %.3, ptr %38, i32 0, i32 6
  store i64 1, ptr %52, align 4
  %53 = getelementptr ptr, ptr %39, i32 6
  store ptr %52, ptr %53, align 8
  %54 = getelementptr %.3, ptr %38, i32 0, i32 7
  store i64 256, ptr %54, align 4
  %55 = getelementptr ptr, ptr %39, i32 7
  store ptr %54, ptr %55, align 8
  %56 = getelementptr %.3, ptr %38, i32 0, i32 8
  store i64 1, ptr %56, align 4
  %57 = getelementptr ptr, ptr %39, i32 8
  store ptr %56, ptr %57, align 8
  %58 = getelementptr %.3, ptr %38, i32 0, i32 9
  store i64 1, ptr %58, align 4
  %59 = getelementptr ptr, ptr %39, i32 9
  store ptr %58, ptr %59, align 8
  %60 = getelementptr %.3, ptr %38, i32 0, i32 10
  store i32 0, ptr %60, align 4
  %61 = getelementptr ptr, ptr %39, i32 10
  store ptr %60, ptr %61, align 8
  %62 = getelementptr %.3, ptr %38, i32 0, i32 11
  store ptr null, ptr %62, align 8
  %63 = getelementptr ptr, ptr %39, i32 11
  store ptr %62, ptr %63, align 8
  %64 = getelementptr %.3, ptr %38, i32 0, i32 12
  store i32 2, ptr %64, align 4
  %65 = getelementptr ptr, ptr %39, i32 12
  store ptr %64, ptr %65, align 8
  %66 = getelementptr %.3, ptr %38, i32 0, i32 13
  store ptr %33, ptr %66, align 8
  %67 = getelementptr ptr, ptr %39, i32 13
  store ptr %66, ptr %67, align 8
  call void @disc_ral_call(ptr %0, ptr @ral_kernel_launch___gpu___pvoid_ppvoid_i64_pvoid_i64_i64_i64_i64_i64_i64_i32_pvoid_i32_ppvoid___void, ptr %39)
  %68 = alloca ptr, align 8
  %69 = getelementptr ptr, ptr %68, i32 0
  store ptr @main_kernel_0_blob_gpu.binary, ptr %69, align 8
  %70 = extractvalue { ptr, ptr, i64, [3 x i64], [3 x i64] } %10, 0
  %71 = extractvalue { ptr, ptr, i64, [3 x i64], [3 x i64] } %10, 1
  %72 = extractvalue { ptr, ptr, i64, [3 x i64], [3 x i64] } %10, 2
  %73 = extractvalue { ptr, ptr, i64, [3 x i64], [3 x i64] } %10, 3, 0
  %74 = extractvalue { ptr, ptr, i64, [3 x i64], [3 x i64] } %10, 3, 1
  %75 = extractvalue { ptr, ptr, i64, [3 x i64], [3 x i64] } %10, 3, 2
  %76 = extractvalue { ptr, ptr, i64, [3 x i64], [3 x i64] } %10, 4, 0
  %77 = extractvalue { ptr, ptr, i64, [3 x i64], [3 x i64] } %10, 4, 1
  %78 = extractvalue { ptr, ptr, i64, [3 x i64], [3 x i64] } %10, 4, 2
  %79 = extractvalue { ptr, ptr, i64, [1 x i64], [1 x i64] } %24, 0
  %80 = extractvalue { ptr, ptr, i64, [1 x i64], [1 x i64] } %24, 1
  %81 = extractvalue { ptr, ptr, i64, [1 x i64], [1 x i64] } %24, 2
  %82 = extractvalue { ptr, ptr, i64, [1 x i64], [1 x i64] } %24, 3, 0
  %83 = extractvalue { ptr, ptr, i64, [1 x i64], [1 x i64] } %24, 4, 0
  %84 = alloca %.4, align 8
  %85 = alloca ptr, i32 4, align 8
  %86 = getelementptr %.4, ptr %84, i32 0, i32 0
  store i64 256, ptr %86, align 4
  %87 = getelementptr ptr, ptr %85, i32 0
  store ptr %86, ptr %87, align 8
  %88 = getelementptr %.4, ptr %84, i32 0, i32 1
  store i64 10496, ptr %88, align 4
  %89 = getelementptr ptr, ptr %85, i32 1
  store ptr %88, ptr %89, align 8
  %90 = getelementptr %.4, ptr %84, i32 0, i32 2
  store ptr %71, ptr %90, align 8
  %91 = getelementptr ptr, ptr %85, i32 2
  store ptr %90, ptr %91, align 8
  %92 = getelementptr %.4, ptr %84, i32 0, i32 3
  store ptr %80, ptr %92, align 8
  %93 = getelementptr ptr, ptr %85, i32 3
  store ptr %92, ptr %93, align 8
  %94 = alloca %.5, align 8
  %95 = alloca ptr, i32 14, align 8
  %96 = getelementptr %.5, ptr %94, i32 0, i32 0
  store ptr %0, ptr %96, align 8
  %97 = getelementptr ptr, ptr %95, i32 0
  store ptr %96, ptr %97, align 8
  %98 = getelementptr %.5, ptr %94, i32 0, i32 1
  store ptr %68, ptr %98, align 8
  %99 = getelementptr ptr, ptr %95, i32 1
  store ptr %98, ptr %99, align 8
  %100 = getelementptr %.5, ptr %94, i32 0, i32 2
  store i64 1, ptr %100, align 4
  %101 = getelementptr ptr, ptr %95, i32 2
  store ptr %100, ptr %101, align 8
  %102 = getelementptr %.5, ptr %94, i32 0, i32 3
  store ptr @main_kernel_0_main_kColReduction_reduce__3_1_0___8w32h_1_kernel_name, ptr %102, align 8
  %103 = getelementptr ptr, ptr %95, i32 3
  store ptr %102, ptr %103, align 8
  %104 = getelementptr %.5, ptr %94, i32 0, i32 4
  store i64 41, ptr %104, align 4
  %105 = getelementptr ptr, ptr %95, i32 4
  store ptr %104, ptr %105, align 8
  %106 = getelementptr %.5, ptr %94, i32 0, i32 5
  store i64 1, ptr %106, align 4
  %107 = getelementptr ptr, ptr %95, i32 5
  store ptr %106, ptr %107, align 8
  %108 = getelementptr %.5, ptr %94, i32 0, i32 6
  store i64 1, ptr %108, align 4
  %109 = getelementptr ptr, ptr %95, i32 6
  store ptr %108, ptr %109, align 8
  %110 = getelementptr %.5, ptr %94, i32 0, i32 7
  store i64 256, ptr %110, align 4
  %111 = getelementptr ptr, ptr %95, i32 7
  store ptr %110, ptr %111, align 8
  %112 = getelementptr %.5, ptr %94, i32 0, i32 8
  store i64 1, ptr %112, align 4
  %113 = getelementptr ptr, ptr %95, i32 8
  store ptr %112, ptr %113, align 8
  %114 = getelementptr %.5, ptr %94, i32 0, i32 9
  store i64 1, ptr %114, align 4
  %115 = getelementptr ptr, ptr %95, i32 9
  store ptr %114, ptr %115, align 8
  %116 = getelementptr %.5, ptr %94, i32 0, i32 10
  store i32 0, ptr %116, align 4
  %117 = getelementptr ptr, ptr %95, i32 10
  store ptr %116, ptr %117, align 8
  %118 = getelementptr %.5, ptr %94, i32 0, i32 11
  store ptr null, ptr %118, align 8
  %119 = getelementptr ptr, ptr %95, i32 11
  store ptr %118, ptr %119, align 8
  %120 = getelementptr %.5, ptr %94, i32 0, i32 12
  store i32 4, ptr %120, align 4
  %121 = getelementptr ptr, ptr %95, i32 12
  store ptr %120, ptr %121, align 8
  %122 = getelementptr %.5, ptr %94, i32 0, i32 13
  store ptr %85, ptr %122, align 8
  %123 = getelementptr ptr, ptr %95, i32 13
  store ptr %122, ptr %123, align 8
  call void @disc_ral_call(ptr %0, ptr @ral_kernel_launch___gpu___pvoid_ppvoid_i64_pvoid_i64_i64_i64_i64_i64_i64_i32_pvoid_i32_ppvoid___void, ptr %95)
  %124 = alloca i64, i64 ptrtoint (ptr getelementptr (i64, ptr null, i64 2) to i64), align 8
  %125 = insertvalue { ptr, ptr, i64, [1 x i64], [1 x i64] } undef, ptr %124, 0
  %126 = insertvalue { ptr, ptr, i64, [1 x i64], [1 x i64] } %125, ptr %124, 1
  %127 = insertvalue { ptr, ptr, i64, [1 x i64], [1 x i64] } %126, i64 0, 2
  %128 = insertvalue { ptr, ptr, i64, [1 x i64], [1 x i64] } %127, i64 2, 3, 0
  %129 = insertvalue { ptr, ptr, i64, [1 x i64], [1 x i64] } %128, i64 1, 4, 0
  %130 = extractvalue { ptr, ptr, i64, [1 x i64], [1 x i64] } %129, 1
  %131 = getelementptr i64, ptr %130, i64 0
  store i64 100, ptr %131, align 4
  %132 = extractvalue { ptr, ptr, i64, [1 x i64], [1 x i64] } %129, 1
  %133 = getelementptr i64, ptr %132, i64 1
  store i64 13, ptr %133, align 4
  %134 = extractvalue { ptr, ptr, i64, [1 x i64], [1 x i64] } %24, 0
  %135 = extractvalue { ptr, ptr, i64, [1 x i64], [1 x i64] } %24, 1
  %136 = extractvalue { ptr, ptr, i64, [1 x i64], [1 x i64] } %24, 2
  %137 = extractvalue { ptr, ptr, i64, [1 x i64], [1 x i64] } %24, 3, 0
  %138 = extractvalue { ptr, ptr, i64, [1 x i64], [1 x i64] } %24, 4, 0
  %139 = extractvalue { ptr, ptr, i64, [1 x i64], [1 x i64] } %129, 0
  %140 = extractvalue { ptr, ptr, i64, [1 x i64], [1 x i64] } %129, 1
  %141 = extractvalue { ptr, ptr, i64, [1 x i64], [1 x i64] } %129, 2
  %142 = extractvalue { ptr, ptr, i64, [1 x i64], [1 x i64] } %129, 3, 0
  %143 = extractvalue { ptr, ptr, i64, [1 x i64], [1 x i64] } %129, 4, 0
  %144 = alloca %.6, align 8
  %145 = alloca ptr, i32 13, align 8
  %146 = getelementptr %.6, ptr %144, i32 0, i32 0
  store ptr %0, ptr %146, align 8
  %147 = getelementptr ptr, ptr %145, i32 0
  store ptr %146, ptr %147, align 8
  %148 = getelementptr %.6, ptr %144, i32 0, i32 1
  store ptr null, ptr %148, align 8
  %149 = getelementptr ptr, ptr %145, i32 1
  store ptr %148, ptr %149, align 8
  %150 = getelementptr %.6, ptr %144, i32 0, i32 2
  store ptr %134, ptr %150, align 8
  %151 = getelementptr ptr, ptr %145, i32 2
  store ptr %150, ptr %151, align 8
  %152 = getelementptr %.6, ptr %144, i32 0, i32 3
  store ptr %135, ptr %152, align 8
  %153 = getelementptr ptr, ptr %145, i32 3
  store ptr %152, ptr %153, align 8
  %154 = getelementptr %.6, ptr %144, i32 0, i32 4
  store i64 %136, ptr %154, align 4
  %155 = getelementptr ptr, ptr %145, i32 4
  store ptr %154, ptr %155, align 8
  %156 = getelementptr %.6, ptr %144, i32 0, i32 5
  store i64 %137, ptr %156, align 4
  %157 = getelementptr ptr, ptr %145, i32 5
  store ptr %156, ptr %157, align 8
  %158 = getelementptr %.6, ptr %144, i32 0, i32 6
  store i64 %138, ptr %158, align 4
  %159 = getelementptr ptr, ptr %145, i32 6
  store ptr %158, ptr %159, align 8
  %160 = getelementptr %.6, ptr %144, i32 0, i32 7
  store ptr %139, ptr %160, align 8
  %161 = getelementptr ptr, ptr %145, i32 7
  store ptr %160, ptr %161, align 8
  %162 = getelementptr %.6, ptr %144, i32 0, i32 8
  store ptr %140, ptr %162, align 8
  %163 = getelementptr ptr, ptr %145, i32 8
  store ptr %162, ptr %163, align 8
  %164 = getelementptr %.6, ptr %144, i32 0, i32 9
  store i64 %141, ptr %164, align 4
  %165 = getelementptr ptr, ptr %145, i32 9
  store ptr %164, ptr %165, align 8
  %166 = getelementptr %.6, ptr %144, i32 0, i32 10
  store i64 %142, ptr %166, align 4
  %167 = getelementptr ptr, ptr %145, i32 10
  store ptr %166, ptr %167, align 8
  %168 = getelementptr %.6, ptr %144, i32 0, i32 11
  store i64 %143, ptr %168, align 4
  %169 = getelementptr ptr, ptr %145, i32 11
  store ptr %168, ptr %169, align 8
  %170 = getelementptr %.6, ptr %144, i32 0, i32 12
  %171 = getelementptr ptr, ptr %145, i32 12
  store ptr %170, ptr %171, align 8
  call void @disc_ral_call(ptr %0, ptr @inc_ref___gpu___pvoid_pvoid_m1df32_m1di64___m2df32, ptr %145)
  %172 = load { ptr, ptr, i64, [2 x i64], [2 x i64] }, ptr %170, align 8
  %173 = extractvalue { ptr, ptr, i64, [2 x i64], [2 x i64] } %172, 0
  %174 = extractvalue { ptr, ptr, i64, [2 x i64], [2 x i64] } %172, 1
  %175 = insertvalue { ptr, ptr, i64, [2 x i64], [2 x i64] } undef, ptr %173, 0
  %176 = insertvalue { ptr, ptr, i64, [2 x i64], [2 x i64] } %175, ptr %174, 1
  %177 = insertvalue { ptr, ptr, i64, [2 x i64], [2 x i64] } %176, i64 0, 2
  %178 = insertvalue { ptr, ptr, i64, [2 x i64], [2 x i64] } %177, i64 100, 3, 0
  %179 = insertvalue { ptr, ptr, i64, [2 x i64], [2 x i64] } %178, i64 13, 4, 0
  %180 = insertvalue { ptr, ptr, i64, [2 x i64], [2 x i64] } %179, i64 13, 3, 1
  %181 = insertvalue { ptr, ptr, i64, [2 x i64], [2 x i64] } %180, i64 1, 4, 1
  %182 = extractvalue { ptr, ptr, i64, [1 x i64], [1 x i64] } %24, 0
  %183 = alloca %.7, align 8
  %184 = alloca ptr, i32 2, align 8
  %185 = getelementptr %.7, ptr %183, i32 0, i32 0
  store ptr %0, ptr %185, align 8
  %186 = getelementptr ptr, ptr %184, i32 0
  store ptr %185, ptr %186, align 8
  %187 = getelementptr %.7, ptr %183, i32 0, i32 1
  store ptr %182, ptr %187, align 8
  %188 = getelementptr ptr, ptr %184, i32 1
  store ptr %187, ptr %188, align 8
  call void @disc_ral_call(ptr %0, ptr @dealloc___gpu___pvoid_pvoid___void, ptr %184)
  %189 = extractvalue { ptr, ptr, i64, [2 x i64], [2 x i64] } %181, 0
  %190 = extractvalue { ptr, ptr, i64, [2 x i64], [2 x i64] } %181, 1
  %191 = extractvalue { ptr, ptr, i64, [2 x i64], [2 x i64] } %181, 2
  %192 = extractvalue { ptr, ptr, i64, [2 x i64], [2 x i64] } %181, 3, 0
  %193 = extractvalue { ptr, ptr, i64, [2 x i64], [2 x i64] } %181, 3, 1
  %194 = extractvalue { ptr, ptr, i64, [2 x i64], [2 x i64] } %181, 4, 0
  %195 = extractvalue { ptr, ptr, i64, [2 x i64], [2 x i64] } %181, 4, 1
  %196 = alloca %.8, align 8
  %197 = alloca ptr, i32 9, align 8
  %198 = getelementptr %.8, ptr %196, i32 0, i32 0
  store ptr %0, ptr %198, align 8
  %199 = getelementptr ptr, ptr %197, i32 0
  store ptr %198, ptr %199, align 8
  %200 = getelementptr %.8, ptr %196, i32 0, i32 1
  store i64 0, ptr %200, align 4
  %201 = getelementptr ptr, ptr %197, i32 1
  store ptr %200, ptr %201, align 8
  %202 = getelementptr %.8, ptr %196, i32 0, i32 2
  store ptr %189, ptr %202, align 8
  %203 = getelementptr ptr, ptr %197, i32 2
  store ptr %202, ptr %203, align 8
  %204 = getelementptr %.8, ptr %196, i32 0, i32 3
  store ptr %190, ptr %204, align 8
  %205 = getelementptr ptr, ptr %197, i32 3
  store ptr %204, ptr %205, align 8
  %206 = getelementptr %.8, ptr %196, i32 0, i32 4
  store i64 %191, ptr %206, align 4
  %207 = getelementptr ptr, ptr %197, i32 4
  store ptr %206, ptr %207, align 8
  %208 = getelementptr %.8, ptr %196, i32 0, i32 5
  store i64 %192, ptr %208, align 4
  %209 = getelementptr ptr, ptr %197, i32 5
  store ptr %208, ptr %209, align 8
  %210 = getelementptr %.8, ptr %196, i32 0, i32 6
  store i64 %193, ptr %210, align 4
  %211 = getelementptr ptr, ptr %197, i32 6
  store ptr %210, ptr %211, align 8
  %212 = getelementptr %.8, ptr %196, i32 0, i32 7
  store i64 %194, ptr %212, align 4
  %213 = getelementptr ptr, ptr %197, i32 7
  store ptr %212, ptr %213, align 8
  %214 = getelementptr %.8, ptr %196, i32 0, i32 8
  store i64 %195, ptr %214, align 4
  %215 = getelementptr ptr, ptr %197, i32 8
  store ptr %214, ptr %215, align 8
  call void @disc_ral_call(ptr %0, ptr @ral_send_output___cpu___pvoid_i64_m2df32___void, ptr %197)
  ret void
}

host default target triple: x86_64-unknown-linux-gnu
host cpu name: icelake-server
host cpu features: -avx512pf,-tsxldtrk,+cx16,+sahf,-tbm,+avx512ifma,+sha,+crc32,-fma4,+vpclmulqdq,+prfchw,+bmi2,-cldemote,+fsgsbase,-avx512bf16,-amx-tile,-raoint,-uintr,+gfni,+popcnt,-ptwrite,+aes,+avx512bitalg,-movdiri,-widekl,+xsaves,-avx512er,-avxvnni,-avx512fp16,+avx512vnni,-amx-bf16,-avxvnniint8,+avx512vpopcntdq,-pconfig,+clwb,-cmpccxadd,+avx512f,+xsavec,-clzero,-pku,-amx-fp16,+mmx,-lwp,+rdpid,-xop,+rdseed,-waitpkg,-prefetchi,-kl,-movdir64b,-sse4a,+avx512bw,-avxneconvert,+clflushopt,+xsave,+avx512vbmi2,+64bit,+avx512vl,-serialize,-hreset,+invpcid,+avx512cd,+avx,+vaes,-amx-int8,+cx8,+fma,-rtm,+bmi,-enqcmd,+rdrnd,-mwaitx,+sse4.1,+sse4.2,+avx2,+fxsr,+wbnoinvd,+sse,+lzcnt,+pclmul,-rdpru,-avxifma,+f16c,+ssse3,-sgx,-prefetchwt1,+cmov,+avx512vbmi,-shstk,+movbe,-avx512vp2intersect,+xsaveopt,+avx512dq,+sse2,+adx,+sse3
after optimize llvm module:
; ModuleID = 'LLVMDialectModule'
source_filename = "LLVMDialectModule"
target datalayout = "e-m:e-p270:32:32-p271:32:32-p272:64:64-i64:64-f80:128-n8:16:32:64-S128"
target triple = "x86_64-unknown-linux-gnu"

%0 = type { ptr, i64, { ptr, ptr, i64, [3 x i64], [3 x i64] } }
%.1 = type { ptr, i64, ptr }
%.2 = type { i64, ptr }
%.3 = type { ptr, ptr, i64, ptr, i64, i64, i64, i64, i64, i64, i32, ptr, i32, ptr }
%.4 = type { i64, i64, ptr, ptr }
%.5 = type { ptr, ptr, i64, ptr, i64, i64, i64, i64, i64, i64, i32, ptr, i32, ptr }
%.6 = type { ptr, ptr, ptr, ptr, i64, i64, i64, ptr, ptr, i64, i64, i64, { ptr, ptr, i64, [2 x i64], [2 x i64] } }
%.7 = type { ptr, ptr }
%.8 = type { ptr, i64, ptr, ptr, i64, i64, i64, i64, i64 }

@ral_send_output___cpu___pvoid_i64_m2df32___void = internal constant [48 x i8] c"ral_send_output___cpu___pvoid_i64_m2df32___void\00"
@dealloc___gpu___pvoid_pvoid___void = internal constant [35 x i8] c"dealloc___gpu___pvoid_pvoid___void\00"
@inc_ref___gpu___pvoid_pvoid_m1df32_m1di64___m2df32 = internal constant [51 x i8] c"inc_ref___gpu___pvoid_pvoid_m1df32_m1di64___m2df32\00"
@main_kernel_0_main_kColReduction_reduce__3_1_0___8w32h_1_kernel_name = internal constant [43 x i8] c"main_kColReduction_reduce__3_1_0___8w32h_1\00"
@main_kernel_0_blob_gpu.binary = internal constant [2280 x i8] c"P\EDU\BA\01\00\10\00\D8\08\00\00\00\00\00\00\02\00\01\01@\00\00\00\98\08\00\00\00\00\00\00\91\08\00\00\00\00\00\00\07\00\01\00P\00\00\00\00\00\00\00\00\00\00\00\11 \00\00\00\00\00\00\00\00\00\00\00\00\00\00\A8\14\00\00\00\00\00\00\A2\7FELF\02\01\013\07\00\01\00g\02\00\BE\00q\00\01\00\22\14\00\01\00\11\11\06\00\F5\0E\00P\05P\00@\008\00\03\00@\00\0C\00\01\00\00.shstrtab\00.\08\00'ym\08\00\F0\1B_shndx\00.nv.info\00.text.main_kColReduction_r\0A\00\F5\03e__3_1_0___8w32h_1:\00\0F4\00\1Doshared6\00\1AOrela\A0\00\1F?rel\D5\00\22\9Fconstant09\00\1A\B2debug_frame{\00\09\11\00!nv\14\00\11aE\00\0F\9E\01 \0F\8A\00\17\0F\C9\01\F4\8F$____wg_3\00\17\00\0C\00/24\02\02'o_param\09\02\1C\0F\01\00\05\8C]\00\00\00\03\00\0A\00\01\00\11\C2\18\00,\0B\00\01\00 \9C\01\18\00,\09\00\01\00\11\DC\18\00,\04\00\01\00\11\FA\18\00,\07\00\01\00f2\00\00\00\12\10x\00!\80\08\07\00v\00\FF\FF\FF\FF(\00\0C\00\00\01\00@\03\00\04|\08\00\D1\0F\0C\81\80\80(\00\08\FF\81\80(\08\0B\00\06(\00/0\00\01\00\03\13pX\00\10\04\9B\00\10\04\CD\04\02E\002\04\D4\01\18\00C/\08\00\06\DF\04\22\04#\0C\00\00\01\00'\04\12\0C\00\17\11\0C\00!7\04 \05\F1\08\015\00\00\04\0A\08\00\03\00\00\00`\01\18\00\03\19\18\00\04\17\0C$\00u\03\00\10\00\00\F0!\10\009\02\00\08\10\00\10\01(\01%\F0\11\10\00\01\01\00\F2\0A\F0\11\00\03\1B\FF\00\04\1C\0C\00P\00\00\00\B0\06\00\00\B0\07\00\00\04\1ET\01#K\00\01\00\B1\02\02\08\10\0A/\22\00\00\00\08\10\00#\00\08\08\00\13\10\08\00\13\18\08\00\13 \08\00\13(\08\00\130\08\00\228\08\84\00\13\00\08\00\13\08\08\00\13\10\08\00\13\18\08\00\13 \08\00\13(\08\00\130\08\00\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\00Z\06b,\00\00\00H\00\01\005\02\00\00d\01\0F\01\00\FF\DA@$v\01\FF\7F\04\B2\FF\00\8E\07\00\C4\0F\00\19y\00\01\00\10%\BB\02a\0E\00\19y\03\00\01\00\10!-\00\F0\04\0E\00$z\00\00\00X\00\00\03\02\8E\07\00\CA\1F\00\0C\10\00\C5Y\00\00p`\F0\03\00\DA\0F\00M\9B\04\B0\80\03\00\EA\0F\00\19x\03\FF\1F\0F\00\F1\00\14\01\00\00\E2\0F\00\B9z\04\00\00F\00\00\F3\04\93\E2\0F\00Ey\00\00\C0\040\00\B1\E2\0F\00$t\05\FF\00\00\80\FF\90\00a\E2\0F\00\11r\03\81\00`\FF@\8F\07\00\C8P\00 \04\FF\C1\02\10\03P\00q\E4\0F\00\12x\03\03\81\04\F1\00\C0\8E\07\00\C6\0F\00'x\02\04}\0C\CE\C7@\00P\C8\0F\00$x4\03B\00\00\03\0AP\00P\19x\07\FF\05\F0\020\16\01\00 \00@\12x\08\00\A0\00\01@\00\D4\E2\0F\04$x\03\07\D7\FF\FF\FF\04\020\00\11\060\00\22\00\16p\00P\0Cx\00\00\7F\D8\04\22@\F4\B0\00@x\03\03 \06\04\000\00\10\E4\10\00\11\02\91\03\22\06\02\90\00`\0Cx\00\03\13\050\00\13\F20\00C\02\02\04\00p\01\96\D8\0F\00G\19\00\00\B0\030\01\95$x\0A\06\00E\01\00\FFP\00%\06\07P\00\10\E4\A0\00t\05\07\E0\22\0A\00\0A \00Dr\07\FF\FFP\00\010\00 \09\04\90\00\14\05 \00\08P\01\10\C4 \00 \04\08\10\01\16\09`\00$\06@@\00\11\CA\B0\00 \06m\E0\00!B\FC\D0\01@$\E8\08\040\00\14\070\005\E4\09\FF\C0\00\93\C8\0F\00%\E6\08\08\00ZP\00p\CA\0F\00\81\E9\0C\08 \00\C0\00\19\1E\0C\00\A2\00\00\10x\0A\06@\000\FF\E0\FF@\00f\04\10x\0D\06\02\10\00\00p\00\12\0Ap\00#\F8\03\10\00\12\0D\10\00\11\F6\10\001\1Cx\00\01\00\F1\04p\F0\F0\03\00\D2\0F\00\10\C8\0A\04\14\05\00\00\07\E0\FF\A0\01:$\C4\0B\90\000\C6\0A\0A\90\00\14\0B\90\005\C9\0E\0A\90\00\94\E2\02\00\10\B8\08\04(\0A@\00G\1F\00$\B4\D0\000\E2\0F\00\A0\00\14\03\A0\00W\C6\0F\00%\B6\E0\00f\CC\0F\00\81\B9\08\E0\00q\22\0F\00\0B\E2\00\0CF\06\A4\80\FA\03\00\C8O\00\1C\E8\00\B0\001\02\00\E4 \00\11\05 \00!\C0\FAP\01A\08\82\0C\0CP\02:\00\80\06\F0\00\02 \00g\10\D8\0D\04<\0F\E0\00'\D4\0A\E0\00`/\00%\D6\0A\0D\90\00\14\0A\90\00&\D9\0A\E0\00a\A2\0E\00$\E2\05\10\02\13\0C\D0\00Q\0B\C2\00\0E\0Eo\00i\FC\03\00\E4\8F\08P\01\00`\01\14\C8\10\00\11p\10\00A\0B\C2\00\050\00#\C0\FC\B0\00\22\0E\0E\B0\00\11\00\B0\01#\0B\B2q\06\00P\00R\C6\0F\01$\C2p\00\13\0Ep\00\0D`\00\1B\B8`\001\B2\00\05@\00\12\C0\F0\01c\10x\07\07P\14p\01\10\D6p\00#\08\08p\00\10\06\C0\01\22$\B2`\00\1F\08`\00\04W\10x\06\06\04P\02A\0B\D2\00\0A\D6\08\10\80`\00aO\08\0B\D2\00\05\10\00\13\C0P\02$\D8\00\90\00\13\02\80\01\10\07p\03BpR\F8\03\80\00\22\0A\0A\80\00!\80\05\80\00\12\D2\80\00\13\0A\80\00`GI\00\00\D0\FCq\04\11\83\B0\03\14A0\05\03\C0\03A\88s\00\02@\00\00\9E\07f\E8\0F\00\1D{\00\01\00\93\EC\0F\00\84\A9\07\02\00\10 \00*\22\0E\D0\00\11\C4\90\00\01\90\041pD\F2 \05T\84\A9\04\02\00P\00qb\0E\00\0B\A2\00\07\CE\09\10\80\D0\00u\1F\08\0B\A2\00\04\07P\01T/\00\1C\A8\00P\00\13\01\E0\00 \00?\10\00#@\F6\E0\00!\07\070\02\00`\01\1B\E4P\01\8F\C6\0F\00\88\A3\00\02\07\C0\00\09!\B9\05O\09\22\00\08 \06+\84\B9\A0\001\B2\00\05\00\01`\80\F4\03\00\C4\1F\10\00(\04\05\A0\00\06\10\02\12\01`\027\05\05\04\F0\01/\88\B3@\01\0B9M\19\00p\019$t\04\C0\03!\84y\CB\08\02@\00\C3\E6\0F\00%v\04\03\00\\\00\00\04\00\05f\84y\03\02\00\04\C0\00E\81y\06\04@\03bb\03\00\0Br\00q\06\22\80\F0`\011r\00\00\10\00#\C0\F2@\01\02\1F\00@\00\00\80\04\90\031\0Br\000\040\00@\F0\E0\05(\0EF\10\02b\E6\0F\00\08r\07 \00\00\01\00p\CC\0F\00\A9s\07\04h\09\C0\07\E1\1E\00\00\A4\0E\00\0Cr\00\07\10\00 pR@\00QO\00$r\06p\02\14\07 \06V\09\00\00\90\FFp\02\1BMp\02TGy\00\00\F0 \00f\C0\0F\00\18y\00\01\00\0F\10\00\90\0F\01\00-#\01\00\A0\01\0B\01\00\22@\00\01\00=\9E\01\000\00\08\01\00\1F\0B@\00\04\13\DE)\00?\09\02\00@\00\0A\13\13<\0B\0C\01\00\13\E8U\00\03\8F\03\01$\00\13\05w\02\00\01\00\22\18\00\01\00.q\01T\00\00\01\00\11\90\95\02O\00\00p\00\80\00\0B\1F)'\00\03\02m\00\00v\0C\01\06\00\06\E4\00*\04\00\01\00\1Fc@\00\04\130@\00\17x@\00\1F\0A@\00\00!\8F\01D\01\0D@\00\13\A8@\00*\D8\00\01\00\1B\08\08\00?~\01\00\CE\0E\00\01\1E\07\01\01\00&\10\00\80\00\17\048\00\04\18\00\138@\01\0C\84\01\13\90@\00\17x1\01\0F\C0\00\01\132T\01\15\06R\00\0A\10\0F&\80\08\80\00j\06\00\00\11\80\00\01\00\13\97\94\00+\03\00\01\00\03\B0\13/\00\04\80\00\0B\14\06\AB\01/\14\00\01\00\02\1B\A8\08\00\17\08\08\02\17\05\E8\00\0C\01\009p\0A\00\08\00\088\00\18\06\A0\00\0F\01\00\05\03\A9\00\80\08\00\00\00\00\00\00\00\00\00\00\00\00\00\00"
@ral_kernel_launch___gpu___pvoid_ppvoid_i64_pvoid_i64_i64_i64_i64_i64_i64_i32_pvoid_i32_ppvoid___void = internal constant [101 x i8] c"ral_kernel_launch___gpu___pvoid_ppvoid_i64_pvoid_i64_i64_i64_i64_i64_i64_i32_pvoid_i32_ppvoid___void\00"
@main_kernel_main_kColReduction_reduce__3_1_0___8w32h_kernel_name = internal constant [41 x i8] c"main_kColReduction_reduce__3_1_0___8w32h\00"
@main_kernel_blob_gpu.binary = internal constant [1096 x i8] c"P\EDU\BA\01\00\10\008\04\00\00\00\00\00\00\02\00\01\01@\00\00\00\F8\03\00\00\00\00\00\00\F3\03\00\00\00\00\00\00\07\00\01\00P\00\00\00\00\00\00\00\00\00\00\00\11 \00\00\00\00\00\00\00\00\00\00\00\00\00\00\E8\0B\00\00\00\00\00\00\A2\7FELF\02\01\013\07\00\01\00f\02\00\BE\00q\00\01\00!@\0B\07\001\00\80\08\07\00\F5\0E\00P\05P\00@\008\00\03\00@\00\0B\00\01\00\00.shstrtab\00.\08\00'ym\08\00\F0\1B_shndx\00.nv.info\00.text.main_kColReduction_r\0A\00\F5\01e__3_1_0___8w32h8\00\0F2\00\1Boshared4\00\1B\9Fconstant07\00\18\FA\01debug_frame\00.rel\11\00!nv\14\00\11aC\00\0F+\01 \0F\88\00\15\0FT\01\BAo_param[\01\1C\0F\01\00\06\8C[\00\00\00\03\00\0A\00\01\00\11\F0\18\00,\09\00\01\00 .\01\18\00,\04\00\01\00\11L\18\00,\07\00\01\00f2\00\00\00\12\10`\00!\80\01\07\00v\00\FF\FF\FF\FF(\00\0C\00\00\01\00@\03\00\04|\08\00\D1\0F\0C\81\80\80(\00\08\FF\81\80(\08\0B\00\06(\00/0\00\01\00\03\13pX\00\10\04\9B\00R\04\14\00\00\00E\00\01\0B\00\00\13\00p/\08\00\05\00\00\00\A7\03\22\04#\0C\00\00\01\00'\04\12\0C\00\17\11\0C\00!7\04\E8\03\F1\08\015\00\00\04\0A\08\00\02\00\00\00`\01\10\00\03\19\10\00\04\17\0C$\00\10\01N\00%\F0!\10\00\01\01\00\F2\02\F0\11\00\03\1B\FF\00\04\1C\08\00P\00\00\00\B0\00\01\00#K\00\01\00s\02\02\08\10\0A/\22\8B\00\00\07\00\03\08\00\13\10\08\00\13\18\08\00\13 \08\00\13(\08\00\130\08\00\228\08|\00\13\00\08\00\13\08\08\00\13\10\08\00\13\18\08\00\13 \08\00\13(\08\00\130\08\00\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\A2\00\00\00\14,\00\00\00H\00\01\00\00 \01/\05\00\01\00\FF\C0A\02z\01\00\1F\04\B1\0F\00\00\00\C4\0F\00\19y\02\00\01\00\10%\9B\02Q\0E\00\19y\03\0F\00\F5\1A\00!\00\00\00$\0E\00$z\02\02\00X\00\00\03\02\8E\07\00\CA\1F\00\0Cx\00\02\13\05\00\00p@\F0\03\00\DA\0F\00MS\04\A0\80\03\00\EA\0F\005t\03\FF\B3\03\10\FF\C0\03P\E2\0F\00\02xF\02B\80\FF\00\0F\10\00r\B9z\04\00\00F\00\84\00\90\D0\0F\00%v\02\02\00Zl\04\00`\00`\0F\00\86y\00\022\00@\04\19\10\0C0\009My\00`\00PGy\00\00\F09\04\A6\FF\83\03\00\C0\0F\00\18y\00\01\00\0F\10\00\90\0F\01\00-\00W\01.\03\00\01\00\22@\00\01\00=+\01\000\00\08\01\00\1F\0B@\00\04\13k)\00\1F[@\00\0C\13\13\1C\04\0C\01\00\13\C8\15\00&\90\00@\04#\04\00\95\04\00\06\05\12\00\01\00\1F\FET\00\00\00\01\00\13X\95\00/p\00\80\00\0B\1F)'\00\03#\00\C8@\00\04P\06\04\E4\00*\04\00\01\00\1Fa@\00\04\13\F81\00&L\00@\00\1F\0A@\00\00!\1C\01D\01\0D@\00\13H)\00*\D8\00\01\00\1B\08\08\00?\0B\01\00\86\07\00Q\00\00 \05\00\01\00&\10\00\80\00\17\048\00\04\18\00\13\C7\14\01\0C\84\01*0\058\07\1F\00\C0\00\04\132@\00+\06\00\01\00\1A\07\D0\07\12\03\00\06:\08\80\00\01\00\13\06\18\06\04(\0B\0C\01\00*\A8\00\08\00\04\F8\00\13\018\00\04\A8\00\0C\01\009P\03\00\08\00\088\00/\06\00\01\00\17\80\08\00\00\00\00\00\00\00\00\00\00\00\00"
@alloc___gpu___pvoid_i64___pvoid = internal constant [32 x i8] c"alloc___gpu___pvoid_i64___pvoid\00"
@ral_recv_input___cpu___pvoid_i64___m3df32 = internal constant [42 x i8] c"ral_recv_input___cpu___pvoid_i64___m3df32\00"

define void @disc_ral_call(ptr nocapture readonly %0, ptr %1, ptr %2) local_unnamed_addr {
entry:
  %3 = load ptr, ptr %0, align 8
  %4 = getelementptr ptr, ptr %0, i64 1
  %5 = load ptr, ptr %4, align 8
  %6 = load ptr, ptr %2, align 8
  store ptr %3, ptr %6, align 8
  tail call void %5(ptr %3, ptr %1, ptr nonnull %2)
  ret void
}

define void @main(ptr %0) local_unnamed_addr {
  %2 = alloca %0, align 8
  %3 = alloca [3 x ptr], align 8
  store ptr %2, ptr %3, align 8
  %4 = getelementptr inbounds %0, ptr %2, i64 0, i32 1
  store i64 0, ptr %4, align 8
  %5 = getelementptr inbounds ptr, ptr %3, i64 1
  store ptr %4, ptr %5, align 8
  %6 = getelementptr inbounds %0, ptr %2, i64 0, i32 2
  %7 = getelementptr inbounds ptr, ptr %3, i64 2
  store ptr %6, ptr %7, align 8
  %8 = load ptr, ptr %0, align 8
  %9 = getelementptr ptr, ptr %0, i64 1
  %10 = load ptr, ptr %9, align 8
  store ptr %8, ptr %2, align 8
  call void %10(ptr %8, ptr nonnull @ral_recv_input___cpu___pvoid_i64___m3df32, ptr nonnull %3)
  %.fca.1.gep4 = getelementptr inbounds %0, ptr %2, i64 0, i32 2, i32 1
  %.fca.1.load5 = load ptr, ptr %.fca.1.gep4, align 8
  %11 = alloca %.1, align 8
  %12 = alloca [3 x ptr], align 8
  store ptr %11, ptr %12, align 8
  %13 = getelementptr inbounds %.1, ptr %11, i64 0, i32 1
  store i64 5200, ptr %13, align 8
  %14 = getelementptr inbounds ptr, ptr %12, i64 1
  store ptr %13, ptr %14, align 8
  %15 = getelementptr inbounds %.1, ptr %11, i64 0, i32 2
  %16 = getelementptr inbounds ptr, ptr %12, i64 2
  store ptr %15, ptr %16, align 8
  %17 = load ptr, ptr %0, align 8
  %18 = load ptr, ptr %9, align 8
  store ptr %17, ptr %11, align 8
  call void %18(ptr %17, ptr nonnull @alloc___gpu___pvoid_i64___pvoid, ptr nonnull %12)
  %19 = load ptr, ptr %15, align 8
  %20 = alloca ptr, align 8
  store ptr @main_kernel_blob_gpu.binary, ptr %20, align 8
  %21 = alloca %.2, align 8
  %22 = alloca [2 x ptr], align 8
  store i64 256, ptr %21, align 8
  store ptr %21, ptr %22, align 8
  %23 = getelementptr inbounds %.2, ptr %21, i64 0, i32 1
  store ptr %19, ptr %23, align 8
  %24 = getelementptr inbounds ptr, ptr %22, i64 1
  store ptr %23, ptr %24, align 8
  %25 = alloca %.3, align 8
  %26 = alloca [14 x ptr], align 8
  store ptr %25, ptr %26, align 8
  %27 = getelementptr inbounds %.3, ptr %25, i64 0, i32 1
  store ptr %20, ptr %27, align 8
  %28 = getelementptr inbounds ptr, ptr %26, i64 1
  store ptr %27, ptr %28, align 8
  %29 = getelementptr inbounds %.3, ptr %25, i64 0, i32 2
  store i64 1, ptr %29, align 8
  %30 = getelementptr inbounds ptr, ptr %26, i64 2
  store ptr %29, ptr %30, align 8
  %31 = getelementptr inbounds %.3, ptr %25, i64 0, i32 3
  store ptr @main_kernel_main_kColReduction_reduce__3_1_0___8w32h_kernel_name, ptr %31, align 8
  %32 = getelementptr inbounds ptr, ptr %26, i64 3
  store ptr %31, ptr %32, align 8
  %33 = getelementptr inbounds %.3, ptr %25, i64 0, i32 4
  store i64 6, ptr %33, align 8
  %34 = getelementptr inbounds ptr, ptr %26, i64 4
  store ptr %33, ptr %34, align 8
  %35 = getelementptr inbounds %.3, ptr %25, i64 0, i32 5
  store i64 1, ptr %35, align 8
  %36 = getelementptr inbounds ptr, ptr %26, i64 5
  store ptr %35, ptr %36, align 8
  %37 = getelementptr inbounds %.3, ptr %25, i64 0, i32 6
  store i64 1, ptr %37, align 8
  %38 = getelementptr inbounds ptr, ptr %26, i64 6
  store ptr %37, ptr %38, align 8
  %39 = getelementptr inbounds %.3, ptr %25, i64 0, i32 7
  store i64 256, ptr %39, align 8
  %40 = getelementptr inbounds ptr, ptr %26, i64 7
  store ptr %39, ptr %40, align 8
  %41 = getelementptr inbounds %.3, ptr %25, i64 0, i32 8
  store i64 1, ptr %41, align 8
  %42 = getelementptr inbounds ptr, ptr %26, i64 8
  store ptr %41, ptr %42, align 8
  %43 = getelementptr inbounds %.3, ptr %25, i64 0, i32 9
  store i64 1, ptr %43, align 8
  %44 = getelementptr inbounds ptr, ptr %26, i64 9
  store ptr %43, ptr %44, align 8
  %45 = getelementptr inbounds %.3, ptr %25, i64 0, i32 10
  store i32 0, ptr %45, align 8
  %46 = getelementptr inbounds ptr, ptr %26, i64 10
  store ptr %45, ptr %46, align 8
  %47 = getelementptr inbounds %.3, ptr %25, i64 0, i32 11
  store ptr null, ptr %47, align 8
  %48 = getelementptr inbounds ptr, ptr %26, i64 11
  store ptr %47, ptr %48, align 8
  %49 = getelementptr inbounds %.3, ptr %25, i64 0, i32 12
  store i32 2, ptr %49, align 8
  %50 = getelementptr inbounds ptr, ptr %26, i64 12
  store ptr %49, ptr %50, align 8
  %51 = getelementptr inbounds %.3, ptr %25, i64 0, i32 13
  store ptr %22, ptr %51, align 8
  %52 = getelementptr inbounds ptr, ptr %26, i64 13
  store ptr %51, ptr %52, align 8
  %53 = load ptr, ptr %0, align 8
  %54 = load ptr, ptr %9, align 8
  store ptr %53, ptr %25, align 8
  call void %54(ptr %53, ptr nonnull @ral_kernel_launch___gpu___pvoid_ppvoid_i64_pvoid_i64_i64_i64_i64_i64_i64_i32_pvoid_i32_ppvoid___void, ptr nonnull %26)
  %55 = alloca ptr, align 8
  store ptr @main_kernel_0_blob_gpu.binary, ptr %55, align 8
  %56 = alloca %.4, align 8
  %57 = alloca [4 x ptr], align 8
  store i64 256, ptr %56, align 8
  store ptr %56, ptr %57, align 8
  %58 = getelementptr inbounds %.4, ptr %56, i64 0, i32 1
  store i64 10496, ptr %58, align 8
  %59 = getelementptr inbounds ptr, ptr %57, i64 1
  store ptr %58, ptr %59, align 8
  %60 = getelementptr inbounds %.4, ptr %56, i64 0, i32 2
  store ptr %.fca.1.load5, ptr %60, align 8
  %61 = getelementptr inbounds ptr, ptr %57, i64 2
  store ptr %60, ptr %61, align 8
  %62 = getelementptr inbounds %.4, ptr %56, i64 0, i32 3
  store ptr %19, ptr %62, align 8
  %63 = getelementptr inbounds ptr, ptr %57, i64 3
  store ptr %62, ptr %63, align 8
  %64 = alloca %.5, align 8
  %65 = alloca [14 x ptr], align 8
  store ptr %64, ptr %65, align 8
  %66 = getelementptr inbounds %.5, ptr %64, i64 0, i32 1
  store ptr %55, ptr %66, align 8
  %67 = getelementptr inbounds ptr, ptr %65, i64 1
  store ptr %66, ptr %67, align 8
  %68 = getelementptr inbounds %.5, ptr %64, i64 0, i32 2
  store i64 1, ptr %68, align 8
  %69 = getelementptr inbounds ptr, ptr %65, i64 2
  store ptr %68, ptr %69, align 8
  %70 = getelementptr inbounds %.5, ptr %64, i64 0, i32 3
  store ptr @main_kernel_0_main_kColReduction_reduce__3_1_0___8w32h_1_kernel_name, ptr %70, align 8
  %71 = getelementptr inbounds ptr, ptr %65, i64 3
  store ptr %70, ptr %71, align 8
  %72 = getelementptr inbounds %.5, ptr %64, i64 0, i32 4
  store i64 41, ptr %72, align 8
  %73 = getelementptr inbounds ptr, ptr %65, i64 4
  store ptr %72, ptr %73, align 8
  %74 = getelementptr inbounds %.5, ptr %64, i64 0, i32 5
  store i64 1, ptr %74, align 8
  %75 = getelementptr inbounds ptr, ptr %65, i64 5
  store ptr %74, ptr %75, align 8
  %76 = getelementptr inbounds %.5, ptr %64, i64 0, i32 6
  store i64 1, ptr %76, align 8
  %77 = getelementptr inbounds ptr, ptr %65, i64 6
  store ptr %76, ptr %77, align 8
  %78 = getelementptr inbounds %.5, ptr %64, i64 0, i32 7
  store i64 256, ptr %78, align 8
  %79 = getelementptr inbounds ptr, ptr %65, i64 7
  store ptr %78, ptr %79, align 8
  %80 = getelementptr inbounds %.5, ptr %64, i64 0, i32 8
  store i64 1, ptr %80, align 8
  %81 = getelementptr inbounds ptr, ptr %65, i64 8
  store ptr %80, ptr %81, align 8
  %82 = getelementptr inbounds %.5, ptr %64, i64 0, i32 9
  store i64 1, ptr %82, align 8
  %83 = getelementptr inbounds ptr, ptr %65, i64 9
  store ptr %82, ptr %83, align 8
  %84 = getelementptr inbounds %.5, ptr %64, i64 0, i32 10
  store i32 0, ptr %84, align 8
  %85 = getelementptr inbounds ptr, ptr %65, i64 10
  store ptr %84, ptr %85, align 8
  %86 = getelementptr inbounds %.5, ptr %64, i64 0, i32 11
  store ptr null, ptr %86, align 8
  %87 = getelementptr inbounds ptr, ptr %65, i64 11
  store ptr %86, ptr %87, align 8
  %88 = getelementptr inbounds %.5, ptr %64, i64 0, i32 12
  store i32 4, ptr %88, align 8
  %89 = getelementptr inbounds ptr, ptr %65, i64 12
  store ptr %88, ptr %89, align 8
  %90 = getelementptr inbounds %.5, ptr %64, i64 0, i32 13
  store ptr %57, ptr %90, align 8
  %91 = getelementptr inbounds ptr, ptr %65, i64 13
  store ptr %90, ptr %91, align 8
  %92 = load ptr, ptr %0, align 8
  %93 = load ptr, ptr %9, align 8
  store ptr %92, ptr %64, align 8
  call void %93(ptr %92, ptr nonnull @ral_kernel_launch___gpu___pvoid_ppvoid_i64_pvoid_i64_i64_i64_i64_i64_i64_i32_pvoid_i32_ppvoid___void, ptr nonnull %65)
  %94 = alloca [16 x i64], align 8
  store i64 100, ptr %94, align 8
  %95 = getelementptr inbounds i64, ptr %94, i64 1
  store i64 13, ptr %95, align 8
  %96 = alloca %.6, align 8
  %97 = alloca [13 x ptr], align 8
  store ptr %96, ptr %97, align 8
  %98 = getelementptr inbounds %.6, ptr %96, i64 0, i32 1
  store ptr null, ptr %98, align 8
  %99 = getelementptr inbounds ptr, ptr %97, i64 1
  store ptr %98, ptr %99, align 8
  %100 = getelementptr inbounds %.6, ptr %96, i64 0, i32 2
  store ptr %19, ptr %100, align 8
  %101 = getelementptr inbounds ptr, ptr %97, i64 2
  store ptr %100, ptr %101, align 8
  %102 = getelementptr inbounds %.6, ptr %96, i64 0, i32 3
  store ptr %19, ptr %102, align 8
  %103 = getelementptr inbounds ptr, ptr %97, i64 3
  store ptr %102, ptr %103, align 8
  %104 = getelementptr inbounds %.6, ptr %96, i64 0, i32 4
  store i64 0, ptr %104, align 8
  %105 = getelementptr inbounds ptr, ptr %97, i64 4
  store ptr %104, ptr %105, align 8
  %106 = getelementptr inbounds %.6, ptr %96, i64 0, i32 5
  store i64 1300, ptr %106, align 8
  %107 = getelementptr inbounds ptr, ptr %97, i64 5
  store ptr %106, ptr %107, align 8
  %108 = getelementptr inbounds %.6, ptr %96, i64 0, i32 6
  store i64 1, ptr %108, align 8
  %109 = getelementptr inbounds ptr, ptr %97, i64 6
  store ptr %108, ptr %109, align 8
  %110 = getelementptr inbounds %.6, ptr %96, i64 0, i32 7
  store ptr %94, ptr %110, align 8
  %111 = getelementptr inbounds ptr, ptr %97, i64 7
  store ptr %110, ptr %111, align 8
  %112 = getelementptr inbounds %.6, ptr %96, i64 0, i32 8
  store ptr %94, ptr %112, align 8
  %113 = getelementptr inbounds ptr, ptr %97, i64 8
  store ptr %112, ptr %113, align 8
  %114 = getelementptr inbounds %.6, ptr %96, i64 0, i32 9
  store i64 0, ptr %114, align 8
  %115 = getelementptr inbounds ptr, ptr %97, i64 9
  store ptr %114, ptr %115, align 8
  %116 = getelementptr inbounds %.6, ptr %96, i64 0, i32 10
  store i64 2, ptr %116, align 8
  %117 = getelementptr inbounds ptr, ptr %97, i64 10
  store ptr %116, ptr %117, align 8
  %118 = getelementptr inbounds %.6, ptr %96, i64 0, i32 11
  store i64 1, ptr %118, align 8
  %119 = getelementptr inbounds ptr, ptr %97, i64 11
  store ptr %118, ptr %119, align 8
  %120 = getelementptr inbounds %.6, ptr %96, i64 0, i32 12
  %121 = getelementptr inbounds ptr, ptr %97, i64 12
  store ptr %120, ptr %121, align 8
  %122 = load ptr, ptr %0, align 8
  %123 = load ptr, ptr %9, align 8
  store ptr %122, ptr %96, align 8
  call void %123(ptr %122, ptr nonnull @inc_ref___gpu___pvoid_pvoid_m1df32_m1di64___m2df32, ptr nonnull %97)
  %.fca.0.load = load ptr, ptr %120, align 8
  %.fca.1.gep = getelementptr inbounds %.6, ptr %96, i64 0, i32 12, i32 1
  %.fca.1.load = load ptr, ptr %.fca.1.gep, align 8
  %124 = alloca %.7, align 8
  %125 = alloca [2 x ptr], align 8
  store ptr %124, ptr %125, align 8
  %126 = getelementptr inbounds %.7, ptr %124, i64 0, i32 1
  store ptr %19, ptr %126, align 8
  %127 = getelementptr inbounds ptr, ptr %125, i64 1
  store ptr %126, ptr %127, align 8
  %128 = load ptr, ptr %0, align 8
  %129 = load ptr, ptr %9, align 8
  store ptr %128, ptr %124, align 8
  call void %129(ptr %128, ptr nonnull @dealloc___gpu___pvoid_pvoid___void, ptr nonnull %125)
  %130 = alloca %.8, align 8
  %131 = alloca [9 x ptr], align 8
  store ptr %130, ptr %131, align 8
  %132 = getelementptr inbounds %.8, ptr %130, i64 0, i32 1
  store i64 0, ptr %132, align 8
  %133 = getelementptr inbounds ptr, ptr %131, i64 1
  store ptr %132, ptr %133, align 8
  %134 = getelementptr inbounds %.8, ptr %130, i64 0, i32 2
  store ptr %.fca.0.load, ptr %134, align 8
  %135 = getelementptr inbounds ptr, ptr %131, i64 2
  store ptr %134, ptr %135, align 8
  %136 = getelementptr inbounds %.8, ptr %130, i64 0, i32 3
  store ptr %.fca.1.load, ptr %136, align 8
  %137 = getelementptr inbounds ptr, ptr %131, i64 3
  store ptr %136, ptr %137, align 8
  %138 = getelementptr inbounds %.8, ptr %130, i64 0, i32 4
  store i64 0, ptr %138, align 8
  %139 = getelementptr inbounds ptr, ptr %131, i64 4
  store ptr %138, ptr %139, align 8
  %140 = getelementptr inbounds %.8, ptr %130, i64 0, i32 5
  store i64 100, ptr %140, align 8
  %141 = getelementptr inbounds ptr, ptr %131, i64 5
  store ptr %140, ptr %141, align 8
  %142 = getelementptr inbounds %.8, ptr %130, i64 0, i32 6
  store i64 13, ptr %142, align 8
  %143 = getelementptr inbounds ptr, ptr %131, i64 6
  store ptr %142, ptr %143, align 8
  %144 = getelementptr inbounds %.8, ptr %130, i64 0, i32 7
  store i64 13, ptr %144, align 8
  %145 = getelementptr inbounds ptr, ptr %131, i64 7
  store ptr %144, ptr %145, align 8
  %146 = getelementptr inbounds %.8, ptr %130, i64 0, i32 8
  store i64 1, ptr %146, align 8
  %147 = getelementptr inbounds ptr, ptr %131, i64 8
  store ptr %146, ptr %147, align 8
  %148 = load ptr, ptr %0, align 8
  %149 = load ptr, ptr %9, align 8
  store ptr %148, ptr %130, align 8
  call void %149(ptr %148, ptr nonnull @ral_send_output___cpu___pvoid_i64_m2df32___void, ptr nonnull %131)
  ret void
}

[DISC] LowerLLVMToBinary takes: 1.413400e-02 s.
object file to shared library command: gcc --shared -o /root/.cache/bazel/_bazel_root/54ece412abe75fa85ab728a2c061e33c/execroot/org_disc_compiler/_tmp/b247c1654ecfc81b23b002417f617beaColReduceStaticShape3DF32_0.so /root/.cache/bazel/_bazel_root/54ece412abe75fa85ab728a2c061e33c/execroot/org_disc_compiler/_tmp/b247c1654ecfc81b23b002417f617beaColReduceStaticShape3DF32_0.so.o
save shared lib file to : /root/.cache/bazel/_bazel_root/54ece412abe75fa85ab728a2c061e33c/execroot/org_disc_compiler/_tmp/b247c1654ecfc81b23b002417f617beaColReduceStaticShape3DF32_0.so
[DISC] BinaryStrToSharedLibrary takes: 9.410000e-03 s.
[DISC] LowerHLOToSharedLibrary takes: 1.004949e+00 s.

============ END ============

2023-06-28 15:08:17.673046: I mlir/disc/tests/mlir_test.cc:275] ret: 0

2023-06-28 15:08:17.673098: I mlir/disc/tests/mlir_test.cc:241] run compiled program

2023-06-28 15:08:18.429139: I external/org_tensorflow/tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-06-28 15:08:18.431897: I mlir/disc/tests/mlir_test.cc:932] --- MLIR Execution uses: 2.848 ms
2023-06-28 15:08:18.431980: I mlir/disc/tests/mlir_test.cc:183] out buffer = 0x7f74e328bc00
2023-06-28 15:08:18.431988: I mlir/disc/tests/mlir_test.cc:184] out shape:
2023-06-28 15:08:18.431991: I mlir/disc/tests/mlir_test.cc:186]   dim #0: 100
2023-06-28 15:08:18.431993: I mlir/disc/tests/mlir_test.cc:186]   dim #1: 13
2023-06-28 15:08:18.432040: I mlir/disc/tests/mlir_test.cc:244] run golden tf

2023-06-28 15:08:18.432048: I mlir/disc/tests/mlir_test.cc:257] program_path: external/org_tensorflow/tensorflow/compiler/mlir/tf-mlir-translate

2023-06-28 15:08:18.485964: I mlir/disc/tests/mlir_test.cc:269] Executed: external/org_tensorflow/tensorflow/compiler/mlir/tf-mlir-translate -mlir-to-graphdef /root/.cache/bazel/_bazel_root/54ece412abe75fa85ab728a2c061e33c/execroot/org_disc_compiler/_tmp/b247c1654ecfc81b23b002417f617bea/tempfile-adf59ec6ac82-139e1c75-42725-5ff31f40a0ef9 -o /root/.cache/bazel/_bazel_root/54ece412abe75fa85ab728a2c061e33c/execroot/org_disc_compiler/_tmp/b247c1654ecfc81b23b002417f617beaColReduceStaticShape3DF32_0.pbtxt 
2023-06-28 15:08:18.485974: I mlir/disc/tests/mlir_test.cc:270] external/org_tensorflow/tensorflow/compiler/mlir/tf-mlir-translate: 0
2023-06-28 15:08:18.485977: I mlir/disc/tests/mlir_test.cc:271] -- stdout:

============ END ============

2023-06-28 15:08:18.485980: I mlir/disc/tests/mlir_test.cc:273] -- stderr:

============ END ============

2023-06-28 15:08:18.485983: I mlir/disc/tests/mlir_test.cc:275] ret: 0

2023-06-28 15:08:18.485990: I mlir/disc/tests/mlir_test.cc:391] graphdef_path: /root/.cache/bazel/_bazel_root/54ece412abe75fa85ab728a2c061e33c/execroot/org_disc_compiler/_tmp/b247c1654ecfc81b23b002417f617beaColReduceStaticShape3DF32_0.pbtxt
2023-06-28 15:08:18.487254: I external/org_tensorflow/tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: SSE3 SSE4.1 SSE4.2 AVX AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-06-28 15:08:19.303111: I external/org_tensorflow/tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-06-28 15:08:19.305564: I external/org_tensorflow/tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-06-28 15:08:19.307946: I external/org_tensorflow/tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-06-28 15:08:19.310537: I external/org_tensorflow/tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-06-28 15:08:19.312868: I external/org_tensorflow/tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-06-28 15:08:19.315196: I external/org_tensorflow/tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-06-28 15:08:19.317534: I external/org_tensorflow/tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-06-28 15:08:19.319860: I external/org_tensorflow/tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-06-28 15:08:19.424523: I external/org_tensorflow/tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-06-28 15:08:19.426917: I external/org_tensorflow/tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-06-28 15:08:19.429284: I external/org_tensorflow/tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-06-28 15:08:19.431616: I external/org_tensorflow/tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-06-28 15:08:19.433949: I external/org_tensorflow/tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-06-28 15:08:19.436266: I external/org_tensorflow/tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-06-28 15:08:19.438604: I external/org_tensorflow/tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-06-28 15:08:19.440923: I external/org_tensorflow/tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-06-28 15:08:19.443246: I external/org_tensorflow/tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-06-28 15:08:19.445577: I external/org_tensorflow/tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-06-28 15:08:19.447903: I external/org_tensorflow/tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-06-28 15:08:19.450224: I external/org_tensorflow/tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-06-28 15:08:19.452544: I external/org_tensorflow/tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-06-28 15:08:19.454864: I external/org_tensorflow/tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-06-28 15:08:19.457172: I external/org_tensorflow/tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-06-28 15:08:19.459493: I external/org_tensorflow/tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-06-28 15:08:21.991524: I external/org_tensorflow/tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-06-28 15:08:21.994044: I external/org_tensorflow/tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-06-28 15:08:21.996479: I external/org_tensorflow/tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-06-28 15:08:21.998853: I external/org_tensorflow/tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-06-28 15:08:22.001206: I external/org_tensorflow/tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-06-28 15:08:22.003570: I external/org_tensorflow/tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-06-28 15:08:22.005933: I external/org_tensorflow/tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-06-28 15:08:22.008286: I external/org_tensorflow/tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-06-28 15:08:22.010683: I external/org_tensorflow/tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-06-28 15:08:22.013023: I external/org_tensorflow/tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-06-28 15:08:22.015369: I external/org_tensorflow/tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-06-28 15:08:22.017696: I external/org_tensorflow/tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-06-28 15:08:22.020017: I external/org_tensorflow/tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-06-28 15:08:22.022359: I external/org_tensorflow/tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-06-28 15:08:22.024681: I external/org_tensorflow/tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-06-28 15:08:22.027008: I external/org_tensorflow/tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-06-28 15:08:22.029336: I external/org_tensorflow/tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-06-28 15:08:22.031665: I external/org_tensorflow/tensorflow/core/common_runtime/gpu/gpu_device.cc:1635] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 79147 MB memory:  -> device: 0, name: NVIDIA A100-SXM4-80GB, pci bus id: 0000:00:08.0, compute capability: 8.0
2023-06-28 15:08:22.033303: I external/org_tensorflow/tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-06-28 15:08:22.035609: I external/org_tensorflow/tensorflow/core/common_runtime/gpu/gpu_device.cc:1635] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 79149 MB memory:  -> device: 1, name: NVIDIA A100-SXM4-80GB, pci bus id: 0000:00:09.0, compute capability: 8.0
2023-06-28 15:08:22.036148: I external/org_tensorflow/tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-06-28 15:08:22.038432: I external/org_tensorflow/tensorflow/core/common_runtime/gpu/gpu_device.cc:1635] Created device /job:localhost/replica:0/task:0/device:GPU:2 with 79149 MB memory:  -> device: 2, name: NVIDIA A100-SXM4-80GB, pci bus id: 0000:00:0a.0, compute capability: 8.0
2023-06-28 15:08:22.038962: I external/org_tensorflow/tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-06-28 15:08:22.041229: I external/org_tensorflow/tensorflow/core/common_runtime/gpu/gpu_device.cc:1635] Created device /job:localhost/replica:0/task:0/device:GPU:3 with 79149 MB memory:  -> device: 3, name: NVIDIA A100-SXM4-80GB, pci bus id: 0000:00:0b.0, compute capability: 8.0
2023-06-28 15:08:22.044574: I external/org_tensorflow/tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-06-28 15:08:22.046876: I external/org_tensorflow/tensorflow/core/common_runtime/gpu/gpu_device.cc:1635] Created device /job:localhost/replica:0/task:0/device:GPU:4 with 79149 MB memory:  -> device: 4, name: NVIDIA A100-SXM4-80GB, pci bus id: 0000:00:0c.0, compute capability: 8.0
2023-06-28 15:08:22.047475: I external/org_tensorflow/tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-06-28 15:08:22.049761: I external/org_tensorflow/tensorflow/core/common_runtime/gpu/gpu_device.cc:1635] Created device /job:localhost/replica:0/task:0/device:GPU:5 with 79149 MB memory:  -> device: 5, name: NVIDIA A100-SXM4-80GB, pci bus id: 0000:00:0d.0, compute capability: 8.0
2023-06-28 15:08:22.050341: I external/org_tensorflow/tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-06-28 15:08:22.052609: I external/org_tensorflow/tensorflow/core/common_runtime/gpu/gpu_device.cc:1635] Created device /job:localhost/replica:0/task:0/device:GPU:6 with 79149 MB memory:  -> device: 6, name: NVIDIA A100-SXM4-80GB, pci bus id: 0000:00:0e.0, compute capability: 8.0
2023-06-28 15:08:22.053172: I external/org_tensorflow/tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-06-28 15:08:22.055459: I external/org_tensorflow/tensorflow/core/common_runtime/gpu/gpu_device.cc:1635] Created device /job:localhost/replica:0/task:0/device:GPU:7 with 79149 MB memory:  -> device: 7, name: NVIDIA A100-SXM4-80GB, pci bus id: 0000:00:0f.0, compute capability: 8.0
2023-06-28 15:08:22.096737: I mlir/disc/tests/mlir_test.cc:467] --- TF Execution uses: 22.557 ms
2023-06-28 15:08:22.096760: I mlir/disc/tests/mlir_test.cc:474] 	output shape #0: [100,13]
2023-06-28 15:08:22.097113: I mlir/disc/tests/mlir_test.cc:467] --- TF Execution uses: 0.344 ms
2023-06-28 15:08:22.097117: I mlir/disc/tests/mlir_test.cc:474] 	output shape #0: [100,13]
2023-06-28 15:08:22.097488: I mlir/disc/tests/mlir_test.cc:467] --- TF Execution uses: 0.366 ms
2023-06-28 15:08:22.097493: I mlir/disc/tests/mlir_test.cc:474] 	output shape #0: [100,13]
2023-06-28 15:08:22.097762: I mlir/disc/tests/mlir_test.cc:467] --- TF Execution uses: 0.265 ms
2023-06-28 15:08:22.097766: I mlir/disc/tests/mlir_test.cc:474] 	output shape #0: [100,13]
2023-06-28 15:08:22.097989: I mlir/disc/tests/mlir_test.cc:467] --- TF Execution uses: 0.218 ms
2023-06-28 15:08:22.097993: I mlir/disc/tests/mlir_test.cc:474] 	output shape #0: [100,13]
2023-06-28 15:08:22.098002: I mlir/disc/tests/mlir_test.cc:484] processing output 0
2023-06-28 15:08:22.100146: I ./mlir/disc/tests/mlir_feature_test.h:37] Unset env setting:
[       OK ] TFMaxOpTest.ColReduceStaticShape3DF32 (5768 ms)
[----------] 1 test from TFMaxOpTest (5768 ms total)

[----------] Global test environment tear-down
[==========] 1 test from 1 test suite ran. (5768 ms total)
[  PASSED  ] 1 test.
