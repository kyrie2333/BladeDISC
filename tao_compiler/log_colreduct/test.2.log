exec ${PAGER:-/usr/bin/less} "$0" || exit 1
Executing tests from //mlir/disc/tests/tensorflow_ops:max.cpp.test
-----------------------------------------------------------------------------
[==========] Running 1 test from 1 test suite.
[----------] Global test environment set-up.
[----------] 1 test from TFMaxOpTest
[ RUN      ] TFMaxOpTest.ColReduceFullyDynamicShape3DF32
2023-06-25 03:54:49.186013: I ./mlir/disc/tests/mlir_feature_test.h:29] Apply env setting:
2023-06-25 03:54:49.186077: I ./mlir/disc/tests/mlir_feature_test.h:31] 	DISC_MEM_INTENSIVE_OPT_EXPERIMENTAL = true
2023-06-25 03:54:49.186081: I ./mlir/disc/tests/mlir_feature_test.h:31] 	DISC_ENABLE_STITCH = true
2023-06-25 03:54:49.186084: I mlir/disc/tests/mlir_feature_test.cc:271] Testing for CUDA backend
2023-06-25 03:54:49.186144: I mlir/disc/tests/mlir_feature_test.cc:145] Original TF code: func.func @main(%arg0: tensor<?x?x?xf32>) -> tensor<?x?xf32> attributes {tf.entry_function = {inputs = "{{INPUTS}}", outputs = "{{OUTPUTS}}", input_placements="{{INPUT_PLACEMENTS}}", output_placements="{{OUTPUT_PLACEMENTS}}"}} {
  %graph = tf_executor.graph {
    %1:2 = tf_executor.island wraps "tf.Const"() {value = dense<[0]> : tensor<1xi32>} : () -> tensor<1xi32>
    %2:2 = tf_executor.island wraps "tf.Abs"(%arg0) : (tensor<?x?x?xf32>) -> tensor<?x?x?xf32>
    %3:2 = tf_executor.island wraps "tf.Max"(%2, %1) : (tensor<?x?x?xf32>, tensor<1xi32>) -> tensor<?x?xf32>
    tf_executor.fetch %3 : tensor<?x?xf32>
  }
  return %graph : tensor<?x?xf32>
}
2023-06-25 03:54:49.186158: I mlir/disc/tests/mlir_feature_test.cc:149] New TF code: func.func @main(%arg0: tensor<?x?x?xf32>) -> tensor<?x?xf32> attributes {tf.entry_function = {inputs = "input0", outputs = "output0", input_placements="gpu", output_placements="gpu"}} {
  %graph = tf_executor.graph {
    %1:2 = tf_executor.island wraps "tf.Const"() {value = dense<[0]> : tensor<1xi32>} : () -> tensor<1xi32>
    %2:2 = tf_executor.island wraps "tf.Abs"(%arg0) : (tensor<?x?x?xf32>) -> tensor<?x?x?xf32>
    %3:2 = tf_executor.island wraps "tf.Max"(%2, %1) : (tensor<?x?x?xf32>, tensor<1xi32>) -> tensor<?x?xf32>
    tf_executor.fetch %3 : tensor<?x?xf32>
  }
  return %graph : tensor<?x?xf32>
}
2023-06-25 03:54:49.186261: I mlir/disc/tests/mlir_test.cc:233] tf_opt_pat: external/org_tensorflow/tensorflow/compiler/mlir/tf-opt
2023-06-25 03:54:49.186266: I mlir/disc/tests/mlir_test.cc:234] mlir_file_path: /root/.cache/bazel/_bazel_root/54ece412abe75fa85ab728a2c061e33c/execroot/org_disc_compiler/_tmp/b247c1654ecfc81b23b002417f617bea/tempfile-adf59ec6ac82-7602f1e3-2553507-5feec320fcb82
2023-06-25 03:54:49.186269: I mlir/disc/tests/mlir_test.cc:235] tmp_dir: /root/.cache/bazel/_bazel_root/54ece412abe75fa85ab728a2c061e33c/execroot/org_disc_compiler/_tmp/b247c1654ecfc81b23b002417f617bea
2023-06-25 03:54:49.186272: I mlir/disc/tests/mlir_test.cc:236] test_name: ColReduceFullyDynamicShape3DF32_0
2023-06-25 03:54:49.186278: I mlir/disc/tests/mlir_test.cc:284] tf_opt_path: external/org_tensorflow/tensorflow/compiler/mlir/tf-opt

2023-06-25 03:54:49.186290: I mlir/disc/tests/mlir_test.cc:257] program_path: external/org_tensorflow/tensorflow/compiler/mlir/tf-opt

2023-06-25 03:54:49.248045: I mlir/disc/tests/mlir_test.cc:269] Executed: external/org_tensorflow/tensorflow/compiler/mlir/tf-opt --tf-standard-pipeline /root/.cache/bazel/_bazel_root/54ece412abe75fa85ab728a2c061e33c/execroot/org_disc_compiler/_tmp/b247c1654ecfc81b23b002417f617bea/tempfile-adf59ec6ac82-7602f1e3-2553507-5feec320fcb82 -o /root/.cache/bazel/_bazel_root/54ece412abe75fa85ab728a2c061e33c/execroot/org_disc_compiler/_tmp/b247c1654ecfc81b23b002417f617beaColReduceFullyDynamicShape3DF32_0_tf_dialect.mlir 
2023-06-25 03:54:49.248068: I mlir/disc/tests/mlir_test.cc:270] external/org_tensorflow/tensorflow/compiler/mlir/tf-opt: 0
2023-06-25 03:54:49.248072: I mlir/disc/tests/mlir_test.cc:271] -- stdout:

============ END ============

2023-06-25 03:54:49.248075: I mlir/disc/tests/mlir_test.cc:273] -- stderr:
2023-06-25 03:54:49.233918: I external/org_tensorflow/tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: SSE3 SSE4.1 SSE4.2 AVX AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.

============ END ============

2023-06-25 03:54:49.248084: I mlir/disc/tests/mlir_test.cc:275] ret: 0

2023-06-25 03:54:49.248099: I mlir/disc/tests/mlir_test.cc:257] program_path: mlir/disc/disc_compiler_main

2023-06-25 03:54:50.132109: I mlir/disc/tests/mlir_test.cc:269] Executed: mlir/disc/disc_compiler_main --mlir-print-elementsattrs-with-hex-if-larger -1 --mlir-elide-elementsattrs-if-larger 8 /root/.cache/bazel/_bazel_root/54ece412abe75fa85ab728a2c061e33c/execroot/org_disc_compiler/_tmp/b247c1654ecfc81b23b002417f617beaColReduceFullyDynamicShape3DF32_0_tf_dialect.mlir /root/.cache/bazel/_bazel_root/54ece412abe75fa85ab728a2c061e33c/execroot/org_disc_compiler/_tmp/b247c1654ecfc81b23b002417f617beaColReduceFullyDynamicShape3DF32_0.so 
2023-06-25 03:54:50.132139: I mlir/disc/tests/mlir_test.cc:270] mlir/disc/disc_compiler_main: 0
2023-06-25 03:54:50.132143: I mlir/disc/tests/mlir_test.cc:271] -- stdout:

============ END ============

2023-06-25 03:54:50.133884: I mlir/disc/tests/mlir_test.cc:273] -- stderr:
======== BEGIN Original Module =========
module {
  func.func @main(%arg0: tensor<?x?x?xf32>) -> tensor<?x?xf32> attributes {tf.entry_function = {input_placements = "gpu", inputs = "input0", output_placements = "gpu", outputs = "output0"}} {
    %cst = "tf.Const"() {value = dense<0> : tensor<1xi32>} : () -> tensor<1xi32>
    %0 = "tf.Abs"(%arg0) : (tensor<?x?x?xf32>) -> tensor<?x?x?xf32>
    %1 = "tf.Max"(%0, %cst) : (tensor<?x?x?xf32>, tensor<1xi32>) -> tensor<?x?xf32>
    return %1 : tensor<?x?xf32>
  }
}

======= END Original Module ==========
[DISC] Load Input IR takes: 2.018000e-03 s.
[[ INFO ]] Running TF2XLA
2023-06-25 03:54:49.297092: I external/org_tensorflow/tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: SSE3 SSE4.1 SSE4.2 AVX AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
// -----// IR Dump After SCCP (sccp) //----- //
module {
  func.func @main(%arg0: tensor<?x?x?xf32>) -> tensor<?x?xf32> attributes {tf.entry_function = {input_placements = "gpu", inputs = "input0", output_placements = "gpu", outputs = "output0"}} {
    %cst = "tf.Const"() {value = dense<0> : tensor<1xi32>} : () -> tensor<1xi32>
    %0 = "tf.Abs"(%arg0) : (tensor<?x?x?xf32>) -> tensor<?x?x?xf32>
    %1 = "tf.Max"(%0, %cst) : (tensor<?x?x?xf32>, tensor<1xi32>) -> tensor<?x?xf32>
    return %1 : tensor<?x?xf32>
  }
}


// -----// IR Dump After SCCP (sccp) //----- //
module {
  func.func @main(%arg0: tensor<?x?x?xf32>) -> tensor<?x?xf32> attributes {tf.entry_function = {input_placements = "gpu", inputs = "input0", output_placements = "gpu", outputs = "output0"}} {
    %cst = "tf.Const"() {value = dense<0> : tensor<1xi32>} : () -> tensor<1xi32>
    %0 = "tf.Abs"(%arg0) : (tensor<?x?x?xf32>) -> tensor<?x?x?xf32>
    %1 = "tf.Max"(%0, %cst) : (tensor<?x?x?xf32>, tensor<1xi32>) -> tensor<?x?xf32>
    return %1 : tensor<?x?xf32>
  }
}


// -----// IR Dump After LegalizeTF (xla-legalize-tf) //----- //
func.func @main(%arg0: tensor<?x?x?xf32>) -> tensor<?x?xf32> attributes {tf.entry_function = {input_placements = "gpu", inputs = "input0", output_placements = "gpu", outputs = "output0"}} {
  %0 = mhlo.constant dense<0> : tensor<1xi32>
  %1 = mhlo.abs %arg0 : tensor<?x?x?xf32>
  %2 = mhlo.convert %1 : tensor<?x?x?xf32>
  %3 = mhlo.constant dense<0xFF800000> : tensor<f32>
  %4 = mhlo.reduce(%2 init: %3) applies mhlo.maximum across dimensions = [0] : (tensor<?x?x?xf32>, tensor<f32>) -> tensor<?x?xf32>
  %5 = mhlo.convert %4 : tensor<?x?xf32>
  return %5 : tensor<?x?xf32>
}

// -----// IR Dump After DiscLowerTfPass (disc-lower-tf) //----- //
func.func @main(%arg0: tensor<?x?x?xf32>) -> tensor<?x?xf32> attributes {tf.entry_function = {input_placements = "gpu", inputs = "input0", output_placements = "gpu", outputs = "output0"}} {
  %0 = mhlo.constant dense<0xFF800000> : tensor<f32>
  %1 = mhlo.abs %arg0 : tensor<?x?x?xf32>
  %2 = mhlo.reduce(%1 init: %0) applies mhlo.maximum across dimensions = [0] : (tensor<?x?x?xf32>, tensor<f32>) -> tensor<?x?xf32>
  return %2 : tensor<?x?xf32>
}

===-------------------------------------------------------------------------===
                         ... Execution time report ...
===-------------------------------------------------------------------------===
  Total Execution Time: 0.0084 seconds

  ----Wall Time----  ----Name----
    0.0000 (  0.6%)  ReviseArgumentsForStaticRankPass
    0.0000 (  0.1%)  FunctionalControlFlowToRegionsPass
    0.0044 ( 51.9%)  Inliner
    0.0000 (  0.2%)    (A) CallGraph
    0.0042 ( 49.3%)  'func.func' Pipeline
    0.0042 ( 49.3%)    Canonicalizer
    0.0001 (  1.1%)  'func.func' Pipeline
    0.0000 (  0.1%)    DropWhileShapeInvariantPass
    0.0000 (  0.0%)    ReplicateTensorListInitOpsPass
    0.0001 (  0.9%)    Canonicalizer
    0.0002 (  2.6%)  SCCP
    0.0000 (  0.2%)  GuaranteeAllFuncsOneUsePass
    0.0000 (  0.0%)    (A) CallGraph
    0.0000 (  0.1%)  TensorFlowShapeInferencePass
    0.0002 (  1.9%)  SCCP
    0.0000 (  0.1%)  TensorListOpsDecompositionPass
    0.0000 (  0.1%)  StackOpsDecompositionPass
    0.0000 (  0.1%)  TensorArrayOpsDecompositionPass
    0.0001 (  0.7%)  'func.func' Pipeline
    0.0001 (  0.7%)    DecomposeResourceOpsPass
    0.0000 (  0.2%)  PromoteResourcesToArgsPass
    0.0000 (  0.1%)  SymbolDCE
    0.0000 (  0.1%)  'func.func' Pipeline
    0.0000 (  0.1%)    SinkConstantsToControlFlowPass
    0.0000 (  0.1%)  TensorFlowShapeInferencePass
    0.0002 (  2.7%)  StablehloLegalizeToHloPass
    0.0000 (  0.6%)  'func.func' Pipeline
    0.0000 (  0.3%)    DiscLowerTfPass
    0.0000 (  0.2%)    LowerQuantizedPass
    0.0000 (  0.2%)  LegalizeTfTypesPass
    0.0010 ( 12.0%)  'func.func' Pipeline
    0.0008 (  9.7%)    LegalizeTF
    0.0002 (  2.1%)    DiscLowerTfPass
    0.0000 (  0.1%)    mlir::mhlo::{anonymous}::AdjustLayout
    0.0000 (  0.3%)  LegalizeTFCollective
    0.0001 (  0.8%)  'func.func' Pipeline
    0.0001 (  0.8%)    Canonicalizer
    0.0000 (  0.1%)  TensorFlowShapeInferencePass
    0.0003 (  4.0%)  'func.func' Pipeline
    0.0003 (  4.0%)    LegalizeTF
    0.0000 (  0.1%)  LegalizeTFCommunicationPass
    0.0000 (  0.5%)  'func.func' Pipeline
    0.0000 (  0.4%)    DiscDynamicSliceConverterPass
    0.0000 (  0.1%)    SinkConstantsToControlFlowPass
   -0.0026 (-30.7%)  Rest
    0.0084 (100.0%)  Total
======== BEGIN After TF2HLO =========
module {
  func.func @main(%arg0: tensor<?x?x?xf32>) -> tensor<?x?xf32> attributes {tf.entry_function = {input_placements = "gpu", inputs = "input0", output_placements = "gpu", outputs = "output0"}} {
    %0 = mhlo.constant dense<0xFF800000> : tensor<f32>
    %1 = mhlo.abs %arg0 : tensor<?x?x?xf32>
    %2 = mhlo.reduce(%1 init: %0) applies mhlo.maximum across dimensions = [0] : (tensor<?x?x?xf32>, tensor<f32>) -> tensor<?x?xf32>
    return %2 : tensor<?x?xf32>
  }
}

======= END After TF2HLO ==========
[DISC] tf2hlo takes: 8.764000e-03 s.
SymbolicDimMgr::save walkRankedTensorValue takes: 4 us
SymbolicDimMgr::save update attributes takes: 1 us
SymbolicDimMgr::updateProductEqualityMap simplifySymbolicDimProductPair takes: 0 us
productSet.size() = 0
SymbolicDimMgr::updateProductEqualityMap propagate graph takes: 2 us
SymbolicDimMgr::updateProductEqualityMap remove multiply takes: 0 us
SymbolicDimMgr::updateProductEqualityMap build toRemove  takes: 0 us
SymbolicDimMgr::updateProductEqualityMap apply toRemove  takes: 0 us
SymbolicDimMgr::save updateProductEqualityMap takes: 13 us
SymbolicDimMgr::save updateFunctionType takes: 1 us
SymbolicDimMgr::save collect symbolicDim ops takes: 4 us
SymbolicDimMgr::save remove symbolicDim ops takes: 1 us
SymbolicDimMgr::save remove unused production takes: 0 us
SymbolicDimMgr::save remove unused production #2 takes: 0 us
SymbolicDimMgr::save canonicalize the name takes: 4 us
SymbolicDimMgr::save replace the name takes: 3 us
SymbolicDimMgr::save updateFunctionType takes: 0 us
// -----// IR Dump After DiscShapeOptimizationPass (disc-shape-optimization) //----- //
module {
  func.func @main(%arg0: tensor<?x?x?xf32, [@S0, @S1, @S2]>) -> tensor<?x?xf32, [@S1, @S2]> attributes {tf.entry_function = {input_placements = "gpu", inputs = "input0", output_placements = "gpu", outputs = "output0"}} {
    %0 = mhlo.constant dense<0xFF800000> : tensor<f32>
    %1 = mhlo.abs %arg0 : tensor<?x?x?xf32, [@S0, @S1, @S2]>
    %2 = mhlo.reduce(%1 init: %0) applies mhlo.maximum across dimensions = [0] : (tensor<?x?x?xf32, [@S0, @S1, @S2]>, tensor<f32>) -> tensor<?x?xf32, [@S1, @S2]>
    return %2 : tensor<?x?xf32, [@S1, @S2]>
  }
  "disc_shape.SymbolicDim"() {knownNegativeOne = false, knownNonNegative = true, knownNonSizeOne = false, knownNonSizeZero = false, sym_name = "S0", value = -9223372036854775808 : i64} : () -> ()
  "disc_shape.SymbolicDim"() {knownNegativeOne = false, knownNonNegative = true, knownNonSizeOne = false, knownNonSizeZero = false, sym_name = "S1", value = -9223372036854775808 : i64} : () -> ()
  "disc_shape.SymbolicDim"() {knownNegativeOne = false, knownNonNegative = true, knownNonSizeOne = false, knownNonSizeZero = false, sym_name = "S2", value = -9223372036854775808 : i64} : () -> ()
  func.func @shape_constraint_graph() {
    return
  }
}


SymbolicDimMgr::save walkRankedTensorValue takes: 4 us
SymbolicDimMgr::save update attributes takes: 1 us
SymbolicDimMgr::updateProductEqualityMap simplifySymbolicDimProductPair takes: 0 us
productSet.size() = 0
SymbolicDimMgr::updateProductEqualityMap propagate graph takes: 1 us
SymbolicDimMgr::updateProductEqualityMap remove multiply takes: 0 us
SymbolicDimMgr::updateProductEqualityMap build toRemove  takes: 0 us
SymbolicDimMgr::updateProductEqualityMap apply toRemove  takes: 0 us
SymbolicDimMgr::save updateProductEqualityMap takes: 12 us
SymbolicDimMgr::save updateFunctionType takes: 1 us
SymbolicDimMgr::save collect symbolicDim ops takes: 3 us
SymbolicDimMgr::save remove symbolicDim ops takes: 0 us
SymbolicDimMgr::save remove unused production takes: 0 us
SymbolicDimMgr::save remove unused production #2 takes: 0 us
SymbolicDimMgr::save canonicalize the name takes: 2 us
SymbolicDimMgr::save replace the name takes: 3 us
SymbolicDimMgr::save updateFunctionType takes: 0 us
// -----// IR Dump After DiscShapeOptimizationPass (disc-shape-optimization) //----- //
module {
  func.func @main(%arg0: tensor<?x?x?xf32, [@S0, @S1, @S2]>) -> tensor<?x?xf32, [@S1, @S2]> attributes {tf.entry_function = {input_placements = "gpu", inputs = "input0", output_placements = "gpu", outputs = "output0"}} {
    %0 = mhlo.constant dense<0xFF800000> : tensor<f32>
    %1 = mhlo.abs %arg0 : tensor<?x?x?xf32, [@S0, @S1, @S2]>
    %2 = mhlo.reduce(%1 init: %0) applies mhlo.maximum across dimensions = [0] : (tensor<?x?x?xf32, [@S0, @S1, @S2]>, tensor<f32>) -> tensor<?x?xf32, [@S1, @S2]>
    return %2 : tensor<?x?xf32, [@S1, @S2]>
  }
  "disc_shape.SymbolicDim"() {knownNegativeOne = false, knownNonNegative = true, knownNonSizeOne = false, knownNonSizeZero = false, sym_name = "S0", value = -9223372036854775808 : i64} : () -> ()
  "disc_shape.SymbolicDim"() {knownNegativeOne = false, knownNonNegative = true, knownNonSizeOne = false, knownNonSizeZero = false, sym_name = "S1", value = -9223372036854775808 : i64} : () -> ()
  "disc_shape.SymbolicDim"() {knownNegativeOne = false, knownNonNegative = true, knownNonSizeOne = false, knownNonSizeZero = false, sym_name = "S2", value = -9223372036854775808 : i64} : () -> ()
  func.func @shape_constraint_graph() {
    return
  }
}


SymbolicDimMgr::save walkRankedTensorValue takes: 3 us
SymbolicDimMgr::save update attributes takes: 0 us
SymbolicDimMgr::updateProductEqualityMap simplifySymbolicDimProductPair takes: 0 us
productSet.size() = 0
SymbolicDimMgr::updateProductEqualityMap propagate graph takes: 1 us
SymbolicDimMgr::updateProductEqualityMap remove multiply takes: 0 us
SymbolicDimMgr::updateProductEqualityMap build toRemove  takes: 0 us
SymbolicDimMgr::updateProductEqualityMap apply toRemove  takes: 0 us
SymbolicDimMgr::save updateProductEqualityMap takes: 12 us
SymbolicDimMgr::save updateFunctionType takes: 1 us
SymbolicDimMgr::save collect symbolicDim ops takes: 3 us
SymbolicDimMgr::save remove symbolicDim ops takes: 0 us
SymbolicDimMgr::save remove unused production takes: 0 us
SymbolicDimMgr::save remove unused production #2 takes: 0 us
SymbolicDimMgr::save canonicalize the name takes: 2 us
SymbolicDimMgr::save replace the name takes: 3 us
SymbolicDimMgr::save updateFunctionType takes: 0 us
// -----// IR Dump After DiscShapeOptimizationPass (disc-shape-optimization) //----- //
module {
  func.func @main(%arg0: tensor<?x?x?xf32, [@S0, @S1, @S2]>) -> tensor<?x?xf32, [@S1, @S2]> attributes {tf.entry_function = {input_placements = "gpu", inputs = "input0", output_placements = "gpu", outputs = "output0"}} {
    %0 = mhlo.constant dense<0xFF800000> : tensor<f32>
    %1 = mhlo.abs %arg0 : tensor<?x?x?xf32, [@S0, @S1, @S2]>
    %2 = mhlo.reduce(%1 init: %0) applies mhlo.maximum across dimensions = [0] : (tensor<?x?x?xf32, [@S0, @S1, @S2]>, tensor<f32>) -> tensor<?x?xf32, [@S1, @S2]>
    return %2 : tensor<?x?xf32, [@S1, @S2]>
  }
  "disc_shape.SymbolicDim"() {knownNegativeOne = false, knownNonNegative = true, knownNonSizeOne = false, knownNonSizeZero = false, sym_name = "S0", value = -9223372036854775808 : i64} : () -> ()
  "disc_shape.SymbolicDim"() {knownNegativeOne = false, knownNonNegative = true, knownNonSizeOne = false, knownNonSizeZero = false, sym_name = "S1", value = -9223372036854775808 : i64} : () -> ()
  "disc_shape.SymbolicDim"() {knownNegativeOne = false, knownNonNegative = true, knownNonSizeOne = false, knownNonSizeZero = false, sym_name = "S2", value = -9223372036854775808 : i64} : () -> ()
  func.func @shape_constraint_graph() {
    return
  }
}


// -----// IR Dump After HloCanonicalizeReductionPass (hlo-canonicalize-reduction) //----- //
func.func @main(%arg0: tensor<?x?x?xf32, [@S0, @S1, @S2]>) -> tensor<?x?xf32, [@S1, @S2]> attributes {tf.entry_function = {input_placements = "gpu", inputs = "input0", output_placements = "gpu", outputs = "output0"}} {
  %0 = mhlo.constant dense<0xFF800000> : tensor<f32>
  %1 = mhlo.abs %arg0 : tensor<?x?x?xf32, [@S0, @S1, @S2]>
  %c1_i32 = arith.constant 1 : i32
  %c0 = arith.constant 0 : index
  %dim = tensor.dim %1, %c0 : tensor<?x?x?xf32, [@S0, @S1, @S2]>
  %2 = arith.index_cast %dim : index to i32
  %3 = arith.muli %c1_i32, %2 : i32
  %c1 = arith.constant 1 : index
  %dim_0 = tensor.dim %1, %c1 : tensor<?x?x?xf32, [@S0, @S1, @S2]>
  %4 = arith.index_cast %dim_0 : index to i32
  %5 = arith.muli %c1_i32, %4 : i32
  %c2 = arith.constant 2 : index
  %dim_1 = tensor.dim %1, %c2 : tensor<?x?x?xf32, [@S0, @S1, @S2]>
  %6 = arith.index_cast %dim_1 : index to i32
  %7 = arith.muli %5, %6 : i32
  %from_elements = tensor.from_elements %3, %7 : tensor<2xi32>
  %8 = mhlo.dynamic_reshape %1, %from_elements : (tensor<?x?x?xf32, [@S0, @S1, @S2]>, tensor<2xi32>) -> tensor<?x?xf32>
  %9 = mhlo.reduce(%8 init: %0) applies mhlo.maximum across dimensions = [0] : (tensor<?x?xf32>, tensor<f32>) -> tensor<?xf32>
  %c1_2 = arith.constant 1 : index
  %dim_3 = tensor.dim %1, %c1_2 : tensor<?x?x?xf32, [@S0, @S1, @S2]>
  %10 = arith.index_cast %dim_3 : index to i32
  %c2_4 = arith.constant 2 : index
  %dim_5 = tensor.dim %1, %c2_4 : tensor<?x?x?xf32, [@S0, @S1, @S2]>
  %11 = arith.index_cast %dim_5 : index to i32
  %from_elements_6 = tensor.from_elements %10, %11 : tensor<2xi32>
  %12 = mhlo.dynamic_reshape %9, %from_elements_6 : (tensor<?xf32>, tensor<2xi32>) -> tensor<?x?xf32, [@S1, @S2]>
  return %12 : tensor<?x?xf32, [@S1, @S2]>
}

// -----// IR Dump After CSE (cse) //----- //
module {
  func.func @main(%arg0: tensor<?x?x?xf32, [@S0, @S1, @S2]>) -> tensor<?x?xf32, [@S1, @S2]> attributes {tf.entry_function = {input_placements = "gpu", inputs = "input0", output_placements = "gpu", outputs = "output0"}} {
    %c0 = arith.constant 0 : index
    %c1 = arith.constant 1 : index
    %c2 = arith.constant 2 : index
    %0 = mhlo.constant dense<0xFF800000> : tensor<f32>
    %dim = tensor.dim %arg0, %c0 : tensor<?x?x?xf32, [@S0, @S1, @S2]>
    %dim_0 = tensor.dim %arg0, %c1 : tensor<?x?x?xf32, [@S0, @S1, @S2]>
    %dim_1 = tensor.dim %arg0, %c2 : tensor<?x?x?xf32, [@S0, @S1, @S2]>
    %1 = "disc_shape.tie_shape"(%arg0, %dim, %dim_0, %dim_1) : (tensor<?x?x?xf32, [@S0, @S1, @S2]>, index, index, index) -> tensor<?x?x?xf32, [@S0, @S1, @S2]>
    %2 = mhlo.abs %1 : tensor<?x?x?xf32, [@S0, @S1, @S2]>
    %3 = "disc_shape.tie_shape"(%2, %dim, %dim_0, %dim_1) : (tensor<?x?x?xf32, [@S0, @S1, @S2]>, index, index, index) -> tensor<?x?x?xf32, [@S0, @S1, @S2]>
    %4 = arith.index_cast %dim : index to i32
    %5 = arith.index_cast %dim_0 : index to i32
    %6 = arith.index_cast %dim_1 : index to i32
    %7 = arith.muli %5, %6 : i32
    %from_elements = tensor.from_elements %4, %7 : tensor<2xi32>
    %8 = "disc_shape.tie_shape"(%from_elements, %c2) : (tensor<2xi32>, index) -> tensor<2xi32>
    %9 = mhlo.dynamic_reshape %3, %8 : (tensor<?x?x?xf32, [@S0, @S1, @S2]>, tensor<2xi32>) -> tensor<?x?xf32>
    %10 = arith.index_cast %7 : i32 to index
    %11 = "disc_shape.tie_shape"(%9, %dim, %10) : (tensor<?x?xf32>, index, index) -> tensor<?x?xf32>
    %12 = mhlo.reduce(%11 init: %0) applies mhlo.maximum across dimensions = [0] : (tensor<?x?xf32>, tensor<f32>) -> tensor<?xf32>
    %13 = "disc_shape.tie_shape"(%12, %10) : (tensor<?xf32>, index) -> tensor<?xf32>
    %from_elements_2 = tensor.from_elements %5, %6 : tensor<2xi32>
    %14 = "disc_shape.tie_shape"(%from_elements_2, %c2) : (tensor<2xi32>, index) -> tensor<2xi32>
    %15 = mhlo.dynamic_reshape %13, %14 : (tensor<?xf32>, tensor<2xi32>) -> tensor<?x?xf32, [@S1, @S2]>
    %16 = "disc_shape.tie_shape"(%15, %dim_0, %dim_1) : (tensor<?x?xf32, [@S1, @S2]>, index, index) -> tensor<?x?xf32, [@S1, @S2]>
    return %16 : tensor<?x?xf32, [@S1, @S2]>
  }
  "disc_shape.SymbolicDim"() {knownNegativeOne = false, knownNonNegative = true, knownNonSizeOne = false, knownNonSizeZero = false, sym_name = "S0", value = -9223372036854775808 : i64} : () -> ()
  "disc_shape.SymbolicDim"() {knownNegativeOne = false, knownNonNegative = true, knownNonSizeOne = false, knownNonSizeZero = false, sym_name = "S1", value = -9223372036854775808 : i64} : () -> ()
  "disc_shape.SymbolicDim"() {knownNegativeOne = false, knownNonNegative = true, knownNonSizeOne = false, knownNonSizeZero = false, sym_name = "S2", value = -9223372036854775808 : i64} : () -> ()
  func.func @shape_constraint_graph() {
    return
  }
}


SymbolicDimMgr::save walkRankedTensorValue takes: 5 us
SymbolicDimMgr::save update attributes takes: 1 us
SymbolicDimMgr::updateProductEqualityMap simplifySymbolicDimProductPair takes: 19 us
productSet.size() = 4
SymbolicDimMgr::updateProductEqualityMap propagate graph takes: 6 us
SymbolicDimMgr::updateProductEqualityMap remove multiply takes: 36 us
SymbolicDimMgr::updateProductEqualityMap build toRemove  takes: 3 us
SymbolicDimMgr::updateProductEqualityMap apply toRemove  takes: 2 us
SymbolicDimMgr::save updateProductEqualityMap takes: 79 us
SymbolicDimMgr::save updateFunctionType takes: 1 us
SymbolicDimMgr::save collect symbolicDim ops takes: 5 us
SymbolicDimMgr::save remove symbolicDim ops takes: 1 us
SymbolicDimMgr::save remove unused production takes: 0 us
SymbolicDimMgr::save remove unused production #2 takes: 1 us
SymbolicDimMgr::save canonicalize the name takes: 4 us
SymbolicDimMgr::save replace the name takes: 6 us
SymbolicDimMgr::save updateFunctionType takes: 0 us
// -----// IR Dump After DiscShapeOptimizationPass (disc-shape-optimization) //----- //
module {
  func.func @main(%arg0: tensor<?x?x?xf32, [@S0, @S1, @S2]>) -> tensor<?x?xf32, [@S1, @S2]> attributes {tf.entry_function = {input_placements = "gpu", inputs = "input0", output_placements = "gpu", outputs = "output0"}} {
    %c0 = arith.constant 0 : index
    %c1 = arith.constant 1 : index
    %c2 = arith.constant 2 : index
    %0 = mhlo.constant dense<0xFF800000> : tensor<f32>
    %dim = tensor.dim %arg0, %c0 : tensor<?x?x?xf32, [@S0, @S1, @S2]>
    %dim_0 = tensor.dim %arg0, %c1 : tensor<?x?x?xf32, [@S0, @S1, @S2]>
    %dim_1 = tensor.dim %arg0, %c2 : tensor<?x?x?xf32, [@S0, @S1, @S2]>
    %1 = mhlo.abs %arg0 : tensor<?x?x?xf32, [@S0, @S1, @S2]>
    %2 = arith.index_cast %dim : index to i32
    %3 = arith.index_cast %dim_0 : index to i32
    %4 = arith.index_cast %dim_1 : index to i32
    %5 = arith.muli %3, %4 : i32
    %from_elements = tensor.from_elements %2, %5 : tensor<2xi32>
    %6 = mhlo.dynamic_reshape %1, %from_elements : (tensor<?x?x?xf32, [@S0, @S1, @S2]>, tensor<2xi32>) -> tensor<?x?xf32, [@S0, @S3]>
    %7 = mhlo.reduce(%6 init: %0) applies mhlo.maximum across dimensions = [0] : (tensor<?x?xf32, [@S0, @S3]>, tensor<f32>) -> tensor<?xf32, [@S3]>
    %from_elements_2 = tensor.from_elements %3, %4 : tensor<2xi32>
    %8 = mhlo.dynamic_reshape %7, %from_elements_2 : (tensor<?xf32, [@S3]>, tensor<2xi32>) -> tensor<?x?xf32, [@S1, @S2]>
    return %8 : tensor<?x?xf32, [@S1, @S2]>
  }
  "disc_shape.SymbolicDim"() {knownNegativeOne = false, knownNonNegative = true, knownNonSizeOne = false, knownNonSizeZero = false, sym_name = "S0", value = -9223372036854775808 : i64} : () -> ()
  "disc_shape.SymbolicDim"() {knownNegativeOne = false, knownNonNegative = true, knownNonSizeOne = false, knownNonSizeZero = false, sym_name = "S1", value = -9223372036854775808 : i64} : () -> ()
  "disc_shape.SymbolicDim"() {knownNegativeOne = false, knownNonNegative = true, knownNonSizeOne = false, knownNonSizeZero = false, sym_name = "S2", value = -9223372036854775808 : i64} : () -> ()
  "disc_shape.SymbolicDim"() {knownNegativeOne = false, knownNonNegative = true, knownNonSizeOne = false, knownNonSizeZero = false, sym_name = "S3", value = -9223372036854775808 : i64} : () -> ()
  func.func @shape_constraint_graph() {
    %0 = "disc_shape.dim"() {name = @S3} : () -> index
    %1 = "disc_shape.dim"() {name = @S1} : () -> index
    %2 = "disc_shape.dim"() {name = @S2} : () -> index
    "disc_shape.tie_product_equal"(%0, %1, %2) {operand_segment_sizes = array<i32: 1, 2>} : (index, index, index) -> ()
    return
  }
}


// -----// IR Dump After DiscMarkShapeCalculationPass (disc-mhlo-mark-shape-calc) //----- //
module {
  func.func @main(%arg0: tensor<?x?x?xf32, [@S0, @S1, @S2]>) -> tensor<?x?xf32, [@S1, @S2]> attributes {tf.entry_function = {input_placements = "gpu", inputs = "input0", output_placements = "gpu", outputs = "output0"}} {
    %c0 = arith.constant 0 : index
    %c1 = arith.constant 1 : index
    %c2 = arith.constant 2 : index
    %0 = mhlo.constant dense<0xFF800000> : tensor<f32>
    %dim = tensor.dim %arg0, %c0 : tensor<?x?x?xf32, [@S0, @S1, @S2]>
    %dim_0 = tensor.dim %arg0, %c1 : tensor<?x?x?xf32, [@S0, @S1, @S2]>
    %dim_1 = tensor.dim %arg0, %c2 : tensor<?x?x?xf32, [@S0, @S1, @S2]>
    %1 = mhlo.abs %arg0 : tensor<?x?x?xf32, [@S0, @S1, @S2]>
    %2 = arith.index_cast %dim : index to i32
    %3 = arith.index_cast %dim_0 : index to i32
    %4 = arith.index_cast %dim_1 : index to i32
    %5 = arith.muli %3, %4 : i32
    %from_elements = tensor.from_elements %2, %5 {disc.shape_op = true} : tensor<2xi32>
    %6 = mhlo.dynamic_reshape %1, %from_elements : (tensor<?x?x?xf32, [@S0, @S1, @S2]>, tensor<2xi32>) -> tensor<?x?xf32, [@S0, @S3]>
    %7 = mhlo.reduce(%6 init: %0) applies mhlo.maximum across dimensions = [0] : (tensor<?x?xf32, [@S0, @S3]>, tensor<f32>) -> tensor<?xf32, [@S3]>
    %from_elements_2 = tensor.from_elements %3, %4 {disc.shape_op = true} : tensor<2xi32>
    %8 = mhlo.dynamic_reshape %7, %from_elements_2 : (tensor<?xf32, [@S3]>, tensor<2xi32>) -> tensor<?x?xf32, [@S1, @S2]>
    return %8 : tensor<?x?xf32, [@S1, @S2]>
  }
  "disc_shape.SymbolicDim"() {knownNegativeOne = false, knownNonNegative = true, knownNonSizeOne = false, knownNonSizeZero = false, sym_name = "S0", value = -9223372036854775808 : i64} : () -> ()
  "disc_shape.SymbolicDim"() {knownNegativeOne = false, knownNonNegative = true, knownNonSizeOne = false, knownNonSizeZero = false, sym_name = "S1", value = -9223372036854775808 : i64} : () -> ()
  "disc_shape.SymbolicDim"() {knownNegativeOne = false, knownNonNegative = true, knownNonSizeOne = false, knownNonSizeZero = false, sym_name = "S2", value = -9223372036854775808 : i64} : () -> ()
  "disc_shape.SymbolicDim"() {knownNegativeOne = false, knownNonNegative = true, knownNonSizeOne = false, knownNonSizeZero = false, sym_name = "S3", value = -9223372036854775808 : i64} : () -> ()
  func.func @shape_constraint_graph() {
    %0 = "disc_shape.dim"() {name = @S3} : () -> index
    %1 = "disc_shape.dim"() {name = @S1} : () -> index
    %2 = "disc_shape.dim"() {name = @S2} : () -> index
    "disc_shape.tie_product_equal"(%0, %1, %2) {operand_segment_sizes = array<i32: 1, 2>} : (index, index, index) -> ()
    return
  }
}


// -----// IR Dump After PlaceOpsPass (mhlo-place-ops) //----- //
module {
  func.func @main(%arg0: tensor<?x?x?xf32, [@S0, @S1, @S2]>) -> tensor<?x?xf32, [@S1, @S2]> attributes {tf.entry_function = {input_placements = "gpu", inputs = "input0", output_placements = "gpu", outputs = "output0"}} {
    %c0 = arith.constant 0 : index
    %c1 = arith.constant 1 : index
    %c2 = arith.constant 2 : index
    %0 = mhlo.constant {disc.device = "gpu"} dense<0xFF800000> : tensor<f32>
    %dim = tensor.dim %arg0, %c0 : tensor<?x?x?xf32, [@S0, @S1, @S2]>
    %dim_0 = tensor.dim %arg0, %c1 : tensor<?x?x?xf32, [@S0, @S1, @S2]>
    %dim_1 = tensor.dim %arg0, %c2 : tensor<?x?x?xf32, [@S0, @S1, @S2]>
    %1 = mhlo.abs %arg0 {disc.device = "gpu"} : tensor<?x?x?xf32, [@S0, @S1, @S2]>
    %2 = arith.index_cast %dim : index to i32
    %3 = arith.index_cast %dim_0 : index to i32
    %4 = arith.index_cast %dim_1 : index to i32
    %5 = arith.muli %3, %4 : i32
    %from_elements = tensor.from_elements %2, %5 {disc.shape_op = true} : tensor<2xi32>
    %6 = mhlo.dynamic_reshape %1, %from_elements {disc.device = "gpu"} : (tensor<?x?x?xf32, [@S0, @S1, @S2]>, tensor<2xi32>) -> tensor<?x?xf32, [@S0, @S3]>
    %7 = mhlo.reduce(%6 init: %0) applies mhlo.maximum across dimensions = [0] : (tensor<?x?xf32, [@S0, @S3]>, tensor<f32>) -> tensor<?xf32, [@S3]>
    %from_elements_2 = tensor.from_elements %3, %4 {disc.shape_op = true} : tensor<2xi32>
    %8 = mhlo.dynamic_reshape %7, %from_elements_2 {disc.device = "gpu"} : (tensor<?xf32, [@S3]>, tensor<2xi32>) -> tensor<?x?xf32, [@S1, @S2]>
    return %8 : tensor<?x?xf32, [@S1, @S2]>
  }
  "disc_shape.SymbolicDim"() {knownNegativeOne = false, knownNonNegative = true, knownNonSizeOne = false, knownNonSizeZero = false, sym_name = "S0", value = -9223372036854775808 : i64} : () -> ()
  "disc_shape.SymbolicDim"() {knownNegativeOne = false, knownNonNegative = true, knownNonSizeOne = false, knownNonSizeZero = false, sym_name = "S1", value = -9223372036854775808 : i64} : () -> ()
  "disc_shape.SymbolicDim"() {knownNegativeOne = false, knownNonNegative = true, knownNonSizeOne = false, knownNonSizeZero = false, sym_name = "S2", value = -9223372036854775808 : i64} : () -> ()
  "disc_shape.SymbolicDim"() {knownNegativeOne = false, knownNonNegative = true, knownNonSizeOne = false, knownNonSizeZero = false, sym_name = "S3", value = -9223372036854775808 : i64} : () -> ()
  func.func @shape_constraint_graph() {
    %0 = "disc_shape.dim"() {name = @S3} : () -> index
    %1 = "disc_shape.dim"() {name = @S1} : () -> index
    %2 = "disc_shape.dim"() {name = @S2} : () -> index
    "disc_shape.tie_product_equal"(%0, %1, %2) {operand_segment_sizes = array<i32: 1, 2>} : (index, index, index) -> ()
    return
  }
}


SymbolicDimMgr::save walkRankedTensorValue takes: 5 us
SymbolicDimMgr::save update attributes takes: 1 us
SymbolicDimMgr::updateProductEqualityMap simplifySymbolicDimProductPair takes: 18 us
productSet.size() = 4
SymbolicDimMgr::updateProductEqualityMap propagate graph takes: 5 us
SymbolicDimMgr::updateProductEqualityMap remove multiply takes: 45 us
SymbolicDimMgr::updateProductEqualityMap build toRemove  takes: 3 us
SymbolicDimMgr::updateProductEqualityMap apply toRemove  takes: 2 us
SymbolicDimMgr::save updateProductEqualityMap takes: 87 us
SymbolicDimMgr::save updateFunctionType takes: 1 us
SymbolicDimMgr::save collect symbolicDim ops takes: 5 us
SymbolicDimMgr::save remove symbolicDim ops takes: 1 us
SymbolicDimMgr::save remove unused production takes: 0 us
SymbolicDimMgr::save remove unused production #2 takes: 1 us
SymbolicDimMgr::save canonicalize the name takes: 3 us
SymbolicDimMgr::save replace the name takes: 5 us
SymbolicDimMgr::save updateFunctionType takes: 0 us
// -----// IR Dump After DiscShapeOptimizationPass (disc-shape-optimization) //----- //
module {
  func.func @main(%arg0: tensor<?x?x?xf32, [@S0, @S1, @S2]>) -> tensor<?x?xf32, [@S1, @S2]> attributes {tf.entry_function = {input_placements = "gpu", inputs = "input0", output_placements = "gpu", outputs = "output0"}} {
    %c0 = arith.constant 0 : index
    %c1 = arith.constant 1 : index
    %c2 = arith.constant 2 : index
    %0 = mhlo.constant {disc.device = "gpu"} dense<0xFF800000> : tensor<f32>
    %dim = tensor.dim %arg0, %c0 : tensor<?x?x?xf32, [@S0, @S1, @S2]>
    %dim_0 = tensor.dim %arg0, %c1 : tensor<?x?x?xf32, [@S0, @S1, @S2]>
    %dim_1 = tensor.dim %arg0, %c2 : tensor<?x?x?xf32, [@S0, @S1, @S2]>
    %1 = mhlo.abs %arg0 {disc.device = "gpu"} : tensor<?x?x?xf32, [@S0, @S1, @S2]>
    %2 = arith.index_cast %dim : index to i32
    %3 = arith.index_cast %dim_0 : index to i32
    %4 = arith.index_cast %dim_1 : index to i32
    %5 = arith.muli %3, %4 : i32
    %from_elements = tensor.from_elements %2, %5 {disc.shape_op = true} : tensor<2xi32>
    %6 = mhlo.dynamic_reshape %1, %from_elements {disc.device = "gpu"} : (tensor<?x?x?xf32, [@S0, @S1, @S2]>, tensor<2xi32>) -> tensor<?x?xf32, [@S0, @S3]>
    %7 = mhlo.reduce(%6 init: %0) applies mhlo.maximum across dimensions = [0] : (tensor<?x?xf32, [@S0, @S3]>, tensor<f32>) -> tensor<?xf32, [@S3]>
    %from_elements_2 = tensor.from_elements %3, %4 {disc.shape_op = true} : tensor<2xi32>
    %8 = mhlo.dynamic_reshape %7, %from_elements_2 {disc.device = "gpu"} : (tensor<?xf32, [@S3]>, tensor<2xi32>) -> tensor<?x?xf32, [@S1, @S2]>
    return %8 : tensor<?x?xf32, [@S1, @S2]>
  }
  "disc_shape.SymbolicDim"() {knownNegativeOne = false, knownNonNegative = true, knownNonSizeOne = false, knownNonSizeZero = false, sym_name = "S0", value = -9223372036854775808 : i64} : () -> ()
  "disc_shape.SymbolicDim"() {knownNegativeOne = false, knownNonNegative = true, knownNonSizeOne = false, knownNonSizeZero = false, sym_name = "S1", value = -9223372036854775808 : i64} : () -> ()
  "disc_shape.SymbolicDim"() {knownNegativeOne = false, knownNonNegative = true, knownNonSizeOne = false, knownNonSizeZero = false, sym_name = "S2", value = -9223372036854775808 : i64} : () -> ()
  "disc_shape.SymbolicDim"() {knownNegativeOne = false, knownNonNegative = true, knownNonSizeOne = false, knownNonSizeZero = false, sym_name = "S3", value = -9223372036854775808 : i64} : () -> ()
  func.func @shape_constraint_graph() {
    %0 = "disc_shape.dim"() {name = @S3} : () -> index
    %1 = "disc_shape.dim"() {name = @S1} : () -> index
    %2 = "disc_shape.dim"() {name = @S2} : () -> index
    "disc_shape.tie_product_equal"(%0, %1, %2) {operand_segment_sizes = array<i32: 1, 2>} : (index, index, index) -> ()
    return
  }
}


SymbolicDimMgr::save walkRankedTensorValue takes: 5 us
SymbolicDimMgr::save update attributes takes: 1 us
SymbolicDimMgr::updateProductEqualityMap simplifySymbolicDimProductPair takes: 15 us
productSet.size() = 4
SymbolicDimMgr::updateProductEqualityMap propagate graph takes: 5 us
SymbolicDimMgr::updateProductEqualityMap remove multiply takes: 32 us
SymbolicDimMgr::updateProductEqualityMap build toRemove  takes: 2 us
SymbolicDimMgr::updateProductEqualityMap apply toRemove  takes: 1 us
SymbolicDimMgr::save updateProductEqualityMap takes: 71 us
SymbolicDimMgr::save updateFunctionType takes: 1 us
SymbolicDimMgr::save collect symbolicDim ops takes: 4 us
SymbolicDimMgr::save remove symbolicDim ops takes: 1 us
SymbolicDimMgr::save remove unused production takes: 0 us
SymbolicDimMgr::save remove unused production #2 takes: 1 us
SymbolicDimMgr::save canonicalize the name takes: 3 us
SymbolicDimMgr::save replace the name takes: 5 us
SymbolicDimMgr::save updateFunctionType takes: 0 us
// -----// IR Dump After DiscShapeOptimizationPass (disc-shape-optimization) //----- //
module {
  func.func @main(%arg0: tensor<?x?x?xf32, [@S0, @S1, @S2]>) -> tensor<?x?xf32, [@S1, @S2]> attributes {tf.entry_function = {input_placements = "gpu", inputs = "input0", output_placements = "gpu", outputs = "output0"}} {
    %c0 = arith.constant 0 : index
    %c1 = arith.constant 1 : index
    %c2 = arith.constant 2 : index
    %0 = mhlo.constant {disc.device = "gpu"} dense<0xFF800000> : tensor<f32>
    %dim = tensor.dim %arg0, %c0 : tensor<?x?x?xf32, [@S0, @S1, @S2]>
    %dim_0 = tensor.dim %arg0, %c1 : tensor<?x?x?xf32, [@S0, @S1, @S2]>
    %dim_1 = tensor.dim %arg0, %c2 : tensor<?x?x?xf32, [@S0, @S1, @S2]>
    %1 = mhlo.abs %arg0 {disc.device = "gpu"} : tensor<?x?x?xf32, [@S0, @S1, @S2]>
    %2 = arith.index_cast %dim : index to i32
    %3 = arith.index_cast %dim_0 : index to i32
    %4 = arith.index_cast %dim_1 : index to i32
    %5 = arith.muli %3, %4 : i32
    %from_elements = tensor.from_elements %2, %5 {disc.shape_op = true} : tensor<2xi32>
    %6 = mhlo.dynamic_reshape %1, %from_elements {disc.device = "gpu"} : (tensor<?x?x?xf32, [@S0, @S1, @S2]>, tensor<2xi32>) -> tensor<?x?xf32, [@S0, @S3]>
    %7 = mhlo.reduce(%6 init: %0) applies mhlo.maximum across dimensions = [0] : (tensor<?x?xf32, [@S0, @S3]>, tensor<f32>) -> tensor<?xf32, [@S3]>
    %from_elements_2 = tensor.from_elements %3, %4 {disc.shape_op = true} : tensor<2xi32>
    %8 = mhlo.dynamic_reshape %7, %from_elements_2 {disc.device = "gpu"} : (tensor<?xf32, [@S3]>, tensor<2xi32>) -> tensor<?x?xf32, [@S1, @S2]>
    return %8 : tensor<?x?xf32, [@S1, @S2]>
  }
  "disc_shape.SymbolicDim"() {knownNegativeOne = false, knownNonNegative = true, knownNonSizeOne = false, knownNonSizeZero = false, sym_name = "S0", value = -9223372036854775808 : i64} : () -> ()
  "disc_shape.SymbolicDim"() {knownNegativeOne = false, knownNonNegative = true, knownNonSizeOne = false, knownNonSizeZero = false, sym_name = "S1", value = -9223372036854775808 : i64} : () -> ()
  "disc_shape.SymbolicDim"() {knownNegativeOne = false, knownNonNegative = true, knownNonSizeOne = false, knownNonSizeZero = false, sym_name = "S2", value = -9223372036854775808 : i64} : () -> ()
  "disc_shape.SymbolicDim"() {knownNegativeOne = false, knownNonNegative = true, knownNonSizeOne = false, knownNonSizeZero = false, sym_name = "S3", value = -9223372036854775808 : i64} : () -> ()
  func.func @shape_constraint_graph() {
    %0 = "disc_shape.dim"() {name = @S3} : () -> index
    %1 = "disc_shape.dim"() {name = @S1} : () -> index
    %2 = "disc_shape.dim"() {name = @S2} : () -> index
    "disc_shape.tie_product_equal"(%0, %1, %2) {operand_segment_sizes = array<i32: 1, 2>} : (index, index, index) -> ()
    return
  }
}


SymbolicDimMgr::save walkRankedTensorValue takes: 5 us
SymbolicDimMgr::save update attributes takes: 1 us
SymbolicDimMgr::updateProductEqualityMap simplifySymbolicDimProductPair takes: 20 us
productSet.size() = 4
SymbolicDimMgr::updateProductEqualityMap propagate graph takes: 5 us
SymbolicDimMgr::updateProductEqualityMap remove multiply takes: 32 us
SymbolicDimMgr::updateProductEqualityMap build toRemove  takes: 2 us
SymbolicDimMgr::updateProductEqualityMap apply toRemove  takes: 1 us
SymbolicDimMgr::save updateProductEqualityMap takes: 76 us
SymbolicDimMgr::save updateFunctionType takes: 1 us
SymbolicDimMgr::save collect symbolicDim ops takes: 5 us
SymbolicDimMgr::save remove symbolicDim ops takes: 1 us
SymbolicDimMgr::save remove unused production takes: 0 us
SymbolicDimMgr::save remove unused production #2 takes: 1 us
SymbolicDimMgr::save canonicalize the name takes: 3 us
SymbolicDimMgr::save replace the name takes: 5 us
SymbolicDimMgr::save updateFunctionType takes: 0 us
// -----// IR Dump After DiscShapeOptimizationPass (disc-shape-optimization) //----- //
module {
  func.func @main(%arg0: tensor<?x?x?xf32, [@S0, @S1, @S2]>) -> tensor<?x?xf32, [@S1, @S2]> attributes {tf.entry_function = {input_placements = "gpu", inputs = "input0", output_placements = "gpu", outputs = "output0"}} {
    %c0 = arith.constant 0 : index
    %c1 = arith.constant 1 : index
    %c2 = arith.constant 2 : index
    %0 = mhlo.constant {disc.device = "gpu"} dense<0xFF800000> : tensor<f32>
    %dim = tensor.dim %arg0, %c0 : tensor<?x?x?xf32, [@S0, @S1, @S2]>
    %dim_0 = tensor.dim %arg0, %c1 : tensor<?x?x?xf32, [@S0, @S1, @S2]>
    %dim_1 = tensor.dim %arg0, %c2 : tensor<?x?x?xf32, [@S0, @S1, @S2]>
    %1 = mhlo.abs %arg0 {disc.device = "gpu"} : tensor<?x?x?xf32, [@S0, @S1, @S2]>
    %2 = arith.index_cast %dim : index to i32
    %3 = arith.index_cast %dim_0 : index to i32
    %4 = arith.index_cast %dim_1 : index to i32
    %5 = arith.muli %3, %4 : i32
    %from_elements = tensor.from_elements %2, %5 {disc.shape_op = true} : tensor<2xi32>
    %6 = mhlo.dynamic_reshape %1, %from_elements {disc.device = "gpu"} : (tensor<?x?x?xf32, [@S0, @S1, @S2]>, tensor<2xi32>) -> tensor<?x?xf32, [@S0, @S3]>
    %7 = mhlo.reduce(%6 init: %0) applies mhlo.maximum across dimensions = [0] : (tensor<?x?xf32, [@S0, @S3]>, tensor<f32>) -> tensor<?xf32, [@S3]>
    %from_elements_2 = tensor.from_elements %3, %4 {disc.shape_op = true} : tensor<2xi32>
    %8 = mhlo.dynamic_reshape %7, %from_elements_2 {disc.device = "gpu"} : (tensor<?xf32, [@S3]>, tensor<2xi32>) -> tensor<?x?xf32, [@S1, @S2]>
    return %8 : tensor<?x?xf32, [@S1, @S2]>
  }
  "disc_shape.SymbolicDim"() {knownNegativeOne = false, knownNonNegative = true, knownNonSizeOne = false, knownNonSizeZero = false, sym_name = "S0", value = -9223372036854775808 : i64} : () -> ()
  "disc_shape.SymbolicDim"() {knownNegativeOne = false, knownNonNegative = true, knownNonSizeOne = false, knownNonSizeZero = false, sym_name = "S1", value = -9223372036854775808 : i64} : () -> ()
  "disc_shape.SymbolicDim"() {knownNegativeOne = false, knownNonNegative = true, knownNonSizeOne = false, knownNonSizeZero = false, sym_name = "S2", value = -9223372036854775808 : i64} : () -> ()
  "disc_shape.SymbolicDim"() {knownNegativeOne = false, knownNonNegative = true, knownNonSizeOne = false, knownNonSizeZero = false, sym_name = "S3", value = -9223372036854775808 : i64} : () -> ()
  func.func @shape_constraint_graph() {
    %0 = "disc_shape.dim"() {name = @S3} : () -> index
    %1 = "disc_shape.dim"() {name = @S1} : () -> index
    %2 = "disc_shape.dim"() {name = @S2} : () -> index
    "disc_shape.tie_product_equal"(%0, %1, %2) {operand_segment_sizes = array<i32: 1, 2>} : (index, index, index) -> ()
    return
  }
}


SymbolicDimMgr::save walkRankedTensorValue takes: 5 us
SymbolicDimMgr::save update attributes takes: 1 us
SymbolicDimMgr::updateProductEqualityMap simplifySymbolicDimProductPair takes: 20 us
productSet.size() = 4
SymbolicDimMgr::updateProductEqualityMap propagate graph takes: 5 us
SymbolicDimMgr::updateProductEqualityMap remove multiply takes: 32 us
SymbolicDimMgr::updateProductEqualityMap build toRemove  takes: 3 us
SymbolicDimMgr::updateProductEqualityMap apply toRemove  takes: 2 us
SymbolicDimMgr::save updateProductEqualityMap takes: 77 us
SymbolicDimMgr::save updateFunctionType takes: 1 us
SymbolicDimMgr::save collect symbolicDim ops takes: 5 us
SymbolicDimMgr::save remove symbolicDim ops takes: 1 us
SymbolicDimMgr::save remove unused production takes: 0 us
SymbolicDimMgr::save remove unused production #2 takes: 1 us
SymbolicDimMgr::save canonicalize the name takes: 3 us
SymbolicDimMgr::save replace the name takes: 5 us
SymbolicDimMgr::save updateFunctionType takes: 0 us
// -----// IR Dump After DiscShapeOptimizationPass (disc-shape-optimization) //----- //
module {
  func.func @main(%arg0: tensor<?x?x?xf32>) -> tensor<?x?xf32> attributes {tf.entry_function = {input_placements = "gpu", inputs = "input0", output_placements = "gpu", outputs = "output0"}} {
    %c0 = arith.constant 0 : index
    %c1 = arith.constant 1 : index
    %c2 = arith.constant 2 : index
    %0 = mhlo.constant {disc.device = "gpu"} dense<0xFF800000> : tensor<f32>
    %dim = tensor.dim %arg0, %c0 : tensor<?x?x?xf32>
    %dim_0 = tensor.dim %arg0, %c1 : tensor<?x?x?xf32>
    %dim_1 = tensor.dim %arg0, %c2 : tensor<?x?x?xf32>
    %1 = "disc_shape.tie_shape"(%arg0, %dim, %dim_0, %dim_1) {kDiscSymbolicDimAttr = [@S0, @S1, @S2]} : (tensor<?x?x?xf32>, index, index, index) -> tensor<?x?x?xf32>
    %2 = mhlo.abs %1 {disc.device = "gpu"} : tensor<?x?x?xf32>
    %3 = "disc_shape.tie_shape"(%2, %dim, %dim_0, %dim_1) {kDiscSymbolicDimAttr = [@S0, @S1, @S2]} : (tensor<?x?x?xf32>, index, index, index) -> tensor<?x?x?xf32>
    %4 = arith.index_cast %dim : index to i32
    %5 = arith.index_cast %dim_0 : index to i32
    %6 = arith.index_cast %dim_1 : index to i32
    %7 = arith.muli %5, %6 : i32
    %from_elements = tensor.from_elements %4, %7 {disc.shape_op = true} : tensor<2xi32>
    %8 = "disc_shape.tie_shape"(%from_elements, %c2) : (tensor<2xi32>, index) -> tensor<2xi32>
    %9 = arith.index_cast %7 : i32 to index
    %10 = mhlo.dynamic_reshape %3, %8 {disc.device = "gpu"} : (tensor<?x?x?xf32>, tensor<2xi32>) -> tensor<?x?xf32>
    %11 = "disc_shape.tie_shape"(%10, %dim, %9) {kDiscSymbolicDimAttr = [@S0, @S3]} : (tensor<?x?xf32>, index, index) -> tensor<?x?xf32>
    %12 = mhlo.reduce(%11 init: %0) applies mhlo.maximum across dimensions = [0] : (tensor<?x?xf32>, tensor<f32>) -> tensor<?xf32>
    %13 = "disc_shape.tie_shape"(%12, %9) {kDiscSymbolicDimAttr = [@S3]} : (tensor<?xf32>, index) -> tensor<?xf32>
    %from_elements_2 = tensor.from_elements %5, %6 {disc.shape_op = true} : tensor<2xi32>
    %14 = "disc_shape.tie_shape"(%from_elements_2, %c2) : (tensor<2xi32>, index) -> tensor<2xi32>
    %15 = mhlo.dynamic_reshape %13, %14 {disc.device = "gpu"} : (tensor<?xf32>, tensor<2xi32>) -> tensor<?x?xf32>
    %16 = "disc_shape.tie_shape"(%15, %dim_0, %dim_1) {kDiscSymbolicDimAttr = [@S1, @S2]} : (tensor<?x?xf32>, index, index) -> tensor<?x?xf32>
    return %16 : tensor<?x?xf32>
  }
  "disc_shape.SymbolicDim"() {knownNegativeOne = false, knownNonNegative = true, knownNonSizeOne = false, knownNonSizeZero = false, sym_name = "S0", value = -9223372036854775808 : i64} : () -> ()
  "disc_shape.SymbolicDim"() {knownNegativeOne = false, knownNonNegative = true, knownNonSizeOne = false, knownNonSizeZero = false, sym_name = "S1", value = -9223372036854775808 : i64} : () -> ()
  "disc_shape.SymbolicDim"() {knownNegativeOne = false, knownNonNegative = true, knownNonSizeOne = false, knownNonSizeZero = false, sym_name = "S2", value = -9223372036854775808 : i64} : () -> ()
  "disc_shape.SymbolicDim"() {knownNegativeOne = false, knownNonNegative = true, knownNonSizeOne = false, knownNonSizeZero = false, sym_name = "S3", value = -9223372036854775808 : i64} : () -> ()
  func.func @shape_constraint_graph() {
    %0 = "disc_shape.dim"() {name = @S3} : () -> index
    %1 = "disc_shape.dim"() {name = @S1} : () -> index
    %2 = "disc_shape.dim"() {name = @S2} : () -> index
    "disc_shape.tie_product_equal"(%0, %1, %2) {operand_segment_sizes = array<i32: 1, 2>} : (index, index, index) -> ()
    return
  }
}


// -----// IR Dump After Canonicalizer (canonicalize) //----- //
func.func @main(%arg0: tensor<?x?x?xf32>) -> tensor<?x?xf32> attributes {tf.entry_function = {input_placements = "gpu", inputs = "input0", output_placements = "gpu", outputs = "output0"}} {
  %c0 = arith.constant 0 : index
  %c1 = arith.constant 1 : index
  %c2 = arith.constant 2 : index
  %0 = mhlo.constant {disc.device = "gpu"} dense<0xFF800000> : tensor<f32>
  %dim = tensor.dim %arg0, %c0 : tensor<?x?x?xf32>
  %dim_0 = tensor.dim %arg0, %c1 : tensor<?x?x?xf32>
  %dim_1 = tensor.dim %arg0, %c2 : tensor<?x?x?xf32>
  %1 = "disc_shape.tie_shape"(%arg0, %dim, %dim_0, %dim_1) {kDiscSymbolicDimAttr = [@S0, @S1, @S2]} : (tensor<?x?x?xf32>, index, index, index) -> tensor<?x?x?xf32>
  %2 = mhlo.abs %1 {disc.device = "gpu"} : tensor<?x?x?xf32>
  %3 = "disc_shape.tie_shape"(%2, %dim, %dim_0, %dim_1) {kDiscSymbolicDimAttr = [@S0, @S1, @S2]} : (tensor<?x?x?xf32>, index, index, index) -> tensor<?x?x?xf32>
  %4 = arith.index_cast %dim : index to i32
  %5 = arith.index_cast %dim_0 : index to i32
  %6 = arith.index_cast %dim_1 : index to i32
  %7 = arith.muli %5, %6 : i32
  %from_elements = tensor.from_elements %4, %7 {disc.shape_op = true} : tensor<2xi32>
  %8 = arith.index_cast %7 : i32 to index
  %9 = mhlo.dynamic_reshape %3, %from_elements {disc.device = "gpu"} : (tensor<?x?x?xf32>, tensor<2xi32>) -> tensor<?x?xf32>
  %10 = "disc_shape.tie_shape"(%9, %dim, %8) {kDiscSymbolicDimAttr = [@S0, @S3]} : (tensor<?x?xf32>, index, index) -> tensor<?x?xf32>
  %11 = mhlo.reduce(%10 init: %0) applies mhlo.maximum across dimensions = [0] : (tensor<?x?xf32>, tensor<f32>) -> tensor<?xf32>
  %12 = "disc_shape.tie_shape"(%11, %8) {kDiscSymbolicDimAttr = [@S3]} : (tensor<?xf32>, index) -> tensor<?xf32>
  %from_elements_2 = tensor.from_elements %5, %6 {disc.shape_op = true} : tensor<2xi32>
  %13 = mhlo.dynamic_reshape %12, %from_elements_2 {disc.device = "gpu"} : (tensor<?xf32>, tensor<2xi32>) -> tensor<?x?xf32>
  %14 = "disc_shape.tie_shape"(%13, %dim_0, %dim_1) {kDiscSymbolicDimAttr = [@S1, @S2]} : (tensor<?x?xf32>, index, index) -> tensor<?x?xf32>
  return %14 : tensor<?x?xf32>
}

// -----// IR Dump After FuncBufferize (func-bufferize) //----- //
module {
  func.func @main(%arg0: memref<?x?x?xf32>) -> memref<?x?xf32> attributes {tf.entry_function = {input_placements = "gpu", inputs = "input0", output_placements = "gpu", outputs = "output0"}} {
    %0 = bufferization.to_tensor %arg0 : memref<?x?x?xf32>
    %c0 = arith.constant 0 : index
    %c1 = arith.constant 1 : index
    %c2 = arith.constant 2 : index
    %1 = mhlo.constant {disc.device = "gpu"} dense<0xFF800000> : tensor<f32>
    %dim = tensor.dim %0, %c0 : tensor<?x?x?xf32>
    %dim_0 = tensor.dim %0, %c1 : tensor<?x?x?xf32>
    %dim_1 = tensor.dim %0, %c2 : tensor<?x?x?xf32>
    %2 = "disc_shape.tie_shape"(%0, %dim, %dim_0, %dim_1) {kDiscSymbolicDimAttr = [@S0, @S1, @S2]} : (tensor<?x?x?xf32>, index, index, index) -> tensor<?x?x?xf32>
    %3 = mhlo.abs %2 {disc.device = "gpu"} : tensor<?x?x?xf32>
    %4 = "disc_shape.tie_shape"(%3, %dim, %dim_0, %dim_1) {kDiscSymbolicDimAttr = [@S0, @S1, @S2]} : (tensor<?x?x?xf32>, index, index, index) -> tensor<?x?x?xf32>
    %5 = arith.index_cast %dim : index to i32
    %6 = arith.index_cast %dim_0 : index to i32
    %7 = arith.index_cast %dim_1 : index to i32
    %8 = arith.muli %6, %7 : i32
    %from_elements = tensor.from_elements %5, %8 {disc.shape_op = true} : tensor<2xi32>
    %9 = arith.index_cast %8 : i32 to index
    %10 = mhlo.dynamic_reshape %4, %from_elements {disc.device = "gpu"} : (tensor<?x?x?xf32>, tensor<2xi32>) -> tensor<?x?xf32>
    %11 = "disc_shape.tie_shape"(%10, %dim, %9) {kDiscSymbolicDimAttr = [@S0, @S3]} : (tensor<?x?xf32>, index, index) -> tensor<?x?xf32>
    %12 = mhlo.reduce(%11 init: %1) applies mhlo.maximum across dimensions = [0] : (tensor<?x?xf32>, tensor<f32>) -> tensor<?xf32>
    %13 = "disc_shape.tie_shape"(%12, %9) {kDiscSymbolicDimAttr = [@S3]} : (tensor<?xf32>, index) -> tensor<?xf32>
    %from_elements_2 = tensor.from_elements %6, %7 {disc.shape_op = true} : tensor<2xi32>
    %14 = mhlo.dynamic_reshape %13, %from_elements_2 {disc.device = "gpu"} : (tensor<?xf32>, tensor<2xi32>) -> tensor<?x?xf32>
    %15 = "disc_shape.tie_shape"(%14, %dim_0, %dim_1) {kDiscSymbolicDimAttr = [@S1, @S2]} : (tensor<?x?xf32>, index, index) -> tensor<?x?xf32>
    %16 = bufferization.to_memref %15 : memref<?x?xf32>
    return %16 : memref<?x?xf32>
  }
  "disc_shape.SymbolicDim"() {knownNegativeOne = false, knownNonNegative = true, knownNonSizeOne = false, knownNonSizeZero = false, sym_name = "S0", value = -9223372036854775808 : i64} : () -> ()
  "disc_shape.SymbolicDim"() {knownNegativeOne = false, knownNonNegative = true, knownNonSizeOne = false, knownNonSizeZero = false, sym_name = "S1", value = -9223372036854775808 : i64} : () -> ()
  "disc_shape.SymbolicDim"() {knownNegativeOne = false, knownNonNegative = true, knownNonSizeOne = false, knownNonSizeZero = false, sym_name = "S2", value = -9223372036854775808 : i64} : () -> ()
  "disc_shape.SymbolicDim"() {knownNegativeOne = false, knownNonNegative = true, knownNonSizeOne = false, knownNonSizeZero = false, sym_name = "S3", value = -9223372036854775808 : i64} : () -> ()
  func.func @shape_constraint_graph() {
    %0 = "disc_shape.dim"() {name = @S3} : () -> index
    %1 = "disc_shape.dim"() {name = @S1} : () -> index
    %2 = "disc_shape.dim"() {name = @S2} : () -> index
    "disc_shape.tie_product_equal"(%0, %1, %2) {operand_segment_sizes = array<i32: 1, 2>} : (index, index, index) -> ()
    return
  }
}


// -----// IR Dump After DiscHloLegalizeToLhloPass (disc-hlo-legalize-to-lhlo) //----- //
module {
  func.func @main(%arg0: memref<?x?x?xf32>) -> memref<?x?xf32> attributes {tf.entry_function = {input_placements = "gpu", inputs = "input0", output_placements = "gpu", outputs = "output0"}} {
    %0 = bufferization.to_tensor %arg0 : memref<?x?x?xf32>
    %1 = bufferization.to_memref %0 : memref<?x?x?xf32>
    %c0 = arith.constant 0 : index
    %c1 = arith.constant 1 : index
    %c2 = arith.constant 2 : index
    %2 = mhlo.constant {disc.device = "gpu"} dense<0xFF800000> : tensor<f32>
    %dim = tensor.dim %0, %c0 : tensor<?x?x?xf32>
    %dim_0 = tensor.dim %0, %c1 : tensor<?x?x?xf32>
    %dim_1 = tensor.dim %0, %c2 : tensor<?x?x?xf32>
    %c1_2 = arith.constant 1 : index
    %3 = arith.muli %c1_2, %dim_1 : index
    %4 = arith.muli %3, %dim_0 : index
    %5 = arith.muli %4, %dim : index
    %reinterpret_cast = memref.reinterpret_cast %1 to offset: [0], sizes: [%dim, %dim_0, %dim_1], strides: [%4, %3, 1] {kDiscSymbolicDimAttr = [@S0, @S1, @S2]} : memref<?x?x?xf32> to memref<?x?x?xf32>
    %6 = bufferization.to_tensor %reinterpret_cast : memref<?x?x?xf32>
    %7 = mhlo.abs %6 {disc.device = "gpu"} : tensor<?x?x?xf32>
    %8 = bufferization.to_memref %7 : memref<?x?x?xf32>
    %c1_3 = arith.constant 1 : index
    %9 = arith.muli %c1_3, %dim_1 : index
    %10 = arith.muli %9, %dim_0 : index
    %11 = arith.muli %10, %dim : index
    %reinterpret_cast_4 = memref.reinterpret_cast %8 to offset: [0], sizes: [%dim, %dim_0, %dim_1], strides: [%10, %9, 1] {kDiscSymbolicDimAttr = [@S0, @S1, @S2]} : memref<?x?x?xf32> to memref<?x?x?xf32>
    %12 = bufferization.to_tensor %reinterpret_cast_4 : memref<?x?x?xf32>
    %13 = arith.index_cast %dim : index to i32
    %14 = arith.index_cast %dim_0 : index to i32
    %15 = arith.index_cast %dim_1 : index to i32
    %16 = arith.muli %14, %15 : i32
    %from_elements = tensor.from_elements %13, %16 {disc.shape_op = true} : tensor<2xi32>
    %17 = arith.index_cast %16 : i32 to index
    %18 = mhlo.dynamic_reshape %12, %from_elements {disc.device = "gpu"} : (tensor<?x?x?xf32>, tensor<2xi32>) -> tensor<?x?xf32>
    %19 = bufferization.to_memref %18 : memref<?x?xf32>
    %c1_5 = arith.constant 1 : index
    %20 = arith.muli %c1_5, %17 : index
    %21 = arith.muli %20, %dim : index
    %reinterpret_cast_6 = memref.reinterpret_cast %19 to offset: [0], sizes: [%dim, %17], strides: [%20, 1] {kDiscSymbolicDimAttr = [@S0, @S3]} : memref<?x?xf32> to memref<?x?xf32>
    %22 = bufferization.to_tensor %reinterpret_cast_6 : memref<?x?xf32>
    %23 = mhlo.reduce(%22 init: %2) applies mhlo.maximum across dimensions = [0] : (tensor<?x?xf32>, tensor<f32>) -> tensor<?xf32>
    %24 = bufferization.to_memref %23 : memref<?xf32>
    %c1_7 = arith.constant 1 : index
    %25 = arith.muli %c1_7, %17 : index
    %reinterpret_cast_8 = memref.reinterpret_cast %24 to offset: [0], sizes: [%17], strides: [1] {kDiscSymbolicDimAttr = [@S3]} : memref<?xf32> to memref<?xf32>
    %26 = bufferization.to_tensor %reinterpret_cast_8 : memref<?xf32>
    %from_elements_9 = tensor.from_elements %14, %15 {disc.shape_op = true} : tensor<2xi32>
    %27 = mhlo.dynamic_reshape %26, %from_elements_9 {disc.device = "gpu"} : (tensor<?xf32>, tensor<2xi32>) -> tensor<?x?xf32>
    %28 = bufferization.to_memref %27 : memref<?x?xf32>
    %c1_10 = arith.constant 1 : index
    %29 = arith.muli %c1_10, %dim_1 : index
    %30 = arith.muli %29, %dim_0 : index
    %reinterpret_cast_11 = memref.reinterpret_cast %28 to offset: [0], sizes: [%dim_0, %dim_1], strides: [%29, 1] {kDiscSymbolicDimAttr = [@S1, @S2]} : memref<?x?xf32> to memref<?x?xf32>
    %31 = bufferization.to_tensor %reinterpret_cast_11 : memref<?x?xf32>
    %32 = bufferization.to_memref %31 : memref<?x?xf32>
    return %32 : memref<?x?xf32>
  }
  "disc_shape.SymbolicDim"() {knownNegativeOne = false, knownNonNegative = true, knownNonSizeOne = false, knownNonSizeZero = false, sym_name = "S0", value = -9223372036854775808 : i64} : () -> ()
  "disc_shape.SymbolicDim"() {knownNegativeOne = false, knownNonNegative = true, knownNonSizeOne = false, knownNonSizeZero = false, sym_name = "S1", value = -9223372036854775808 : i64} : () -> ()
  "disc_shape.SymbolicDim"() {knownNegativeOne = false, knownNonNegative = true, knownNonSizeOne = false, knownNonSizeZero = false, sym_name = "S2", value = -9223372036854775808 : i64} : () -> ()
  "disc_shape.SymbolicDim"() {knownNegativeOne = false, knownNonNegative = true, knownNonSizeOne = false, knownNonSizeZero = false, sym_name = "S3", value = -9223372036854775808 : i64} : () -> ()
  func.func @shape_constraint_graph() {
    %0 = "disc_shape.dim"() {name = @S3} : () -> index
    %1 = "disc_shape.dim"() {name = @S1} : () -> index
    %2 = "disc_shape.dim"() {name = @S2} : () -> index
    "disc_shape.tie_product_equal"(%0, %1, %2) {operand_segment_sizes = array<i32: 1, 2>} : (index, index, index) -> ()
    return
  }
}


// -----// IR Dump After HloLegalizeToLhloPass (hlo-legalize-to-lhlo) //----- //
module {
  func.func @main(%arg0: memref<?x?x?xf32>) -> memref<?x?xf32> attributes {tf.entry_function = {input_placements = "gpu", inputs = "input0", output_placements = "gpu", outputs = "output0"}} {
    %0 = bufferization.to_tensor %arg0 : memref<?x?x?xf32>
    %c0 = arith.constant 0 : index
    %c1 = arith.constant 1 : index
    %c2 = arith.constant 2 : index
    %alloc = memref.alloc() : memref<f32>
    "lmhlo.constant"(%alloc) {disc.device = "gpu", value = dense<0xFF800000> : tensor<f32>} : (memref<f32>) -> ()
    %dim = tensor.dim %0, %c0 : tensor<?x?x?xf32>
    %dim_0 = tensor.dim %0, %c1 : tensor<?x?x?xf32>
    %dim_1 = tensor.dim %0, %c2 : tensor<?x?x?xf32>
    %c1_2 = arith.constant 1 : index
    %1 = arith.muli %c1_2, %dim_1 : index
    %2 = arith.muli %1, %dim_0 : index
    %3 = arith.muli %2, %dim : index
    %reinterpret_cast = memref.reinterpret_cast %arg0 to offset: [0], sizes: [%dim, %dim_0, %dim_1], strides: [%2, %1, 1] {kDiscSymbolicDimAttr = [@S0, @S1, @S2]} : memref<?x?x?xf32> to memref<?x?x?xf32>
    %4 = bufferization.to_tensor %reinterpret_cast : memref<?x?x?xf32>
    %5 = bufferization.to_memref %4 : memref<?x?x?xf32>
    %6 = bufferization.to_tensor %5 : memref<?x?x?xf32>
    %7 = shape.shape_of %6 : tensor<?x?x?xf32> -> tensor<3xindex>
    %c0_3 = arith.constant 0 : index
    %extracted = tensor.extract %7[%c0_3] : tensor<3xindex>
    %c1_4 = arith.constant 1 : index
    %extracted_5 = tensor.extract %7[%c1_4] : tensor<3xindex>
    %c2_6 = arith.constant 2 : index
    %extracted_7 = tensor.extract %7[%c2_6] : tensor<3xindex>
    %alloc_8 = memref.alloc(%extracted, %extracted_5, %extracted_7) : memref<?x?x?xf32>
    "lmhlo.abs"(%5, %alloc_8) {disc.device = "gpu"} : (memref<?x?x?xf32>, memref<?x?x?xf32>) -> ()
    %8 = bufferization.to_tensor %alloc_8 : memref<?x?x?xf32>
    %9 = bufferization.to_memref %8 : memref<?x?x?xf32>
    %c1_9 = arith.constant 1 : index
    %10 = arith.muli %c1_9, %dim_1 : index
    %11 = arith.muli %10, %dim_0 : index
    %12 = arith.muli %11, %dim : index
    %reinterpret_cast_10 = memref.reinterpret_cast %9 to offset: [0], sizes: [%dim, %dim_0, %dim_1], strides: [%11, %10, 1] {kDiscSymbolicDimAttr = [@S0, @S1, @S2]} : memref<?x?x?xf32> to memref<?x?x?xf32>
    %13 = bufferization.to_tensor %reinterpret_cast_10 : memref<?x?x?xf32>
    %14 = bufferization.to_memref %13 : memref<?x?x?xf32>
    %15 = arith.index_cast %dim : index to i32
    %16 = arith.index_cast %dim_0 : index to i32
    %17 = arith.index_cast %dim_1 : index to i32
    %18 = arith.muli %16, %17 : i32
    %from_elements = tensor.from_elements %15, %18 {disc.shape_op = true} : tensor<2xi32>
    %19 = bufferization.to_memref %from_elements : memref<2xi32>
    %20 = arith.index_cast %18 : i32 to index
    %21 = bufferization.to_tensor %14 : memref<?x?x?xf32>
    %22 = bufferization.to_tensor %19 : memref<2xi32>
    %23 = arith.index_cast %22 : tensor<2xi32> to tensor<2xindex>
    %c0_11 = arith.constant 0 : index
    %extracted_12 = tensor.extract %23[%c0_11] : tensor<2xindex>
    %c1_13 = arith.constant 1 : index
    %extracted_14 = tensor.extract %23[%c1_13] : tensor<2xindex>
    %alloc_15 = memref.alloc(%extracted_12, %extracted_14) : memref<?x?xf32>
    "lmhlo.dynamic_reshape"(%14, %19, %alloc_15) {disc.device = "gpu"} : (memref<?x?x?xf32>, memref<2xi32>, memref<?x?xf32>) -> ()
    %24 = bufferization.to_tensor %alloc_15 : memref<?x?xf32>
    %25 = bufferization.to_memref %24 : memref<?x?xf32>
    %c1_16 = arith.constant 1 : index
    %26 = arith.muli %c1_16, %20 : index
    %27 = arith.muli %26, %dim : index
    %reinterpret_cast_17 = memref.reinterpret_cast %25 to offset: [0], sizes: [%dim, %20], strides: [%26, 1] {kDiscSymbolicDimAttr = [@S0, @S3]} : memref<?x?xf32> to memref<?x?xf32>
    %28 = bufferization.to_tensor %reinterpret_cast_17 : memref<?x?xf32>
    %29 = bufferization.to_memref %28 : memref<?x?xf32>
    %30 = bufferization.to_tensor %29 : memref<?x?xf32>
    %31 = bufferization.to_tensor %alloc : memref<f32>
    %c1_18 = arith.constant 1 : index
    %dim_19 = tensor.dim %30, %c1_18 : tensor<?x?xf32>
    %from_elements_20 = tensor.from_elements %dim_19 : tensor<1xindex>
    %c0_21 = arith.constant 0 : index
    %extracted_22 = tensor.extract %from_elements_20[%c0_21] : tensor<1xindex>
    %alloc_23 = memref.alloc(%extracted_22) : memref<?xf32>
    "lmhlo.reduce"(%29, %alloc, %alloc_23) ({
    ^bb0(%arg1: memref<f32>, %arg2: memref<f32>, %arg3: memref<f32>):
      %alloc_34 = memref.alloc() : memref<f32>
      "lmhlo.maximum"(%arg1, %arg2, %alloc_34) {disc.device = "gpu"} : (memref<f32>, memref<f32>, memref<f32>) -> ()
      "lmhlo.copy"(%alloc_34, %arg3) : (memref<f32>, memref<f32>) -> ()
      "lmhlo.terminator"() : () -> ()
    }) {dimensions = dense<0> : tensor<1xi64>, disc.device = "gpu"} : (memref<?x?xf32>, memref<f32>, memref<?xf32>) -> ()
    %32 = bufferization.to_tensor %alloc_23 : memref<?xf32>
    %33 = bufferization.to_memref %32 : memref<?xf32>
    %c1_24 = arith.constant 1 : index
    %34 = arith.muli %c1_24, %20 : index
    %reinterpret_cast_25 = memref.reinterpret_cast %33 to offset: [0], sizes: [%20], strides: [1] {kDiscSymbolicDimAttr = [@S3]} : memref<?xf32> to memref<?xf32>
    %35 = bufferization.to_tensor %reinterpret_cast_25 : memref<?xf32>
    %36 = bufferization.to_memref %35 : memref<?xf32>
    %from_elements_26 = tensor.from_elements %16, %17 {disc.shape_op = true} : tensor<2xi32>
    %37 = bufferization.to_memref %from_elements_26 : memref<2xi32>
    %38 = bufferization.to_tensor %36 : memref<?xf32>
    %39 = bufferization.to_tensor %37 : memref<2xi32>
    %40 = arith.index_cast %39 : tensor<2xi32> to tensor<2xindex>
    %c0_27 = arith.constant 0 : index
    %extracted_28 = tensor.extract %40[%c0_27] : tensor<2xindex>
    %c1_29 = arith.constant 1 : index
    %extracted_30 = tensor.extract %40[%c1_29] : tensor<2xindex>
    %alloc_31 = memref.alloc(%extracted_28, %extracted_30) : memref<?x?xf32>
    "lmhlo.dynamic_reshape"(%36, %37, %alloc_31) {disc.device = "gpu"} : (memref<?xf32>, memref<2xi32>, memref<?x?xf32>) -> ()
    %41 = bufferization.to_tensor %alloc_31 : memref<?x?xf32>
    %42 = bufferization.to_memref %41 : memref<?x?xf32>
    %c1_32 = arith.constant 1 : index
    %43 = arith.muli %c1_32, %dim_1 : index
    %44 = arith.muli %43, %dim_0 : index
    %reinterpret_cast_33 = memref.reinterpret_cast %42 to offset: [0], sizes: [%dim_0, %dim_1], strides: [%43, 1] {kDiscSymbolicDimAttr = [@S1, @S2]} : memref<?x?xf32> to memref<?x?xf32>
    return %reinterpret_cast_33 : memref<?x?xf32>
  }
  "disc_shape.SymbolicDim"() {knownNegativeOne = false, knownNonNegative = true, knownNonSizeOne = false, knownNonSizeZero = false, sym_name = "S0", value = -9223372036854775808 : i64} : () -> ()
  "disc_shape.SymbolicDim"() {knownNegativeOne = false, knownNonNegative = true, knownNonSizeOne = false, knownNonSizeZero = false, sym_name = "S1", value = -9223372036854775808 : i64} : () -> ()
  "disc_shape.SymbolicDim"() {knownNegativeOne = false, knownNonNegative = true, knownNonSizeOne = false, knownNonSizeZero = false, sym_name = "S2", value = -9223372036854775808 : i64} : () -> ()
  "disc_shape.SymbolicDim"() {knownNegativeOne = false, knownNonNegative = true, knownNonSizeOne = false, knownNonSizeZero = false, sym_name = "S3", value = -9223372036854775808 : i64} : () -> ()
  func.func @shape_constraint_graph() {
    %0 = "disc_shape.dim"() {name = @S3} : () -> index
    %1 = "disc_shape.dim"() {name = @S1} : () -> index
    %2 = "disc_shape.dim"() {name = @S2} : () -> index
    "disc_shape.tie_product_equal"(%0, %1, %2) {operand_segment_sizes = array<i32: 1, 2>} : (index, index, index) -> ()
    return
  }
}


// -----// IR Dump After Canonicalizer (canonicalize) //----- //
func.func @main(%arg0: memref<?x?x?xf32>) -> memref<?x?xf32> attributes {tf.entry_function = {input_placements = "gpu", inputs = "input0", output_placements = "gpu", outputs = "output0"}} {
  %c2 = arith.constant 2 : index
  %c1 = arith.constant 1 : index
  %c0 = arith.constant 0 : index
  %alloc = memref.alloc() : memref<f32>
  "lmhlo.constant"(%alloc) {disc.device = "gpu", value = dense<0xFF800000> : tensor<f32>} : (memref<f32>) -> ()
  %dim = memref.dim %arg0, %c0 : memref<?x?x?xf32>
  %dim_0 = memref.dim %arg0, %c1 : memref<?x?x?xf32>
  %dim_1 = memref.dim %arg0, %c2 : memref<?x?x?xf32>
  %0 = arith.muli %dim_1, %dim_0 : index
  %reinterpret_cast = memref.reinterpret_cast %arg0 to offset: [0], sizes: [%dim, %dim_0, %dim_1], strides: [%0, %dim_1, 1] {kDiscSymbolicDimAttr = [@S0, @S1, @S2]} : memref<?x?x?xf32> to memref<?x?x?xf32>
  %alloc_2 = memref.alloc(%dim, %dim_0, %dim_1) : memref<?x?x?xf32>
  "lmhlo.abs"(%reinterpret_cast, %alloc_2) {disc.device = "gpu"} : (memref<?x?x?xf32>, memref<?x?x?xf32>) -> ()
  %1 = arith.muli %dim_1, %dim_0 : index
  %reinterpret_cast_3 = memref.reinterpret_cast %alloc_2 to offset: [0], sizes: [%dim, %dim_0, %dim_1], strides: [%1, %dim_1, 1] {kDiscSymbolicDimAttr = [@S0, @S1, @S2]} : memref<?x?x?xf32> to memref<?x?x?xf32>
  %2 = arith.index_cast %dim : index to i32
  %3 = arith.index_cast %dim_0 : index to i32
  %4 = arith.index_cast %dim_1 : index to i32
  %5 = arith.muli %3, %4 : i32
  %from_elements = tensor.from_elements %2, %5 {disc.shape_op = true} : tensor<2xi32>
  %6 = bufferization.to_memref %from_elements : memref<2xi32>
  %7 = arith.index_cast %5 : i32 to index
  %8 = bufferization.to_tensor %6 : memref<2xi32>
  %extracted = tensor.extract %8[%c0] : tensor<2xi32>
  %9 = arith.index_cast %extracted : i32 to index
  %extracted_4 = tensor.extract %8[%c1] : tensor<2xi32>
  %10 = arith.index_cast %extracted_4 : i32 to index
  %alloc_5 = memref.alloc(%9, %10) : memref<?x?xf32>
  "lmhlo.dynamic_reshape"(%reinterpret_cast_3, %6, %alloc_5) {disc.device = "gpu"} : (memref<?x?x?xf32>, memref<2xi32>, memref<?x?xf32>) -> ()
  %reinterpret_cast_6 = memref.reinterpret_cast %alloc_5 to offset: [0], sizes: [%dim, %7], strides: [%7, 1] {kDiscSymbolicDimAttr = [@S0, @S3]} : memref<?x?xf32> to memref<?x?xf32>
  %alloc_7 = memref.alloc(%7) : memref<?xf32>
  "lmhlo.reduce"(%reinterpret_cast_6, %alloc, %alloc_7) ({
  ^bb0(%arg1: memref<f32>, %arg2: memref<f32>, %arg3: memref<f32>):
    "lmhlo.maximum"(%arg1, %arg2, %arg3) {disc.device = "gpu"} : (memref<f32>, memref<f32>, memref<f32>) -> ()
    "lmhlo.terminator"() : () -> ()
  }) {dimensions = dense<0> : tensor<1xi64>, disc.device = "gpu"} : (memref<?x?xf32>, memref<f32>, memref<?xf32>) -> ()
  %reinterpret_cast_8 = memref.reinterpret_cast %alloc_7 to offset: [0], sizes: [%7], strides: [1] {kDiscSymbolicDimAttr = [@S3]} : memref<?xf32> to memref<?xf32>
  %from_elements_9 = tensor.from_elements %3, %4 {disc.shape_op = true} : tensor<2xi32>
  %11 = bufferization.to_memref %from_elements_9 : memref<2xi32>
  %alloc_10 = memref.alloc(%dim_0, %dim_1) : memref<?x?xf32>
  "lmhlo.dynamic_reshape"(%reinterpret_cast_8, %11, %alloc_10) {disc.device = "gpu"} : (memref<?xf32>, memref<2xi32>, memref<?x?xf32>) -> ()
  %reinterpret_cast_11 = memref.reinterpret_cast %alloc_10 to offset: [0], sizes: [%dim_0, %dim_1], strides: [%dim_1, 1] {kDiscSymbolicDimAttr = [@S1, @S2]} : memref<?x?xf32> to memref<?x?xf32>
  return %reinterpret_cast_11 : memref<?x?xf32>
}

// -----// IR Dump After CSE (cse) //----- //
func.func @main(%arg0: memref<?x?x?xf32>) -> memref<?x?xf32> attributes {tf.entry_function = {input_placements = "gpu", inputs = "input0", output_placements = "gpu", outputs = "output0"}} {
  %c2 = arith.constant 2 : index
  %c1 = arith.constant 1 : index
  %c0 = arith.constant 0 : index
  %alloc = memref.alloc() : memref<f32>
  "lmhlo.constant"(%alloc) {disc.device = "gpu", value = dense<0xFF800000> : tensor<f32>} : (memref<f32>) -> ()
  %dim = memref.dim %arg0, %c0 : memref<?x?x?xf32>
  %dim_0 = memref.dim %arg0, %c1 : memref<?x?x?xf32>
  %dim_1 = memref.dim %arg0, %c2 : memref<?x?x?xf32>
  %0 = arith.muli %dim_1, %dim_0 : index
  %reinterpret_cast = memref.reinterpret_cast %arg0 to offset: [0], sizes: [%dim, %dim_0, %dim_1], strides: [%0, %dim_1, 1] {kDiscSymbolicDimAttr = [@S0, @S1, @S2]} : memref<?x?x?xf32> to memref<?x?x?xf32>
  %alloc_2 = memref.alloc(%dim, %dim_0, %dim_1) : memref<?x?x?xf32>
  "lmhlo.abs"(%reinterpret_cast, %alloc_2) {disc.device = "gpu"} : (memref<?x?x?xf32>, memref<?x?x?xf32>) -> ()
  %reinterpret_cast_3 = memref.reinterpret_cast %alloc_2 to offset: [0], sizes: [%dim, %dim_0, %dim_1], strides: [%0, %dim_1, 1] {kDiscSymbolicDimAttr = [@S0, @S1, @S2]} : memref<?x?x?xf32> to memref<?x?x?xf32>
  %1 = arith.index_cast %dim : index to i32
  %2 = arith.index_cast %dim_0 : index to i32
  %3 = arith.index_cast %dim_1 : index to i32
  %4 = arith.muli %2, %3 : i32
  %from_elements = tensor.from_elements %1, %4 {disc.shape_op = true} : tensor<2xi32>
  %5 = bufferization.to_memref %from_elements : memref<2xi32>
  %6 = arith.index_cast %4 : i32 to index
  %7 = bufferization.to_tensor %5 : memref<2xi32>
  %extracted = tensor.extract %7[%c0] : tensor<2xi32>
  %8 = arith.index_cast %extracted : i32 to index
  %extracted_4 = tensor.extract %7[%c1] : tensor<2xi32>
  %9 = arith.index_cast %extracted_4 : i32 to index
  %alloc_5 = memref.alloc(%8, %9) : memref<?x?xf32>
  "lmhlo.dynamic_reshape"(%reinterpret_cast_3, %5, %alloc_5) {disc.device = "gpu"} : (memref<?x?x?xf32>, memref<2xi32>, memref<?x?xf32>) -> ()
  %reinterpret_cast_6 = memref.reinterpret_cast %alloc_5 to offset: [0], sizes: [%dim, %6], strides: [%6, 1] {kDiscSymbolicDimAttr = [@S0, @S3]} : memref<?x?xf32> to memref<?x?xf32>
  %alloc_7 = memref.alloc(%6) : memref<?xf32>
  "lmhlo.reduce"(%reinterpret_cast_6, %alloc, %alloc_7) ({
  ^bb0(%arg1: memref<f32>, %arg2: memref<f32>, %arg3: memref<f32>):
    "lmhlo.maximum"(%arg1, %arg2, %arg3) {disc.device = "gpu"} : (memref<f32>, memref<f32>, memref<f32>) -> ()
    "lmhlo.terminator"() : () -> ()
  }) {dimensions = dense<0> : tensor<1xi64>, disc.device = "gpu"} : (memref<?x?xf32>, memref<f32>, memref<?xf32>) -> ()
  %reinterpret_cast_8 = memref.reinterpret_cast %alloc_7 to offset: [0], sizes: [%6], strides: [1] {kDiscSymbolicDimAttr = [@S3]} : memref<?xf32> to memref<?xf32>
  %from_elements_9 = tensor.from_elements %2, %3 {disc.shape_op = true} : tensor<2xi32>
  %10 = bufferization.to_memref %from_elements_9 : memref<2xi32>
  %alloc_10 = memref.alloc(%dim_0, %dim_1) : memref<?x?xf32>
  "lmhlo.dynamic_reshape"(%reinterpret_cast_8, %10, %alloc_10) {disc.device = "gpu"} : (memref<?xf32>, memref<2xi32>, memref<?x?xf32>) -> ()
  %reinterpret_cast_11 = memref.reinterpret_cast %alloc_10 to offset: [0], sizes: [%dim_0, %dim_1], strides: [%dim_1, 1] {kDiscSymbolicDimAttr = [@S1, @S2]} : memref<?x?xf32> to memref<?x?xf32>
  return %reinterpret_cast_11 : memref<?x?xf32>
}

// -----// IR Dump After LegalizeToTensorOpPass (lhlo-legalize-to-tensor-op) //----- //
func.func @main(%arg0: memref<?x?x?xf32>) -> memref<?x?xf32> attributes {tf.entry_function = {input_placements = "gpu", inputs = "input0", output_placements = "gpu", outputs = "output0"}} {
  %c2 = arith.constant 2 : index
  %c1 = arith.constant 1 : index
  %c0 = arith.constant 0 : index
  %alloc = memref.alloc() : memref<f32>
  "lmhlo.constant"(%alloc) {disc.device = "gpu", value = dense<0xFF800000> : tensor<f32>} : (memref<f32>) -> ()
  %dim = memref.dim %arg0, %c0 : memref<?x?x?xf32>
  %dim_0 = memref.dim %arg0, %c1 : memref<?x?x?xf32>
  %dim_1 = memref.dim %arg0, %c2 : memref<?x?x?xf32>
  %0 = arith.muli %dim_1, %dim_0 : index
  %reinterpret_cast = memref.reinterpret_cast %arg0 to offset: [0], sizes: [%dim, %dim_0, %dim_1], strides: [%0, %dim_1, 1] {kDiscSymbolicDimAttr = [@S0, @S1, @S2]} : memref<?x?x?xf32> to memref<?x?x?xf32>
  %alloc_2 = memref.alloc(%dim, %dim_0, %dim_1) : memref<?x?x?xf32>
  "lmhlo.abs"(%reinterpret_cast, %alloc_2) {disc.device = "gpu"} : (memref<?x?x?xf32>, memref<?x?x?xf32>) -> ()
  %reinterpret_cast_3 = memref.reinterpret_cast %alloc_2 to offset: [0], sizes: [%dim, %dim_0, %dim_1], strides: [%0, %dim_1, 1] {kDiscSymbolicDimAttr = [@S0, @S1, @S2]} : memref<?x?x?xf32> to memref<?x?x?xf32>
  %1 = arith.index_cast %dim : index to i32
  %2 = arith.index_cast %dim_0 : index to i32
  %3 = arith.index_cast %dim_1 : index to i32
  %4 = arith.muli %2, %3 : i32
  %from_elements = tensor.from_elements %1, %4 {disc.shape_op = true} : tensor<2xi32>
  %5 = bufferization.to_memref %from_elements : memref<2xi32>
  %6 = arith.index_cast %4 : i32 to index
  %7 = memref.load %5[%c0] : memref<2xi32>
  %8 = arith.index_cast %7 : i32 to index
  %9 = memref.load %5[%c1] : memref<2xi32>
  %10 = arith.index_cast %9 : i32 to index
  %alloc_4 = memref.alloc(%8, %10) : memref<?x?xf32>
  "lmhlo.dynamic_reshape"(%reinterpret_cast_3, %5, %alloc_4) {disc.device = "gpu"} : (memref<?x?x?xf32>, memref<2xi32>, memref<?x?xf32>) -> ()
  %reinterpret_cast_5 = memref.reinterpret_cast %alloc_4 to offset: [0], sizes: [%dim, %6], strides: [%6, 1] {kDiscSymbolicDimAttr = [@S0, @S3]} : memref<?x?xf32> to memref<?x?xf32>
  %alloc_6 = memref.alloc(%6) : memref<?xf32>
  "lmhlo.reduce"(%reinterpret_cast_5, %alloc, %alloc_6) ({
  ^bb0(%arg1: memref<f32>, %arg2: memref<f32>, %arg3: memref<f32>):
    "lmhlo.maximum"(%arg1, %arg2, %arg3) {disc.device = "gpu"} : (memref<f32>, memref<f32>, memref<f32>) -> ()
    "lmhlo.terminator"() : () -> ()
  }) {dimensions = dense<0> : tensor<1xi64>, disc.device = "gpu"} : (memref<?x?xf32>, memref<f32>, memref<?xf32>) -> ()
  %reinterpret_cast_7 = memref.reinterpret_cast %alloc_6 to offset: [0], sizes: [%6], strides: [1] {kDiscSymbolicDimAttr = [@S3]} : memref<?xf32> to memref<?xf32>
  %from_elements_8 = tensor.from_elements %2, %3 {disc.shape_op = true} : tensor<2xi32>
  %11 = bufferization.to_memref %from_elements_8 : memref<2xi32>
  %alloc_9 = memref.alloc(%dim_0, %dim_1) : memref<?x?xf32>
  "lmhlo.dynamic_reshape"(%reinterpret_cast_7, %11, %alloc_9) {disc.device = "gpu"} : (memref<?xf32>, memref<2xi32>, memref<?x?xf32>) -> ()
  %reinterpret_cast_10 = memref.reinterpret_cast %alloc_9 to offset: [0], sizes: [%dim_0, %dim_1], strides: [%dim_1, 1] {kDiscSymbolicDimAttr = [@S1, @S2]} : memref<?x?xf32> to memref<?x?xf32>
  return %reinterpret_cast_10 : memref<?x?xf32>
}

// -----// IR Dump After Canonicalizer (canonicalize) //----- //
func.func @main(%arg0: memref<?x?x?xf32>) -> memref<?x?xf32> attributes {tf.entry_function = {input_placements = "gpu", inputs = "input0", output_placements = "gpu", outputs = "output0"}} {
  %c2 = arith.constant 2 : index
  %c1 = arith.constant 1 : index
  %c0 = arith.constant 0 : index
  %alloc = memref.alloc() : memref<f32>
  "lmhlo.constant"(%alloc) {disc.device = "gpu", value = dense<0xFF800000> : tensor<f32>} : (memref<f32>) -> ()
  %dim = memref.dim %arg0, %c0 : memref<?x?x?xf32>
  %dim_0 = memref.dim %arg0, %c1 : memref<?x?x?xf32>
  %dim_1 = memref.dim %arg0, %c2 : memref<?x?x?xf32>
  %0 = arith.muli %dim_1, %dim_0 : index
  %reinterpret_cast = memref.reinterpret_cast %arg0 to offset: [0], sizes: [%dim, %dim_0, %dim_1], strides: [%0, %dim_1, 1] {kDiscSymbolicDimAttr = [@S0, @S1, @S2]} : memref<?x?x?xf32> to memref<?x?x?xf32>
  %alloc_2 = memref.alloc(%dim, %dim_0, %dim_1) : memref<?x?x?xf32>
  "lmhlo.abs"(%reinterpret_cast, %alloc_2) {disc.device = "gpu"} : (memref<?x?x?xf32>, memref<?x?x?xf32>) -> ()
  %reinterpret_cast_3 = memref.reinterpret_cast %alloc_2 to offset: [0], sizes: [%dim, %dim_0, %dim_1], strides: [%0, %dim_1, 1] {kDiscSymbolicDimAttr = [@S0, @S1, @S2]} : memref<?x?x?xf32> to memref<?x?x?xf32>
  %1 = arith.index_cast %dim : index to i32
  %2 = arith.index_cast %dim_0 : index to i32
  %3 = arith.index_cast %dim_1 : index to i32
  %4 = arith.muli %2, %3 : i32
  %from_elements = tensor.from_elements %1, %4 {disc.shape_op = true} : tensor<2xi32>
  %5 = bufferization.to_memref %from_elements : memref<2xi32>
  %6 = arith.index_cast %4 : i32 to index
  %7 = arith.index_cast %4 : i32 to index
  %alloc_4 = memref.alloc(%dim, %7) : memref<?x?xf32>
  "lmhlo.dynamic_reshape"(%reinterpret_cast_3, %5, %alloc_4) {disc.device = "gpu"} : (memref<?x?x?xf32>, memref<2xi32>, memref<?x?xf32>) -> ()
  %reinterpret_cast_5 = memref.reinterpret_cast %alloc_4 to offset: [0], sizes: [%dim, %6], strides: [%6, 1] {kDiscSymbolicDimAttr = [@S0, @S3]} : memref<?x?xf32> to memref<?x?xf32>
  %alloc_6 = memref.alloc(%6) : memref<?xf32>
  "lmhlo.reduce"(%reinterpret_cast_5, %alloc, %alloc_6) ({
  ^bb0(%arg1: memref<f32>, %arg2: memref<f32>, %arg3: memref<f32>):
    "lmhlo.maximum"(%arg1, %arg2, %arg3) {disc.device = "gpu"} : (memref<f32>, memref<f32>, memref<f32>) -> ()
    "lmhlo.terminator"() : () -> ()
  }) {dimensions = dense<0> : tensor<1xi64>, disc.device = "gpu"} : (memref<?x?xf32>, memref<f32>, memref<?xf32>) -> ()
  %reinterpret_cast_7 = memref.reinterpret_cast %alloc_6 to offset: [0], sizes: [%6], strides: [1] {kDiscSymbolicDimAttr = [@S3]} : memref<?xf32> to memref<?xf32>
  %from_elements_8 = tensor.from_elements %2, %3 {disc.shape_op = true} : tensor<2xi32>
  %8 = bufferization.to_memref %from_elements_8 : memref<2xi32>
  %alloc_9 = memref.alloc(%dim_0, %dim_1) : memref<?x?xf32>
  "lmhlo.dynamic_reshape"(%reinterpret_cast_7, %8, %alloc_9) {disc.device = "gpu"} : (memref<?xf32>, memref<2xi32>, memref<?x?xf32>) -> ()
  %reinterpret_cast_10 = memref.reinterpret_cast %alloc_9 to offset: [0], sizes: [%dim_0, %dim_1], strides: [%dim_1, 1] {kDiscSymbolicDimAttr = [@S1, @S2]} : memref<?x?xf32> to memref<?x?xf32>
  return %reinterpret_cast_10 : memref<?x?xf32>
}

// -----// IR Dump After TensorBufferize (tensor-bufferize) //----- //
func.func @main(%arg0: memref<?x?x?xf32>) -> memref<?x?xf32> attributes {tf.entry_function = {input_placements = "gpu", inputs = "input0", output_placements = "gpu", outputs = "output0"}} {
  %c2 = arith.constant 2 : index
  %c1 = arith.constant 1 : index
  %c0 = arith.constant 0 : index
  %alloc = memref.alloc() : memref<f32>
  "lmhlo.constant"(%alloc) {disc.device = "gpu", value = dense<0xFF800000> : tensor<f32>} : (memref<f32>) -> ()
  %dim = memref.dim %arg0, %c0 : memref<?x?x?xf32>
  %dim_0 = memref.dim %arg0, %c1 : memref<?x?x?xf32>
  %dim_1 = memref.dim %arg0, %c2 : memref<?x?x?xf32>
  %0 = arith.muli %dim_1, %dim_0 : index
  %reinterpret_cast = memref.reinterpret_cast %arg0 to offset: [0], sizes: [%dim, %dim_0, %dim_1], strides: [%0, %dim_1, 1] {kDiscSymbolicDimAttr = [@S0, @S1, @S2]} : memref<?x?x?xf32> to memref<?x?x?xf32>
  %alloc_2 = memref.alloc(%dim, %dim_0, %dim_1) : memref<?x?x?xf32>
  "lmhlo.abs"(%reinterpret_cast, %alloc_2) {disc.device = "gpu"} : (memref<?x?x?xf32>, memref<?x?x?xf32>) -> ()
  %reinterpret_cast_3 = memref.reinterpret_cast %alloc_2 to offset: [0], sizes: [%dim, %dim_0, %dim_1], strides: [%0, %dim_1, 1] {kDiscSymbolicDimAttr = [@S0, @S1, @S2]} : memref<?x?x?xf32> to memref<?x?x?xf32>
  %1 = arith.index_cast %dim : index to i32
  %2 = arith.index_cast %dim_0 : index to i32
  %3 = arith.index_cast %dim_1 : index to i32
  %4 = arith.muli %2, %3 : i32
  %alloc_4 = memref.alloc() {alignment = 64 : i64} : memref<2xi32>
  %c0_5 = arith.constant 0 : index
  %c1_6 = arith.constant 1 : index
  memref.store %1, %alloc_4[%c0_5] : memref<2xi32>
  memref.store %4, %alloc_4[%c1_6] : memref<2xi32>
  %5 = arith.index_cast %4 : i32 to index
  %6 = arith.index_cast %4 : i32 to index
  %alloc_7 = memref.alloc(%dim, %6) : memref<?x?xf32>
  "lmhlo.dynamic_reshape"(%reinterpret_cast_3, %alloc_4, %alloc_7) {disc.device = "gpu"} : (memref<?x?x?xf32>, memref<2xi32>, memref<?x?xf32>) -> ()
  %reinterpret_cast_8 = memref.reinterpret_cast %alloc_7 to offset: [0], sizes: [%dim, %5], strides: [%5, 1] {kDiscSymbolicDimAttr = [@S0, @S3]} : memref<?x?xf32> to memref<?x?xf32>
  %alloc_9 = memref.alloc(%5) : memref<?xf32>
  "lmhlo.reduce"(%reinterpret_cast_8, %alloc, %alloc_9) ({
  ^bb0(%arg1: memref<f32>, %arg2: memref<f32>, %arg3: memref<f32>):
    "lmhlo.maximum"(%arg1, %arg2, %arg3) {disc.device = "gpu"} : (memref<f32>, memref<f32>, memref<f32>) -> ()
    "lmhlo.terminator"() : () -> ()
  }) {dimensions = dense<0> : tensor<1xi64>, disc.device = "gpu"} : (memref<?x?xf32>, memref<f32>, memref<?xf32>) -> ()
  %reinterpret_cast_10 = memref.reinterpret_cast %alloc_9 to offset: [0], sizes: [%5], strides: [1] {kDiscSymbolicDimAttr = [@S3]} : memref<?xf32> to memref<?xf32>
  %alloc_11 = memref.alloc() {alignment = 64 : i64} : memref<2xi32>
  %c0_12 = arith.constant 0 : index
  %c1_13 = arith.constant 1 : index
  memref.store %2, %alloc_11[%c0_12] : memref<2xi32>
  memref.store %3, %alloc_11[%c1_13] : memref<2xi32>
  %alloc_14 = memref.alloc(%dim_0, %dim_1) : memref<?x?xf32>
  "lmhlo.dynamic_reshape"(%reinterpret_cast_10, %alloc_11, %alloc_14) {disc.device = "gpu"} : (memref<?xf32>, memref<2xi32>, memref<?x?xf32>) -> ()
  %reinterpret_cast_15 = memref.reinterpret_cast %alloc_14 to offset: [0], sizes: [%dim_0, %dim_1], strides: [%dim_1, 1] {kDiscSymbolicDimAttr = [@S1, @S2]} : memref<?x?xf32> to memref<?x?xf32>
  return %reinterpret_cast_15 : memref<?x?xf32>
}

// -----// IR Dump After Canonicalizer (canonicalize) //----- //
func.func @main(%arg0: memref<?x?x?xf32>) -> memref<?x?xf32> attributes {tf.entry_function = {input_placements = "gpu", inputs = "input0", output_placements = "gpu", outputs = "output0"}} {
  %c2 = arith.constant 2 : index
  %c1 = arith.constant 1 : index
  %c0 = arith.constant 0 : index
  %alloc = memref.alloc() : memref<f32>
  "lmhlo.constant"(%alloc) {disc.device = "gpu", value = dense<0xFF800000> : tensor<f32>} : (memref<f32>) -> ()
  %dim = memref.dim %arg0, %c0 : memref<?x?x?xf32>
  %dim_0 = memref.dim %arg0, %c1 : memref<?x?x?xf32>
  %dim_1 = memref.dim %arg0, %c2 : memref<?x?x?xf32>
  %0 = arith.muli %dim_1, %dim_0 : index
  %reinterpret_cast = memref.reinterpret_cast %arg0 to offset: [0], sizes: [%dim, %dim_0, %dim_1], strides: [%0, %dim_1, 1] {kDiscSymbolicDimAttr = [@S0, @S1, @S2]} : memref<?x?x?xf32> to memref<?x?x?xf32>
  %alloc_2 = memref.alloc(%dim, %dim_0, %dim_1) : memref<?x?x?xf32>
  "lmhlo.abs"(%reinterpret_cast, %alloc_2) {disc.device = "gpu"} : (memref<?x?x?xf32>, memref<?x?x?xf32>) -> ()
  %reinterpret_cast_3 = memref.reinterpret_cast %alloc_2 to offset: [0], sizes: [%dim, %dim_0, %dim_1], strides: [%0, %dim_1, 1] {kDiscSymbolicDimAttr = [@S0, @S1, @S2]} : memref<?x?x?xf32> to memref<?x?x?xf32>
  %1 = arith.index_cast %dim : index to i32
  %2 = arith.index_cast %dim_0 : index to i32
  %3 = arith.index_cast %dim_1 : index to i32
  %4 = arith.muli %2, %3 : i32
  %alloc_4 = memref.alloc() {alignment = 64 : i64} : memref<2xi32>
  memref.store %1, %alloc_4[%c0] : memref<2xi32>
  memref.store %4, %alloc_4[%c1] : memref<2xi32>
  %5 = arith.index_cast %4 : i32 to index
  %6 = arith.index_cast %4 : i32 to index
  %alloc_5 = memref.alloc(%dim, %6) : memref<?x?xf32>
  "lmhlo.dynamic_reshape"(%reinterpret_cast_3, %alloc_4, %alloc_5) {disc.device = "gpu"} : (memref<?x?x?xf32>, memref<2xi32>, memref<?x?xf32>) -> ()
  %reinterpret_cast_6 = memref.reinterpret_cast %alloc_5 to offset: [0], sizes: [%dim, %5], strides: [%5, 1] {kDiscSymbolicDimAttr = [@S0, @S3]} : memref<?x?xf32> to memref<?x?xf32>
  %alloc_7 = memref.alloc(%5) : memref<?xf32>
  "lmhlo.reduce"(%reinterpret_cast_6, %alloc, %alloc_7) ({
  ^bb0(%arg1: memref<f32>, %arg2: memref<f32>, %arg3: memref<f32>):
    "lmhlo.maximum"(%arg1, %arg2, %arg3) {disc.device = "gpu"} : (memref<f32>, memref<f32>, memref<f32>) -> ()
    "lmhlo.terminator"() : () -> ()
  }) {dimensions = dense<0> : tensor<1xi64>, disc.device = "gpu"} : (memref<?x?xf32>, memref<f32>, memref<?xf32>) -> ()
  %reinterpret_cast_8 = memref.reinterpret_cast %alloc_7 to offset: [0], sizes: [%5], strides: [1] {kDiscSymbolicDimAttr = [@S3]} : memref<?xf32> to memref<?xf32>
  %alloc_9 = memref.alloc() {alignment = 64 : i64} : memref<2xi32>
  memref.store %2, %alloc_9[%c0] : memref<2xi32>
  memref.store %3, %alloc_9[%c1] : memref<2xi32>
  %alloc_10 = memref.alloc(%dim_0, %dim_1) : memref<?x?xf32>
  "lmhlo.dynamic_reshape"(%reinterpret_cast_8, %alloc_9, %alloc_10) {disc.device = "gpu"} : (memref<?xf32>, memref<2xi32>, memref<?x?xf32>) -> ()
  %reinterpret_cast_11 = memref.reinterpret_cast %alloc_10 to offset: [0], sizes: [%dim_0, %dim_1], strides: [%dim_1, 1] {kDiscSymbolicDimAttr = [@S1, @S2]} : memref<?x?xf32> to memref<?x?xf32>
  return %reinterpret_cast_11 : memref<?x?xf32>
}

// -----// IR Dump After CSE (cse) //----- //
func.func @main(%arg0: memref<?x?x?xf32>) -> memref<?x?xf32> attributes {tf.entry_function = {input_placements = "gpu", inputs = "input0", output_placements = "gpu", outputs = "output0"}} {
  %c2 = arith.constant 2 : index
  %c1 = arith.constant 1 : index
  %c0 = arith.constant 0 : index
  %alloc = memref.alloc() : memref<f32>
  "lmhlo.constant"(%alloc) {disc.device = "gpu", value = dense<0xFF800000> : tensor<f32>} : (memref<f32>) -> ()
  %dim = memref.dim %arg0, %c0 : memref<?x?x?xf32>
  %dim_0 = memref.dim %arg0, %c1 : memref<?x?x?xf32>
  %dim_1 = memref.dim %arg0, %c2 : memref<?x?x?xf32>
  %0 = arith.muli %dim_1, %dim_0 : index
  %reinterpret_cast = memref.reinterpret_cast %arg0 to offset: [0], sizes: [%dim, %dim_0, %dim_1], strides: [%0, %dim_1, 1] {kDiscSymbolicDimAttr = [@S0, @S1, @S2]} : memref<?x?x?xf32> to memref<?x?x?xf32>
  %alloc_2 = memref.alloc(%dim, %dim_0, %dim_1) : memref<?x?x?xf32>
  "lmhlo.abs"(%reinterpret_cast, %alloc_2) {disc.device = "gpu"} : (memref<?x?x?xf32>, memref<?x?x?xf32>) -> ()
  %reinterpret_cast_3 = memref.reinterpret_cast %alloc_2 to offset: [0], sizes: [%dim, %dim_0, %dim_1], strides: [%0, %dim_1, 1] {kDiscSymbolicDimAttr = [@S0, @S1, @S2]} : memref<?x?x?xf32> to memref<?x?x?xf32>
  %1 = arith.index_cast %dim : index to i32
  %2 = arith.index_cast %dim_0 : index to i32
  %3 = arith.index_cast %dim_1 : index to i32
  %4 = arith.muli %2, %3 : i32
  %alloc_4 = memref.alloc() {alignment = 64 : i64} : memref<2xi32>
  memref.store %1, %alloc_4[%c0] : memref<2xi32>
  memref.store %4, %alloc_4[%c1] : memref<2xi32>
  %5 = arith.index_cast %4 : i32 to index
  %alloc_5 = memref.alloc(%dim, %5) : memref<?x?xf32>
  "lmhlo.dynamic_reshape"(%reinterpret_cast_3, %alloc_4, %alloc_5) {disc.device = "gpu"} : (memref<?x?x?xf32>, memref<2xi32>, memref<?x?xf32>) -> ()
  %reinterpret_cast_6 = memref.reinterpret_cast %alloc_5 to offset: [0], sizes: [%dim, %5], strides: [%5, 1] {kDiscSymbolicDimAttr = [@S0, @S3]} : memref<?x?xf32> to memref<?x?xf32>
  %alloc_7 = memref.alloc(%5) : memref<?xf32>
  "lmhlo.reduce"(%reinterpret_cast_6, %alloc, %alloc_7) ({
  ^bb0(%arg1: memref<f32>, %arg2: memref<f32>, %arg3: memref<f32>):
    "lmhlo.maximum"(%arg1, %arg2, %arg3) {disc.device = "gpu"} : (memref<f32>, memref<f32>, memref<f32>) -> ()
    "lmhlo.terminator"() : () -> ()
  }) {dimensions = dense<0> : tensor<1xi64>, disc.device = "gpu"} : (memref<?x?xf32>, memref<f32>, memref<?xf32>) -> ()
  %reinterpret_cast_8 = memref.reinterpret_cast %alloc_7 to offset: [0], sizes: [%5], strides: [1] {kDiscSymbolicDimAttr = [@S3]} : memref<?xf32> to memref<?xf32>
  %alloc_9 = memref.alloc() {alignment = 64 : i64} : memref<2xi32>
  memref.store %2, %alloc_9[%c0] : memref<2xi32>
  memref.store %3, %alloc_9[%c1] : memref<2xi32>
  %alloc_10 = memref.alloc(%dim_0, %dim_1) : memref<?x?xf32>
  "lmhlo.dynamic_reshape"(%reinterpret_cast_8, %alloc_9, %alloc_10) {disc.device = "gpu"} : (memref<?xf32>, memref<2xi32>, memref<?x?xf32>) -> ()
  %reinterpret_cast_11 = memref.reinterpret_cast %alloc_10 to offset: [0], sizes: [%dim_0, %dim_1], strides: [%dim_1, 1] {kDiscSymbolicDimAttr = [@S1, @S2]} : memref<?x?xf32> to memref<?x?xf32>
  return %reinterpret_cast_11 : memref<?x?xf32>
}

// -----// IR Dump After DiscMemrefCanonicalizer (disc-memref-canonicalize) //----- //
func.func @main(%arg0: memref<?x?x?xf32>) -> memref<?x?xf32> attributes {tf.entry_function = {input_placements = "gpu", inputs = "input0", output_placements = "gpu", outputs = "output0"}} {
  %c2 = arith.constant 2 : index
  %c1 = arith.constant 1 : index
  %c0 = arith.constant 0 : index
  %alloc = memref.alloc() : memref<f32>
  "lmhlo.constant"(%alloc) {disc.device = "gpu", value = dense<0xFF800000> : tensor<f32>} : (memref<f32>) -> ()
  %dim = memref.dim %arg0, %c0 : memref<?x?x?xf32>
  %dim_0 = memref.dim %arg0, %c1 : memref<?x?x?xf32>
  %dim_1 = memref.dim %arg0, %c2 : memref<?x?x?xf32>
  %0 = arith.muli %dim_1, %dim_0 : index
  %reinterpret_cast = memref.reinterpret_cast %arg0 to offset: [0], sizes: [%dim, %dim_0, %dim_1], strides: [%0, %dim_1, 1] {kDiscSymbolicDimAttr = [@S0, @S1, @S2]} : memref<?x?x?xf32> to memref<?x?x?xf32>
  %alloc_2 = memref.alloc(%dim, %dim_0, %dim_1) {kDiscSymbolicDimAttr = [@S0, @S1, @S2]} : memref<?x?x?xf32>
  "lmhlo.abs"(%reinterpret_cast, %alloc_2) {disc.device = "gpu"} : (memref<?x?x?xf32>, memref<?x?x?xf32>) -> ()
  %1 = arith.index_cast %dim : index to i32
  %2 = arith.index_cast %dim_0 : index to i32
  %3 = arith.index_cast %dim_1 : index to i32
  %4 = arith.muli %2, %3 : i32
  %alloc_3 = memref.alloc() {alignment = 64 : i64} : memref<2xi32>
  memref.store %1, %alloc_3[%c0] : memref<2xi32>
  memref.store %4, %alloc_3[%c1] : memref<2xi32>
  %5 = arith.index_cast %4 : i32 to index
  %alloc_4 = memref.alloc(%dim, %5) {kDiscSymbolicDimAttr = [@S0, @S3]} : memref<?x?xf32>
  "lmhlo.dynamic_reshape"(%alloc_2, %alloc_3, %alloc_4) {disc.device = "gpu"} : (memref<?x?x?xf32>, memref<2xi32>, memref<?x?xf32>) -> ()
  %alloc_5 = memref.alloc(%5) {kDiscSymbolicDimAttr = [@S3]} : memref<?xf32>
  "lmhlo.reduce"(%alloc_4, %alloc, %alloc_5) ({
  ^bb0(%arg1: memref<f32>, %arg2: memref<f32>, %arg3: memref<f32>):
    "lmhlo.maximum"(%arg1, %arg2, %arg3) {disc.device = "gpu"} : (memref<f32>, memref<f32>, memref<f32>) -> ()
    "lmhlo.terminator"() : () -> ()
  }) {dimensions = dense<0> : tensor<1xi64>, disc.device = "gpu"} : (memref<?x?xf32>, memref<f32>, memref<?xf32>) -> ()
  %alloc_6 = memref.alloc() {alignment = 64 : i64} : memref<2xi32>
  memref.store %2, %alloc_6[%c0] : memref<2xi32>
  memref.store %3, %alloc_6[%c1] : memref<2xi32>
  %alloc_7 = memref.alloc(%dim_0, %dim_1) {kDiscSymbolicDimAttr = [@S1, @S2]} : memref<?x?xf32>
  "lmhlo.dynamic_reshape"(%alloc_5, %alloc_6, %alloc_7) {disc.device = "gpu"} : (memref<?xf32>, memref<2xi32>, memref<?x?xf32>) -> ()
  return %alloc_7 : memref<?x?xf32>
}

// -----// IR Dump After DiscAssignMemorySpacePass (disc-assign-memory-space) //----- //
module {
  func.func @main(%arg0: memref<?x?x?xf32, "gpu">) -> memref<?x?xf32, "gpu"> attributes {tf.entry_function = {input_placements = "gpu", inputs = "input0", output_placements = "gpu", outputs = "output0"}} {
    %c2 = arith.constant 2 : index
    %c1 = arith.constant 1 : index
    %c0 = arith.constant 0 : index
    %alloc = memref.alloc() : memref<f32, "gpu">
    "lmhlo.constant"(%alloc) {disc.device = "gpu", value = dense<0xFF800000> : tensor<f32>} : (memref<f32, "gpu">) -> ()
    %dim = memref.dim %arg0, %c0 : memref<?x?x?xf32, "gpu">
    %dim_0 = memref.dim %arg0, %c1 : memref<?x?x?xf32, "gpu">
    %dim_1 = memref.dim %arg0, %c2 : memref<?x?x?xf32, "gpu">
    %0 = arith.muli %dim_1, %dim_0 : index
    %reinterpret_cast = memref.reinterpret_cast %arg0 to offset: [0], sizes: [%dim, %dim_0, %dim_1], strides: [%0, %dim_1, 1] {kDiscSymbolicDimAttr = [@S0, @S1, @S2]} : memref<?x?x?xf32, "gpu"> to memref<?x?x?xf32, "gpu">
    %alloc_2 = memref.alloc(%dim, %dim_0, %dim_1) {kDiscSymbolicDimAttr = [@S0, @S1, @S2]} : memref<?x?x?xf32, "gpu">
    "lmhlo.abs"(%reinterpret_cast, %alloc_2) {disc.device = "gpu"} : (memref<?x?x?xf32, "gpu">, memref<?x?x?xf32, "gpu">) -> ()
    %1 = arith.index_cast %dim : index to i32
    %2 = arith.index_cast %dim_0 : index to i32
    %3 = arith.index_cast %dim_1 : index to i32
    %4 = arith.muli %2, %3 : i32
    %alloc_3 = memref.alloc() {alignment = 64 : i64} : memref<2xi32, "cpu">
    memref.store %1, %alloc_3[%c0] : memref<2xi32, "cpu">
    memref.store %4, %alloc_3[%c1] : memref<2xi32, "cpu">
    %5 = arith.index_cast %4 : i32 to index
    %alloc_4 = memref.alloc(%dim, %5) {kDiscSymbolicDimAttr = [@S0, @S3]} : memref<?x?xf32, "gpu">
    "lmhlo.dynamic_reshape"(%alloc_2, %alloc_3, %alloc_4) {disc.device = "gpu"} : (memref<?x?x?xf32, "gpu">, memref<2xi32, "cpu">, memref<?x?xf32, "gpu">) -> ()
    %alloc_5 = memref.alloc(%5) {kDiscSymbolicDimAttr = [@S3]} : memref<?xf32, "gpu">
    "lmhlo.reduce"(%alloc_4, %alloc, %alloc_5) ({
    ^bb0(%arg1: memref<f32>, %arg2: memref<f32>, %arg3: memref<f32>):
      "lmhlo.maximum"(%arg1, %arg2, %arg3) {disc.device = "gpu"} : (memref<f32>, memref<f32>, memref<f32>) -> ()
      "lmhlo.terminator"() : () -> ()
    }) {dimensions = dense<0> : tensor<1xi64>, disc.device = "gpu"} : (memref<?x?xf32, "gpu">, memref<f32, "gpu">, memref<?xf32, "gpu">) -> ()
    %alloc_6 = memref.alloc() {alignment = 64 : i64} : memref<2xi32, "cpu">
    memref.store %2, %alloc_6[%c0] : memref<2xi32, "cpu">
    memref.store %3, %alloc_6[%c1] : memref<2xi32, "cpu">
    %alloc_7 = memref.alloc(%dim_0, %dim_1) {kDiscSymbolicDimAttr = [@S1, @S2]} : memref<?x?xf32, "gpu">
    "lmhlo.dynamic_reshape"(%alloc_5, %alloc_6, %alloc_7) {disc.device = "gpu"} : (memref<?xf32, "gpu">, memref<2xi32, "cpu">, memref<?x?xf32, "gpu">) -> ()
    return %alloc_7 : memref<?x?xf32, "gpu">
  }
  "disc_shape.SymbolicDim"() {knownNegativeOne = false, knownNonNegative = true, knownNonSizeOne = false, knownNonSizeZero = false, sym_name = "S0", value = -9223372036854775808 : i64} : () -> ()
  "disc_shape.SymbolicDim"() {knownNegativeOne = false, knownNonNegative = true, knownNonSizeOne = false, knownNonSizeZero = false, sym_name = "S1", value = -9223372036854775808 : i64} : () -> ()
  "disc_shape.SymbolicDim"() {knownNegativeOne = false, knownNonNegative = true, knownNonSizeOne = false, knownNonSizeZero = false, sym_name = "S2", value = -9223372036854775808 : i64} : () -> ()
  "disc_shape.SymbolicDim"() {knownNegativeOne = false, knownNonNegative = true, knownNonSizeOne = false, knownNonSizeZero = false, sym_name = "S3", value = -9223372036854775808 : i64} : () -> ()
  func.func @shape_constraint_graph() {
    %0 = "disc_shape.dim"() {name = @S3} : () -> index
    %1 = "disc_shape.dim"() {name = @S1} : () -> index
    %2 = "disc_shape.dim"() {name = @S2} : () -> index
    "disc_shape.tie_product_equal"(%0, %1, %2) {operand_segment_sizes = array<i32: 1, 2>} : (index, index, index) -> ()
    return
  }
}


// -----// IR Dump After PromoteBuffersToStack (promote-buffers-to-stack) //----- //
func.func @main(%arg0: memref<?x?x?xf32, "gpu">) -> memref<?x?xf32, "gpu"> attributes {tf.entry_function = {input_placements = "gpu", inputs = "input0", output_placements = "gpu", outputs = "output0"}} {
  %c2 = arith.constant 2 : index
  %c1 = arith.constant 1 : index
  %c0 = arith.constant 0 : index
  %alloc = memref.alloc() : memref<f32, "gpu">
  "lmhlo.constant"(%alloc) {disc.device = "gpu", value = dense<0xFF800000> : tensor<f32>} : (memref<f32, "gpu">) -> ()
  %dim = memref.dim %arg0, %c0 : memref<?x?x?xf32, "gpu">
  %dim_0 = memref.dim %arg0, %c1 : memref<?x?x?xf32, "gpu">
  %dim_1 = memref.dim %arg0, %c2 : memref<?x?x?xf32, "gpu">
  %0 = arith.muli %dim_1, %dim_0 : index
  %reinterpret_cast = memref.reinterpret_cast %arg0 to offset: [0], sizes: [%dim, %dim_0, %dim_1], strides: [%0, %dim_1, 1] {kDiscSymbolicDimAttr = [@S0, @S1, @S2]} : memref<?x?x?xf32, "gpu"> to memref<?x?x?xf32, "gpu">
  %alloc_2 = memref.alloc(%dim, %dim_0, %dim_1) {kDiscSymbolicDimAttr = [@S0, @S1, @S2]} : memref<?x?x?xf32, "gpu">
  "lmhlo.abs"(%reinterpret_cast, %alloc_2) {disc.device = "gpu"} : (memref<?x?x?xf32, "gpu">, memref<?x?x?xf32, "gpu">) -> ()
  %1 = arith.index_cast %dim : index to i32
  %2 = arith.index_cast %dim_0 : index to i32
  %3 = arith.index_cast %dim_1 : index to i32
  %4 = arith.muli %2, %3 : i32
  %alloca = memref.alloca() : memref<2xi32, "cpu">
  memref.store %1, %alloca[%c0] : memref<2xi32, "cpu">
  memref.store %4, %alloca[%c1] : memref<2xi32, "cpu">
  %5 = arith.index_cast %4 : i32 to index
  %alloc_3 = memref.alloc(%dim, %5) {kDiscSymbolicDimAttr = [@S0, @S3]} : memref<?x?xf32, "gpu">
  "lmhlo.dynamic_reshape"(%alloc_2, %alloca, %alloc_3) {disc.device = "gpu"} : (memref<?x?x?xf32, "gpu">, memref<2xi32, "cpu">, memref<?x?xf32, "gpu">) -> ()
  %alloc_4 = memref.alloc(%5) {kDiscSymbolicDimAttr = [@S3]} : memref<?xf32, "gpu">
  "lmhlo.reduce"(%alloc_3, %alloc, %alloc_4) ({
  ^bb0(%arg1: memref<f32>, %arg2: memref<f32>, %arg3: memref<f32>):
    "lmhlo.maximum"(%arg1, %arg2, %arg3) {disc.device = "gpu"} : (memref<f32>, memref<f32>, memref<f32>) -> ()
    "lmhlo.terminator"() : () -> ()
  }) {dimensions = dense<0> : tensor<1xi64>, disc.device = "gpu"} : (memref<?x?xf32, "gpu">, memref<f32, "gpu">, memref<?xf32, "gpu">) -> ()
  %alloca_5 = memref.alloca() : memref<2xi32, "cpu">
  memref.store %2, %alloca_5[%c0] : memref<2xi32, "cpu">
  memref.store %3, %alloca_5[%c1] : memref<2xi32, "cpu">
  %alloc_6 = memref.alloc(%dim_0, %dim_1) {kDiscSymbolicDimAttr = [@S1, @S2]} : memref<?x?xf32, "gpu">
  "lmhlo.dynamic_reshape"(%alloc_4, %alloca_5, %alloc_6) {disc.device = "gpu"} : (memref<?xf32, "gpu">, memref<2xi32, "cpu">, memref<?x?xf32, "gpu">) -> ()
  return %alloc_6 : memref<?x?xf32, "gpu">
}

SymbolicDimMgr::save walkRankedTensorValue takes: 1 us
SymbolicDimMgr::save update attributes takes: 5 us
SymbolicDimMgr::updateProductEqualityMap simplifySymbolicDimProductPair takes: 8 us
productSet.size() = 2
SymbolicDimMgr::updateProductEqualityMap propagate graph takes: 5 us
SymbolicDimMgr::updateProductEqualityMap remove multiply takes: 12 us
SymbolicDimMgr::updateProductEqualityMap build toRemove  takes: 0 us
SymbolicDimMgr::updateProductEqualityMap apply toRemove  takes: 0 us
SymbolicDimMgr::save updateProductEqualityMap takes: 40 us
SymbolicDimMgr::save updateFunctionType takes: 2 us
SymbolicDimMgr::save collect symbolicDim ops takes: 4 us
SymbolicDimMgr::save remove symbolicDim ops takes: 0 us
SymbolicDimMgr::save remove unused production takes: 0 us
SymbolicDimMgr::save remove unused production #2 takes: 1 us
SymbolicDimMgr::save canonicalize the name takes: 3 us
SymbolicDimMgr::save replace the name takes: 4 us
SymbolicDimMgr::save updateFunctionType takes: 0 us
// -----// IR Dump After DiscFusionPass (disc-fusion) //----- //
func.func @main(%arg0: memref<?x?x?xf32, "gpu">) -> memref<?x?xf32, "gpu"> attributes {tf.entry_function = {input_placements = "gpu", inputs = "input0", output_placements = "gpu", outputs = "output0"}} {
  %c2 = arith.constant 2 : index
  %dim = memref.dim %arg0, %c2 : memref<?x?x?xf32, "gpu">
  %c1 = arith.constant 1 : index
  %dim_0 = memref.dim %arg0, %c1 : memref<?x?x?xf32, "gpu">
  %c0 = arith.constant 0 : index
  %dim_1 = memref.dim %arg0, %c0 : memref<?x?x?xf32, "gpu">
  %alloc = memref.alloc() : memref<f32, "gpu">
  %0 = arith.muli %dim, %dim_0 : index
  %reinterpret_cast = memref.reinterpret_cast %arg0 to offset: [0], sizes: [%dim_1, %dim_0, %dim], strides: [%0, %dim, 1] {kDiscSymbolicDimAttr = [@S0, @S1, @S2]} : memref<?x?x?xf32, "gpu"> to memref<?x?x?xf32, "gpu">
  %alloc_2 = memref.alloc(%dim_1, %dim_0, %dim) {kDiscSymbolicDimAttr = [@S0, @S1, @S2]} : memref<?x?x?xf32, "gpu">
  %1 = arith.index_cast %dim_1 : index to i32
  %2 = arith.index_cast %dim_0 : index to i32
  %3 = arith.index_cast %dim : index to i32
  %4 = arith.muli %2, %3 : i32
  %alloca = memref.alloca() : memref<2xi32, "cpu">
  memref.store %1, %alloca[%c0] : memref<2xi32, "cpu">
  memref.store %4, %alloca[%c1] : memref<2xi32, "cpu">
  %5 = arith.index_cast %4 : i32 to index
  %alloc_3 = memref.alloc(%dim_1, %5) {kDiscSymbolicDimAttr = [@S0, @S3]} : memref<?x?xf32, "gpu">
  %alloc_4 = memref.alloc(%5) {kDiscSymbolicDimAttr = [@S3]} : memref<?xf32, "gpu">
  "lmhlo.fusion"() ({
    "lmhlo.constant"(%alloc) {disc.device = "gpu", value = dense<0xFF800000> : tensor<f32>} : (memref<f32, "gpu">) -> ()
    "lmhlo.abs"(%reinterpret_cast, %alloc_2) {disc.device = "gpu"} : (memref<?x?x?xf32, "gpu">, memref<?x?x?xf32, "gpu">) -> ()
    "lmhlo.dynamic_reshape"(%alloc_2, %alloca, %alloc_3) {disc.device = "gpu"} : (memref<?x?x?xf32, "gpu">, memref<2xi32, "cpu">, memref<?x?xf32, "gpu">) -> ()
    "lmhlo.reduce"(%alloc_3, %alloc, %alloc_4) ({
    ^bb0(%arg1: memref<f32>, %arg2: memref<f32>, %arg3: memref<f32>):
      "lmhlo.maximum"(%arg1, %arg2, %arg3) {disc.device = "gpu"} : (memref<f32>, memref<f32>, memref<f32>) -> ()
      "lmhlo.terminator"() : () -> ()
    }) {dimensions = dense<0> : tensor<1xi64>, disc.device = "gpu"} : (memref<?x?xf32, "gpu">, memref<f32, "gpu">, memref<?xf32, "gpu">) -> ()
    "lmhlo.terminator"() : () -> ()
  }) {disc.device = "gpu", disc.fusion.name = "main_kColReduction_reduce__4_1_0", disc.fusion_type = "kColReduction"} : () -> ()
  %alloca_5 = memref.alloca() : memref<2xi32, "cpu">
  memref.store %2, %alloca_5[%c0] : memref<2xi32, "cpu">
  memref.store %3, %alloca_5[%c1] : memref<2xi32, "cpu">
  %alloc_6 = memref.alloc(%dim_0, %dim) {kDiscSymbolicDimAttr = [@S1, @S2]} : memref<?x?xf32, "gpu">
  "lmhlo.dynamic_reshape"(%alloc_4, %alloca_5, %alloc_6) {disc.device = "gpu"} : (memref<?xf32, "gpu">, memref<2xi32, "cpu">, memref<?x?xf32, "gpu">) -> ()
  return %alloc_6 : memref<?x?xf32, "gpu">
}

SymbolicDimMgr::save walkRankedTensorValue takes: 1 us
SymbolicDimMgr::save update attributes takes: 5 us
SymbolicDimMgr::updateProductEqualityMap simplifySymbolicDimProductPair takes: 6 us
productSet.size() = 2
SymbolicDimMgr::updateProductEqualityMap propagate graph takes: 3 us
SymbolicDimMgr::updateProductEqualityMap remove multiply takes: 11 us
SymbolicDimMgr::updateProductEqualityMap build toRemove  takes: 0 us
SymbolicDimMgr::updateProductEqualityMap apply toRemove  takes: 0 us
SymbolicDimMgr::save updateProductEqualityMap takes: 36 us
SymbolicDimMgr::save updateFunctionType takes: 1 us
SymbolicDimMgr::save collect symbolicDim ops takes: 4 us
SymbolicDimMgr::save remove symbolicDim ops takes: 0 us
SymbolicDimMgr::save remove unused production takes: 0 us
SymbolicDimMgr::save remove unused production #2 takes: 0 us
SymbolicDimMgr::save canonicalize the name takes: 3 us
SymbolicDimMgr::save replace the name takes: 5 us
SymbolicDimMgr::save updateFunctionType takes: 0 us
// -----// IR Dump After DiscSpecializeFusionWithSpeculationPass (disc-specialize-fusion-with-speculation) //----- //
func.func @main(%arg0: memref<?x?x?xf32, "gpu">) -> memref<?x?xf32, "gpu"> attributes {tf.entry_function = {input_placements = "gpu", inputs = "input0", output_placements = "gpu", outputs = "output0"}} {
  %c2 = arith.constant 2 : index
  %dim = memref.dim %arg0, %c2 : memref<?x?x?xf32, "gpu">
  %c1 = arith.constant 1 : index
  %dim_0 = memref.dim %arg0, %c1 : memref<?x?x?xf32, "gpu">
  %c0 = arith.constant 0 : index
  %dim_1 = memref.dim %arg0, %c0 : memref<?x?x?xf32, "gpu">
  %alloc = memref.alloc() : memref<f32, "gpu">
  %0 = arith.muli %dim, %dim_0 : index
  %reinterpret_cast = memref.reinterpret_cast %arg0 to offset: [0], sizes: [%dim_1, %dim_0, %dim], strides: [%0, %dim, 1] {kDiscSymbolicDimAttr = [@S0, @S1, @S2]} : memref<?x?x?xf32, "gpu"> to memref<?x?x?xf32, "gpu">
  %alloc_2 = memref.alloc(%dim_1, %dim_0, %dim) {kDiscSymbolicDimAttr = [@S0, @S1, @S2]} : memref<?x?x?xf32, "gpu">
  %1 = arith.index_cast %dim_1 : index to i32
  %2 = arith.index_cast %dim_0 : index to i32
  %3 = arith.index_cast %dim : index to i32
  %4 = arith.muli %2, %3 : i32
  %alloca = memref.alloca() : memref<2xi32, "cpu">
  memref.store %1, %alloca[%c0] : memref<2xi32, "cpu">
  memref.store %4, %alloca[%c1] : memref<2xi32, "cpu">
  %5 = arith.index_cast %4 : i32 to index
  %alloc_3 = memref.alloc(%dim_1, %5) {kDiscSymbolicDimAttr = [@S0, @S3]} : memref<?x?xf32, "gpu">
  %alloc_4 = memref.alloc(%5) {kDiscSymbolicDimAttr = [@S3]} : memref<?xf32, "gpu">
  %c0_5 = arith.constant 0 : index
  %dim_6 = memref.dim %alloc_3, %c0_5 : memref<?x?xf32, "gpu">
  %c1_7 = arith.constant 1 : index
  %dim_8 = memref.dim %alloc_3, %c1_7 : memref<?x?xf32, "gpu">
  %6 = arith.muli %dim_6, %dim_8 : index
  %c256 = arith.constant 256 : index
  %7 = arith.ceildivsi %6, %c256 : index
  %c108 = arith.constant 108 : index
  %8 = arith.cmpi sgt, %7, %c108 : index
  scf.if %8 {
    "lmhlo.fusion"() ({
      "lmhlo.constant"(%alloc) {disc.device = "gpu", value = dense<0xFF800000> : tensor<f32>} : (memref<f32, "gpu">) -> ()
      "lmhlo.abs"(%reinterpret_cast, %alloc_2) {disc.device = "gpu"} : (memref<?x?x?xf32, "gpu">, memref<?x?x?xf32, "gpu">) -> ()
      "lmhlo.dynamic_reshape"(%alloc_2, %alloca, %alloc_3) {disc.device = "gpu"} : (memref<?x?x?xf32, "gpu">, memref<2xi32, "cpu">, memref<?x?xf32, "gpu">) -> ()
      "lmhlo.reduce"(%alloc_3, %alloc, %alloc_4) ({
      ^bb0(%arg1: memref<f32>, %arg2: memref<f32>, %arg3: memref<f32>):
        "lmhlo.maximum"(%arg1, %arg2, %arg3) {disc.device = "gpu"} : (memref<f32>, memref<f32>, memref<f32>) -> ()
        "lmhlo.terminator"() : () -> ()
      }) {dimensions = dense<0> : tensor<1xi64>, disc.device = "gpu"} : (memref<?x?xf32, "gpu">, memref<f32, "gpu">, memref<?xf32, "gpu">) -> ()
      "lmhlo.terminator"() : () -> ()
    }) {disc.device = "gpu", disc.fusion.name = "main_kColReduction_reduce__4_1_0", disc.fusion.tag = "8w32h", disc.fusion_type = "kColReduction", disc_col_reduction_schedule_hint = 1 : i32, disc_thread_per_block_hint = 256 : i32} : () -> ()
  } else {
    "lmhlo.fusion"() ({
      "lmhlo.constant"(%alloc) {disc.device = "gpu", value = dense<0xFF800000> : tensor<f32>} : (memref<f32, "gpu">) -> ()
      "lmhlo.abs"(%reinterpret_cast, %alloc_2) {disc.device = "gpu"} : (memref<?x?x?xf32, "gpu">, memref<?x?x?xf32, "gpu">) -> ()
      "lmhlo.dynamic_reshape"(%alloc_2, %alloca, %alloc_3) {disc.device = "gpu"} : (memref<?x?x?xf32, "gpu">, memref<2xi32, "cpu">, memref<?x?xf32, "gpu">) -> ()
      "lmhlo.reduce"(%alloc_3, %alloc, %alloc_4) ({
      ^bb0(%arg1: memref<f32>, %arg2: memref<f32>, %arg3: memref<f32>):
        "lmhlo.maximum"(%arg1, %arg2, %arg3) {disc.device = "gpu"} : (memref<f32>, memref<f32>, memref<f32>) -> ()
        "lmhlo.terminator"() : () -> ()
      }) {dimensions = dense<0> : tensor<1xi64>, disc.device = "gpu"} : (memref<?x?xf32, "gpu">, memref<f32, "gpu">, memref<?xf32, "gpu">) -> ()
      "lmhlo.terminator"() : () -> ()
    }) {disc.device = "gpu", disc.fusion.name = "main_kColReduction_reduce__4_1_0", disc.fusion.tag = "8w16h", disc.fusion_type = "kColReduction", disc_col_reduction_schedule_hint = 2 : i32, disc_thread_per_block_hint = 128 : i32} : () -> ()
  }
  %alloca_9 = memref.alloca() : memref<2xi32, "cpu">
  memref.store %2, %alloca_9[%c0] : memref<2xi32, "cpu">
  memref.store %3, %alloca_9[%c1] : memref<2xi32, "cpu">
  %alloc_10 = memref.alloc(%dim_0, %dim) {kDiscSymbolicDimAttr = [@S1, @S2]} : memref<?x?xf32, "gpu">
  "lmhlo.dynamic_reshape"(%alloc_4, %alloca_9, %alloc_10) {disc.device = "gpu"} : (memref<?xf32, "gpu">, memref<2xi32, "cpu">, memref<?x?xf32, "gpu">) -> ()
  return %alloc_10 : memref<?x?xf32, "gpu">
}

// -----// IR Dump After Canonicalizer (canonicalize) //----- //
func.func @main(%arg0: memref<?x?x?xf32, "gpu">) -> memref<?x?xf32, "gpu"> attributes {tf.entry_function = {input_placements = "gpu", inputs = "input0", output_placements = "gpu", outputs = "output0"}} {
  %c108 = arith.constant 108 : index
  %c256 = arith.constant 256 : index
  %c0 = arith.constant 0 : index
  %c1 = arith.constant 1 : index
  %c2 = arith.constant 2 : index
  %dim = memref.dim %arg0, %c2 : memref<?x?x?xf32, "gpu">
  %dim_0 = memref.dim %arg0, %c1 : memref<?x?x?xf32, "gpu">
  %dim_1 = memref.dim %arg0, %c0 : memref<?x?x?xf32, "gpu">
  %alloc = memref.alloc() : memref<f32, "gpu">
  %0 = arith.muli %dim, %dim_0 : index
  %reinterpret_cast = memref.reinterpret_cast %arg0 to offset: [0], sizes: [%dim_1, %dim_0, %dim], strides: [%0, %dim, 1] {kDiscSymbolicDimAttr = [@S0, @S1, @S2]} : memref<?x?x?xf32, "gpu"> to memref<?x?x?xf32, "gpu">
  %alloc_2 = memref.alloc(%dim_1, %dim_0, %dim) {kDiscSymbolicDimAttr = [@S0, @S1, @S2]} : memref<?x?x?xf32, "gpu">
  %1 = arith.index_cast %dim_1 : index to i32
  %2 = arith.index_cast %dim_0 : index to i32
  %3 = arith.index_cast %dim : index to i32
  %4 = arith.muli %2, %3 : i32
  %alloca = memref.alloca() : memref<2xi32, "cpu">
  memref.store %1, %alloca[%c0] : memref<2xi32, "cpu">
  memref.store %4, %alloca[%c1] : memref<2xi32, "cpu">
  %5 = arith.index_cast %4 : i32 to index
  %alloc_3 = memref.alloc(%dim_1, %5) {kDiscSymbolicDimAttr = [@S0, @S3]} : memref<?x?xf32, "gpu">
  %alloc_4 = memref.alloc(%5) {kDiscSymbolicDimAttr = [@S3]} : memref<?xf32, "gpu">
  %6 = arith.muli %dim_1, %5 : index
  %7 = arith.ceildivsi %6, %c256 : index
  %8 = arith.cmpi sgt, %7, %c108 : index
  scf.if %8 {
    "lmhlo.fusion"() ({
      "lmhlo.constant"(%alloc) {disc.device = "gpu", value = dense<0xFF800000> : tensor<f32>} : (memref<f32, "gpu">) -> ()
      "lmhlo.abs"(%reinterpret_cast, %alloc_2) {disc.device = "gpu"} : (memref<?x?x?xf32, "gpu">, memref<?x?x?xf32, "gpu">) -> ()
      "lmhlo.dynamic_reshape"(%alloc_2, %alloca, %alloc_3) {disc.device = "gpu"} : (memref<?x?x?xf32, "gpu">, memref<2xi32, "cpu">, memref<?x?xf32, "gpu">) -> ()
      "lmhlo.reduce"(%alloc_3, %alloc, %alloc_4) ({
      ^bb0(%arg1: memref<f32>, %arg2: memref<f32>, %arg3: memref<f32>):
        "lmhlo.maximum"(%arg1, %arg2, %arg3) {disc.device = "gpu"} : (memref<f32>, memref<f32>, memref<f32>) -> ()
        "lmhlo.terminator"() : () -> ()
      }) {dimensions = dense<0> : tensor<1xi64>, disc.device = "gpu"} : (memref<?x?xf32, "gpu">, memref<f32, "gpu">, memref<?xf32, "gpu">) -> ()
      "lmhlo.terminator"() : () -> ()
    }) {disc.device = "gpu", disc.fusion.name = "main_kColReduction_reduce__4_1_0", disc.fusion.tag = "8w32h", disc.fusion_type = "kColReduction", disc_col_reduction_schedule_hint = 1 : i32, disc_thread_per_block_hint = 256 : i32} : () -> ()
  } else {
    "lmhlo.fusion"() ({
      "lmhlo.constant"(%alloc) {disc.device = "gpu", value = dense<0xFF800000> : tensor<f32>} : (memref<f32, "gpu">) -> ()
      "lmhlo.abs"(%reinterpret_cast, %alloc_2) {disc.device = "gpu"} : (memref<?x?x?xf32, "gpu">, memref<?x?x?xf32, "gpu">) -> ()
      "lmhlo.dynamic_reshape"(%alloc_2, %alloca, %alloc_3) {disc.device = "gpu"} : (memref<?x?x?xf32, "gpu">, memref<2xi32, "cpu">, memref<?x?xf32, "gpu">) -> ()
      "lmhlo.reduce"(%alloc_3, %alloc, %alloc_4) ({
      ^bb0(%arg1: memref<f32>, %arg2: memref<f32>, %arg3: memref<f32>):
        "lmhlo.maximum"(%arg1, %arg2, %arg3) {disc.device = "gpu"} : (memref<f32>, memref<f32>, memref<f32>) -> ()
        "lmhlo.terminator"() : () -> ()
      }) {dimensions = dense<0> : tensor<1xi64>, disc.device = "gpu"} : (memref<?x?xf32, "gpu">, memref<f32, "gpu">, memref<?xf32, "gpu">) -> ()
      "lmhlo.terminator"() : () -> ()
    }) {disc.device = "gpu", disc.fusion.name = "main_kColReduction_reduce__4_1_0", disc.fusion.tag = "8w16h", disc.fusion_type = "kColReduction", disc_col_reduction_schedule_hint = 2 : i32, disc_thread_per_block_hint = 128 : i32} : () -> ()
  }
  %alloca_5 = memref.alloca() : memref<2xi32, "cpu">
  memref.store %2, %alloca_5[%c0] : memref<2xi32, "cpu">
  memref.store %3, %alloca_5[%c1] : memref<2xi32, "cpu">
  %alloc_6 = memref.alloc(%dim_0, %dim) {kDiscSymbolicDimAttr = [@S1, @S2]} : memref<?x?xf32, "gpu">
  "lmhlo.dynamic_reshape"(%alloc_4, %alloca_5, %alloc_6) {disc.device = "gpu"} : (memref<?xf32, "gpu">, memref<2xi32, "cpu">, memref<?x?xf32, "gpu">) -> ()
  return %alloc_6 : memref<?x?xf32, "gpu">
}

// -----// IR Dump After BufferDeallocation (buffer-deallocation) //----- //
func.func @main(%arg0: memref<?x?x?xf32, "gpu">) -> memref<?x?xf32, "gpu"> attributes {tf.entry_function = {input_placements = "gpu", inputs = "input0", output_placements = "gpu", outputs = "output0"}} {
  %c108 = arith.constant 108 : index
  %c256 = arith.constant 256 : index
  %c0 = arith.constant 0 : index
  %c1 = arith.constant 1 : index
  %c2 = arith.constant 2 : index
  %dim = memref.dim %arg0, %c2 : memref<?x?x?xf32, "gpu">
  %dim_0 = memref.dim %arg0, %c1 : memref<?x?x?xf32, "gpu">
  %dim_1 = memref.dim %arg0, %c0 : memref<?x?x?xf32, "gpu">
  %alloc = memref.alloc() : memref<f32, "gpu">
  %0 = arith.muli %dim, %dim_0 : index
  %reinterpret_cast = memref.reinterpret_cast %arg0 to offset: [0], sizes: [%dim_1, %dim_0, %dim], strides: [%0, %dim, 1] {kDiscSymbolicDimAttr = [@S0, @S1, @S2]} : memref<?x?x?xf32, "gpu"> to memref<?x?x?xf32, "gpu">
  %alloc_2 = memref.alloc(%dim_1, %dim_0, %dim) {kDiscSymbolicDimAttr = [@S0, @S1, @S2]} : memref<?x?x?xf32, "gpu">
  %1 = arith.index_cast %dim_1 : index to i32
  %2 = arith.index_cast %dim_0 : index to i32
  %3 = arith.index_cast %dim : index to i32
  %4 = arith.muli %2, %3 : i32
  %alloca = memref.alloca() : memref<2xi32, "cpu">
  memref.store %1, %alloca[%c0] : memref<2xi32, "cpu">
  memref.store %4, %alloca[%c1] : memref<2xi32, "cpu">
  %5 = arith.index_cast %4 : i32 to index
  %alloc_3 = memref.alloc(%dim_1, %5) {kDiscSymbolicDimAttr = [@S0, @S3]} : memref<?x?xf32, "gpu">
  %alloc_4 = memref.alloc(%5) {kDiscSymbolicDimAttr = [@S3]} : memref<?xf32, "gpu">
  %6 = arith.muli %dim_1, %5 : index
  %7 = arith.ceildivsi %6, %c256 : index
  %8 = arith.cmpi sgt, %7, %c108 : index
  scf.if %8 {
    "lmhlo.fusion"() ({
      "lmhlo.constant"(%alloc) {disc.device = "gpu", value = dense<0xFF800000> : tensor<f32>} : (memref<f32, "gpu">) -> ()
      "lmhlo.abs"(%reinterpret_cast, %alloc_2) {disc.device = "gpu"} : (memref<?x?x?xf32, "gpu">, memref<?x?x?xf32, "gpu">) -> ()
      "lmhlo.dynamic_reshape"(%alloc_2, %alloca, %alloc_3) {disc.device = "gpu"} : (memref<?x?x?xf32, "gpu">, memref<2xi32, "cpu">, memref<?x?xf32, "gpu">) -> ()
      "lmhlo.reduce"(%alloc_3, %alloc, %alloc_4) ({
      ^bb0(%arg1: memref<f32>, %arg2: memref<f32>, %arg3: memref<f32>):
        "lmhlo.maximum"(%arg1, %arg2, %arg3) {disc.device = "gpu"} : (memref<f32>, memref<f32>, memref<f32>) -> ()
        "lmhlo.terminator"() : () -> ()
      }) {dimensions = dense<0> : tensor<1xi64>, disc.device = "gpu"} : (memref<?x?xf32, "gpu">, memref<f32, "gpu">, memref<?xf32, "gpu">) -> ()
      "lmhlo.terminator"() : () -> ()
    }) {disc.device = "gpu", disc.fusion.name = "main_kColReduction_reduce__4_1_0", disc.fusion.tag = "8w32h", disc.fusion_type = "kColReduction", disc_col_reduction_schedule_hint = 1 : i32, disc_thread_per_block_hint = 256 : i32} : () -> ()
  } else {
    "lmhlo.fusion"() ({
      "lmhlo.constant"(%alloc) {disc.device = "gpu", value = dense<0xFF800000> : tensor<f32>} : (memref<f32, "gpu">) -> ()
      "lmhlo.abs"(%reinterpret_cast, %alloc_2) {disc.device = "gpu"} : (memref<?x?x?xf32, "gpu">, memref<?x?x?xf32, "gpu">) -> ()
      "lmhlo.dynamic_reshape"(%alloc_2, %alloca, %alloc_3) {disc.device = "gpu"} : (memref<?x?x?xf32, "gpu">, memref<2xi32, "cpu">, memref<?x?xf32, "gpu">) -> ()
      "lmhlo.reduce"(%alloc_3, %alloc, %alloc_4) ({
      ^bb0(%arg1: memref<f32>, %arg2: memref<f32>, %arg3: memref<f32>):
        "lmhlo.maximum"(%arg1, %arg2, %arg3) {disc.device = "gpu"} : (memref<f32>, memref<f32>, memref<f32>) -> ()
        "lmhlo.terminator"() : () -> ()
      }) {dimensions = dense<0> : tensor<1xi64>, disc.device = "gpu"} : (memref<?x?xf32, "gpu">, memref<f32, "gpu">, memref<?xf32, "gpu">) -> ()
      "lmhlo.terminator"() : () -> ()
    }) {disc.device = "gpu", disc.fusion.name = "main_kColReduction_reduce__4_1_0", disc.fusion.tag = "8w16h", disc.fusion_type = "kColReduction", disc_col_reduction_schedule_hint = 2 : i32, disc_thread_per_block_hint = 128 : i32} : () -> ()
  }
  memref.dealloc %alloc_3 : memref<?x?xf32, "gpu">
  memref.dealloc %alloc_2 : memref<?x?x?xf32, "gpu">
  memref.dealloc %alloc : memref<f32, "gpu">
  %alloca_5 = memref.alloca() : memref<2xi32, "cpu">
  memref.store %2, %alloca_5[%c0] : memref<2xi32, "cpu">
  memref.store %3, %alloca_5[%c1] : memref<2xi32, "cpu">
  %alloc_6 = memref.alloc(%dim_0, %dim) {kDiscSymbolicDimAttr = [@S1, @S2]} : memref<?x?xf32, "gpu">
  "lmhlo.dynamic_reshape"(%alloc_4, %alloca_5, %alloc_6) {disc.device = "gpu"} : (memref<?xf32, "gpu">, memref<2xi32, "cpu">, memref<?x?xf32, "gpu">) -> ()
  memref.dealloc %alloc_4 : memref<?xf32, "gpu">
  return %alloc_6 : memref<?x?xf32, "gpu">
}

// -----// IR Dump After RalInjectExecutionContextPass (disc-ral-inject-execution-context) //----- //
module {
  func.func @main(%arg0: !disc_ral.context) attributes {tf.entry_function = {input_placements = "gpu", inputs = "input0", output_placements = "gpu", outputs = "output0"}} {
    %c0 = arith.constant 0 : index
    %0 = "disc_ral.recv_input"(%arg0, %c0) : (!disc_ral.context, index) -> memref<?x?x?xf32, "gpu">
    %c108 = arith.constant 108 : index
    %c256 = arith.constant 256 : index
    %c0_0 = arith.constant 0 : index
    %c1 = arith.constant 1 : index
    %c2 = arith.constant 2 : index
    %dim = memref.dim %0, %c2 : memref<?x?x?xf32, "gpu">
    %dim_1 = memref.dim %0, %c1 : memref<?x?x?xf32, "gpu">
    %dim_2 = memref.dim %0, %c0_0 : memref<?x?x?xf32, "gpu">
    %alloc = memref.alloc() : memref<f32, "gpu">
    %1 = arith.muli %dim, %dim_1 : index
    %reinterpret_cast = memref.reinterpret_cast %0 to offset: [0], sizes: [%dim_2, %dim_1, %dim], strides: [%1, %dim, 1] {kDiscSymbolicDimAttr = [@S0, @S1, @S2]} : memref<?x?x?xf32, "gpu"> to memref<?x?x?xf32, "gpu">
    %alloc_3 = memref.alloc(%dim_2, %dim_1, %dim) {kDiscSymbolicDimAttr = [@S0, @S1, @S2]} : memref<?x?x?xf32, "gpu">
    %2 = arith.index_cast %dim_2 : index to i32
    %3 = arith.index_cast %dim_1 : index to i32
    %4 = arith.index_cast %dim : index to i32
    %5 = arith.muli %3, %4 : i32
    %alloca = memref.alloca() : memref<2xi32, "cpu">
    memref.store %2, %alloca[%c0_0] : memref<2xi32, "cpu">
    memref.store %5, %alloca[%c1] : memref<2xi32, "cpu">
    %6 = arith.index_cast %5 : i32 to index
    %alloc_4 = memref.alloc(%dim_2, %6) {kDiscSymbolicDimAttr = [@S0, @S3]} : memref<?x?xf32, "gpu">
    %alloc_5 = memref.alloc(%6) {kDiscSymbolicDimAttr = [@S3]} : memref<?xf32, "gpu">
    %7 = arith.muli %dim_2, %6 : index
    %8 = arith.ceildivsi %7, %c256 : index
    %9 = arith.cmpi sgt, %8, %c108 : index
    scf.if %9 {
      "lmhlo.fusion"() ({
        "lmhlo.constant"(%alloc) {disc.device = "gpu", value = dense<0xFF800000> : tensor<f32>} : (memref<f32, "gpu">) -> ()
        "lmhlo.abs"(%reinterpret_cast, %alloc_3) {disc.device = "gpu"} : (memref<?x?x?xf32, "gpu">, memref<?x?x?xf32, "gpu">) -> ()
        "lmhlo.dynamic_reshape"(%alloc_3, %alloca, %alloc_4) {disc.device = "gpu"} : (memref<?x?x?xf32, "gpu">, memref<2xi32, "cpu">, memref<?x?xf32, "gpu">) -> ()
        "lmhlo.reduce"(%alloc_4, %alloc, %alloc_5) ({
        ^bb0(%arg1: memref<f32>, %arg2: memref<f32>, %arg3: memref<f32>):
          "lmhlo.maximum"(%arg1, %arg2, %arg3) {disc.device = "gpu"} : (memref<f32>, memref<f32>, memref<f32>) -> ()
          "lmhlo.terminator"() : () -> ()
        }) {dimensions = dense<0> : tensor<1xi64>, disc.device = "gpu"} : (memref<?x?xf32, "gpu">, memref<f32, "gpu">, memref<?xf32, "gpu">) -> ()
        "lmhlo.terminator"() : () -> ()
      }) {disc.device = "gpu", disc.fusion.name = "main_kColReduction_reduce__4_1_0", disc.fusion.tag = "8w32h", disc.fusion_type = "kColReduction", disc_col_reduction_schedule_hint = 1 : i32, disc_thread_per_block_hint = 256 : i32} : () -> ()
    } else {
      "lmhlo.fusion"() ({
        "lmhlo.constant"(%alloc) {disc.device = "gpu", value = dense<0xFF800000> : tensor<f32>} : (memref<f32, "gpu">) -> ()
        "lmhlo.abs"(%reinterpret_cast, %alloc_3) {disc.device = "gpu"} : (memref<?x?x?xf32, "gpu">, memref<?x?x?xf32, "gpu">) -> ()
        "lmhlo.dynamic_reshape"(%alloc_3, %alloca, %alloc_4) {disc.device = "gpu"} : (memref<?x?x?xf32, "gpu">, memref<2xi32, "cpu">, memref<?x?xf32, "gpu">) -> ()
        "lmhlo.reduce"(%alloc_4, %alloc, %alloc_5) ({
        ^bb0(%arg1: memref<f32>, %arg2: memref<f32>, %arg3: memref<f32>):
          "lmhlo.maximum"(%arg1, %arg2, %arg3) {disc.device = "gpu"} : (memref<f32>, memref<f32>, memref<f32>) -> ()
          "lmhlo.terminator"() : () -> ()
        }) {dimensions = dense<0> : tensor<1xi64>, disc.device = "gpu"} : (memref<?x?xf32, "gpu">, memref<f32, "gpu">, memref<?xf32, "gpu">) -> ()
        "lmhlo.terminator"() : () -> ()
      }) {disc.device = "gpu", disc.fusion.name = "main_kColReduction_reduce__4_1_0", disc.fusion.tag = "8w16h", disc.fusion_type = "kColReduction", disc_col_reduction_schedule_hint = 2 : i32, disc_thread_per_block_hint = 128 : i32} : () -> ()
    }
    memref.dealloc %alloc_4 : memref<?x?xf32, "gpu">
    memref.dealloc %alloc_3 : memref<?x?x?xf32, "gpu">
    memref.dealloc %alloc : memref<f32, "gpu">
    %alloca_6 = memref.alloca() : memref<2xi32, "cpu">
    memref.store %3, %alloca_6[%c0_0] : memref<2xi32, "cpu">
    memref.store %4, %alloca_6[%c1] : memref<2xi32, "cpu">
    %alloc_7 = memref.alloc(%dim_1, %dim) {kDiscSymbolicDimAttr = [@S1, @S2]} : memref<?x?xf32, "gpu">
    "lmhlo.dynamic_reshape"(%alloc_5, %alloca_6, %alloc_7) {disc.device = "gpu"} : (memref<?xf32, "gpu">, memref<2xi32, "cpu">, memref<?x?xf32, "gpu">) -> ()
    memref.dealloc %alloc_5 : memref<?xf32, "gpu">
    %c0_8 = arith.constant 0 : index
    "disc_ral.send_output"(%arg0, %c0_8, %alloc_7) : (!disc_ral.context, index, memref<?x?xf32, "gpu">) -> ()
    return
  }
  "disc_shape.SymbolicDim"() {knownNegativeOne = false, knownNonNegative = true, knownNonSizeOne = false, knownNonSizeZero = false, sym_name = "S0", value = -9223372036854775808 : i64} : () -> ()
  "disc_shape.SymbolicDim"() {knownNegativeOne = false, knownNonNegative = true, knownNonSizeOne = false, knownNonSizeZero = false, sym_name = "S1", value = -9223372036854775808 : i64} : () -> ()
  "disc_shape.SymbolicDim"() {knownNegativeOne = false, knownNonNegative = true, knownNonSizeOne = false, knownNonSizeZero = false, sym_name = "S2", value = -9223372036854775808 : i64} : () -> ()
  "disc_shape.SymbolicDim"() {knownNegativeOne = false, knownNonNegative = true, knownNonSizeOne = false, knownNonSizeZero = false, sym_name = "S3", value = -9223372036854775808 : i64} : () -> ()
  func.func @shape_constraint_graph() {
    %0 = "disc_shape.dim"() {name = @S3} : () -> index
    %1 = "disc_shape.dim"() {name = @S1} : () -> index
    %2 = "disc_shape.dim"() {name = @S2} : () -> index
    "disc_shape.tie_product_equal"(%0, %1, %2) {operand_segment_sizes = array<i32: 1, 2>} : (index, index, index) -> ()
    return
  }
}


// -----// IR Dump After DiscLowerToLibraryCallPass (disc-lower-to-library-call) //----- //
func.func @main(%arg0: !disc_ral.context) attributes {tf.entry_function = {input_placements = "gpu", inputs = "input0", output_placements = "gpu", outputs = "output0"}} {
  %0 = llvm.mlir.constant(0 : i32) : i32
  %c0 = arith.constant 0 : index
  %c1 = arith.constant 1 : index
  %c2 = arith.constant 2 : index
  %c256 = arith.constant 256 : index
  %c108 = arith.constant 108 : index
  %1 = "disc_ral.dispatch"(%arg0, %c0) {backend_config = "", call_target_name = "ral_recv_input", device = "cpu", has_side_effect = false} : (!disc_ral.context, index) -> memref<?x?x?xf32, "gpu">
  %dim = memref.dim %1, %c2 : memref<?x?x?xf32, "gpu">
  %dim_0 = memref.dim %1, %c1 : memref<?x?x?xf32, "gpu">
  %dim_1 = memref.dim %1, %c0 : memref<?x?x?xf32, "gpu">
  %alloc = memref.alloc() : memref<f32, "gpu">
  %2 = arith.muli %dim, %dim_0 : index
  %reinterpret_cast = memref.reinterpret_cast %1 to offset: [0], sizes: [%dim_1, %dim_0, %dim], strides: [%2, %dim, 1] {kDiscSymbolicDimAttr = [@S0, @S1, @S2]} : memref<?x?x?xf32, "gpu"> to memref<?x?x?xf32, "gpu">
  %alloc_2 = memref.alloc(%dim_1, %dim_0, %dim) {kDiscSymbolicDimAttr = [@S0, @S1, @S2]} : memref<?x?x?xf32, "gpu">
  %3 = arith.index_cast %dim_1 : index to i32
  %4 = arith.index_cast %dim_0 : index to i32
  %5 = arith.index_cast %dim : index to i32
  %6 = arith.muli %4, %5 : i32
  %alloca = memref.alloca() : memref<2xi32, "cpu">
  memref.store %3, %alloca[%c0] : memref<2xi32, "cpu">
  memref.store %6, %alloca[%c1] : memref<2xi32, "cpu">
  %7 = arith.index_cast %6 : i32 to index
  %alloc_3 = memref.alloc(%dim_1, %7) {kDiscSymbolicDimAttr = [@S0, @S3]} : memref<?x?xf32, "gpu">
  %alloc_4 = memref.alloc(%7) {kDiscSymbolicDimAttr = [@S3]} : memref<?xf32, "gpu">
  %8 = arith.muli %dim_1, %7 : index
  %9 = arith.ceildivsi %8, %c256 : index
  %10 = arith.cmpi sgt, %9, %c108 : index
  scf.if %10 {
    "lmhlo.fusion"() ({
      "lmhlo.constant"(%alloc) {disc.device = "gpu", value = dense<0xFF800000> : tensor<f32>} : (memref<f32, "gpu">) -> ()
      "lmhlo.abs"(%reinterpret_cast, %alloc_2) {disc.device = "gpu"} : (memref<?x?x?xf32, "gpu">, memref<?x?x?xf32, "gpu">) -> ()
      "lmhlo.dynamic_reshape"(%alloc_2, %alloca, %alloc_3) {disc.device = "gpu"} : (memref<?x?x?xf32, "gpu">, memref<2xi32, "cpu">, memref<?x?xf32, "gpu">) -> ()
      "lmhlo.reduce"(%alloc_3, %alloc, %alloc_4) ({
      ^bb0(%arg1: memref<f32>, %arg2: memref<f32>, %arg3: memref<f32>):
        "lmhlo.maximum"(%arg1, %arg2, %arg3) {disc.device = "gpu"} : (memref<f32>, memref<f32>, memref<f32>) -> ()
        "lmhlo.terminator"() : () -> ()
      }) {dimensions = dense<0> : tensor<1xi64>, disc.device = "gpu"} : (memref<?x?xf32, "gpu">, memref<f32, "gpu">, memref<?xf32, "gpu">) -> ()
      "lmhlo.terminator"() : () -> ()
    }) {disc.device = "gpu", disc.fusion.name = "main_kColReduction_reduce__4_1_0", disc.fusion.tag = "8w32h", disc.fusion_type = "kColReduction", disc_col_reduction_schedule_hint = 1 : i32, disc_thread_per_block_hint = 256 : i32} : () -> ()
  } else {
    "lmhlo.fusion"() ({
      "lmhlo.constant"(%alloc) {disc.device = "gpu", value = dense<0xFF800000> : tensor<f32>} : (memref<f32, "gpu">) -> ()
      "lmhlo.abs"(%reinterpret_cast, %alloc_2) {disc.device = "gpu"} : (memref<?x?x?xf32, "gpu">, memref<?x?x?xf32, "gpu">) -> ()
      "lmhlo.dynamic_reshape"(%alloc_2, %alloca, %alloc_3) {disc.device = "gpu"} : (memref<?x?x?xf32, "gpu">, memref<2xi32, "cpu">, memref<?x?xf32, "gpu">) -> ()
      "lmhlo.reduce"(%alloc_3, %alloc, %alloc_4) ({
      ^bb0(%arg1: memref<f32>, %arg2: memref<f32>, %arg3: memref<f32>):
        "lmhlo.maximum"(%arg1, %arg2, %arg3) {disc.device = "gpu"} : (memref<f32>, memref<f32>, memref<f32>) -> ()
        "lmhlo.terminator"() : () -> ()
      }) {dimensions = dense<0> : tensor<1xi64>, disc.device = "gpu"} : (memref<?x?xf32, "gpu">, memref<f32, "gpu">, memref<?xf32, "gpu">) -> ()
      "lmhlo.terminator"() : () -> ()
    }) {disc.device = "gpu", disc.fusion.name = "main_kColReduction_reduce__4_1_0", disc.fusion.tag = "8w16h", disc.fusion_type = "kColReduction", disc_col_reduction_schedule_hint = 2 : i32, disc_thread_per_block_hint = 128 : i32} : () -> ()
  }
  memref.dealloc %alloc_3 : memref<?x?xf32, "gpu">
  memref.dealloc %alloc_2 : memref<?x?x?xf32, "gpu">
  memref.dealloc %alloc : memref<f32, "gpu">
  %alloca_5 = memref.alloca() : memref<2xi32, "cpu">
  memref.store %4, %alloca_5[%c0] : memref<2xi32, "cpu">
  memref.store %5, %alloca_5[%c1] : memref<2xi32, "cpu">
  %11 = llvm.inttoptr %0 : i32 to !llvm.ptr<i8>
  %alloca_6 = memref.alloca() : memref<2xindex, "cpu">
  memref.store %dim_0, %alloca_6[%c0] : memref<2xindex, "cpu">
  memref.store %dim, %alloca_6[%c1] : memref<2xindex, "cpu">
  %12 = "disc_ral.dispatch"(%arg0, %11, %alloc_4, %alloca_6) {backend_config = "", call_target_name = "inc_ref", device = "gpu", has_side_effect = false} : (!disc_ral.context, !llvm.ptr<i8>, memref<?xf32, "gpu">, memref<2xindex, "cpu">) -> memref<?x?xf32, "gpu">
  %reinterpret_cast_7 = memref.reinterpret_cast %12 to offset: [0], sizes: [%dim_0, %dim], strides: [%dim, 1] {kDiscSymbolicDimAttr = [@S1, @S2]} : memref<?x?xf32, "gpu"> to memref<?x?xf32, "gpu">
  memref.dealloc %alloc_4 : memref<?xf32, "gpu">
  "disc_ral.dispatch"(%arg0, %c0, %reinterpret_cast_7) {backend_config = "", call_target_name = "ral_send_output", device = "cpu", has_side_effect = false} : (!disc_ral.context, index, memref<?x?xf32, "gpu">) -> ()
  return
}

kColReduction <main_kColReduction_reduce__4_1_0___8w32h>, use_new: 0 schedule_hint: 1
kColReduction <main_kColReduction_reduce__4_1_0___8w16h>, use_new: 0 schedule_hint: 2
SymbolicDimMgr::save walkRankedTensorValue takes: 4 us
SymbolicDimMgr::save update attributes takes: 10 us
SymbolicDimMgr::updateProductEqualityMap simplifySymbolicDimProductPair takes: 7 us
productSet.size() = 2
SymbolicDimMgr::updateProductEqualityMap propagate graph takes: 5 us
SymbolicDimMgr::updateProductEqualityMap remove multiply takes: 12 us
SymbolicDimMgr::updateProductEqualityMap build toRemove  takes: 0 us
SymbolicDimMgr::updateProductEqualityMap apply toRemove  takes: 0 us
SymbolicDimMgr::save updateProductEqualityMap takes: 40 us
SymbolicDimMgr::save updateFunctionType takes: 4 us
SymbolicDimMgr::save collect symbolicDim ops takes: 9 us
SymbolicDimMgr::save remove symbolicDim ops takes: 0 us
SymbolicDimMgr::save remove unused production takes: 0 us
SymbolicDimMgr::save remove unused production #2 takes: 0 us
SymbolicDimMgr::save canonicalize the name takes: 3 us
SymbolicDimMgr::save replace the name takes: 10 us
SymbolicDimMgr::save updateFunctionType takes: 2 us
// -----// IR Dump After DiscLhloLegalizeRootsToParallelLoopsPass (disc-lhlo-legalize-roots-to-parallel-loops) //----- //
func.func @main(%arg0: !disc_ral.context) attributes {tf.entry_function = {input_placements = "gpu", inputs = "input0", output_placements = "gpu", outputs = "output0"}} {
  %c64 = arith.constant 64 : index
  %c128 = arith.constant 128 : index
  %c4 = arith.constant 4 : index
  %c16 = arith.constant 16 : index
  %c32 = arith.constant 32 : index
  %c8 = arith.constant 8 : index
  %0 = llvm.mlir.constant(0 : i32) : i32
  %c0 = arith.constant 0 : index
  %c1 = arith.constant 1 : index
  %c2 = arith.constant 2 : index
  %c256 = arith.constant 256 : index
  %c108 = arith.constant 108 : index
  %1 = "disc_ral.dispatch"(%arg0, %c0) {backend_config = "", call_target_name = "ral_recv_input", device = "cpu", has_side_effect = false} : (!disc_ral.context, index) -> memref<?x?x?xf32, "gpu">
  %dim = memref.dim %1, %c2 : memref<?x?x?xf32, "gpu">
  %dim_0 = memref.dim %1, %c1 : memref<?x?x?xf32, "gpu">
  %dim_1 = memref.dim %1, %c0 : memref<?x?x?xf32, "gpu">
  %alloc = memref.alloc() : memref<f32, "gpu">
  %2 = arith.muli %dim, %dim_0 : index
  %reinterpret_cast = memref.reinterpret_cast %1 to offset: [0], sizes: [%dim_1, %dim_0, %dim], strides: [%2, %dim, 1] {kDiscSymbolicDimAttr = [@S0, @S1, @S2]} : memref<?x?x?xf32, "gpu"> to memref<?x?x?xf32, "gpu">
  %alloc_2 = memref.alloc(%dim_1, %dim_0, %dim) {kDiscSymbolicDimAttr = [@S0, @S1, @S2]} : memref<?x?x?xf32, "gpu">
  %3 = arith.index_cast %dim_1 : index to i32
  %4 = arith.index_cast %dim_0 : index to i32
  %5 = arith.index_cast %dim : index to i32
  %6 = arith.muli %4, %5 : i32
  %alloca = memref.alloca() : memref<2xi32, "cpu">
  memref.store %3, %alloca[%c0] : memref<2xi32, "cpu">
  memref.store %6, %alloca[%c1] : memref<2xi32, "cpu">
  %7 = arith.index_cast %6 : i32 to index
  %alloc_3 = memref.alloc(%dim_1, %7) {kDiscSymbolicDimAttr = [@S0, @S3]} : memref<?x?xf32, "gpu">
  %alloc_4 = memref.alloc(%7) {kDiscSymbolicDimAttr = [@S3]} : memref<?xf32, "gpu">
  %8 = arith.muli %dim_1, %7 : index
  %9 = arith.ceildivsi %8, %c256 : index
  %10 = arith.cmpi sgt, %9, %c108 : index
  scf.if %10 {
    "lmhlo.fusion"() ({
      "lmhlo.constant"(%alloc) {disc.device = "gpu", value = dense<0xFF800000> : tensor<f32>} : (memref<f32, "gpu">) -> ()
      "lmhlo.abs"(%reinterpret_cast, %alloc_2) {disc.device = "gpu"} : (memref<?x?x?xf32, "gpu">, memref<?x?x?xf32, "gpu">) -> ()
      "lmhlo.dynamic_reshape"(%alloc_2, %alloca, %alloc_3) {disc.device = "gpu"} : (memref<?x?x?xf32, "gpu">, memref<2xi32, "cpu">, memref<?x?xf32, "gpu">) -> ()
      scf.parallel (%arg1) = (%c0) to (%7) step (%c1) {
        %16 = "disc_shape.delinearize"(%arg1, %7) : (index, index) -> index
        %17 = memref.load %alloc[] : memref<f32, "gpu">
        memref.store %17, %alloc_4[%16] : memref<?xf32, "gpu">
        scf.yield
      }
      %13 = arith.ceildivsi %7, %c8 : index // 1300/8, expect 1300/32 = 41
      %14 = arith.ceildivsi %dim_1, %c32 : index // 110 / 32, expect 1300/8/64 = 1
      %15 = arith.muli %13, %14 : index
      scf.parallel (%arg1, %arg2) = (%c0, %c0) to (%15, %c256) step (%c1, %c1) {
        %16 = memref.load %alloc[] : memref<f32, "gpu">
        %17 = arith.divui %arg2, %c8 : index
        %18 = arith.remui %arg2, %c8 : index
        %19 = arith.divui %arg1, %13 : index
        %20 = arith.remui %arg1, %13 : index
        %21 = arith.muli %19, %c32 : index
        %22 = arith.addi %21, %17 : index
        %23 = arith.muli %20, %c8 : index
        %24 = arith.addi %23, %18 : index
        %alloc_8 = memref.alloc() : memref<256xf32, #gpu.address_space<workgroup>>
        %25 = arith.cmpi ult, %22, %dim_1 : index
        %26 = arith.cmpi ult, %24, %7 : index
        %27 = arith.andi %25, %26 : i1
        scf.if %27 {
          %46 = memref.load %alloc_3[%22, %24] : memref<?x?xf32, "gpu">
          %47 = arith.maxf %46, %16 : f32
          memref.store %47, %alloc_8[%arg2] : memref<256xf32, #gpu.address_space<workgroup>>
        } else {
          memref.store %16, %alloc_8[%arg2] : memref<256xf32, #gpu.address_space<workgroup>>
        }
        gpu.barrier
        %28 = arith.cmpi ult, %17, %c16 : index
        %29 = arith.addi %22, %c16 : index
        %30 = arith.cmpi ult, %29, %dim_1 : index
        %31 = arith.andi %28, %30 : i1
        scf.if %31 {
          %46 = arith.addi %arg2, %c128 : index
          %47 = memref.load %alloc_8[%arg2] : memref<256xf32, #gpu.address_space<workgroup>>
          %48 = memref.load %alloc_8[%46] : memref<256xf32, #gpu.address_space<workgroup>>
          %49 = arith.maxf %47, %48 : f32
          memref.store %49, %alloc_8[%arg2] : memref<256xf32, #gpu.address_space<workgroup>>
        }
        gpu.barrier
        %32 = arith.cmpi ult, %17, %c8 : index
        %33 = arith.addi %22, %c8 : index
        %34 = arith.cmpi ult, %33, %dim_1 : index
        %35 = arith.andi %32, %34 : i1
        scf.if %35 {
          %46 = arith.addi %arg2, %c64 : index
          %47 = memref.load %alloc_8[%arg2] : memref<256xf32, #gpu.address_space<workgroup>>
          %48 = memref.load %alloc_8[%46] : memref<256xf32, #gpu.address_space<workgroup>>
          %49 = arith.maxf %47, %48 : f32
          memref.store %49, %alloc_8[%arg2] : memref<256xf32, #gpu.address_space<workgroup>>
        }
        gpu.barrier
        %36 = arith.cmpi ult, %17, %c4 : index
        %37 = arith.addi %22, %c4 : index
        %38 = arith.cmpi ult, %37, %dim_1 : index
        %39 = arith.andi %36, %38 : i1
        scf.if %39 {
          %46 = arith.addi %arg2, %c32 : index
          %47 = memref.load %alloc_8[%arg2] : memref<256xf32, #gpu.address_space<workgroup>>
          %48 = memref.load %alloc_8[%46] : memref<256xf32, #gpu.address_space<workgroup>>
          %49 = arith.maxf %47, %48 : f32
          memref.store %49, %alloc_8[%arg2] : memref<256xf32, #gpu.address_space<workgroup>>
        }
        gpu.barrier
        %40 = arith.cmpi ult, %17, %c2 : index
        %41 = arith.addi %22, %c2 : index
        %42 = arith.cmpi ult, %41, %dim_1 : index
        %43 = arith.andi %40, %42 : i1
        scf.if %43 {
          %46 = arith.addi %arg2, %c16 : index
          %47 = memref.load %alloc_8[%arg2] : memref<256xf32, #gpu.address_space<workgroup>>
          %48 = memref.load %alloc_8[%46] : memref<256xf32, #gpu.address_space<workgroup>>
          %49 = arith.maxf %47, %48 : f32
          memref.store %49, %alloc_8[%arg2] : memref<256xf32, #gpu.address_space<workgroup>>
        }
        gpu.barrier
        %44 = arith.cmpi eq, %17, %c0 : index
        %45 = arith.andi %44, %27 : i1
        scf.if %45 {
          %46 = arith.addi %arg2, %c8 : index
          %47 = memref.load %alloc_8[%arg2] : memref<256xf32, #gpu.address_space<workgroup>>
          %48 = memref.load %alloc_8[%46] : memref<256xf32, #gpu.address_space<workgroup>>
          %49 = arith.maxf %47, %48 : f32
          %50 = memref.atomic_rmw maxf %49, %alloc_4[%24] : (f32, memref<?xf32, "gpu">) -> f32
        }
        scf.yield
      }
      "lmhlo.terminator"() : () -> ()
    }) {disc.device = "gpu", disc.fusion.name = "main_kColReduction_reduce__4_1_0", disc.fusion.tag = "8w32h", disc.fusion_type = "kColReduction", disc_col_reduction_schedule_hint = 1 : i32, disc_thread_per_block_hint = 256 : i32} : () -> ()
  } else {
    "lmhlo.fusion"() ({
      "lmhlo.constant"(%alloc) {disc.device = "gpu", value = dense<0xFF800000> : tensor<f32>} : (memref<f32, "gpu">) -> ()
      "lmhlo.abs"(%reinterpret_cast, %alloc_2) {disc.device = "gpu"} : (memref<?x?x?xf32, "gpu">, memref<?x?x?xf32, "gpu">) -> ()
      "lmhlo.dynamic_reshape"(%alloc_2, %alloca, %alloc_3) {disc.device = "gpu"} : (memref<?x?x?xf32, "gpu">, memref<2xi32, "cpu">, memref<?x?xf32, "gpu">) -> ()
      scf.parallel (%arg1) = (%c0) to (%7) step (%c1) {
        %16 = "disc_shape.delinearize"(%arg1, %7) : (index, index) -> index
        %17 = memref.load %alloc[] : memref<f32, "gpu">
        memref.store %17, %alloc_4[%16] : memref<?xf32, "gpu">
        scf.yield
      }
      %13 = arith.ceildivsi %7, %c8 : index
      %14 = arith.ceildivsi %dim_1, %c16 : index
      %15 = arith.muli %13, %14 : index
      scf.parallel (%arg1, %arg2) = (%c0, %c0) to (%15, %c128) step (%c1, %c1) {
        %16 = memref.load %alloc[] : memref<f32, "gpu">
        %17 = arith.divui %arg2, %c8 : index
        %18 = arith.remui %arg2, %c8 : index
        %19 = arith.divui %arg1, %13 : index
        %20 = arith.remui %arg1, %13 : index
        %21 = arith.muli %19, %c16 : index
        %22 = arith.addi %21, %17 : index
        %23 = arith.muli %20, %c8 : index
        %24 = arith.addi %23, %18 : index
        %alloc_8 = memref.alloc() : memref<128xf32, #gpu.address_space<workgroup>>
        %25 = arith.cmpi ult, %22, %dim_1 : index
        %26 = arith.cmpi ult, %24, %7 : index
        %27 = arith.andi %25, %26 : i1
        scf.if %27 {
          %42 = memref.load %alloc_3[%22, %24] : memref<?x?xf32, "gpu">
          %43 = arith.maxf %42, %16 : f32
          memref.store %43, %alloc_8[%arg2] : memref<128xf32, #gpu.address_space<workgroup>>
        } else {
          memref.store %16, %alloc_8[%arg2] : memref<128xf32, #gpu.address_space<workgroup>>
        }
        gpu.barrier
        %28 = arith.cmpi ult, %17, %c8 : index
        %29 = arith.addi %22, %c8 : index
        %30 = arith.cmpi ult, %29, %dim_1 : index
        %31 = arith.andi %28, %30 : i1
        scf.if %31 {
          %42 = arith.addi %arg2, %c64 : index
          %43 = memref.load %alloc_8[%arg2] : memref<128xf32, #gpu.address_space<workgroup>>
          %44 = memref.load %alloc_8[%42] : memref<128xf32, #gpu.address_space<workgroup>>
          %45 = arith.maxf %43, %44 : f32
          memref.store %45, %alloc_8[%arg2] : memref<128xf32, #gpu.address_space<workgroup>>
        }
        gpu.barrier
        %32 = arith.cmpi ult, %17, %c4 : index
        %33 = arith.addi %22, %c4 : index
        %34 = arith.cmpi ult, %33, %dim_1 : index
        %35 = arith.andi %32, %34 : i1
        scf.if %35 {
          %42 = arith.addi %arg2, %c32 : index
          %43 = memref.load %alloc_8[%arg2] : memref<128xf32, #gpu.address_space<workgroup>>
          %44 = memref.load %alloc_8[%42] : memref<128xf32, #gpu.address_space<workgroup>>
          %45 = arith.maxf %43, %44 : f32
          memref.store %45, %alloc_8[%arg2] : memref<128xf32, #gpu.address_space<workgroup>>
        }
        gpu.barrier
        %36 = arith.cmpi ult, %17, %c2 : index
        %37 = arith.addi %22, %c2 : index
        %38 = arith.cmpi ult, %37, %dim_1 : index
        %39 = arith.andi %36, %38 : i1
        scf.if %39 {
          %42 = arith.addi %arg2, %c16 : index
          %43 = memref.load %alloc_8[%arg2] : memref<128xf32, #gpu.address_space<workgroup>>
          %44 = memref.load %alloc_8[%42] : memref<128xf32, #gpu.address_space<workgroup>>
          %45 = arith.maxf %43, %44 : f32
          memref.store %45, %alloc_8[%arg2] : memref<128xf32, #gpu.address_space<workgroup>>
        }
        gpu.barrier
        %40 = arith.cmpi eq, %17, %c0 : index
        %41 = arith.andi %40, %27 : i1
        scf.if %41 {
          %42 = arith.addi %arg2, %c8 : index
          %43 = memref.load %alloc_8[%arg2] : memref<128xf32, #gpu.address_space<workgroup>>
          %44 = memref.load %alloc_8[%42] : memref<128xf32, #gpu.address_space<workgroup>>
          %45 = arith.maxf %43, %44 : f32
          %46 = memref.atomic_rmw maxf %45, %alloc_4[%24] : (f32, memref<?xf32, "gpu">) -> f32
        }
        scf.yield
      }
      "lmhlo.terminator"() : () -> ()
    }) {disc.device = "gpu", disc.fusion.name = "main_kColReduction_reduce__4_1_0", disc.fusion.tag = "8w16h", disc.fusion_type = "kColReduction", disc_col_reduction_schedule_hint = 2 : i32, disc_thread_per_block_hint = 128 : i32} : () -> ()
  }
  memref.dealloc %alloc_3 : memref<?x?xf32, "gpu">
  memref.dealloc %alloc_2 : memref<?x?x?xf32, "gpu">
  memref.dealloc %alloc : memref<f32, "gpu">
  %alloca_5 = memref.alloca() : memref<2xi32, "cpu">
  memref.store %4, %alloca_5[%c0] : memref<2xi32, "cpu">
  memref.store %5, %alloca_5[%c1] : memref<2xi32, "cpu">
  %11 = llvm.inttoptr %0 : i32 to !llvm.ptr<i8>
  %alloca_6 = memref.alloca() : memref<2xindex, "cpu">
  memref.store %dim_0, %alloca_6[%c0] : memref<2xindex, "cpu">
  memref.store %dim, %alloca_6[%c1] : memref<2xindex, "cpu">
  %12 = "disc_ral.dispatch"(%arg0, %11, %alloc_4, %alloca_6) {backend_config = "", call_target_name = "inc_ref", device = "gpu", has_side_effect = false} : (!disc_ral.context, !llvm.ptr<i8>, memref<?xf32, "gpu">, memref<2xindex, "cpu">) -> memref<?x?xf32, "gpu">
  %reinterpret_cast_7 = memref.reinterpret_cast %12 to offset: [0], sizes: [%dim_0, %dim], strides: [%dim, 1] {kDiscSymbolicDimAttr = [@S1, @S2]} : memref<?x?xf32, "gpu"> to memref<?x?xf32, "gpu">
  memref.dealloc %alloc_4 : memref<?xf32, "gpu">
  "disc_ral.dispatch"(%arg0, %c0, %reinterpret_cast_7) {backend_config = "", call_target_name = "ral_send_output", device = "cpu", has_side_effect = false} : (!disc_ral.context, index, memref<?x?xf32, "gpu">) -> ()
  return
}

// -----// IR Dump After ExpandOps (memref-expand) //----- //
func.func @main(%arg0: !disc_ral.context) attributes {tf.entry_function = {input_placements = "gpu", inputs = "input0", output_placements = "gpu", outputs = "output0"}} {
  %c64 = arith.constant 64 : index
  %c128 = arith.constant 128 : index
  %c4 = arith.constant 4 : index
  %c16 = arith.constant 16 : index
  %c32 = arith.constant 32 : index
  %c8 = arith.constant 8 : index
  %0 = llvm.mlir.constant(0 : i32) : i32
  %c0 = arith.constant 0 : index
  %c1 = arith.constant 1 : index
  %c2 = arith.constant 2 : index
  %c256 = arith.constant 256 : index
  %c108 = arith.constant 108 : index
  %1 = "disc_ral.dispatch"(%arg0, %c0) {backend_config = "", call_target_name = "ral_recv_input", device = "cpu", has_side_effect = false} : (!disc_ral.context, index) -> memref<?x?x?xf32, "gpu">
  %dim = memref.dim %1, %c2 : memref<?x?x?xf32, "gpu">
  %dim_0 = memref.dim %1, %c1 : memref<?x?x?xf32, "gpu">
  %dim_1 = memref.dim %1, %c0 : memref<?x?x?xf32, "gpu">
  %alloc = memref.alloc() : memref<f32, "gpu">
  %2 = arith.muli %dim, %dim_0 : index
  %reinterpret_cast = memref.reinterpret_cast %1 to offset: [0], sizes: [%dim_1, %dim_0, %dim], strides: [%2, %dim, 1] {kDiscSymbolicDimAttr = [@S0, @S1, @S2]} : memref<?x?x?xf32, "gpu"> to memref<?x?x?xf32, "gpu">
  %alloc_2 = memref.alloc(%dim_1, %dim_0, %dim) {kDiscSymbolicDimAttr = [@S0, @S1, @S2]} : memref<?x?x?xf32, "gpu">
  %3 = arith.index_cast %dim_1 : index to i32
  %4 = arith.index_cast %dim_0 : index to i32
  %5 = arith.index_cast %dim : index to i32
  %6 = arith.muli %4, %5 : i32
  %alloca = memref.alloca() : memref<2xi32, "cpu">
  memref.store %3, %alloca[%c0] : memref<2xi32, "cpu">
  memref.store %6, %alloca[%c1] : memref<2xi32, "cpu">
  %7 = arith.index_cast %6 : i32 to index
  %alloc_3 = memref.alloc(%dim_1, %7) {kDiscSymbolicDimAttr = [@S0, @S3]} : memref<?x?xf32, "gpu">
  %alloc_4 = memref.alloc(%7) {kDiscSymbolicDimAttr = [@S3]} : memref<?xf32, "gpu">
  %8 = arith.muli %dim_1, %7 : index
  %9 = arith.ceildivsi %8, %c256 : index
  %10 = arith.cmpi sgt, %9, %c108 : index
  scf.if %10 {
    "lmhlo.fusion"() ({
      "lmhlo.constant"(%alloc) {disc.device = "gpu", value = dense<0xFF800000> : tensor<f32>} : (memref<f32, "gpu">) -> ()
      "lmhlo.abs"(%reinterpret_cast, %alloc_2) {disc.device = "gpu"} : (memref<?x?x?xf32, "gpu">, memref<?x?x?xf32, "gpu">) -> ()
      "lmhlo.dynamic_reshape"(%alloc_2, %alloca, %alloc_3) {disc.device = "gpu"} : (memref<?x?x?xf32, "gpu">, memref<2xi32, "cpu">, memref<?x?xf32, "gpu">) -> ()
      scf.parallel (%arg1) = (%c0) to (%7) step (%c1) {
        %16 = "disc_shape.delinearize"(%arg1, %7) : (index, index) -> index
        %17 = memref.load %alloc[] : memref<f32, "gpu">
        memref.store %17, %alloc_4[%16] : memref<?xf32, "gpu">
        scf.yield
      }
      %13 = arith.ceildivsi %7, %c8 : index
      %14 = arith.ceildivsi %dim_1, %c32 : index
      %15 = arith.muli %13, %14 : index
      scf.parallel (%arg1, %arg2) = (%c0, %c0) to (%15, %c256) step (%c1, %c1) {
        %16 = memref.load %alloc[] : memref<f32, "gpu">
        %17 = arith.divui %arg2, %c8 : index
        %18 = arith.remui %arg2, %c8 : index
        %19 = arith.divui %arg1, %13 : index
        %20 = arith.remui %arg1, %13 : index
        %21 = arith.muli %19, %c32 : index
        %22 = arith.addi %21, %17 : index
        %23 = arith.muli %20, %c8 : index
        %24 = arith.addi %23, %18 : index
        %alloc_8 = memref.alloc() : memref<256xf32, #gpu.address_space<workgroup>>
        %25 = arith.cmpi ult, %22, %dim_1 : index
        %26 = arith.cmpi ult, %24, %7 : index
        %27 = arith.andi %25, %26 : i1
        scf.if %27 {
          %46 = memref.load %alloc_3[%22, %24] : memref<?x?xf32, "gpu">
          %47 = arith.maxf %46, %16 : f32
          memref.store %47, %alloc_8[%arg2] : memref<256xf32, #gpu.address_space<workgroup>>
        } else {
          memref.store %16, %alloc_8[%arg2] : memref<256xf32, #gpu.address_space<workgroup>>
        }
        gpu.barrier
        %28 = arith.cmpi ult, %17, %c16 : index
        %29 = arith.addi %22, %c16 : index
        %30 = arith.cmpi ult, %29, %dim_1 : index
        %31 = arith.andi %28, %30 : i1
        scf.if %31 {
          %46 = arith.addi %arg2, %c128 : index
          %47 = memref.load %alloc_8[%arg2] : memref<256xf32, #gpu.address_space<workgroup>>
          %48 = memref.load %alloc_8[%46] : memref<256xf32, #gpu.address_space<workgroup>>
          %49 = arith.maxf %47, %48 : f32
          memref.store %49, %alloc_8[%arg2] : memref<256xf32, #gpu.address_space<workgroup>>
        }
        gpu.barrier
        %32 = arith.cmpi ult, %17, %c8 : index
        %33 = arith.addi %22, %c8 : index
        %34 = arith.cmpi ult, %33, %dim_1 : index
        %35 = arith.andi %32, %34 : i1
        scf.if %35 {
          %46 = arith.addi %arg2, %c64 : index
          %47 = memref.load %alloc_8[%arg2] : memref<256xf32, #gpu.address_space<workgroup>>
          %48 = memref.load %alloc_8[%46] : memref<256xf32, #gpu.address_space<workgroup>>
          %49 = arith.maxf %47, %48 : f32
          memref.store %49, %alloc_8[%arg2] : memref<256xf32, #gpu.address_space<workgroup>>
        }
        gpu.barrier
        %36 = arith.cmpi ult, %17, %c4 : index
        %37 = arith.addi %22, %c4 : index
        %38 = arith.cmpi ult, %37, %dim_1 : index
        %39 = arith.andi %36, %38 : i1
        scf.if %39 {
          %46 = arith.addi %arg2, %c32 : index
          %47 = memref.load %alloc_8[%arg2] : memref<256xf32, #gpu.address_space<workgroup>>
          %48 = memref.load %alloc_8[%46] : memref<256xf32, #gpu.address_space<workgroup>>
          %49 = arith.maxf %47, %48 : f32
          memref.store %49, %alloc_8[%arg2] : memref<256xf32, #gpu.address_space<workgroup>>
        }
        gpu.barrier
        %40 = arith.cmpi ult, %17, %c2 : index
        %41 = arith.addi %22, %c2 : index
        %42 = arith.cmpi ult, %41, %dim_1 : index
        %43 = arith.andi %40, %42 : i1
        scf.if %43 {
          %46 = arith.addi %arg2, %c16 : index
          %47 = memref.load %alloc_8[%arg2] : memref<256xf32, #gpu.address_space<workgroup>>
          %48 = memref.load %alloc_8[%46] : memref<256xf32, #gpu.address_space<workgroup>>
          %49 = arith.maxf %47, %48 : f32
          memref.store %49, %alloc_8[%arg2] : memref<256xf32, #gpu.address_space<workgroup>>
        }
        gpu.barrier
        %44 = arith.cmpi eq, %17, %c0 : index
        %45 = arith.andi %44, %27 : i1
        scf.if %45 {
          %46 = arith.addi %arg2, %c8 : index
          %47 = memref.load %alloc_8[%arg2] : memref<256xf32, #gpu.address_space<workgroup>>
          %48 = memref.load %alloc_8[%46] : memref<256xf32, #gpu.address_space<workgroup>>
          %49 = arith.maxf %47, %48 : f32
          %50 = memref.generic_atomic_rmw %alloc_4[%24] : memref<?xf32, "gpu"> {
          ^bb0(%arg3: f32):
            %51 = arith.cmpf ogt, %arg3, %49 : f32
            %52 = arith.select %51, %arg3, %49 : f32
            memref.atomic_yield %52 : f32
          }
        }
        scf.yield
      }
      "lmhlo.terminator"() : () -> ()
    }) {disc.device = "gpu", disc.fusion.name = "main_kColReduction_reduce__4_1_0", disc.fusion.tag = "8w32h", disc.fusion_type = "kColReduction", disc_col_reduction_schedule_hint = 1 : i32, disc_thread_per_block_hint = 256 : i32} : () -> ()
  } else {
    "lmhlo.fusion"() ({
      "lmhlo.constant"(%alloc) {disc.device = "gpu", value = dense<0xFF800000> : tensor<f32>} : (memref<f32, "gpu">) -> ()
      "lmhlo.abs"(%reinterpret_cast, %alloc_2) {disc.device = "gpu"} : (memref<?x?x?xf32, "gpu">, memref<?x?x?xf32, "gpu">) -> ()
      "lmhlo.dynamic_reshape"(%alloc_2, %alloca, %alloc_3) {disc.device = "gpu"} : (memref<?x?x?xf32, "gpu">, memref<2xi32, "cpu">, memref<?x?xf32, "gpu">) -> ()
      scf.parallel (%arg1) = (%c0) to (%7) step (%c1) {
        %16 = "disc_shape.delinearize"(%arg1, %7) : (index, index) -> index
        %17 = memref.load %alloc[] : memref<f32, "gpu">
        memref.store %17, %alloc_4[%16] : memref<?xf32, "gpu">
        scf.yield
      }
      %13 = arith.ceildivsi %7, %c8 : index
      %14 = arith.ceildivsi %dim_1, %c16 : index
      %15 = arith.muli %13, %14 : index
      scf.parallel (%arg1, %arg2) = (%c0, %c0) to (%15, %c128) step (%c1, %c1) {
        %16 = memref.load %alloc[] : memref<f32, "gpu">
        %17 = arith.divui %arg2, %c8 : index
        %18 = arith.remui %arg2, %c8 : index
        %19 = arith.divui %arg1, %13 : index
        %20 = arith.remui %arg1, %13 : index
        %21 = arith.muli %19, %c16 : index
        %22 = arith.addi %21, %17 : index
        %23 = arith.muli %20, %c8 : index
        %24 = arith.addi %23, %18 : index
        %alloc_8 = memref.alloc() : memref<128xf32, #gpu.address_space<workgroup>>
        %25 = arith.cmpi ult, %22, %dim_1 : index
        %26 = arith.cmpi ult, %24, %7 : index
        %27 = arith.andi %25, %26 : i1
        scf.if %27 {
          %42 = memref.load %alloc_3[%22, %24] : memref<?x?xf32, "gpu">
          %43 = arith.maxf %42, %16 : f32
          memref.store %43, %alloc_8[%arg2] : memref<128xf32, #gpu.address_space<workgroup>>
        } else {
          memref.store %16, %alloc_8[%arg2] : memref<128xf32, #gpu.address_space<workgroup>>
        }
        gpu.barrier
        %28 = arith.cmpi ult, %17, %c8 : index
        %29 = arith.addi %22, %c8 : index
        %30 = arith.cmpi ult, %29, %dim_1 : index
        %31 = arith.andi %28, %30 : i1
        scf.if %31 {
          %42 = arith.addi %arg2, %c64 : index
          %43 = memref.load %alloc_8[%arg2] : memref<128xf32, #gpu.address_space<workgroup>>
          %44 = memref.load %alloc_8[%42] : memref<128xf32, #gpu.address_space<workgroup>>
          %45 = arith.maxf %43, %44 : f32
          memref.store %45, %alloc_8[%arg2] : memref<128xf32, #gpu.address_space<workgroup>>
        }
        gpu.barrier
        %32 = arith.cmpi ult, %17, %c4 : index
        %33 = arith.addi %22, %c4 : index
        %34 = arith.cmpi ult, %33, %dim_1 : index
        %35 = arith.andi %32, %34 : i1
        scf.if %35 {
          %42 = arith.addi %arg2, %c32 : index
          %43 = memref.load %alloc_8[%arg2] : memref<128xf32, #gpu.address_space<workgroup>>
          %44 = memref.load %alloc_8[%42] : memref<128xf32, #gpu.address_space<workgroup>>
          %45 = arith.maxf %43, %44 : f32
          memref.store %45, %alloc_8[%arg2] : memref<128xf32, #gpu.address_space<workgroup>>
        }
        gpu.barrier
        %36 = arith.cmpi ult, %17, %c2 : index
        %37 = arith.addi %22, %c2 : index
        %38 = arith.cmpi ult, %37, %dim_1 : index
        %39 = arith.andi %36, %38 : i1
        scf.if %39 {
          %42 = arith.addi %arg2, %c16 : index
          %43 = memref.load %alloc_8[%arg2] : memref<128xf32, #gpu.address_space<workgroup>>
          %44 = memref.load %alloc_8[%42] : memref<128xf32, #gpu.address_space<workgroup>>
          %45 = arith.maxf %43, %44 : f32
          memref.store %45, %alloc_8[%arg2] : memref<128xf32, #gpu.address_space<workgroup>>
        }
        gpu.barrier
        %40 = arith.cmpi eq, %17, %c0 : index
        %41 = arith.andi %40, %27 : i1
        scf.if %41 {
          %42 = arith.addi %arg2, %c8 : index
          %43 = memref.load %alloc_8[%arg2] : memref<128xf32, #gpu.address_space<workgroup>>
          %44 = memref.load %alloc_8[%42] : memref<128xf32, #gpu.address_space<workgroup>>
          %45 = arith.maxf %43, %44 : f32
          %46 = memref.generic_atomic_rmw %alloc_4[%24] : memref<?xf32, "gpu"> {
          ^bb0(%arg3: f32):
            %47 = arith.cmpf ogt, %arg3, %45 : f32
            %48 = arith.select %47, %arg3, %45 : f32
            memref.atomic_yield %48 : f32
          }
        }
        scf.yield
      }
      "lmhlo.terminator"() : () -> ()
    }) {disc.device = "gpu", disc.fusion.name = "main_kColReduction_reduce__4_1_0", disc.fusion.tag = "8w16h", disc.fusion_type = "kColReduction", disc_col_reduction_schedule_hint = 2 : i32, disc_thread_per_block_hint = 128 : i32} : () -> ()
  }
  memref.dealloc %alloc_3 : memref<?x?xf32, "gpu">
  memref.dealloc %alloc_2 : memref<?x?x?xf32, "gpu">
  memref.dealloc %alloc : memref<f32, "gpu">
  %alloca_5 = memref.alloca() : memref<2xi32, "cpu">
  memref.store %4, %alloca_5[%c0] : memref<2xi32, "cpu">
  memref.store %5, %alloca_5[%c1] : memref<2xi32, "cpu">
  %11 = llvm.inttoptr %0 : i32 to !llvm.ptr<i8>
  %alloca_6 = memref.alloca() : memref<2xindex, "cpu">
  memref.store %dim_0, %alloca_6[%c0] : memref<2xindex, "cpu">
  memref.store %dim, %alloca_6[%c1] : memref<2xindex, "cpu">
  %12 = "disc_ral.dispatch"(%arg0, %11, %alloc_4, %alloca_6) {backend_config = "", call_target_name = "inc_ref", device = "gpu", has_side_effect = false} : (!disc_ral.context, !llvm.ptr<i8>, memref<?xf32, "gpu">, memref<2xindex, "cpu">) -> memref<?x?xf32, "gpu">
  %reinterpret_cast_7 = memref.reinterpret_cast %12 to offset: [0], sizes: [%dim_0, %dim], strides: [%dim, 1] {kDiscSymbolicDimAttr = [@S1, @S2]} : memref<?x?xf32, "gpu"> to memref<?x?xf32, "gpu">
  memref.dealloc %alloc_4 : memref<?xf32, "gpu">
  "disc_ral.dispatch"(%arg0, %c0, %reinterpret_cast_7) {backend_config = "", call_target_name = "ral_send_output", device = "cpu", has_side_effect = false} : (!disc_ral.context, index, memref<?x?xf32, "gpu">) -> ()
  return
}

// -----// IR Dump After InputInlineFusionPass (disc-input-inline-fusion) //----- //
func.func @main(%arg0: !disc_ral.context) attributes {tf.entry_function = {input_placements = "gpu", inputs = "input0", output_placements = "gpu", outputs = "output0"}} {
  %cst = arith.constant 0xFF800000 : f32
  %c64 = arith.constant 64 : index
  %c128 = arith.constant 128 : index
  %c4 = arith.constant 4 : index
  %c16 = arith.constant 16 : index
  %c32 = arith.constant 32 : index
  %c8 = arith.constant 8 : index
  %0 = llvm.mlir.constant(0 : i32) : i32
  %c0 = arith.constant 0 : index
  %c1 = arith.constant 1 : index
  %c2 = arith.constant 2 : index
  %c256 = arith.constant 256 : index
  %c108 = arith.constant 108 : index
  %1 = "disc_ral.dispatch"(%arg0, %c0) {backend_config = "", call_target_name = "ral_recv_input", device = "cpu", has_side_effect = false} : (!disc_ral.context, index) -> memref<?x?x?xf32, "gpu">
  %dim = memref.dim %1, %c2 : memref<?x?x?xf32, "gpu">
  %dim_0 = memref.dim %1, %c1 : memref<?x?x?xf32, "gpu">
  %dim_1 = memref.dim %1, %c0 : memref<?x?x?xf32, "gpu">
  %alloc = memref.alloc() : memref<f32, "gpu">
  %2 = arith.muli %dim, %dim_0 : index
  %reinterpret_cast = memref.reinterpret_cast %1 to offset: [0], sizes: [%dim_1, %dim_0, %dim], strides: [%2, %dim, 1] {kDiscSymbolicDimAttr = [@S0, @S1, @S2]} : memref<?x?x?xf32, "gpu"> to memref<?x?x?xf32, "gpu">
  %alloc_2 = memref.alloc(%dim_1, %dim_0, %dim) {kDiscSymbolicDimAttr = [@S0, @S1, @S2]} : memref<?x?x?xf32, "gpu">
  %3 = arith.index_cast %dim_1 : index to i32
  %4 = arith.index_cast %dim_0 : index to i32
  %5 = arith.index_cast %dim : index to i32
  %6 = arith.muli %4, %5 : i32
  %alloca = memref.alloca() : memref<2xi32, "cpu">
  memref.store %3, %alloca[%c0] : memref<2xi32, "cpu">
  memref.store %6, %alloca[%c1] : memref<2xi32, "cpu">
  %7 = arith.index_cast %6 : i32 to index
  %alloc_3 = memref.alloc(%dim_1, %7) {kDiscSymbolicDimAttr = [@S0, @S3]} : memref<?x?xf32, "gpu">
  %alloc_4 = memref.alloc(%7) {kDiscSymbolicDimAttr = [@S3]} : memref<?xf32, "gpu">
  %8 = arith.muli %dim_1, %7 : index
  %9 = arith.ceildivsi %8, %c256 : index
  %10 = arith.cmpi sgt, %9, %c108 : index
  scf.if %10 {
    "lmhlo.fusion"() ({
      scf.parallel (%arg1) = (%c0) to (%7) step (%c1) {
        %16 = "disc_shape.delinearize"(%arg1, %7) : (index, index) -> index
        memref.store %cst, %alloc_4[%16] : memref<?xf32, "gpu">
        scf.yield
      }
      %13 = arith.ceildivsi %7, %c8 : index
      %14 = arith.ceildivsi %dim_1, %c32 : index
      %15 = arith.muli %13, %14 : index
      scf.parallel (%arg1, %arg2) = (%c0, %c0) to (%15, %c256) step (%c1, %c1) {
        %16 = arith.divui %arg2, %c8 : index
        %17 = arith.remui %arg2, %c8 : index
        %18 = arith.divui %arg1, %13 : index
        %19 = arith.remui %arg1, %13 : index
        %20 = arith.muli %18, %c32 : index
        %21 = arith.addi %20, %16 : index
        %22 = arith.muli %19, %c8 : index
        %23 = arith.addi %22, %17 : index
        %alloc_8 = memref.alloc() : memref<256xf32, #gpu.address_space<workgroup>>
        %24 = arith.cmpi ult, %21, %dim_1 : index
        %25 = arith.cmpi ult, %23, %7 : index
        %26 = arith.andi %24, %25 : i1
        scf.if %26 {
          %45 = "disc_shape.linearize"(%21, %23, %dim_1, %7) {operand_segment_sizes = array<i32: 2, 2>} : (index, index, index, index) -> index
          %46:3 = "disc_shape.delinearize"(%45, %dim_1, %dim_0, %dim) : (index, index, index, index) -> (index, index, index)
          %47 = memref.load %reinterpret_cast[%46#0, %46#1, %46#2] : memref<?x?x?xf32, "gpu">
          %48 = math.absf %47 : f32
          memref.store %48, %alloc_8[%arg2] : memref<256xf32, #gpu.address_space<workgroup>>
        } else {
          memref.store %cst, %alloc_8[%arg2] : memref<256xf32, #gpu.address_space<workgroup>>
        }
        gpu.barrier
        %27 = arith.cmpi ult, %16, %c16 : index
        %28 = arith.addi %21, %c16 : index
        %29 = arith.cmpi ult, %28, %dim_1 : index
        %30 = arith.andi %27, %29 : i1
        scf.if %30 {
          %45 = arith.addi %arg2, %c128 : index
          %46 = memref.load %alloc_8[%arg2] : memref<256xf32, #gpu.address_space<workgroup>>
          %47 = memref.load %alloc_8[%45] : memref<256xf32, #gpu.address_space<workgroup>>
          %48 = arith.maxf %46, %47 : f32
          memref.store %48, %alloc_8[%arg2] : memref<256xf32, #gpu.address_space<workgroup>>
        }
        gpu.barrier
        %31 = arith.cmpi ult, %16, %c8 : index
        %32 = arith.addi %21, %c8 : index
        %33 = arith.cmpi ult, %32, %dim_1 : index
        %34 = arith.andi %31, %33 : i1
        scf.if %34 {
          %45 = arith.addi %arg2, %c64 : index
          %46 = memref.load %alloc_8[%arg2] : memref<256xf32, #gpu.address_space<workgroup>>
          %47 = memref.load %alloc_8[%45] : memref<256xf32, #gpu.address_space<workgroup>>
          %48 = arith.maxf %46, %47 : f32
          memref.store %48, %alloc_8[%arg2] : memref<256xf32, #gpu.address_space<workgroup>>
        }
        gpu.barrier
        %35 = arith.cmpi ult, %16, %c4 : index
        %36 = arith.addi %21, %c4 : index
        %37 = arith.cmpi ult, %36, %dim_1 : index
        %38 = arith.andi %35, %37 : i1
        scf.if %38 {
          %45 = arith.addi %arg2, %c32 : index
          %46 = memref.load %alloc_8[%arg2] : memref<256xf32, #gpu.address_space<workgroup>>
          %47 = memref.load %alloc_8[%45] : memref<256xf32, #gpu.address_space<workgroup>>
          %48 = arith.maxf %46, %47 : f32
          memref.store %48, %alloc_8[%arg2] : memref<256xf32, #gpu.address_space<workgroup>>
        }
        gpu.barrier
        %39 = arith.cmpi ult, %16, %c2 : index
        %40 = arith.addi %21, %c2 : index
        %41 = arith.cmpi ult, %40, %dim_1 : index
        %42 = arith.andi %39, %41 : i1
        scf.if %42 {
          %45 = arith.addi %arg2, %c16 : index
          %46 = memref.load %alloc_8[%arg2] : memref<256xf32, #gpu.address_space<workgroup>>
          %47 = memref.load %alloc_8[%45] : memref<256xf32, #gpu.address_space<workgroup>>
          %48 = arith.maxf %46, %47 : f32
          memref.store %48, %alloc_8[%arg2] : memref<256xf32, #gpu.address_space<workgroup>>
        }
        gpu.barrier
        %43 = arith.cmpi eq, %16, %c0 : index
        %44 = arith.andi %43, %26 : i1
        scf.if %44 {
          %45 = arith.addi %arg2, %c8 : index
          %46 = memref.load %alloc_8[%arg2] : memref<256xf32, #gpu.address_space<workgroup>>
          %47 = memref.load %alloc_8[%45] : memref<256xf32, #gpu.address_space<workgroup>>
          %48 = arith.maxf %46, %47 : f32
          %49 = memref.generic_atomic_rmw %alloc_4[%23] : memref<?xf32, "gpu"> {
          ^bb0(%arg3: f32):
            %50 = arith.cmpf ogt, %arg3, %48 : f32
            %51 = arith.select %50, %arg3, %48 : f32
            memref.atomic_yield %51 : f32
          }
        }
        scf.yield
      }
      "lmhlo.terminator"() : () -> ()
    }) {disc.device = "gpu", disc.fusion.name = "main_kColReduction_reduce__4_1_0", disc.fusion.tag = "8w32h", disc.fusion_type = "kColReduction", disc_col_reduction_schedule_hint = 1 : i32, disc_thread_per_block_hint = 256 : i32} : () -> ()
  } else {
    "lmhlo.fusion"() ({
      scf.parallel (%arg1) = (%c0) to (%7) step (%c1) {
        %16 = "disc_shape.delinearize"(%arg1, %7) : (index, index) -> index
        memref.store %cst, %alloc_4[%16] : memref<?xf32, "gpu">
        scf.yield
      }
      %13 = arith.ceildivsi %7, %c8 : index
      %14 = arith.ceildivsi %dim_1, %c16 : index
      %15 = arith.muli %13, %14 : index
      scf.parallel (%arg1, %arg2) = (%c0, %c0) to (%15, %c128) step (%c1, %c1) {
        %16 = arith.divui %arg2, %c8 : index
        %17 = arith.remui %arg2, %c8 : index
        %18 = arith.divui %arg1, %13 : index
        %19 = arith.remui %arg1, %13 : index
        %20 = arith.muli %18, %c16 : index
        %21 = arith.addi %20, %16 : index
        %22 = arith.muli %19, %c8 : index
        %23 = arith.addi %22, %17 : index
        %alloc_8 = memref.alloc() : memref<128xf32, #gpu.address_space<workgroup>>
        %24 = arith.cmpi ult, %21, %dim_1 : index
        %25 = arith.cmpi ult, %23, %7 : index
        %26 = arith.andi %24, %25 : i1
        scf.if %26 {
          %41 = "disc_shape.linearize"(%21, %23, %dim_1, %7) {operand_segment_sizes = array<i32: 2, 2>} : (index, index, index, index) -> index
          %42:3 = "disc_shape.delinearize"(%41, %dim_1, %dim_0, %dim) : (index, index, index, index) -> (index, index, index)
          %43 = memref.load %reinterpret_cast[%42#0, %42#1, %42#2] : memref<?x?x?xf32, "gpu">
          %44 = math.absf %43 : f32
          memref.store %44, %alloc_8[%arg2] : memref<128xf32, #gpu.address_space<workgroup>>
        } else {
          memref.store %cst, %alloc_8[%arg2] : memref<128xf32, #gpu.address_space<workgroup>>
        }
        gpu.barrier
        %27 = arith.cmpi ult, %16, %c8 : index
        %28 = arith.addi %21, %c8 : index
        %29 = arith.cmpi ult, %28, %dim_1 : index
        %30 = arith.andi %27, %29 : i1
        scf.if %30 {
          %41 = arith.addi %arg2, %c64 : index
          %42 = memref.load %alloc_8[%arg2] : memref<128xf32, #gpu.address_space<workgroup>>
          %43 = memref.load %alloc_8[%41] : memref<128xf32, #gpu.address_space<workgroup>>
          %44 = arith.maxf %42, %43 : f32
          memref.store %44, %alloc_8[%arg2] : memref<128xf32, #gpu.address_space<workgroup>>
        }
        gpu.barrier
        %31 = arith.cmpi ult, %16, %c4 : index
        %32 = arith.addi %21, %c4 : index
        %33 = arith.cmpi ult, %32, %dim_1 : index
        %34 = arith.andi %31, %33 : i1
        scf.if %34 {
          %41 = arith.addi %arg2, %c32 : index
          %42 = memref.load %alloc_8[%arg2] : memref<128xf32, #gpu.address_space<workgroup>>
          %43 = memref.load %alloc_8[%41] : memref<128xf32, #gpu.address_space<workgroup>>
          %44 = arith.maxf %42, %43 : f32
          memref.store %44, %alloc_8[%arg2] : memref<128xf32, #gpu.address_space<workgroup>>
        }
        gpu.barrier
        %35 = arith.cmpi ult, %16, %c2 : index
        %36 = arith.addi %21, %c2 : index
        %37 = arith.cmpi ult, %36, %dim_1 : index
        %38 = arith.andi %35, %37 : i1
        scf.if %38 {
          %41 = arith.addi %arg2, %c16 : index
          %42 = memref.load %alloc_8[%arg2] : memref<128xf32, #gpu.address_space<workgroup>>
          %43 = memref.load %alloc_8[%41] : memref<128xf32, #gpu.address_space<workgroup>>
          %44 = arith.maxf %42, %43 : f32
          memref.store %44, %alloc_8[%arg2] : memref<128xf32, #gpu.address_space<workgroup>>
        }
        gpu.barrier
        %39 = arith.cmpi eq, %16, %c0 : index
        %40 = arith.andi %39, %26 : i1
        scf.if %40 {
          %41 = arith.addi %arg2, %c8 : index
          %42 = memref.load %alloc_8[%arg2] : memref<128xf32, #gpu.address_space<workgroup>>
          %43 = memref.load %alloc_8[%41] : memref<128xf32, #gpu.address_space<workgroup>>
          %44 = arith.maxf %42, %43 : f32
          %45 = memref.generic_atomic_rmw %alloc_4[%23] : memref<?xf32, "gpu"> {
          ^bb0(%arg3: f32):
            %46 = arith.cmpf ogt, %arg3, %44 : f32
            %47 = arith.select %46, %arg3, %44 : f32
            memref.atomic_yield %47 : f32
          }
        }
        scf.yield
      }
      "lmhlo.terminator"() : () -> ()
    }) {disc.device = "gpu", disc.fusion.name = "main_kColReduction_reduce__4_1_0", disc.fusion.tag = "8w16h", disc.fusion_type = "kColReduction", disc_col_reduction_schedule_hint = 2 : i32, disc_thread_per_block_hint = 128 : i32} : () -> ()
  }
  memref.dealloc %alloc_3 : memref<?x?xf32, "gpu">
  memref.dealloc %alloc_2 : memref<?x?x?xf32, "gpu">
  memref.dealloc %alloc : memref<f32, "gpu">
  %alloca_5 = memref.alloca() : memref<2xi32, "cpu">
  memref.store %4, %alloca_5[%c0] : memref<2xi32, "cpu">
  memref.store %5, %alloca_5[%c1] : memref<2xi32, "cpu">
  %11 = llvm.inttoptr %0 : i32 to !llvm.ptr<i8>
  %alloca_6 = memref.alloca() : memref<2xindex, "cpu">
  memref.store %dim_0, %alloca_6[%c0] : memref<2xindex, "cpu">
  memref.store %dim, %alloca_6[%c1] : memref<2xindex, "cpu">
  %12 = "disc_ral.dispatch"(%arg0, %11, %alloc_4, %alloca_6) {backend_config = "", call_target_name = "inc_ref", device = "gpu", has_side_effect = false} : (!disc_ral.context, !llvm.ptr<i8>, memref<?xf32, "gpu">, memref<2xindex, "cpu">) -> memref<?x?xf32, "gpu">
  %reinterpret_cast_7 = memref.reinterpret_cast %12 to offset: [0], sizes: [%dim_0, %dim], strides: [%dim, 1] {kDiscSymbolicDimAttr = [@S1, @S2]} : memref<?x?xf32, "gpu"> to memref<?x?xf32, "gpu">
  memref.dealloc %alloc_4 : memref<?xf32, "gpu">
  "disc_ral.dispatch"(%arg0, %c0, %reinterpret_cast_7) {backend_config = "", call_target_name = "ral_send_output", device = "cpu", has_side_effect = false} : (!disc_ral.context, index, memref<?x?xf32, "gpu">) -> ()
  return
}

// -----// IR Dump After ArithExpandOps (arith-expand) //----- //
func.func @main(%arg0: !disc_ral.context) attributes {tf.entry_function = {input_placements = "gpu", inputs = "input0", output_placements = "gpu", outputs = "output0"}} {
  %cst = arith.constant 0xFF800000 : f32
  %c64 = arith.constant 64 : index
  %c128 = arith.constant 128 : index
  %c4 = arith.constant 4 : index
  %c16 = arith.constant 16 : index
  %c32 = arith.constant 32 : index
  %c8 = arith.constant 8 : index
  %0 = llvm.mlir.constant(0 : i32) : i32
  %c0 = arith.constant 0 : index
  %c1 = arith.constant 1 : index
  %c2 = arith.constant 2 : index
  %c256 = arith.constant 256 : index
  %c108 = arith.constant 108 : index
  %1 = "disc_ral.dispatch"(%arg0, %c0) {backend_config = "", call_target_name = "ral_recv_input", device = "cpu", has_side_effect = false} : (!disc_ral.context, index) -> memref<?x?x?xf32, "gpu">
  %dim = memref.dim %1, %c2 : memref<?x?x?xf32, "gpu">
  %dim_0 = memref.dim %1, %c1 : memref<?x?x?xf32, "gpu">
  %dim_1 = memref.dim %1, %c0 : memref<?x?x?xf32, "gpu">
  %alloc = memref.alloc() : memref<f32, "gpu">
  %2 = arith.muli %dim, %dim_0 : index
  %reinterpret_cast = memref.reinterpret_cast %1 to offset: [0], sizes: [%dim_1, %dim_0, %dim], strides: [%2, %dim, 1] {kDiscSymbolicDimAttr = [@S0, @S1, @S2]} : memref<?x?x?xf32, "gpu"> to memref<?x?x?xf32, "gpu">
  %alloc_2 = memref.alloc(%dim_1, %dim_0, %dim) {kDiscSymbolicDimAttr = [@S0, @S1, @S2]} : memref<?x?x?xf32, "gpu">
  %3 = arith.index_cast %dim_1 : index to i32
  %4 = arith.index_cast %dim_0 : index to i32
  %5 = arith.index_cast %dim : index to i32
  %6 = arith.muli %4, %5 : i32
  %alloca = memref.alloca() : memref<2xi32, "cpu">
  memref.store %3, %alloca[%c0] : memref<2xi32, "cpu">
  memref.store %6, %alloca[%c1] : memref<2xi32, "cpu">
  %7 = arith.index_cast %6 : i32 to index
  %alloc_3 = memref.alloc(%dim_1, %7) {kDiscSymbolicDimAttr = [@S0, @S3]} : memref<?x?xf32, "gpu">
  %alloc_4 = memref.alloc(%7) {kDiscSymbolicDimAttr = [@S3]} : memref<?xf32, "gpu">
  %8 = arith.muli %dim_1, %7 : index
  %c1_5 = arith.constant 1 : index
  %c0_6 = arith.constant 0 : index
  %c-1 = arith.constant -1 : index
  %9 = arith.cmpi sgt, %c256, %c0_6 : index
  %10 = arith.select %9, %c-1, %c1_5 : index
  %11 = arith.addi %10, %8 : index
  %12 = arith.divsi %11, %c256 : index
  %13 = arith.addi %c1_5, %12 : index
  %14 = arith.subi %c0_6, %8 : index
  %15 = arith.divsi %14, %c256 : index
  %16 = arith.subi %c0_6, %15 : index
  %17 = arith.cmpi slt, %8, %c0_6 : index
  %18 = arith.cmpi sgt, %8, %c0_6 : index
  %19 = arith.cmpi slt, %c256, %c0_6 : index
  %20 = arith.cmpi sgt, %c256, %c0_6 : index
  %21 = arith.andi %17, %19 : i1
  %22 = arith.andi %18, %20 : i1
  %23 = arith.ori %21, %22 : i1
  %24 = arith.select %23, %13, %16 : index
  %25 = arith.cmpi sgt, %24, %c108 : index
  scf.if %25 {
    "lmhlo.fusion"() ({
      scf.parallel (%arg1) = (%c0) to (%7) step (%c1) {
        %61 = "disc_shape.delinearize"(%arg1, %7) : (index, index) -> index
        memref.store %cst, %alloc_4[%61] : memref<?xf32, "gpu">
        scf.yield
      }
      %c1_10 = arith.constant 1 : index
      %c0_11 = arith.constant 0 : index
      %c-1_12 = arith.constant -1 : index
      %28 = arith.cmpi sgt, %c8, %c0_11 : index
      %29 = arith.select %28, %c-1_12, %c1_10 : index
      %30 = arith.addi %29, %7 : index
      %31 = arith.divsi %30, %c8 : index
      %32 = arith.addi %c1_10, %31 : index
      %33 = arith.subi %c0_11, %7 : index
      %34 = arith.divsi %33, %c8 : index
      %35 = arith.subi %c0_11, %34 : index
      %36 = arith.cmpi slt, %7, %c0_11 : index
      %37 = arith.cmpi sgt, %7, %c0_11 : index
      %38 = arith.cmpi slt, %c8, %c0_11 : index
      %39 = arith.cmpi sgt, %c8, %c0_11 : index
      %40 = arith.andi %36, %38 : i1
      %41 = arith.andi %37, %39 : i1
      %42 = arith.ori %40, %41 : i1
      %43 = arith.select %42, %32, %35 : index
      %c1_13 = arith.constant 1 : index
      %c0_14 = arith.constant 0 : index
      %c-1_15 = arith.constant -1 : index
      %44 = arith.cmpi sgt, %c32, %c0_14 : index
      %45 = arith.select %44, %c-1_15, %c1_13 : index
      %46 = arith.addi %45, %dim_1 : index
      %47 = arith.divsi %46, %c32 : index
      %48 = arith.addi %c1_13, %47 : index
      %49 = arith.subi %c0_14, %dim_1 : index
      %50 = arith.divsi %49, %c32 : index
      %51 = arith.subi %c0_14, %50 : index
      %52 = arith.cmpi slt, %dim_1, %c0_14 : index
      %53 = arith.cmpi sgt, %dim_1, %c0_14 : index
      %54 = arith.cmpi slt, %c32, %c0_14 : index
      %55 = arith.cmpi sgt, %c32, %c0_14 : index
      %56 = arith.andi %52, %54 : i1
      %57 = arith.andi %53, %55 : i1
      %58 = arith.ori %56, %57 : i1
      %59 = arith.select %58, %48, %51 : index
      %60 = arith.muli %43, %59 : index
      scf.parallel (%arg1, %arg2) = (%c0, %c0) to (%60, %c256) step (%c1, %c1) {
        %61 = arith.divui %arg2, %c8 : index
        %62 = arith.remui %arg2, %c8 : index
        %63 = arith.divui %arg1, %43 : index
        %64 = arith.remui %arg1, %43 : index
        %65 = arith.muli %63, %c32 : index
        %66 = arith.addi %65, %61 : index
        %67 = arith.muli %64, %c8 : index
        %68 = arith.addi %67, %62 : index
        %alloc_16 = memref.alloc() : memref<256xf32, #gpu.address_space<workgroup>>
        %69 = arith.cmpi ult, %66, %dim_1 : index
        %70 = arith.cmpi ult, %68, %7 : index
        %71 = arith.andi %69, %70 : i1
        scf.if %71 {
          %90 = "disc_shape.linearize"(%66, %68, %dim_1, %7) {operand_segment_sizes = array<i32: 2, 2>} : (index, index, index, index) -> index
          %91:3 = "disc_shape.delinearize"(%90, %dim_1, %dim_0, %dim) : (index, index, index, index) -> (index, index, index)
          %92 = memref.load %reinterpret_cast[%91#0, %91#1, %91#2] : memref<?x?x?xf32, "gpu">
          %93 = math.absf %92 : f32
          memref.store %93, %alloc_16[%arg2] : memref<256xf32, #gpu.address_space<workgroup>>
        } else {
          memref.store %cst, %alloc_16[%arg2] : memref<256xf32, #gpu.address_space<workgroup>>
        }
        gpu.barrier
        %72 = arith.cmpi ult, %61, %c16 : index
        %73 = arith.addi %66, %c16 : index
        %74 = arith.cmpi ult, %73, %dim_1 : index
        %75 = arith.andi %72, %74 : i1
        scf.if %75 {
          %90 = arith.addi %arg2, %c128 : index
          %91 = memref.load %alloc_16[%arg2] : memref<256xf32, #gpu.address_space<workgroup>>
          %92 = memref.load %alloc_16[%90] : memref<256xf32, #gpu.address_space<workgroup>>
          %93 = arith.cmpf ugt, %91, %92 : f32
          %94 = arith.select %93, %91, %92 : f32
          %95 = arith.cmpf uno, %92, %92 : f32
          %96 = arith.select %95, %92, %94 : f32
          memref.store %96, %alloc_16[%arg2] : memref<256xf32, #gpu.address_space<workgroup>>
        }
        gpu.barrier
        %76 = arith.cmpi ult, %61, %c8 : index
        %77 = arith.addi %66, %c8 : index
        %78 = arith.cmpi ult, %77, %dim_1 : index
        %79 = arith.andi %76, %78 : i1
        scf.if %79 {
          %90 = arith.addi %arg2, %c64 : index
          %91 = memref.load %alloc_16[%arg2] : memref<256xf32, #gpu.address_space<workgroup>>
          %92 = memref.load %alloc_16[%90] : memref<256xf32, #gpu.address_space<workgroup>>
          %93 = arith.cmpf ugt, %91, %92 : f32
          %94 = arith.select %93, %91, %92 : f32
          %95 = arith.cmpf uno, %92, %92 : f32
          %96 = arith.select %95, %92, %94 : f32
          memref.store %96, %alloc_16[%arg2] : memref<256xf32, #gpu.address_space<workgroup>>
        }
        gpu.barrier
        %80 = arith.cmpi ult, %61, %c4 : index
        %81 = arith.addi %66, %c4 : index
        %82 = arith.cmpi ult, %81, %dim_1 : index
        %83 = arith.andi %80, %82 : i1
        scf.if %83 {
          %90 = arith.addi %arg2, %c32 : index
          %91 = memref.load %alloc_16[%arg2] : memref<256xf32, #gpu.address_space<workgroup>>
          %92 = memref.load %alloc_16[%90] : memref<256xf32, #gpu.address_space<workgroup>>
          %93 = arith.cmpf ugt, %91, %92 : f32
          %94 = arith.select %93, %91, %92 : f32
          %95 = arith.cmpf uno, %92, %92 : f32
          %96 = arith.select %95, %92, %94 : f32
          memref.store %96, %alloc_16[%arg2] : memref<256xf32, #gpu.address_space<workgroup>>
        }
        gpu.barrier
        %84 = arith.cmpi ult, %61, %c2 : index
        %85 = arith.addi %66, %c2 : index
        %86 = arith.cmpi ult, %85, %dim_1 : index
        %87 = arith.andi %84, %86 : i1
        scf.if %87 {
          %90 = arith.addi %arg2, %c16 : index
          %91 = memref.load %alloc_16[%arg2] : memref<256xf32, #gpu.address_space<workgroup>>
          %92 = memref.load %alloc_16[%90] : memref<256xf32, #gpu.address_space<workgroup>>
          %93 = arith.cmpf ugt, %91, %92 : f32
          %94 = arith.select %93, %91, %92 : f32
          %95 = arith.cmpf uno, %92, %92 : f32
          %96 = arith.select %95, %92, %94 : f32
          memref.store %96, %alloc_16[%arg2] : memref<256xf32, #gpu.address_space<workgroup>>
        }
        gpu.barrier
        %88 = arith.cmpi eq, %61, %c0 : index
        %89 = arith.andi %88, %71 : i1
        scf.if %89 {
          %90 = arith.addi %arg2, %c8 : index
          %91 = memref.load %alloc_16[%arg2] : memref<256xf32, #gpu.address_space<workgroup>>
          %92 = memref.load %alloc_16[%90] : memref<256xf32, #gpu.address_space<workgroup>>
          %93 = arith.cmpf ugt, %91, %92 : f32
          %94 = arith.select %93, %91, %92 : f32
          %95 = arith.cmpf uno, %92, %92 : f32
          %96 = arith.select %95, %92, %94 : f32
          %97 = memref.generic_atomic_rmw %alloc_4[%68] : memref<?xf32, "gpu"> {
          ^bb0(%arg3: f32):
            %98 = arith.cmpf ogt, %arg3, %96 : f32
            %99 = arith.select %98, %arg3, %96 : f32
            memref.atomic_yield %99 : f32
          }
        }
        scf.yield
      }
      "lmhlo.terminator"() : () -> ()
    }) {disc.device = "gpu", disc.fusion.name = "main_kColReduction_reduce__4_1_0", disc.fusion.tag = "8w32h", disc.fusion_type = "kColReduction", disc_col_reduction_schedule_hint = 1 : i32, disc_thread_per_block_hint = 256 : i32} : () -> ()
  } else {
    "lmhlo.fusion"() ({
      scf.parallel (%arg1) = (%c0) to (%7) step (%c1) {
        %61 = "disc_shape.delinearize"(%arg1, %7) : (index, index) -> index
        memref.store %cst, %alloc_4[%61] : memref<?xf32, "gpu">
        scf.yield
      }
      %c1_10 = arith.constant 1 : index
      %c0_11 = arith.constant 0 : index
      %c-1_12 = arith.constant -1 : index
      %28 = arith.cmpi sgt, %c8, %c0_11 : index
      %29 = arith.select %28, %c-1_12, %c1_10 : index
      %30 = arith.addi %29, %7 : index
      %31 = arith.divsi %30, %c8 : index
      %32 = arith.addi %c1_10, %31 : index
      %33 = arith.subi %c0_11, %7 : index
      %34 = arith.divsi %33, %c8 : index
      %35 = arith.subi %c0_11, %34 : index
      %36 = arith.cmpi slt, %7, %c0_11 : index
      %37 = arith.cmpi sgt, %7, %c0_11 : index
      %38 = arith.cmpi slt, %c8, %c0_11 : index
      %39 = arith.cmpi sgt, %c8, %c0_11 : index
      %40 = arith.andi %36, %38 : i1
      %41 = arith.andi %37, %39 : i1
      %42 = arith.ori %40, %41 : i1
      %43 = arith.select %42, %32, %35 : index
      %c1_13 = arith.constant 1 : index
      %c0_14 = arith.constant 0 : index
      %c-1_15 = arith.constant -1 : index
      %44 = arith.cmpi sgt, %c16, %c0_14 : index
      %45 = arith.select %44, %c-1_15, %c1_13 : index
      %46 = arith.addi %45, %dim_1 : index
      %47 = arith.divsi %46, %c16 : index
      %48 = arith.addi %c1_13, %47 : index
      %49 = arith.subi %c0_14, %dim_1 : index
      %50 = arith.divsi %49, %c16 : index
      %51 = arith.subi %c0_14, %50 : index
      %52 = arith.cmpi slt, %dim_1, %c0_14 : index
      %53 = arith.cmpi sgt, %dim_1, %c0_14 : index
      %54 = arith.cmpi slt, %c16, %c0_14 : index
      %55 = arith.cmpi sgt, %c16, %c0_14 : index
      %56 = arith.andi %52, %54 : i1
      %57 = arith.andi %53, %55 : i1
      %58 = arith.ori %56, %57 : i1
      %59 = arith.select %58, %48, %51 : index
      %60 = arith.muli %43, %59 : index
      scf.parallel (%arg1, %arg2) = (%c0, %c0) to (%60, %c128) step (%c1, %c1) {
        %61 = arith.divui %arg2, %c8 : index
        %62 = arith.remui %arg2, %c8 : index
        %63 = arith.divui %arg1, %43 : index
        %64 = arith.remui %arg1, %43 : index
        %65 = arith.muli %63, %c16 : index
        %66 = arith.addi %65, %61 : index
        %67 = arith.muli %64, %c8 : index
        %68 = arith.addi %67, %62 : index
        %alloc_16 = memref.alloc() : memref<128xf32, #gpu.address_space<workgroup>>
        %69 = arith.cmpi ult, %66, %dim_1 : index
        %70 = arith.cmpi ult, %68, %7 : index
        %71 = arith.andi %69, %70 : i1
        scf.if %71 {
          %86 = "disc_shape.linearize"(%66, %68, %dim_1, %7) {operand_segment_sizes = array<i32: 2, 2>} : (index, index, index, index) -> index
          %87:3 = "disc_shape.delinearize"(%86, %dim_1, %dim_0, %dim) : (index, index, index, index) -> (index, index, index)
          %88 = memref.load %reinterpret_cast[%87#0, %87#1, %87#2] : memref<?x?x?xf32, "gpu">
          %89 = math.absf %88 : f32
          memref.store %89, %alloc_16[%arg2] : memref<128xf32, #gpu.address_space<workgroup>>
        } else {
          memref.store %cst, %alloc_16[%arg2] : memref<128xf32, #gpu.address_space<workgroup>>
        }
        gpu.barrier
        %72 = arith.cmpi ult, %61, %c8 : index
        %73 = arith.addi %66, %c8 : index
        %74 = arith.cmpi ult, %73, %dim_1 : index
        %75 = arith.andi %72, %74 : i1
        scf.if %75 {
          %86 = arith.addi %arg2, %c64 : index
          %87 = memref.load %alloc_16[%arg2] : memref<128xf32, #gpu.address_space<workgroup>>
          %88 = memref.load %alloc_16[%86] : memref<128xf32, #gpu.address_space<workgroup>>
          %89 = arith.cmpf ugt, %87, %88 : f32
          %90 = arith.select %89, %87, %88 : f32
          %91 = arith.cmpf uno, %88, %88 : f32
          %92 = arith.select %91, %88, %90 : f32
          memref.store %92, %alloc_16[%arg2] : memref<128xf32, #gpu.address_space<workgroup>>
        }
        gpu.barrier
        %76 = arith.cmpi ult, %61, %c4 : index
        %77 = arith.addi %66, %c4 : index
        %78 = arith.cmpi ult, %77, %dim_1 : index
        %79 = arith.andi %76, %78 : i1
        scf.if %79 {
          %86 = arith.addi %arg2, %c32 : index
          %87 = memref.load %alloc_16[%arg2] : memref<128xf32, #gpu.address_space<workgroup>>
          %88 = memref.load %alloc_16[%86] : memref<128xf32, #gpu.address_space<workgroup>>
          %89 = arith.cmpf ugt, %87, %88 : f32
          %90 = arith.select %89, %87, %88 : f32
          %91 = arith.cmpf uno, %88, %88 : f32
          %92 = arith.select %91, %88, %90 : f32
          memref.store %92, %alloc_16[%arg2] : memref<128xf32, #gpu.address_space<workgroup>>
        }
        gpu.barrier
        %80 = arith.cmpi ult, %61, %c2 : index
        %81 = arith.addi %66, %c2 : index
        %82 = arith.cmpi ult, %81, %dim_1 : index
        %83 = arith.andi %80, %82 : i1
        scf.if %83 {
          %86 = arith.addi %arg2, %c16 : index
          %87 = memref.load %alloc_16[%arg2] : memref<128xf32, #gpu.address_space<workgroup>>
          %88 = memref.load %alloc_16[%86] : memref<128xf32, #gpu.address_space<workgroup>>
          %89 = arith.cmpf ugt, %87, %88 : f32
          %90 = arith.select %89, %87, %88 : f32
          %91 = arith.cmpf uno, %88, %88 : f32
          %92 = arith.select %91, %88, %90 : f32
          memref.store %92, %alloc_16[%arg2] : memref<128xf32, #gpu.address_space<workgroup>>
        }
        gpu.barrier
        %84 = arith.cmpi eq, %61, %c0 : index
        %85 = arith.andi %84, %71 : i1
        scf.if %85 {
          %86 = arith.addi %arg2, %c8 : index
          %87 = memref.load %alloc_16[%arg2] : memref<128xf32, #gpu.address_space<workgroup>>
          %88 = memref.load %alloc_16[%86] : memref<128xf32, #gpu.address_space<workgroup>>
          %89 = arith.cmpf ugt, %87, %88 : f32
          %90 = arith.select %89, %87, %88 : f32
          %91 = arith.cmpf uno, %88, %88 : f32
          %92 = arith.select %91, %88, %90 : f32
          %93 = memref.generic_atomic_rmw %alloc_4[%68] : memref<?xf32, "gpu"> {
          ^bb0(%arg3: f32):
            %94 = arith.cmpf ogt, %arg3, %92 : f32
            %95 = arith.select %94, %arg3, %92 : f32
            memref.atomic_yield %95 : f32
          }
        }
        scf.yield
      }
      "lmhlo.terminator"() : () -> ()
    }) {disc.device = "gpu", disc.fusion.name = "main_kColReduction_reduce__4_1_0", disc.fusion.tag = "8w16h", disc.fusion_type = "kColReduction", disc_col_reduction_schedule_hint = 2 : i32, disc_thread_per_block_hint = 128 : i32} : () -> ()
  }
  memref.dealloc %alloc_3 : memref<?x?xf32, "gpu">
  memref.dealloc %alloc_2 : memref<?x?x?xf32, "gpu">
  memref.dealloc %alloc : memref<f32, "gpu">
  %alloca_7 = memref.alloca() : memref<2xi32, "cpu">
  memref.store %4, %alloca_7[%c0] : memref<2xi32, "cpu">
  memref.store %5, %alloca_7[%c1] : memref<2xi32, "cpu">
  %26 = llvm.inttoptr %0 : i32 to !llvm.ptr<i8>
  %alloca_8 = memref.alloca() : memref<2xindex, "cpu">
  memref.store %dim_0, %alloca_8[%c0] : memref<2xindex, "cpu">
  memref.store %dim, %alloca_8[%c1] : memref<2xindex, "cpu">
  %27 = "disc_ral.dispatch"(%arg0, %26, %alloc_4, %alloca_8) {backend_config = "", call_target_name = "inc_ref", device = "gpu", has_side_effect = false} : (!disc_ral.context, !llvm.ptr<i8>, memref<?xf32, "gpu">, memref<2xindex, "cpu">) -> memref<?x?xf32, "gpu">
  %reinterpret_cast_9 = memref.reinterpret_cast %27 to offset: [0], sizes: [%dim_0, %dim], strides: [%dim, 1] {kDiscSymbolicDimAttr = [@S1, @S2]} : memref<?x?xf32, "gpu"> to memref<?x?xf32, "gpu">
  memref.dealloc %alloc_4 : memref<?xf32, "gpu">
  "disc_ral.dispatch"(%arg0, %c0, %reinterpret_cast_9) {backend_config = "", call_target_name = "ral_send_output", device = "cpu", has_side_effect = false} : (!disc_ral.context, index, memref<?x?xf32, "gpu">) -> ()
  return
}

// -----// IR Dump After DiscBF16ExpansionPass (disc-bf16-expansion) //----- //
func.func @main(%arg0: !disc_ral.context) attributes {tf.entry_function = {input_placements = "gpu", inputs = "input0", output_placements = "gpu", outputs = "output0"}} {
  %c-1 = arith.constant -1 : index
  %cst = arith.constant 0xFF800000 : f32
  %c64 = arith.constant 64 : index
  %c128 = arith.constant 128 : index
  %c4 = arith.constant 4 : index
  %c16 = arith.constant 16 : index
  %c32 = arith.constant 32 : index
  %c8 = arith.constant 8 : index
  %0 = llvm.mlir.constant(0 : i32) : i32
  %c0 = arith.constant 0 : index
  %c1 = arith.constant 1 : index
  %c2 = arith.constant 2 : index
  %c256 = arith.constant 256 : index
  %c108 = arith.constant 108 : index
  %1 = "disc_ral.dispatch"(%arg0, %c0) {backend_config = "", call_target_name = "ral_recv_input", device = "cpu", has_side_effect = false} : (!disc_ral.context, index) -> memref<?x?x?xf32, "gpu">
  %dim = memref.dim %1, %c2 : memref<?x?x?xf32, "gpu">
  %dim_0 = memref.dim %1, %c1 : memref<?x?x?xf32, "gpu">
  %dim_1 = memref.dim %1, %c0 : memref<?x?x?xf32, "gpu">
  %alloc = memref.alloc() : memref<f32, "gpu">
  %2 = arith.muli %dim, %dim_0 : index
  %reinterpret_cast = memref.reinterpret_cast %1 to offset: [0], sizes: [%dim_1, %dim_0, %dim], strides: [%2, %dim, 1] {kDiscSymbolicDimAttr = [@S0, @S1, @S2]} : memref<?x?x?xf32, "gpu"> to memref<?x?x?xf32, "gpu">
  %alloc_2 = memref.alloc(%dim_1, %dim_0, %dim) {kDiscSymbolicDimAttr = [@S0, @S1, @S2]} : memref<?x?x?xf32, "gpu">
  %3 = arith.index_cast %dim_1 : index to i32
  %4 = arith.index_cast %dim_0 : index to i32
  %5 = arith.index_cast %dim : index to i32
  %6 = arith.muli %4, %5 : i32
  %alloca = memref.alloca() : memref<2xi32, "cpu">
  memref.store %3, %alloca[%c0] : memref<2xi32, "cpu">
  memref.store %6, %alloca[%c1] : memref<2xi32, "cpu">
  %7 = arith.index_cast %6 : i32 to index
  %alloc_3 = memref.alloc(%dim_1, %7) {kDiscSymbolicDimAttr = [@S0, @S3]} : memref<?x?xf32, "gpu">
  %alloc_4 = memref.alloc(%7) {kDiscSymbolicDimAttr = [@S3]} : memref<?xf32, "gpu">
  %8 = arith.muli %dim_1, %7 : index
  %9 = arith.addi %8, %c-1 : index
  %10 = arith.divsi %9, %c256 : index
  %11 = arith.addi %10, %c1 : index
  %12 = arith.subi %c0, %8 : index
  %13 = arith.divsi %12, %c256 : index
  %14 = arith.subi %c0, %13 : index
  %15 = arith.cmpi sgt, %8, %c0 : index
  %16 = arith.select %15, %11, %14 : index
  %17 = arith.cmpi sgt, %16, %c108 : index
  scf.if %17 {
    "lmhlo.fusion"() ({
      scf.parallel (%arg1) = (%c0) to (%7) step (%c1) {
        %37 = "disc_shape.delinearize"(%arg1, %7) : (index, index) -> index
        memref.store %cst, %alloc_4[%37] : memref<?xf32, "gpu">
        scf.yield
      }
      %20 = arith.addi %7, %c-1 : index
      %21 = arith.divsi %20, %c8 : index
      %22 = arith.addi %21, %c1 : index
      %23 = arith.subi %c0, %7 : index
      %24 = arith.divsi %23, %c8 : index
      %25 = arith.subi %c0, %24 : index
      %26 = arith.cmpi sgt, %7, %c0 : index
      %27 = arith.select %26, %22, %25 : index
      %28 = arith.addi %dim_1, %c-1 : index
      %29 = arith.divsi %28, %c32 : index
      %30 = arith.addi %29, %c1 : index
      %31 = arith.subi %c0, %dim_1 : index
      %32 = arith.divsi %31, %c32 : index
      %33 = arith.subi %c0, %32 : index
      %34 = arith.cmpi sgt, %dim_1, %c0 : index
      %35 = arith.select %34, %30, %33 : index
      %36 = arith.muli %27, %35 : index
      scf.parallel (%arg1, %arg2) = (%c0, %c0) to (%36, %c256) step (%c1, %c1) {
        %37 = arith.divui %arg2, %c8 : index
        %38 = arith.remui %arg2, %c8 : index
        %39 = arith.divui %arg1, %27 : index
        %40 = arith.remui %arg1, %27 : index
        %41 = arith.muli %39, %c32 : index
        %42 = arith.addi %41, %37 : index
        %43 = arith.muli %40, %c8 : index
        %44 = arith.addi %43, %38 : index
        %alloc_8 = memref.alloc() : memref<256xf32, #gpu.address_space<workgroup>>
        %45 = arith.cmpi ult, %42, %dim_1 : index
        %46 = arith.cmpi ult, %44, %7 : index
        %47 = arith.andi %45, %46 : i1
        scf.if %47 {
          %66 = "disc_shape.linearize"(%42, %44, %dim_1, %7) {operand_segment_sizes = array<i32: 2, 2>} : (index, index, index, index) -> index
          %67:3 = "disc_shape.delinearize"(%66, %dim_1, %dim_0, %dim) : (index, index, index, index) -> (index, index, index)
          %68 = memref.load %reinterpret_cast[%67#0, %67#1, %67#2] : memref<?x?x?xf32, "gpu">
          %69 = math.absf %68 : f32
          memref.store %69, %alloc_8[%arg2] : memref<256xf32, #gpu.address_space<workgroup>>
        } else {
          memref.store %cst, %alloc_8[%arg2] : memref<256xf32, #gpu.address_space<workgroup>>
        }
        gpu.barrier
        %48 = arith.cmpi ult, %37, %c16 : index
        %49 = arith.addi %42, %c16 : index
        %50 = arith.cmpi ult, %49, %dim_1 : index
        %51 = arith.andi %48, %50 : i1
        scf.if %51 {
          %66 = arith.addi %arg2, %c128 : index
          %67 = memref.load %alloc_8[%arg2] : memref<256xf32, #gpu.address_space<workgroup>>
          %68 = memref.load %alloc_8[%66] : memref<256xf32, #gpu.address_space<workgroup>>
          %69 = arith.cmpf ugt, %67, %68 : f32
          %70 = arith.select %69, %67, %68 : f32
          %71 = arith.cmpf uno, %68, %68 : f32
          %72 = arith.select %71, %68, %70 : f32
          memref.store %72, %alloc_8[%arg2] : memref<256xf32, #gpu.address_space<workgroup>>
        }
        gpu.barrier
        %52 = arith.cmpi ult, %37, %c8 : index
        %53 = arith.addi %42, %c8 : index
        %54 = arith.cmpi ult, %53, %dim_1 : index
        %55 = arith.andi %52, %54 : i1
        scf.if %55 {
          %66 = arith.addi %arg2, %c64 : index
          %67 = memref.load %alloc_8[%arg2] : memref<256xf32, #gpu.address_space<workgroup>>
          %68 = memref.load %alloc_8[%66] : memref<256xf32, #gpu.address_space<workgroup>>
          %69 = arith.cmpf ugt, %67, %68 : f32
          %70 = arith.select %69, %67, %68 : f32
          %71 = arith.cmpf uno, %68, %68 : f32
          %72 = arith.select %71, %68, %70 : f32
          memref.store %72, %alloc_8[%arg2] : memref<256xf32, #gpu.address_space<workgroup>>
        }
        gpu.barrier
        %56 = arith.cmpi ult, %37, %c4 : index
        %57 = arith.addi %42, %c4 : index
        %58 = arith.cmpi ult, %57, %dim_1 : index
        %59 = arith.andi %56, %58 : i1
        scf.if %59 {
          %66 = arith.addi %arg2, %c32 : index
          %67 = memref.load %alloc_8[%arg2] : memref<256xf32, #gpu.address_space<workgroup>>
          %68 = memref.load %alloc_8[%66] : memref<256xf32, #gpu.address_space<workgroup>>
          %69 = arith.cmpf ugt, %67, %68 : f32
          %70 = arith.select %69, %67, %68 : f32
          %71 = arith.cmpf uno, %68, %68 : f32
          %72 = arith.select %71, %68, %70 : f32
          memref.store %72, %alloc_8[%arg2] : memref<256xf32, #gpu.address_space<workgroup>>
        }
        gpu.barrier
        %60 = arith.cmpi ult, %37, %c2 : index
        %61 = arith.addi %42, %c2 : index
        %62 = arith.cmpi ult, %61, %dim_1 : index
        %63 = arith.andi %60, %62 : i1
        scf.if %63 {
          %66 = arith.addi %arg2, %c16 : index
          %67 = memref.load %alloc_8[%arg2] : memref<256xf32, #gpu.address_space<workgroup>>
          %68 = memref.load %alloc_8[%66] : memref<256xf32, #gpu.address_space<workgroup>>
          %69 = arith.cmpf ugt, %67, %68 : f32
          %70 = arith.select %69, %67, %68 : f32
          %71 = arith.cmpf uno, %68, %68 : f32
          %72 = arith.select %71, %68, %70 : f32
          memref.store %72, %alloc_8[%arg2] : memref<256xf32, #gpu.address_space<workgroup>>
        }
        gpu.barrier
        %64 = arith.cmpi eq, %37, %c0 : index
        %65 = arith.andi %64, %47 : i1
        scf.if %65 {
          %66 = arith.addi %arg2, %c8 : index
          %67 = memref.load %alloc_8[%arg2] : memref<256xf32, #gpu.address_space<workgroup>>
          %68 = memref.load %alloc_8[%66] : memref<256xf32, #gpu.address_space<workgroup>>
          %69 = arith.cmpf ugt, %67, %68 : f32
          %70 = arith.select %69, %67, %68 : f32
          %71 = arith.cmpf uno, %68, %68 : f32
          %72 = arith.select %71, %68, %70 : f32
          %73 = memref.generic_atomic_rmw %alloc_4[%44] : memref<?xf32, "gpu"> {
          ^bb0(%arg3: f32):
            %74 = arith.cmpf ogt, %arg3, %72 : f32
            %75 = arith.select %74, %arg3, %72 : f32
            memref.atomic_yield %75 : f32
          }
        }
        scf.yield
      }
      "lmhlo.terminator"() : () -> ()
    }) {disc.device = "gpu", disc.fusion.name = "main_kColReduction_reduce__4_1_0", disc.fusion.tag = "8w32h", disc.fusion_type = "kColReduction", disc_col_reduction_schedule_hint = 1 : i32, disc_thread_per_block_hint = 256 : i32} : () -> ()
  } else {
    "lmhlo.fusion"() ({
      scf.parallel (%arg1) = (%c0) to (%7) step (%c1) {
        %37 = "disc_shape.delinearize"(%arg1, %7) : (index, index) -> index
        memref.store %cst, %alloc_4[%37] : memref<?xf32, "gpu">
        scf.yield
      }
      %20 = arith.addi %7, %c-1 : index
      %21 = arith.divsi %20, %c8 : index
      %22 = arith.addi %21, %c1 : index
      %23 = arith.subi %c0, %7 : index
      %24 = arith.divsi %23, %c8 : index
      %25 = arith.subi %c0, %24 : index
      %26 = arith.cmpi sgt, %7, %c0 : index
      %27 = arith.select %26, %22, %25 : index
      %28 = arith.addi %dim_1, %c-1 : index
      %29 = arith.divsi %28, %c16 : index
      %30 = arith.addi %29, %c1 : index
      %31 = arith.subi %c0, %dim_1 : index
      %32 = arith.divsi %31, %c16 : index
      %33 = arith.subi %c0, %32 : index
      %34 = arith.cmpi sgt, %dim_1, %c0 : index
      %35 = arith.select %34, %30, %33 : index
      %36 = arith.muli %27, %35 : index
      scf.parallel (%arg1, %arg2) = (%c0, %c0) to (%36, %c128) step (%c1, %c1) {
        %37 = arith.divui %arg2, %c8 : index
        %38 = arith.remui %arg2, %c8 : index
        %39 = arith.divui %arg1, %27 : index
        %40 = arith.remui %arg1, %27 : index
        %41 = arith.muli %39, %c16 : index
        %42 = arith.addi %41, %37 : index
        %43 = arith.muli %40, %c8 : index
        %44 = arith.addi %43, %38 : index
        %alloc_8 = memref.alloc() : memref<128xf32, #gpu.address_space<workgroup>>
        %45 = arith.cmpi ult, %42, %dim_1 : index
        %46 = arith.cmpi ult, %44, %7 : index
        %47 = arith.andi %45, %46 : i1
        scf.if %47 {
          %62 = "disc_shape.linearize"(%42, %44, %dim_1, %7) {operand_segment_sizes = array<i32: 2, 2>} : (index, index, index, index) -> index
          %63:3 = "disc_shape.delinearize"(%62, %dim_1, %dim_0, %dim) : (index, index, index, index) -> (index, index, index)
          %64 = memref.load %reinterpret_cast[%63#0, %63#1, %63#2] : memref<?x?x?xf32, "gpu">
          %65 = math.absf %64 : f32
          memref.store %65, %alloc_8[%arg2] : memref<128xf32, #gpu.address_space<workgroup>>
        } else {
          memref.store %cst, %alloc_8[%arg2] : memref<128xf32, #gpu.address_space<workgroup>>
        }
        gpu.barrier
        %48 = arith.cmpi ult, %37, %c8 : index
        %49 = arith.addi %42, %c8 : index
        %50 = arith.cmpi ult, %49, %dim_1 : index
        %51 = arith.andi %48, %50 : i1
        scf.if %51 {
          %62 = arith.addi %arg2, %c64 : index
          %63 = memref.load %alloc_8[%arg2] : memref<128xf32, #gpu.address_space<workgroup>>
          %64 = memref.load %alloc_8[%62] : memref<128xf32, #gpu.address_space<workgroup>>
          %65 = arith.cmpf ugt, %63, %64 : f32
          %66 = arith.select %65, %63, %64 : f32
          %67 = arith.cmpf uno, %64, %64 : f32
          %68 = arith.select %67, %64, %66 : f32
          memref.store %68, %alloc_8[%arg2] : memref<128xf32, #gpu.address_space<workgroup>>
        }
        gpu.barrier
        %52 = arith.cmpi ult, %37, %c4 : index
        %53 = arith.addi %42, %c4 : index
        %54 = arith.cmpi ult, %53, %dim_1 : index
        %55 = arith.andi %52, %54 : i1
        scf.if %55 {
          %62 = arith.addi %arg2, %c32 : index
          %63 = memref.load %alloc_8[%arg2] : memref<128xf32, #gpu.address_space<workgroup>>
          %64 = memref.load %alloc_8[%62] : memref<128xf32, #gpu.address_space<workgroup>>
          %65 = arith.cmpf ugt, %63, %64 : f32
          %66 = arith.select %65, %63, %64 : f32
          %67 = arith.cmpf uno, %64, %64 : f32
          %68 = arith.select %67, %64, %66 : f32
          memref.store %68, %alloc_8[%arg2] : memref<128xf32, #gpu.address_space<workgroup>>
        }
        gpu.barrier
        %56 = arith.cmpi ult, %37, %c2 : index
        %57 = arith.addi %42, %c2 : index
        %58 = arith.cmpi ult, %57, %dim_1 : index
        %59 = arith.andi %56, %58 : i1
        scf.if %59 {
          %62 = arith.addi %arg2, %c16 : index
          %63 = memref.load %alloc_8[%arg2] : memref<128xf32, #gpu.address_space<workgroup>>
          %64 = memref.load %alloc_8[%62] : memref<128xf32, #gpu.address_space<workgroup>>
          %65 = arith.cmpf ugt, %63, %64 : f32
          %66 = arith.select %65, %63, %64 : f32
          %67 = arith.cmpf uno, %64, %64 : f32
          %68 = arith.select %67, %64, %66 : f32
          memref.store %68, %alloc_8[%arg2] : memref<128xf32, #gpu.address_space<workgroup>>
        }
        gpu.barrier
        %60 = arith.cmpi eq, %37, %c0 : index
        %61 = arith.andi %60, %47 : i1
        scf.if %61 {
          %62 = arith.addi %arg2, %c8 : index
          %63 = memref.load %alloc_8[%arg2] : memref<128xf32, #gpu.address_space<workgroup>>
          %64 = memref.load %alloc_8[%62] : memref<128xf32, #gpu.address_space<workgroup>>
          %65 = arith.cmpf ugt, %63, %64 : f32
          %66 = arith.select %65, %63, %64 : f32
          %67 = arith.cmpf uno, %64, %64 : f32
          %68 = arith.select %67, %64, %66 : f32
          %69 = memref.generic_atomic_rmw %alloc_4[%44] : memref<?xf32, "gpu"> {
          ^bb0(%arg3: f32):
            %70 = arith.cmpf ogt, %arg3, %68 : f32
            %71 = arith.select %70, %arg3, %68 : f32
            memref.atomic_yield %71 : f32
          }
        }
        scf.yield
      }
      "lmhlo.terminator"() : () -> ()
    }) {disc.device = "gpu", disc.fusion.name = "main_kColReduction_reduce__4_1_0", disc.fusion.tag = "8w16h", disc.fusion_type = "kColReduction", disc_col_reduction_schedule_hint = 2 : i32, disc_thread_per_block_hint = 128 : i32} : () -> ()
  }
  memref.dealloc %alloc_3 : memref<?x?xf32, "gpu">
  memref.dealloc %alloc_2 : memref<?x?x?xf32, "gpu">
  memref.dealloc %alloc : memref<f32, "gpu">
  %alloca_5 = memref.alloca() : memref<2xi32, "cpu">
  memref.store %4, %alloca_5[%c0] : memref<2xi32, "cpu">
  memref.store %5, %alloca_5[%c1] : memref<2xi32, "cpu">
  %18 = llvm.inttoptr %0 : i32 to !llvm.ptr<i8>
  %alloca_6 = memref.alloca() : memref<2xindex, "cpu">
  memref.store %dim_0, %alloca_6[%c0] : memref<2xindex, "cpu">
  memref.store %dim, %alloca_6[%c1] : memref<2xindex, "cpu">
  %19 = "disc_ral.dispatch"(%arg0, %18, %alloc_4, %alloca_6) {backend_config = "", call_target_name = "inc_ref", device = "gpu", has_side_effect = false} : (!disc_ral.context, !llvm.ptr<i8>, memref<?xf32, "gpu">, memref<2xindex, "cpu">) -> memref<?x?xf32, "gpu">
  %reinterpret_cast_7 = memref.reinterpret_cast %19 to offset: [0], sizes: [%dim_0, %dim], strides: [%dim, 1] {kDiscSymbolicDimAttr = [@S1, @S2]} : memref<?x?xf32, "gpu"> to memref<?x?xf32, "gpu">
  memref.dealloc %alloc_4 : memref<?xf32, "gpu">
  "disc_ral.dispatch"(%arg0, %c0, %reinterpret_cast_7) {backend_config = "", call_target_name = "ral_send_output", device = "cpu", has_side_effect = false} : (!disc_ral.context, index, memref<?x?xf32, "gpu">) -> ()
  return
}

// -----// IR Dump After DiscFlattenMemrefAccessPass (disc-flatten-memref-access) //----- //
func.func @main(%arg0: !disc_ral.context) attributes {tf.entry_function = {input_placements = "gpu", inputs = "input0", output_placements = "gpu", outputs = "output0"}} {
  %c-1 = arith.constant -1 : index
  %cst = arith.constant 0xFF800000 : f32
  %c64 = arith.constant 64 : index
  %c128 = arith.constant 128 : index
  %c4 = arith.constant 4 : index
  %c16 = arith.constant 16 : index
  %c32 = arith.constant 32 : index
  %c8 = arith.constant 8 : index
  %0 = llvm.mlir.constant(0 : i32) : i32
  %c0 = arith.constant 0 : index
  %c1 = arith.constant 1 : index
  %c2 = arith.constant 2 : index
  %c256 = arith.constant 256 : index
  %c108 = arith.constant 108 : index
  %1 = "disc_ral.dispatch"(%arg0, %c0) {backend_config = "", call_target_name = "ral_recv_input", device = "cpu", has_side_effect = false} : (!disc_ral.context, index) -> memref<?x?x?xf32, "gpu">
  %dim = memref.dim %1, %c2 : memref<?x?x?xf32, "gpu">
  %dim_0 = memref.dim %1, %c1 : memref<?x?x?xf32, "gpu">
  %dim_1 = memref.dim %1, %c0 : memref<?x?x?xf32, "gpu">
  %alloc = memref.alloc() : memref<f32, "gpu">
  %2 = arith.muli %dim, %dim_0 : index
  %reinterpret_cast = memref.reinterpret_cast %1 to offset: [0], sizes: [%dim_1, %dim_0, %dim], strides: [%2, %dim, 1] {kDiscSymbolicDimAttr = [@S0, @S1, @S2]} : memref<?x?x?xf32, "gpu"> to memref<?x?x?xf32, "gpu">
  %alloc_2 = memref.alloc(%dim_1, %dim_0, %dim) {kDiscSymbolicDimAttr = [@S0, @S1, @S2]} : memref<?x?x?xf32, "gpu">
  %3 = arith.index_cast %dim_1 : index to i32
  %4 = arith.index_cast %dim_0 : index to i32
  %5 = arith.index_cast %dim : index to i32
  %6 = arith.muli %4, %5 : i32
  %alloca = memref.alloca() : memref<2xi32, "cpu">
  memref.store %3, %alloca[%c0] : memref<2xi32, "cpu">
  memref.store %6, %alloca[%c1] : memref<2xi32, "cpu">
  %7 = arith.index_cast %6 : i32 to index
  %alloc_3 = memref.alloc(%dim_1, %7) {kDiscSymbolicDimAttr = [@S0, @S3]} : memref<?x?xf32, "gpu">
  %alloc_4 = memref.alloc(%7) {kDiscSymbolicDimAttr = [@S3]} : memref<?xf32, "gpu">
  %8 = arith.muli %dim_1, %7 : index
  %9 = arith.addi %8, %c-1 : index
  %10 = arith.divsi %9, %c256 : index
  %11 = arith.addi %10, %c1 : index
  %12 = arith.subi %c0, %8 : index
  %13 = arith.divsi %12, %c256 : index
  %14 = arith.subi %c0, %13 : index
  %15 = arith.cmpi sgt, %8, %c0 : index
  %16 = arith.select %15, %11, %14 : index
  %17 = arith.cmpi sgt, %16, %c108 : index
  scf.if %17 {
    "lmhlo.fusion"() ({
      scf.parallel (%arg1) = (%c0) to (%7) step (%c1) {
        %37 = "disc_shape.delinearize"(%arg1, %7) : (index, index) -> index
        %c0_8 = arith.constant 0 : index
        %dim_9 = memref.dim %alloc_4, %c0_8 : memref<?xf32, "gpu">
        %38 = "disc_shape.linearize"(%37, %dim_9) {operand_segment_sizes = array<i32: 1, 1>} : (index, index) -> index
        %c1_10 = arith.constant 1 : index
        %c0_11 = arith.constant 0 : index
        %dim_12 = memref.dim %alloc_4, %c0_11 : memref<?xf32, "gpu">
        %39 = arith.muli %c1_10, %dim_12 : index
        %c1_13 = arith.constant 1 : index
        %c0_14 = arith.constant 0 : index
        %reinterpret_cast_15 = memref.reinterpret_cast %alloc_4 to offset: [%c0_14], sizes: [%39], strides: [%c1_13] : memref<?xf32, "gpu"> to memref<?xf32, "gpu">
        memref.store %cst, %reinterpret_cast_15[%38] : memref<?xf32, "gpu">
        scf.yield
      }
      %20 = arith.addi %7, %c-1 : index
      %21 = arith.divsi %20, %c8 : index
      %22 = arith.addi %21, %c1 : index
      %23 = arith.subi %c0, %7 : index
      %24 = arith.divsi %23, %c8 : index
      %25 = arith.subi %c0, %24 : index
      %26 = arith.cmpi sgt, %7, %c0 : index
      %27 = arith.select %26, %22, %25 : index
      %28 = arith.addi %dim_1, %c-1 : index
      %29 = arith.divsi %28, %c32 : index
      %30 = arith.addi %29, %c1 : index
      %31 = arith.subi %c0, %dim_1 : index
      %32 = arith.divsi %31, %c32 : index
      %33 = arith.subi %c0, %32 : index
      %34 = arith.cmpi sgt, %dim_1, %c0 : index
      %35 = arith.select %34, %30, %33 : index
      %36 = arith.muli %27, %35 : index
      scf.parallel (%arg1, %arg2) = (%c0, %c0) to (%36, %c256) step (%c1, %c1) {
        %37 = arith.divui %arg2, %c8 : index
        %38 = arith.remui %arg2, %c8 : index
        %39 = arith.divui %arg1, %27 : index
        %40 = arith.remui %arg1, %27 : index
        %41 = arith.muli %39, %c32 : index
        %42 = arith.addi %41, %37 : index
        %43 = arith.muli %40, %c8 : index
        %44 = arith.addi %43, %38 : index
        %alloc_8 = memref.alloc() : memref<256xf32, #gpu.address_space<workgroup>>
        %45 = arith.cmpi ult, %42, %dim_1 : index
        %46 = arith.cmpi ult, %44, %7 : index
        %47 = arith.andi %45, %46 : i1
        scf.if %47 {
          %66 = "disc_shape.linearize"(%42, %44, %dim_1, %7) {operand_segment_sizes = array<i32: 2, 2>} : (index, index, index, index) -> index
          %67:3 = "disc_shape.delinearize"(%66, %dim_1, %dim_0, %dim) : (index, index, index, index) -> (index, index, index)
          %c0_9 = arith.constant 0 : index
          %dim_10 = memref.dim %reinterpret_cast, %c0_9 : memref<?x?x?xf32, "gpu">
          %c1_11 = arith.constant 1 : index
          %dim_12 = memref.dim %reinterpret_cast, %c1_11 : memref<?x?x?xf32, "gpu">
          %c2_13 = arith.constant 2 : index
          %dim_14 = memref.dim %reinterpret_cast, %c2_13 : memref<?x?x?xf32, "gpu">
          %68 = "disc_shape.linearize"(%67#0, %67#1, %67#2, %dim_10, %dim_12, %dim_14) {operand_segment_sizes = array<i32: 3, 3>} : (index, index, index, index, index, index) -> index
          %c1_15 = arith.constant 1 : index
          %c0_16 = arith.constant 0 : index
          %dim_17 = memref.dim %reinterpret_cast, %c0_16 : memref<?x?x?xf32, "gpu">
          %69 = arith.muli %c1_15, %dim_17 : index
          %c1_18 = arith.constant 1 : index
          %dim_19 = memref.dim %reinterpret_cast, %c1_18 : memref<?x?x?xf32, "gpu">
          %70 = arith.muli %69, %dim_19 : index
          %c2_20 = arith.constant 2 : index
          %dim_21 = memref.dim %reinterpret_cast, %c2_20 : memref<?x?x?xf32, "gpu">
          %71 = arith.muli %70, %dim_21 : index
          %c1_22 = arith.constant 1 : index
          %c0_23 = arith.constant 0 : index
          %reinterpret_cast_24 = memref.reinterpret_cast %reinterpret_cast to offset: [%c0_23], sizes: [%71], strides: [%c1_22] : memref<?x?x?xf32, "gpu"> to memref<?xf32, "gpu">
          %72 = memref.load %reinterpret_cast_24[%68] : memref<?xf32, "gpu">
          %73 = math.absf %72 : f32
          %c256_25 = arith.constant 256 : index
          %74 = "disc_shape.linearize"(%arg2, %c256_25) {operand_segment_sizes = array<i32: 1, 1>} : (index, index) -> index
          %reinterpret_cast_26 = memref.reinterpret_cast %alloc_8 to offset: [0], sizes: [256], strides: [1] : memref<256xf32, #gpu.address_space<workgroup>> to memref<256xf32, #gpu.address_space<workgroup>>
          memref.store %73, %reinterpret_cast_26[%74] : memref<256xf32, #gpu.address_space<workgroup>>
        } else {
          %c256_9 = arith.constant 256 : index
          %66 = "disc_shape.linearize"(%arg2, %c256_9) {operand_segment_sizes = array<i32: 1, 1>} : (index, index) -> index
          %reinterpret_cast_10 = memref.reinterpret_cast %alloc_8 to offset: [0], sizes: [256], strides: [1] : memref<256xf32, #gpu.address_space<workgroup>> to memref<256xf32, #gpu.address_space<workgroup>>
          memref.store %cst, %reinterpret_cast_10[%66] : memref<256xf32, #gpu.address_space<workgroup>>
        }
        gpu.barrier
        %48 = arith.cmpi ult, %37, %c16 : index
        %49 = arith.addi %42, %c16 : index
        %50 = arith.cmpi ult, %49, %dim_1 : index
        %51 = arith.andi %48, %50 : i1
        scf.if %51 {
          %66 = arith.addi %arg2, %c128 : index
          %c256_9 = arith.constant 256 : index
          %67 = "disc_shape.linearize"(%arg2, %c256_9) {operand_segment_sizes = array<i32: 1, 1>} : (index, index) -> index
          %reinterpret_cast_10 = memref.reinterpret_cast %alloc_8 to offset: [0], sizes: [256], strides: [1] : memref<256xf32, #gpu.address_space<workgroup>> to memref<256xf32, #gpu.address_space<workgroup>>
          %68 = memref.load %reinterpret_cast_10[%67] : memref<256xf32, #gpu.address_space<workgroup>>
          %c256_11 = arith.constant 256 : index
          %69 = "disc_shape.linearize"(%66, %c256_11) {operand_segment_sizes = array<i32: 1, 1>} : (index, index) -> index
          %reinterpret_cast_12 = memref.reinterpret_cast %alloc_8 to offset: [0], sizes: [256], strides: [1] : memref<256xf32, #gpu.address_space<workgroup>> to memref<256xf32, #gpu.address_space<workgroup>>
          %70 = memref.load %reinterpret_cast_12[%69] : memref<256xf32, #gpu.address_space<workgroup>>
          %71 = arith.cmpf ugt, %68, %70 : f32
          %72 = arith.select %71, %68, %70 : f32
          %73 = arith.cmpf uno, %70, %70 : f32
          %74 = arith.select %73, %70, %72 : f32
          %c256_13 = arith.constant 256 : index
          %75 = "disc_shape.linearize"(%arg2, %c256_13) {operand_segment_sizes = array<i32: 1, 1>} : (index, index) -> index
          %reinterpret_cast_14 = memref.reinterpret_cast %alloc_8 to offset: [0], sizes: [256], strides: [1] : memref<256xf32, #gpu.address_space<workgroup>> to memref<256xf32, #gpu.address_space<workgroup>>
          memref.store %74, %reinterpret_cast_14[%75] : memref<256xf32, #gpu.address_space<workgroup>>
        }
        gpu.barrier
        %52 = arith.cmpi ult, %37, %c8 : index
        %53 = arith.addi %42, %c8 : index
        %54 = arith.cmpi ult, %53, %dim_1 : index
        %55 = arith.andi %52, %54 : i1
        scf.if %55 {
          %66 = arith.addi %arg2, %c64 : index
          %c256_9 = arith.constant 256 : index
          %67 = "disc_shape.linearize"(%arg2, %c256_9) {operand_segment_sizes = array<i32: 1, 1>} : (index, index) -> index
          %reinterpret_cast_10 = memref.reinterpret_cast %alloc_8 to offset: [0], sizes: [256], strides: [1] : memref<256xf32, #gpu.address_space<workgroup>> to memref<256xf32, #gpu.address_space<workgroup>>
          %68 = memref.load %reinterpret_cast_10[%67] : memref<256xf32, #gpu.address_space<workgroup>>
          %c256_11 = arith.constant 256 : index
          %69 = "disc_shape.linearize"(%66, %c256_11) {operand_segment_sizes = array<i32: 1, 1>} : (index, index) -> index
          %reinterpret_cast_12 = memref.reinterpret_cast %alloc_8 to offset: [0], sizes: [256], strides: [1] : memref<256xf32, #gpu.address_space<workgroup>> to memref<256xf32, #gpu.address_space<workgroup>>
          %70 = memref.load %reinterpret_cast_12[%69] : memref<256xf32, #gpu.address_space<workgroup>>
          %71 = arith.cmpf ugt, %68, %70 : f32
          %72 = arith.select %71, %68, %70 : f32
          %73 = arith.cmpf uno, %70, %70 : f32
          %74 = arith.select %73, %70, %72 : f32
          %c256_13 = arith.constant 256 : index
          %75 = "disc_shape.linearize"(%arg2, %c256_13) {operand_segment_sizes = array<i32: 1, 1>} : (index, index) -> index
          %reinterpret_cast_14 = memref.reinterpret_cast %alloc_8 to offset: [0], sizes: [256], strides: [1] : memref<256xf32, #gpu.address_space<workgroup>> to memref<256xf32, #gpu.address_space<workgroup>>
          memref.store %74, %reinterpret_cast_14[%75] : memref<256xf32, #gpu.address_space<workgroup>>
        }
        gpu.barrier
        %56 = arith.cmpi ult, %37, %c4 : index
        %57 = arith.addi %42, %c4 : index
        %58 = arith.cmpi ult, %57, %dim_1 : index
        %59 = arith.andi %56, %58 : i1
        scf.if %59 {
          %66 = arith.addi %arg2, %c32 : index
          %c256_9 = arith.constant 256 : index
          %67 = "disc_shape.linearize"(%arg2, %c256_9) {operand_segment_sizes = array<i32: 1, 1>} : (index, index) -> index
          %reinterpret_cast_10 = memref.reinterpret_cast %alloc_8 to offset: [0], sizes: [256], strides: [1] : memref<256xf32, #gpu.address_space<workgroup>> to memref<256xf32, #gpu.address_space<workgroup>>
          %68 = memref.load %reinterpret_cast_10[%67] : memref<256xf32, #gpu.address_space<workgroup>>
          %c256_11 = arith.constant 256 : index
          %69 = "disc_shape.linearize"(%66, %c256_11) {operand_segment_sizes = array<i32: 1, 1>} : (index, index) -> index
          %reinterpret_cast_12 = memref.reinterpret_cast %alloc_8 to offset: [0], sizes: [256], strides: [1] : memref<256xf32, #gpu.address_space<workgroup>> to memref<256xf32, #gpu.address_space<workgroup>>
          %70 = memref.load %reinterpret_cast_12[%69] : memref<256xf32, #gpu.address_space<workgroup>>
          %71 = arith.cmpf ugt, %68, %70 : f32
          %72 = arith.select %71, %68, %70 : f32
          %73 = arith.cmpf uno, %70, %70 : f32
          %74 = arith.select %73, %70, %72 : f32
          %c256_13 = arith.constant 256 : index
          %75 = "disc_shape.linearize"(%arg2, %c256_13) {operand_segment_sizes = array<i32: 1, 1>} : (index, index) -> index
          %reinterpret_cast_14 = memref.reinterpret_cast %alloc_8 to offset: [0], sizes: [256], strides: [1] : memref<256xf32, #gpu.address_space<workgroup>> to memref<256xf32, #gpu.address_space<workgroup>>
          memref.store %74, %reinterpret_cast_14[%75] : memref<256xf32, #gpu.address_space<workgroup>>
        }
        gpu.barrier
        %60 = arith.cmpi ult, %37, %c2 : index
        %61 = arith.addi %42, %c2 : index
        %62 = arith.cmpi ult, %61, %dim_1 : index
        %63 = arith.andi %60, %62 : i1
        scf.if %63 {
          %66 = arith.addi %arg2, %c16 : index
          %c256_9 = arith.constant 256 : index
          %67 = "disc_shape.linearize"(%arg2, %c256_9) {operand_segment_sizes = array<i32: 1, 1>} : (index, index) -> index
          %reinterpret_cast_10 = memref.reinterpret_cast %alloc_8 to offset: [0], sizes: [256], strides: [1] : memref<256xf32, #gpu.address_space<workgroup>> to memref<256xf32, #gpu.address_space<workgroup>>
          %68 = memref.load %reinterpret_cast_10[%67] : memref<256xf32, #gpu.address_space<workgroup>>
          %c256_11 = arith.constant 256 : index
          %69 = "disc_shape.linearize"(%66, %c256_11) {operand_segment_sizes = array<i32: 1, 1>} : (index, index) -> index
          %reinterpret_cast_12 = memref.reinterpret_cast %alloc_8 to offset: [0], sizes: [256], strides: [1] : memref<256xf32, #gpu.address_space<workgroup>> to memref<256xf32, #gpu.address_space<workgroup>>
          %70 = memref.load %reinterpret_cast_12[%69] : memref<256xf32, #gpu.address_space<workgroup>>
          %71 = arith.cmpf ugt, %68, %70 : f32
          %72 = arith.select %71, %68, %70 : f32
          %73 = arith.cmpf uno, %70, %70 : f32
          %74 = arith.select %73, %70, %72 : f32
          %c256_13 = arith.constant 256 : index
          %75 = "disc_shape.linearize"(%arg2, %c256_13) {operand_segment_sizes = array<i32: 1, 1>} : (index, index) -> index
          %reinterpret_cast_14 = memref.reinterpret_cast %alloc_8 to offset: [0], sizes: [256], strides: [1] : memref<256xf32, #gpu.address_space<workgroup>> to memref<256xf32, #gpu.address_space<workgroup>>
          memref.store %74, %reinterpret_cast_14[%75] : memref<256xf32, #gpu.address_space<workgroup>>
        }
        gpu.barrier
        %64 = arith.cmpi eq, %37, %c0 : index
        %65 = arith.andi %64, %47 : i1
        scf.if %65 {
          %66 = arith.addi %arg2, %c8 : index
          %c256_9 = arith.constant 256 : index
          %67 = "disc_shape.linearize"(%arg2, %c256_9) {operand_segment_sizes = array<i32: 1, 1>} : (index, index) -> index
          %reinterpret_cast_10 = memref.reinterpret_cast %alloc_8 to offset: [0], sizes: [256], strides: [1] : memref<256xf32, #gpu.address_space<workgroup>> to memref<256xf32, #gpu.address_space<workgroup>>
          %68 = memref.load %reinterpret_cast_10[%67] : memref<256xf32, #gpu.address_space<workgroup>>
          %c256_11 = arith.constant 256 : index
          %69 = "disc_shape.linearize"(%66, %c256_11) {operand_segment_sizes = array<i32: 1, 1>} : (index, index) -> index
          %reinterpret_cast_12 = memref.reinterpret_cast %alloc_8 to offset: [0], sizes: [256], strides: [1] : memref<256xf32, #gpu.address_space<workgroup>> to memref<256xf32, #gpu.address_space<workgroup>>
          %70 = memref.load %reinterpret_cast_12[%69] : memref<256xf32, #gpu.address_space<workgroup>>
          %71 = arith.cmpf ugt, %68, %70 : f32
          %72 = arith.select %71, %68, %70 : f32
          %73 = arith.cmpf uno, %70, %70 : f32
          %74 = arith.select %73, %70, %72 : f32
          %75 = memref.generic_atomic_rmw %alloc_4[%44] : memref<?xf32, "gpu"> {
          ^bb0(%arg3: f32):
            %76 = arith.cmpf ogt, %arg3, %74 : f32
            %77 = arith.select %76, %arg3, %74 : f32
            memref.atomic_yield %77 : f32
          }
        }
        scf.yield
      }
      "lmhlo.terminator"() : () -> ()
    }) {disc.device = "gpu", disc.fusion.name = "main_kColReduction_reduce__4_1_0", disc.fusion.tag = "8w32h", disc.fusion_type = "kColReduction", disc_col_reduction_schedule_hint = 1 : i32, disc_thread_per_block_hint = 256 : i32} : () -> ()
  } else {
    "lmhlo.fusion"() ({
      scf.parallel (%arg1) = (%c0) to (%7) step (%c1) {
        %37 = "disc_shape.delinearize"(%arg1, %7) : (index, index) -> index
        %c0_8 = arith.constant 0 : index
        %dim_9 = memref.dim %alloc_4, %c0_8 : memref<?xf32, "gpu">
        %38 = "disc_shape.linearize"(%37, %dim_9) {operand_segment_sizes = array<i32: 1, 1>} : (index, index) -> index
        %c1_10 = arith.constant 1 : index
        %c0_11 = arith.constant 0 : index
        %dim_12 = memref.dim %alloc_4, %c0_11 : memref<?xf32, "gpu">
        %39 = arith.muli %c1_10, %dim_12 : index
        %c1_13 = arith.constant 1 : index
        %c0_14 = arith.constant 0 : index
        %reinterpret_cast_15 = memref.reinterpret_cast %alloc_4 to offset: [%c0_14], sizes: [%39], strides: [%c1_13] : memref<?xf32, "gpu"> to memref<?xf32, "gpu">
        memref.store %cst, %reinterpret_cast_15[%38] : memref<?xf32, "gpu">
        scf.yield
      }
      %20 = arith.addi %7, %c-1 : index
      %21 = arith.divsi %20, %c8 : index
      %22 = arith.addi %21, %c1 : index
      %23 = arith.subi %c0, %7 : index
      %24 = arith.divsi %23, %c8 : index
      %25 = arith.subi %c0, %24 : index
      %26 = arith.cmpi sgt, %7, %c0 : index
      %27 = arith.select %26, %22, %25 : index
      %28 = arith.addi %dim_1, %c-1 : index
      %29 = arith.divsi %28, %c16 : index
      %30 = arith.addi %29, %c1 : index
      %31 = arith.subi %c0, %dim_1 : index
      %32 = arith.divsi %31, %c16 : index
      %33 = arith.subi %c0, %32 : index
      %34 = arith.cmpi sgt, %dim_1, %c0 : index
      %35 = arith.select %34, %30, %33 : index
      %36 = arith.muli %27, %35 : index
      scf.parallel (%arg1, %arg2) = (%c0, %c0) to (%36, %c128) step (%c1, %c1) {
        %37 = arith.divui %arg2, %c8 : index
        %38 = arith.remui %arg2, %c8 : index
        %39 = arith.divui %arg1, %27 : index
        %40 = arith.remui %arg1, %27 : index
        %41 = arith.muli %39, %c16 : index
        %42 = arith.addi %41, %37 : index
        %43 = arith.muli %40, %c8 : index
        %44 = arith.addi %43, %38 : index
        %alloc_8 = memref.alloc() : memref<128xf32, #gpu.address_space<workgroup>>
        %45 = arith.cmpi ult, %42, %dim_1 : index
        %46 = arith.cmpi ult, %44, %7 : index
        %47 = arith.andi %45, %46 : i1
        scf.if %47 {
          %62 = "disc_shape.linearize"(%42, %44, %dim_1, %7) {operand_segment_sizes = array<i32: 2, 2>} : (index, index, index, index) -> index
          %63:3 = "disc_shape.delinearize"(%62, %dim_1, %dim_0, %dim) : (index, index, index, index) -> (index, index, index)
          %c0_9 = arith.constant 0 : index
          %dim_10 = memref.dim %reinterpret_cast, %c0_9 : memref<?x?x?xf32, "gpu">
          %c1_11 = arith.constant 1 : index
          %dim_12 = memref.dim %reinterpret_cast, %c1_11 : memref<?x?x?xf32, "gpu">
          %c2_13 = arith.constant 2 : index
          %dim_14 = memref.dim %reinterpret_cast, %c2_13 : memref<?x?x?xf32, "gpu">
          %64 = "disc_shape.linearize"(%63#0, %63#1, %63#2, %dim_10, %dim_12, %dim_14) {operand_segment_sizes = array<i32: 3, 3>} : (index, index, index, index, index, index) -> index
          %c1_15 = arith.constant 1 : index
          %c0_16 = arith.constant 0 : index
          %dim_17 = memref.dim %reinterpret_cast, %c0_16 : memref<?x?x?xf32, "gpu">
          %65 = arith.muli %c1_15, %dim_17 : index
          %c1_18 = arith.constant 1 : index
          %dim_19 = memref.dim %reinterpret_cast, %c1_18 : memref<?x?x?xf32, "gpu">
          %66 = arith.muli %65, %dim_19 : index
          %c2_20 = arith.constant 2 : index
          %dim_21 = memref.dim %reinterpret_cast, %c2_20 : memref<?x?x?xf32, "gpu">
          %67 = arith.muli %66, %dim_21 : index
          %c1_22 = arith.constant 1 : index
          %c0_23 = arith.constant 0 : index
          %reinterpret_cast_24 = memref.reinterpret_cast %reinterpret_cast to offset: [%c0_23], sizes: [%67], strides: [%c1_22] : memref<?x?x?xf32, "gpu"> to memref<?xf32, "gpu">
          %68 = memref.load %reinterpret_cast_24[%64] : memref<?xf32, "gpu">
          %69 = math.absf %68 : f32
          %c128_25 = arith.constant 128 : index
          %70 = "disc_shape.linearize"(%arg2, %c128_25) {operand_segment_sizes = array<i32: 1, 1>} : (index, index) -> index
          %reinterpret_cast_26 = memref.reinterpret_cast %alloc_8 to offset: [0], sizes: [128], strides: [1] : memref<128xf32, #gpu.address_space<workgroup>> to memref<128xf32, #gpu.address_space<workgroup>>
          memref.store %69, %reinterpret_cast_26[%70] : memref<128xf32, #gpu.address_space<workgroup>>
        } else {
          %c128_9 = arith.constant 128 : index
          %62 = "disc_shape.linearize"(%arg2, %c128_9) {operand_segment_sizes = array<i32: 1, 1>} : (index, index) -> index
          %reinterpret_cast_10 = memref.reinterpret_cast %alloc_8 to offset: [0], sizes: [128], strides: [1] : memref<128xf32, #gpu.address_space<workgroup>> to memref<128xf32, #gpu.address_space<workgroup>>
          memref.store %cst, %reinterpret_cast_10[%62] : memref<128xf32, #gpu.address_space<workgroup>>
        }
        gpu.barrier
        %48 = arith.cmpi ult, %37, %c8 : index
        %49 = arith.addi %42, %c8 : index
        %50 = arith.cmpi ult, %49, %dim_1 : index
        %51 = arith.andi %48, %50 : i1
        scf.if %51 {
          %62 = arith.addi %arg2, %c64 : index
          %c128_9 = arith.constant 128 : index
          %63 = "disc_shape.linearize"(%arg2, %c128_9) {operand_segment_sizes = array<i32: 1, 1>} : (index, index) -> index
          %reinterpret_cast_10 = memref.reinterpret_cast %alloc_8 to offset: [0], sizes: [128], strides: [1] : memref<128xf32, #gpu.address_space<workgroup>> to memref<128xf32, #gpu.address_space<workgroup>>
          %64 = memref.load %reinterpret_cast_10[%63] : memref<128xf32, #gpu.address_space<workgroup>>
          %c128_11 = arith.constant 128 : index
          %65 = "disc_shape.linearize"(%62, %c128_11) {operand_segment_sizes = array<i32: 1, 1>} : (index, index) -> index
          %reinterpret_cast_12 = memref.reinterpret_cast %alloc_8 to offset: [0], sizes: [128], strides: [1] : memref<128xf32, #gpu.address_space<workgroup>> to memref<128xf32, #gpu.address_space<workgroup>>
          %66 = memref.load %reinterpret_cast_12[%65] : memref<128xf32, #gpu.address_space<workgroup>>
          %67 = arith.cmpf ugt, %64, %66 : f32
          %68 = arith.select %67, %64, %66 : f32
          %69 = arith.cmpf uno, %66, %66 : f32
          %70 = arith.select %69, %66, %68 : f32
          %c128_13 = arith.constant 128 : index
          %71 = "disc_shape.linearize"(%arg2, %c128_13) {operand_segment_sizes = array<i32: 1, 1>} : (index, index) -> index
          %reinterpret_cast_14 = memref.reinterpret_cast %alloc_8 to offset: [0], sizes: [128], strides: [1] : memref<128xf32, #gpu.address_space<workgroup>> to memref<128xf32, #gpu.address_space<workgroup>>
          memref.store %70, %reinterpret_cast_14[%71] : memref<128xf32, #gpu.address_space<workgroup>>
        }
        gpu.barrier
        %52 = arith.cmpi ult, %37, %c4 : index
        %53 = arith.addi %42, %c4 : index
        %54 = arith.cmpi ult, %53, %dim_1 : index
        %55 = arith.andi %52, %54 : i1
        scf.if %55 {
          %62 = arith.addi %arg2, %c32 : index
          %c128_9 = arith.constant 128 : index
          %63 = "disc_shape.linearize"(%arg2, %c128_9) {operand_segment_sizes = array<i32: 1, 1>} : (index, index) -> index
          %reinterpret_cast_10 = memref.reinterpret_cast %alloc_8 to offset: [0], sizes: [128], strides: [1] : memref<128xf32, #gpu.address_space<workgroup>> to memref<128xf32, #gpu.address_space<workgroup>>
          %64 = memref.load %reinterpret_cast_10[%63] : memref<128xf32, #gpu.address_space<workgroup>>
          %c128_11 = arith.constant 128 : index
          %65 = "disc_shape.linearize"(%62, %c128_11) {operand_segment_sizes = array<i32: 1, 1>} : (index, index) -> index
          %reinterpret_cast_12 = memref.reinterpret_cast %alloc_8 to offset: [0], sizes: [128], strides: [1] : memref<128xf32, #gpu.address_space<workgroup>> to memref<128xf32, #gpu.address_space<workgroup>>
          %66 = memref.load %reinterpret_cast_12[%65] : memref<128xf32, #gpu.address_space<workgroup>>
          %67 = arith.cmpf ugt, %64, %66 : f32
          %68 = arith.select %67, %64, %66 : f32
          %69 = arith.cmpf uno, %66, %66 : f32
          %70 = arith.select %69, %66, %68 : f32
          %c128_13 = arith.constant 128 : index
          %71 = "disc_shape.linearize"(%arg2, %c128_13) {operand_segment_sizes = array<i32: 1, 1>} : (index, index) -> index
          %reinterpret_cast_14 = memref.reinterpret_cast %alloc_8 to offset: [0], sizes: [128], strides: [1] : memref<128xf32, #gpu.address_space<workgroup>> to memref<128xf32, #gpu.address_space<workgroup>>
          memref.store %70, %reinterpret_cast_14[%71] : memref<128xf32, #gpu.address_space<workgroup>>
        }
        gpu.barrier
        %56 = arith.cmpi ult, %37, %c2 : index
        %57 = arith.addi %42, %c2 : index
        %58 = arith.cmpi ult, %57, %dim_1 : index
        %59 = arith.andi %56, %58 : i1
        scf.if %59 {
          %62 = arith.addi %arg2, %c16 : index
          %c128_9 = arith.constant 128 : index
          %63 = "disc_shape.linearize"(%arg2, %c128_9) {operand_segment_sizes = array<i32: 1, 1>} : (index, index) -> index
          %reinterpret_cast_10 = memref.reinterpret_cast %alloc_8 to offset: [0], sizes: [128], strides: [1] : memref<128xf32, #gpu.address_space<workgroup>> to memref<128xf32, #gpu.address_space<workgroup>>
          %64 = memref.load %reinterpret_cast_10[%63] : memref<128xf32, #gpu.address_space<workgroup>>
          %c128_11 = arith.constant 128 : index
          %65 = "disc_shape.linearize"(%62, %c128_11) {operand_segment_sizes = array<i32: 1, 1>} : (index, index) -> index
          %reinterpret_cast_12 = memref.reinterpret_cast %alloc_8 to offset: [0], sizes: [128], strides: [1] : memref<128xf32, #gpu.address_space<workgroup>> to memref<128xf32, #gpu.address_space<workgroup>>
          %66 = memref.load %reinterpret_cast_12[%65] : memref<128xf32, #gpu.address_space<workgroup>>
          %67 = arith.cmpf ugt, %64, %66 : f32
          %68 = arith.select %67, %64, %66 : f32
          %69 = arith.cmpf uno, %66, %66 : f32
          %70 = arith.select %69, %66, %68 : f32
          %c128_13 = arith.constant 128 : index
          %71 = "disc_shape.linearize"(%arg2, %c128_13) {operand_segment_sizes = array<i32: 1, 1>} : (index, index) -> index
          %reinterpret_cast_14 = memref.reinterpret_cast %alloc_8 to offset: [0], sizes: [128], strides: [1] : memref<128xf32, #gpu.address_space<workgroup>> to memref<128xf32, #gpu.address_space<workgroup>>
          memref.store %70, %reinterpret_cast_14[%71] : memref<128xf32, #gpu.address_space<workgroup>>
        }
        gpu.barrier
        %60 = arith.cmpi eq, %37, %c0 : index
        %61 = arith.andi %60, %47 : i1
        scf.if %61 {
          %62 = arith.addi %arg2, %c8 : index
          %c128_9 = arith.constant 128 : index
          %63 = "disc_shape.linearize"(%arg2, %c128_9) {operand_segment_sizes = array<i32: 1, 1>} : (index, index) -> index
          %reinterpret_cast_10 = memref.reinterpret_cast %alloc_8 to offset: [0], sizes: [128], strides: [1] : memref<128xf32, #gpu.address_space<workgroup>> to memref<128xf32, #gpu.address_space<workgroup>>
          %64 = memref.load %reinterpret_cast_10[%63] : memref<128xf32, #gpu.address_space<workgroup>>
          %c128_11 = arith.constant 128 : index
          %65 = "disc_shape.linearize"(%62, %c128_11) {operand_segment_sizes = array<i32: 1, 1>} : (index, index) -> index
          %reinterpret_cast_12 = memref.reinterpret_cast %alloc_8 to offset: [0], sizes: [128], strides: [1] : memref<128xf32, #gpu.address_space<workgroup>> to memref<128xf32, #gpu.address_space<workgroup>>
          %66 = memref.load %reinterpret_cast_12[%65] : memref<128xf32, #gpu.address_space<workgroup>>
          %67 = arith.cmpf ugt, %64, %66 : f32
          %68 = arith.select %67, %64, %66 : f32
          %69 = arith.cmpf uno, %66, %66 : f32
          %70 = arith.select %69, %66, %68 : f32
          %71 = memref.generic_atomic_rmw %alloc_4[%44] : memref<?xf32, "gpu"> {
          ^bb0(%arg3: f32):
            %72 = arith.cmpf ogt, %arg3, %70 : f32
            %73 = arith.select %72, %arg3, %70 : f32
            memref.atomic_yield %73 : f32
          }
        }
        scf.yield
      }
      "lmhlo.terminator"() : () -> ()
    }) {disc.device = "gpu", disc.fusion.name = "main_kColReduction_reduce__4_1_0", disc.fusion.tag = "8w16h", disc.fusion_type = "kColReduction", disc_col_reduction_schedule_hint = 2 : i32, disc_thread_per_block_hint = 128 : i32} : () -> ()
  }
  memref.dealloc %alloc_3 : memref<?x?xf32, "gpu">
  memref.dealloc %alloc_2 : memref<?x?x?xf32, "gpu">
  memref.dealloc %alloc : memref<f32, "gpu">
  %alloca_5 = memref.alloca() : memref<2xi32, "cpu">
  memref.store %4, %alloca_5[%c0] : memref<2xi32, "cpu">
  memref.store %5, %alloca_5[%c1] : memref<2xi32, "cpu">
  %18 = llvm.inttoptr %0 : i32 to !llvm.ptr<i8>
  %alloca_6 = memref.alloca() : memref<2xindex, "cpu">
  memref.store %dim_0, %alloca_6[%c0] : memref<2xindex, "cpu">
  memref.store %dim, %alloca_6[%c1] : memref<2xindex, "cpu">
  %19 = "disc_ral.dispatch"(%arg0, %18, %alloc_4, %alloca_6) {backend_config = "", call_target_name = "inc_ref", device = "gpu", has_side_effect = false} : (!disc_ral.context, !llvm.ptr<i8>, memref<?xf32, "gpu">, memref<2xindex, "cpu">) -> memref<?x?xf32, "gpu">
  %reinterpret_cast_7 = memref.reinterpret_cast %19 to offset: [0], sizes: [%dim_0, %dim], strides: [%dim, 1] {kDiscSymbolicDimAttr = [@S1, @S2]} : memref<?x?xf32, "gpu"> to memref<?x?xf32, "gpu">
  memref.dealloc %alloc_4 : memref<?xf32, "gpu">
  "disc_ral.dispatch"(%arg0, %c0, %reinterpret_cast_7) {backend_config = "", call_target_name = "ral_send_output", device = "cpu", has_side_effect = false} : (!disc_ral.context, index, memref<?x?xf32, "gpu">) -> ()
  return
}

// -----// IR Dump After Canonicalizer (disc-canonicalize) //----- //
func.func @main(%arg0: !disc_ral.context) attributes {tf.entry_function = {input_placements = "gpu", inputs = "input0", output_placements = "gpu", outputs = "output0"}} {
  %c-1 = arith.constant -1 : index
  %cst = arith.constant 0xFF800000 : f32
  %c64 = arith.constant 64 : index
  %c128 = arith.constant 128 : index
  %c4 = arith.constant 4 : index
  %c16 = arith.constant 16 : index
  %c32 = arith.constant 32 : index
  %c8 = arith.constant 8 : index
  %0 = llvm.mlir.constant(0 : i32) : i32
  %c0 = arith.constant 0 : index
  %c1 = arith.constant 1 : index
  %c2 = arith.constant 2 : index
  %c256 = arith.constant 256 : index
  %c108 = arith.constant 108 : index
  %1 = "disc_ral.dispatch"(%arg0, %c0) {backend_config = "", call_target_name = "ral_recv_input", device = "cpu", has_side_effect = false} : (!disc_ral.context, index) -> memref<?x?x?xf32, "gpu">
  %dim = memref.dim %1, %c2 : memref<?x?x?xf32, "gpu">
  %dim_0 = memref.dim %1, %c1 : memref<?x?x?xf32, "gpu">
  %dim_1 = memref.dim %1, %c0 : memref<?x?x?xf32, "gpu">
  %2 = arith.index_cast %dim_0 : index to i32
  %3 = arith.index_cast %dim : index to i32
  %4 = arith.muli %2, %3 : i32
  %5 = arith.index_cast %4 : i32 to index
  %alloc = memref.alloc(%5) {kDiscSymbolicDimAttr = [@S3]} : memref<?xf32, "gpu">
  %6 = arith.muli %dim_1, %5 : index
  %7 = arith.addi %6, %c-1 : index
  %8 = arith.divsi %7, %c256 : index
  %9 = arith.addi %8, %c1 : index
  %10 = arith.subi %c0, %6 : index
  %11 = arith.divsi %10, %c256 : index
  %12 = arith.subi %c0, %11 : index
  %13 = arith.cmpi sgt, %6, %c0 : index
  %14 = arith.select %13, %9, %12 : index
  %15 = arith.cmpi sgt, %14, %c108 : index
  scf.if %15 {
    "lmhlo.fusion"() ({
      scf.parallel (%arg1) = (%c0) to (%5) step (%c1) {
        %reinterpret_cast_2 = memref.reinterpret_cast %alloc to offset: [%c0], sizes: [%5], strides: [%c1] : memref<?xf32, "gpu"> to memref<?xf32, "gpu">
        memref.store %cst, %reinterpret_cast_2[%arg1] : memref<?xf32, "gpu">
        scf.yield
      }
      %18 = arith.addi %5, %c-1 : index
      %19 = arith.divsi %18, %c8 : index
      %20 = arith.addi %19, %c1 : index
      %21 = arith.subi %c0, %5 : index
      %22 = arith.divsi %21, %c8 : index
      %23 = arith.subi %c0, %22 : index
      %24 = arith.cmpi sgt, %5, %c0 : index
      %25 = arith.select %24, %20, %23 : index
      %26 = arith.addi %dim_1, %c-1 : index
      %27 = arith.divsi %26, %c32 : index
      %28 = arith.addi %27, %c1 : index
      %29 = arith.subi %c0, %dim_1 : index
      %30 = arith.divsi %29, %c32 : index
      %31 = arith.subi %c0, %30 : index
      %32 = arith.cmpi sgt, %dim_1, %c0 : index
      %33 = arith.select %32, %28, %31 : index
      %34 = arith.muli %25, %33 : index
      scf.parallel (%arg1, %arg2) = (%c0, %c0) to (%34, %c256) step (%c1, %c1) {
        %35 = arith.divui %arg2, %c8 : index
        %36 = arith.remui %arg2, %c8 : index
        %37 = arith.divui %arg1, %25 : index
        %38 = arith.remui %arg1, %25 : index
        %39 = arith.muli %37, %c32 : index
        %40 = arith.addi %39, %35 : index
        %41 = arith.muli %38, %c8 : index
        %42 = arith.addi %41, %36 : index
        %alloc_2 = memref.alloc() : memref<256xf32, #gpu.address_space<workgroup>>
        %43 = arith.cmpi ult, %40, %dim_1 : index
        %44 = arith.cmpi ult, %42, %5 : index
        %45 = arith.andi %43, %44 : i1
        scf.if %45 {
          %64 = "disc_shape.linearize"(%40, %42, %dim_1, %5) {operand_segment_sizes = array<i32: 2, 2>} : (index, index, index, index) -> index
          %65 = arith.muli %dim_1, %dim_0 : index
          %66 = arith.muli %65, %dim : index
          %reinterpret_cast_3 = memref.reinterpret_cast %1 to offset: [%c0], sizes: [%66], strides: [%c1] : memref<?x?x?xf32, "gpu"> to memref<?xf32, "gpu">
          %67 = memref.load %reinterpret_cast_3[%64] : memref<?xf32, "gpu">
          %68 = math.absf %67 : f32
          %69 = "disc_shape.linearize"(%arg2, %c256) {operand_segment_sizes = array<i32: 1, 1>} : (index, index) -> index
          %reinterpret_cast_4 = memref.reinterpret_cast %alloc_2 to offset: [0], sizes: [256], strides: [1] : memref<256xf32, #gpu.address_space<workgroup>> to memref<256xf32, #gpu.address_space<workgroup>>
          memref.store %68, %reinterpret_cast_4[%69] : memref<256xf32, #gpu.address_space<workgroup>>
        } else {
          %64 = "disc_shape.linearize"(%arg2, %c256) {operand_segment_sizes = array<i32: 1, 1>} : (index, index) -> index
          %reinterpret_cast_3 = memref.reinterpret_cast %alloc_2 to offset: [0], sizes: [256], strides: [1] : memref<256xf32, #gpu.address_space<workgroup>> to memref<256xf32, #gpu.address_space<workgroup>>
          memref.store %cst, %reinterpret_cast_3[%64] : memref<256xf32, #gpu.address_space<workgroup>>
        }
        gpu.barrier
        %46 = arith.cmpi ult, %35, %c16 : index
        %47 = arith.addi %40, %c16 : index
        %48 = arith.cmpi ult, %47, %dim_1 : index
        %49 = arith.andi %46, %48 : i1
        scf.if %49 {
          %64 = arith.addi %arg2, %c128 : index
          %65 = "disc_shape.linearize"(%arg2, %c256) {operand_segment_sizes = array<i32: 1, 1>} : (index, index) -> index
          %reinterpret_cast_3 = memref.reinterpret_cast %alloc_2 to offset: [0], sizes: [256], strides: [1] : memref<256xf32, #gpu.address_space<workgroup>> to memref<256xf32, #gpu.address_space<workgroup>>
          %66 = memref.load %reinterpret_cast_3[%65] : memref<256xf32, #gpu.address_space<workgroup>>
          %67 = "disc_shape.linearize"(%64, %c256) {operand_segment_sizes = array<i32: 1, 1>} : (index, index) -> index
          %reinterpret_cast_4 = memref.reinterpret_cast %alloc_2 to offset: [0], sizes: [256], strides: [1] : memref<256xf32, #gpu.address_space<workgroup>> to memref<256xf32, #gpu.address_space<workgroup>>
          %68 = memref.load %reinterpret_cast_4[%67] : memref<256xf32, #gpu.address_space<workgroup>>
          %69 = arith.cmpf ugt, %66, %68 : f32
          %70 = arith.select %69, %66, %68 : f32
          %71 = arith.cmpf uno, %68, %68 : f32
          %72 = arith.select %71, %68, %70 : f32
          %73 = "disc_shape.linearize"(%arg2, %c256) {operand_segment_sizes = array<i32: 1, 1>} : (index, index) -> index
          %reinterpret_cast_5 = memref.reinterpret_cast %alloc_2 to offset: [0], sizes: [256], strides: [1] : memref<256xf32, #gpu.address_space<workgroup>> to memref<256xf32, #gpu.address_space<workgroup>>
          memref.store %72, %reinterpret_cast_5[%73] : memref<256xf32, #gpu.address_space<workgroup>>
        }
        gpu.barrier
        %50 = arith.cmpi ult, %35, %c8 : index
        %51 = arith.addi %40, %c8 : index
        %52 = arith.cmpi ult, %51, %dim_1 : index
        %53 = arith.andi %50, %52 : i1
        scf.if %53 {
          %64 = arith.addi %arg2, %c64 : index
          %65 = "disc_shape.linearize"(%arg2, %c256) {operand_segment_sizes = array<i32: 1, 1>} : (index, index) -> index
          %reinterpret_cast_3 = memref.reinterpret_cast %alloc_2 to offset: [0], sizes: [256], strides: [1] : memref<256xf32, #gpu.address_space<workgroup>> to memref<256xf32, #gpu.address_space<workgroup>>
          %66 = memref.load %reinterpret_cast_3[%65] : memref<256xf32, #gpu.address_space<workgroup>>
          %67 = "disc_shape.linearize"(%64, %c256) {operand_segment_sizes = array<i32: 1, 1>} : (index, index) -> index
          %reinterpret_cast_4 = memref.reinterpret_cast %alloc_2 to offset: [0], sizes: [256], strides: [1] : memref<256xf32, #gpu.address_space<workgroup>> to memref<256xf32, #gpu.address_space<workgroup>>
          %68 = memref.load %reinterpret_cast_4[%67] : memref<256xf32, #gpu.address_space<workgroup>>
          %69 = arith.cmpf ugt, %66, %68 : f32
          %70 = arith.select %69, %66, %68 : f32
          %71 = arith.cmpf uno, %68, %68 : f32
          %72 = arith.select %71, %68, %70 : f32
          %73 = "disc_shape.linearize"(%arg2, %c256) {operand_segment_sizes = array<i32: 1, 1>} : (index, index) -> index
          %reinterpret_cast_5 = memref.reinterpret_cast %alloc_2 to offset: [0], sizes: [256], strides: [1] : memref<256xf32, #gpu.address_space<workgroup>> to memref<256xf32, #gpu.address_space<workgroup>>
          memref.store %72, %reinterpret_cast_5[%73] : memref<256xf32, #gpu.address_space<workgroup>>
        }
        gpu.barrier
        %54 = arith.cmpi ult, %35, %c4 : index
        %55 = arith.addi %40, %c4 : index
        %56 = arith.cmpi ult, %55, %dim_1 : index
        %57 = arith.andi %54, %56 : i1
        scf.if %57 {
          %64 = arith.addi %arg2, %c32 : index
          %65 = "disc_shape.linearize"(%arg2, %c256) {operand_segment_sizes = array<i32: 1, 1>} : (index, index) -> index
          %reinterpret_cast_3 = memref.reinterpret_cast %alloc_2 to offset: [0], sizes: [256], strides: [1] : memref<256xf32, #gpu.address_space<workgroup>> to memref<256xf32, #gpu.address_space<workgroup>>
          %66 = memref.load %reinterpret_cast_3[%65] : memref<256xf32, #gpu.address_space<workgroup>>
          %67 = "disc_shape.linearize"(%64, %c256) {operand_segment_sizes = array<i32: 1, 1>} : (index, index) -> index
          %reinterpret_cast_4 = memref.reinterpret_cast %alloc_2 to offset: [0], sizes: [256], strides: [1] : memref<256xf32, #gpu.address_space<workgroup>> to memref<256xf32, #gpu.address_space<workgroup>>
          %68 = memref.load %reinterpret_cast_4[%67] : memref<256xf32, #gpu.address_space<workgroup>>
          %69 = arith.cmpf ugt, %66, %68 : f32
          %70 = arith.select %69, %66, %68 : f32
          %71 = arith.cmpf uno, %68, %68 : f32
          %72 = arith.select %71, %68, %70 : f32
          %73 = "disc_shape.linearize"(%arg2, %c256) {operand_segment_sizes = array<i32: 1, 1>} : (index, index) -> index
          %reinterpret_cast_5 = memref.reinterpret_cast %alloc_2 to offset: [0], sizes: [256], strides: [1] : memref<256xf32, #gpu.address_space<workgroup>> to memref<256xf32, #gpu.address_space<workgroup>>
          memref.store %72, %reinterpret_cast_5[%73] : memref<256xf32, #gpu.address_space<workgroup>>
        }
        gpu.barrier
        %58 = arith.cmpi ult, %35, %c2 : index
        %59 = arith.addi %40, %c2 : index
        %60 = arith.cmpi ult, %59, %dim_1 : index
        %61 = arith.andi %58, %60 : i1
        scf.if %61 {
          %64 = arith.addi %arg2, %c16 : index
          %65 = "disc_shape.linearize"(%arg2, %c256) {operand_segment_sizes = array<i32: 1, 1>} : (index, index) -> index
          %reinterpret_cast_3 = memref.reinterpret_cast %alloc_2 to offset: [0], sizes: [256], strides: [1] : memref<256xf32, #gpu.address_space<workgroup>> to memref<256xf32, #gpu.address_space<workgroup>>
          %66 = memref.load %reinterpret_cast_3[%65] : memref<256xf32, #gpu.address_space<workgroup>>
          %67 = "disc_shape.linearize"(%64, %c256) {operand_segment_sizes = array<i32: 1, 1>} : (index, index) -> index
          %reinterpret_cast_4 = memref.reinterpret_cast %alloc_2 to offset: [0], sizes: [256], strides: [1] : memref<256xf32, #gpu.address_space<workgroup>> to memref<256xf32, #gpu.address_space<workgroup>>
          %68 = memref.load %reinterpret_cast_4[%67] : memref<256xf32, #gpu.address_space<workgroup>>
          %69 = arith.cmpf ugt, %66, %68 : f32
          %70 = arith.select %69, %66, %68 : f32
          %71 = arith.cmpf uno, %68, %68 : f32
          %72 = arith.select %71, %68, %70 : f32
          %73 = "disc_shape.linearize"(%arg2, %c256) {operand_segment_sizes = array<i32: 1, 1>} : (index, index) -> index
          %reinterpret_cast_5 = memref.reinterpret_cast %alloc_2 to offset: [0], sizes: [256], strides: [1] : memref<256xf32, #gpu.address_space<workgroup>> to memref<256xf32, #gpu.address_space<workgroup>>
          memref.store %72, %reinterpret_cast_5[%73] : memref<256xf32, #gpu.address_space<workgroup>>
        }
        gpu.barrier
        %62 = arith.cmpi eq, %35, %c0 : index
        %63 = arith.andi %62, %45 : i1
        scf.if %63 {
          %64 = arith.addi %arg2, %c8 : index
          %65 = "disc_shape.linearize"(%arg2, %c256) {operand_segment_sizes = array<i32: 1, 1>} : (index, index) -> index
          %reinterpret_cast_3 = memref.reinterpret_cast %alloc_2 to offset: [0], sizes: [256], strides: [1] : memref<256xf32, #gpu.address_space<workgroup>> to memref<256xf32, #gpu.address_space<workgroup>>
          %66 = memref.load %reinterpret_cast_3[%65] : memref<256xf32, #gpu.address_space<workgroup>>
          %67 = "disc_shape.linearize"(%64, %c256) {operand_segment_sizes = array<i32: 1, 1>} : (index, index) -> index
          %reinterpret_cast_4 = memref.reinterpret_cast %alloc_2 to offset: [0], sizes: [256], strides: [1] : memref<256xf32, #gpu.address_space<workgroup>> to memref<256xf32, #gpu.address_space<workgroup>>
          %68 = memref.load %reinterpret_cast_4[%67] : memref<256xf32, #gpu.address_space<workgroup>>
          %69 = arith.cmpf ugt, %66, %68 : f32
          %70 = arith.select %69, %66, %68 : f32
          %71 = arith.cmpf uno, %68, %68 : f32
          %72 = arith.select %71, %68, %70 : f32
          %73 = memref.generic_atomic_rmw %alloc[%42] : memref<?xf32, "gpu"> {
          ^bb0(%arg3: f32):
            %74 = arith.cmpf ogt, %arg3, %72 : f32
            %75 = arith.select %74, %arg3, %72 : f32
            memref.atomic_yield %75 : f32
          }
        }
        scf.yield
      }
      "lmhlo.terminator"() : () -> ()
    }) {disc.device = "gpu", disc.fusion.name = "main_kColReduction_reduce__4_1_0", disc.fusion.tag = "8w32h", disc.fusion_type = "kColReduction", disc_col_reduction_schedule_hint = 1 : i32, disc_thread_per_block_hint = 256 : i32} : () -> ()
  } else {
    "lmhlo.fusion"() ({
      scf.parallel (%arg1) = (%c0) to (%5) step (%c1) {
        %reinterpret_cast_2 = memref.reinterpret_cast %alloc to offset: [%c0], sizes: [%5], strides: [%c1] : memref<?xf32, "gpu"> to memref<?xf32, "gpu">
        memref.store %cst, %reinterpret_cast_2[%arg1] : memref<?xf32, "gpu">
        scf.yield
      }
      %18 = arith.addi %5, %c-1 : index
      %19 = arith.divsi %18, %c8 : index
      %20 = arith.addi %19, %c1 : index
      %21 = arith.subi %c0, %5 : index
      %22 = arith.divsi %21, %c8 : index
      %23 = arith.subi %c0, %22 : index
      %24 = arith.cmpi sgt, %5, %c0 : index
      %25 = arith.select %24, %20, %23 : index
      %26 = arith.addi %dim_1, %c-1 : index
      %27 = arith.divsi %26, %c16 : index
      %28 = arith.addi %27, %c1 : index
      %29 = arith.subi %c0, %dim_1 : index
      %30 = arith.divsi %29, %c16 : index
      %31 = arith.subi %c0, %30 : index
      %32 = arith.cmpi sgt, %dim_1, %c0 : index
      %33 = arith.select %32, %28, %31 : index
      %34 = arith.muli %25, %33 : index
      scf.parallel (%arg1, %arg2) = (%c0, %c0) to (%34, %c128) step (%c1, %c1) {
        %35 = arith.divui %arg2, %c8 : index
        %36 = arith.remui %arg2, %c8 : index
        %37 = arith.divui %arg1, %25 : index
        %38 = arith.remui %arg1, %25 : index
        %39 = arith.muli %37, %c16 : index
        %40 = arith.addi %39, %35 : index
        %41 = arith.muli %38, %c8 : index
        %42 = arith.addi %41, %36 : index
        %alloc_2 = memref.alloc() : memref<128xf32, #gpu.address_space<workgroup>>
        %43 = arith.cmpi ult, %40, %dim_1 : index
        %44 = arith.cmpi ult, %42, %5 : index
        %45 = arith.andi %43, %44 : i1
        scf.if %45 {
          %60 = "disc_shape.linearize"(%40, %42, %dim_1, %5) {operand_segment_sizes = array<i32: 2, 2>} : (index, index, index, index) -> index
          %61 = arith.muli %dim_1, %dim_0 : index
          %62 = arith.muli %61, %dim : index
          %reinterpret_cast_3 = memref.reinterpret_cast %1 to offset: [%c0], sizes: [%62], strides: [%c1] : memref<?x?x?xf32, "gpu"> to memref<?xf32, "gpu">
          %63 = memref.load %reinterpret_cast_3[%60] : memref<?xf32, "gpu">
          %64 = math.absf %63 : f32
          %65 = "disc_shape.linearize"(%arg2, %c128) {operand_segment_sizes = array<i32: 1, 1>} : (index, index) -> index
          %reinterpret_cast_4 = memref.reinterpret_cast %alloc_2 to offset: [0], sizes: [128], strides: [1] : memref<128xf32, #gpu.address_space<workgroup>> to memref<128xf32, #gpu.address_space<workgroup>>
          memref.store %64, %reinterpret_cast_4[%65] : memref<128xf32, #gpu.address_space<workgroup>>
        } else {
          %60 = "disc_shape.linearize"(%arg2, %c128) {operand_segment_sizes = array<i32: 1, 1>} : (index, index) -> index
          %reinterpret_cast_3 = memref.reinterpret_cast %alloc_2 to offset: [0], sizes: [128], strides: [1] : memref<128xf32, #gpu.address_space<workgroup>> to memref<128xf32, #gpu.address_space<workgroup>>
          memref.store %cst, %reinterpret_cast_3[%60] : memref<128xf32, #gpu.address_space<workgroup>>
        }
        gpu.barrier
        %46 = arith.cmpi ult, %35, %c8 : index
        %47 = arith.addi %40, %c8 : index
        %48 = arith.cmpi ult, %47, %dim_1 : index
        %49 = arith.andi %46, %48 : i1
        scf.if %49 {
          %60 = arith.addi %arg2, %c64 : index
          %61 = "disc_shape.linearize"(%arg2, %c128) {operand_segment_sizes = array<i32: 1, 1>} : (index, index) -> index
          %reinterpret_cast_3 = memref.reinterpret_cast %alloc_2 to offset: [0], sizes: [128], strides: [1] : memref<128xf32, #gpu.address_space<workgroup>> to memref<128xf32, #gpu.address_space<workgroup>>
          %62 = memref.load %reinterpret_cast_3[%61] : memref<128xf32, #gpu.address_space<workgroup>>
          %63 = "disc_shape.linearize"(%60, %c128) {operand_segment_sizes = array<i32: 1, 1>} : (index, index) -> index
          %reinterpret_cast_4 = memref.reinterpret_cast %alloc_2 to offset: [0], sizes: [128], strides: [1] : memref<128xf32, #gpu.address_space<workgroup>> to memref<128xf32, #gpu.address_space<workgroup>>
          %64 = memref.load %reinterpret_cast_4[%63] : memref<128xf32, #gpu.address_space<workgroup>>
          %65 = arith.cmpf ugt, %62, %64 : f32
          %66 = arith.select %65, %62, %64 : f32
          %67 = arith.cmpf uno, %64, %64 : f32
          %68 = arith.select %67, %64, %66 : f32
          %69 = "disc_shape.linearize"(%arg2, %c128) {operand_segment_sizes = array<i32: 1, 1>} : (index, index) -> index
          %reinterpret_cast_5 = memref.reinterpret_cast %alloc_2 to offset: [0], sizes: [128], strides: [1] : memref<128xf32, #gpu.address_space<workgroup>> to memref<128xf32, #gpu.address_space<workgroup>>
          memref.store %68, %reinterpret_cast_5[%69] : memref<128xf32, #gpu.address_space<workgroup>>
        }
        gpu.barrier
        %50 = arith.cmpi ult, %35, %c4 : index
        %51 = arith.addi %40, %c4 : index
        %52 = arith.cmpi ult, %51, %dim_1 : index
        %53 = arith.andi %50, %52 : i1
        scf.if %53 {
          %60 = arith.addi %arg2, %c32 : index
          %61 = "disc_shape.linearize"(%arg2, %c128) {operand_segment_sizes = array<i32: 1, 1>} : (index, index) -> index
          %reinterpret_cast_3 = memref.reinterpret_cast %alloc_2 to offset: [0], sizes: [128], strides: [1] : memref<128xf32, #gpu.address_space<workgroup>> to memref<128xf32, #gpu.address_space<workgroup>>
          %62 = memref.load %reinterpret_cast_3[%61] : memref<128xf32, #gpu.address_space<workgroup>>
          %63 = "disc_shape.linearize"(%60, %c128) {operand_segment_sizes = array<i32: 1, 1>} : (index, index) -> index
          %reinterpret_cast_4 = memref.reinterpret_cast %alloc_2 to offset: [0], sizes: [128], strides: [1] : memref<128xf32, #gpu.address_space<workgroup>> to memref<128xf32, #gpu.address_space<workgroup>>
          %64 = memref.load %reinterpret_cast_4[%63] : memref<128xf32, #gpu.address_space<workgroup>>
          %65 = arith.cmpf ugt, %62, %64 : f32
          %66 = arith.select %65, %62, %64 : f32
          %67 = arith.cmpf uno, %64, %64 : f32
          %68 = arith.select %67, %64, %66 : f32
          %69 = "disc_shape.linearize"(%arg2, %c128) {operand_segment_sizes = array<i32: 1, 1>} : (index, index) -> index
          %reinterpret_cast_5 = memref.reinterpret_cast %alloc_2 to offset: [0], sizes: [128], strides: [1] : memref<128xf32, #gpu.address_space<workgroup>> to memref<128xf32, #gpu.address_space<workgroup>>
          memref.store %68, %reinterpret_cast_5[%69] : memref<128xf32, #gpu.address_space<workgroup>>
        }
        gpu.barrier
        %54 = arith.cmpi ult, %35, %c2 : index
        %55 = arith.addi %40, %c2 : index
        %56 = arith.cmpi ult, %55, %dim_1 : index
        %57 = arith.andi %54, %56 : i1
        scf.if %57 {
          %60 = arith.addi %arg2, %c16 : index
          %61 = "disc_shape.linearize"(%arg2, %c128) {operand_segment_sizes = array<i32: 1, 1>} : (index, index) -> index
          %reinterpret_cast_3 = memref.reinterpret_cast %alloc_2 to offset: [0], sizes: [128], strides: [1] : memref<128xf32, #gpu.address_space<workgroup>> to memref<128xf32, #gpu.address_space<workgroup>>
          %62 = memref.load %reinterpret_cast_3[%61] : memref<128xf32, #gpu.address_space<workgroup>>
          %63 = "disc_shape.linearize"(%60, %c128) {operand_segment_sizes = array<i32: 1, 1>} : (index, index) -> index
          %reinterpret_cast_4 = memref.reinterpret_cast %alloc_2 to offset: [0], sizes: [128], strides: [1] : memref<128xf32, #gpu.address_space<workgroup>> to memref<128xf32, #gpu.address_space<workgroup>>
          %64 = memref.load %reinterpret_cast_4[%63] : memref<128xf32, #gpu.address_space<workgroup>>
          %65 = arith.cmpf ugt, %62, %64 : f32
          %66 = arith.select %65, %62, %64 : f32
          %67 = arith.cmpf uno, %64, %64 : f32
          %68 = arith.select %67, %64, %66 : f32
          %69 = "disc_shape.linearize"(%arg2, %c128) {operand_segment_sizes = array<i32: 1, 1>} : (index, index) -> index
          %reinterpret_cast_5 = memref.reinterpret_cast %alloc_2 to offset: [0], sizes: [128], strides: [1] : memref<128xf32, #gpu.address_space<workgroup>> to memref<128xf32, #gpu.address_space<workgroup>>
          memref.store %68, %reinterpret_cast_5[%69] : memref<128xf32, #gpu.address_space<workgroup>>
        }
        gpu.barrier
        %58 = arith.cmpi eq, %35, %c0 : index
        %59 = arith.andi %58, %45 : i1
        scf.if %59 {
          %60 = arith.addi %arg2, %c8 : index
          %61 = "disc_shape.linearize"(%arg2, %c128) {operand_segment_sizes = array<i32: 1, 1>} : (index, index) -> index
          %reinterpret_cast_3 = memref.reinterpret_cast %alloc_2 to offset: [0], sizes: [128], strides: [1] : memref<128xf32, #gpu.address_space<workgroup>> to memref<128xf32, #gpu.address_space<workgroup>>
          %62 = memref.load %reinterpret_cast_3[%61] : memref<128xf32, #gpu.address_space<workgroup>>
          %63 = "disc_shape.linearize"(%60, %c128) {operand_segment_sizes = array<i32: 1, 1>} : (index, index) -> index
          %reinterpret_cast_4 = memref.reinterpret_cast %alloc_2 to offset: [0], sizes: [128], strides: [1] : memref<128xf32, #gpu.address_space<workgroup>> to memref<128xf32, #gpu.address_space<workgroup>>
          %64 = memref.load %reinterpret_cast_4[%63] : memref<128xf32, #gpu.address_space<workgroup>>
          %65 = arith.cmpf ugt, %62, %64 : f32
          %66 = arith.select %65, %62, %64 : f32
          %67 = arith.cmpf uno, %64, %64 : f32
          %68 = arith.select %67, %64, %66 : f32
          %69 = memref.generic_atomic_rmw %alloc[%42] : memref<?xf32, "gpu"> {
          ^bb0(%arg3: f32):
            %70 = arith.cmpf ogt, %arg3, %68 : f32
            %71 = arith.select %70, %arg3, %68 : f32
            memref.atomic_yield %71 : f32
          }
        }
        scf.yield
      }
      "lmhlo.terminator"() : () -> ()
    }) {disc.device = "gpu", disc.fusion.name = "main_kColReduction_reduce__4_1_0", disc.fusion.tag = "8w16h", disc.fusion_type = "kColReduction", disc_col_reduction_schedule_hint = 2 : i32, disc_thread_per_block_hint = 128 : i32} : () -> ()
  }
  %16 = llvm.inttoptr %0 : i32 to !llvm.ptr<i8>
  %alloca = memref.alloca() : memref<2xindex, "cpu">
  memref.store %dim_0, %alloca[%c0] : memref<2xindex, "cpu">
  memref.store %dim, %alloca[%c1] : memref<2xindex, "cpu">
  %17 = "disc_ral.dispatch"(%arg0, %16, %alloc, %alloca) {backend_config = "", call_target_name = "inc_ref", device = "gpu", has_side_effect = false} : (!disc_ral.context, !llvm.ptr<i8>, memref<?xf32, "gpu">, memref<2xindex, "cpu">) -> memref<?x?xf32, "gpu">
  %reinterpret_cast = memref.reinterpret_cast %17 to offset: [0], sizes: [%dim_0, %dim], strides: [%dim, 1] {kDiscSymbolicDimAttr = [@S1, @S2]} : memref<?x?xf32, "gpu"> to memref<?x?xf32, "gpu">
  memref.dealloc %alloc : memref<?xf32, "gpu">
  "disc_ral.dispatch"(%arg0, %c0, %reinterpret_cast) {backend_config = "", call_target_name = "ral_send_output", device = "cpu", has_side_effect = false} : (!disc_ral.context, index, memref<?x?xf32, "gpu">) -> ()
  return
}

// -----// IR Dump After CSE (cse) //----- //
func.func @main(%arg0: !disc_ral.context) attributes {tf.entry_function = {input_placements = "gpu", inputs = "input0", output_placements = "gpu", outputs = "output0"}} {
  %c-1 = arith.constant -1 : index
  %cst = arith.constant 0xFF800000 : f32
  %c64 = arith.constant 64 : index
  %c128 = arith.constant 128 : index
  %c4 = arith.constant 4 : index
  %c16 = arith.constant 16 : index
  %c32 = arith.constant 32 : index
  %c8 = arith.constant 8 : index
  %0 = llvm.mlir.constant(0 : i32) : i32
  %c0 = arith.constant 0 : index
  %c1 = arith.constant 1 : index
  %c2 = arith.constant 2 : index
  %c256 = arith.constant 256 : index
  %c108 = arith.constant 108 : index
  %1 = "disc_ral.dispatch"(%arg0, %c0) {backend_config = "", call_target_name = "ral_recv_input", device = "cpu", has_side_effect = false} : (!disc_ral.context, index) -> memref<?x?x?xf32, "gpu">
  %dim = memref.dim %1, %c2 : memref<?x?x?xf32, "gpu">
  %dim_0 = memref.dim %1, %c1 : memref<?x?x?xf32, "gpu">
  %dim_1 = memref.dim %1, %c0 : memref<?x?x?xf32, "gpu">
  %2 = arith.index_cast %dim_0 : index to i32
  %3 = arith.index_cast %dim : index to i32
  %4 = arith.muli %2, %3 : i32
  %5 = arith.index_cast %4 : i32 to index
  %alloc = memref.alloc(%5) {kDiscSymbolicDimAttr = [@S3]} : memref<?xf32, "gpu">
  %6 = arith.muli %dim_1, %5 : index
  %7 = arith.addi %6, %c-1 : index
  %8 = arith.divsi %7, %c256 : index
  %9 = arith.addi %8, %c1 : index
  %10 = arith.subi %c0, %6 : index
  %11 = arith.divsi %10, %c256 : index
  %12 = arith.subi %c0, %11 : index
  %13 = arith.cmpi sgt, %6, %c0 : index
  %14 = arith.select %13, %9, %12 : index
  %15 = arith.cmpi sgt, %14, %c108 : index
  scf.if %15 {
    "lmhlo.fusion"() ({
      scf.parallel (%arg1) = (%c0) to (%5) step (%c1) {
        %reinterpret_cast_2 = memref.reinterpret_cast %alloc to offset: [%c0], sizes: [%5], strides: [%c1] : memref<?xf32, "gpu"> to memref<?xf32, "gpu">
        memref.store %cst, %reinterpret_cast_2[%arg1] : memref<?xf32, "gpu">
        scf.yield
      }
      %18 = arith.addi %5, %c-1 : index
      %19 = arith.divsi %18, %c8 : index
      %20 = arith.addi %19, %c1 : index
      %21 = arith.subi %c0, %5 : index
      %22 = arith.divsi %21, %c8 : index
      %23 = arith.subi %c0, %22 : index
      %24 = arith.cmpi sgt, %5, %c0 : index
      %25 = arith.select %24, %20, %23 : index
      %26 = arith.addi %dim_1, %c-1 : index
      %27 = arith.divsi %26, %c32 : index
      %28 = arith.addi %27, %c1 : index
      %29 = arith.subi %c0, %dim_1 : index
      %30 = arith.divsi %29, %c32 : index
      %31 = arith.subi %c0, %30 : index
      %32 = arith.cmpi sgt, %dim_1, %c0 : index
      %33 = arith.select %32, %28, %31 : index
      %34 = arith.muli %25, %33 : index
      scf.parallel (%arg1, %arg2) = (%c0, %c0) to (%34, %c256) step (%c1, %c1) {
        %35 = arith.divui %arg2, %c8 : index
        %36 = arith.remui %arg2, %c8 : index
        %37 = arith.divui %arg1, %25 : index
        %38 = arith.remui %arg1, %25 : index
        %39 = arith.muli %37, %c32 : index
        %40 = arith.addi %39, %35 : index
        %41 = arith.muli %38, %c8 : index
        %42 = arith.addi %41, %36 : index
        %alloc_2 = memref.alloc() : memref<256xf32, #gpu.address_space<workgroup>>
        %43 = arith.cmpi ult, %40, %dim_1 : index
        %44 = arith.cmpi ult, %42, %5 : index
        %45 = arith.andi %43, %44 : i1
        scf.if %45 {
          %64 = "disc_shape.linearize"(%40, %42, %dim_1, %5) {operand_segment_sizes = array<i32: 2, 2>} : (index, index, index, index) -> index
          %65 = arith.muli %dim_1, %dim_0 : index
          %66 = arith.muli %65, %dim : index
          %reinterpret_cast_3 = memref.reinterpret_cast %1 to offset: [%c0], sizes: [%66], strides: [%c1] : memref<?x?x?xf32, "gpu"> to memref<?xf32, "gpu">
          %67 = memref.load %reinterpret_cast_3[%64] : memref<?xf32, "gpu">
          %68 = math.absf %67 : f32
          %69 = "disc_shape.linearize"(%arg2, %c256) {operand_segment_sizes = array<i32: 1, 1>} : (index, index) -> index
          %reinterpret_cast_4 = memref.reinterpret_cast %alloc_2 to offset: [0], sizes: [256], strides: [1] : memref<256xf32, #gpu.address_space<workgroup>> to memref<256xf32, #gpu.address_space<workgroup>>
          memref.store %68, %reinterpret_cast_4[%69] : memref<256xf32, #gpu.address_space<workgroup>>
        } else {
          %64 = "disc_shape.linearize"(%arg2, %c256) {operand_segment_sizes = array<i32: 1, 1>} : (index, index) -> index
          %reinterpret_cast_3 = memref.reinterpret_cast %alloc_2 to offset: [0], sizes: [256], strides: [1] : memref<256xf32, #gpu.address_space<workgroup>> to memref<256xf32, #gpu.address_space<workgroup>>
          memref.store %cst, %reinterpret_cast_3[%64] : memref<256xf32, #gpu.address_space<workgroup>>
        }
        gpu.barrier
        %46 = arith.cmpi ult, %35, %c16 : index
        %47 = arith.addi %40, %c16 : index
        %48 = arith.cmpi ult, %47, %dim_1 : index
        %49 = arith.andi %46, %48 : i1
        scf.if %49 {
          %64 = arith.addi %arg2, %c128 : index
          %65 = "disc_shape.linearize"(%arg2, %c256) {operand_segment_sizes = array<i32: 1, 1>} : (index, index) -> index
          %reinterpret_cast_3 = memref.reinterpret_cast %alloc_2 to offset: [0], sizes: [256], strides: [1] : memref<256xf32, #gpu.address_space<workgroup>> to memref<256xf32, #gpu.address_space<workgroup>>
          %66 = memref.load %reinterpret_cast_3[%65] : memref<256xf32, #gpu.address_space<workgroup>>
          %67 = "disc_shape.linearize"(%64, %c256) {operand_segment_sizes = array<i32: 1, 1>} : (index, index) -> index
          %68 = memref.load %reinterpret_cast_3[%67] : memref<256xf32, #gpu.address_space<workgroup>>
          %69 = arith.cmpf ugt, %66, %68 : f32
          %70 = arith.select %69, %66, %68 : f32
          %71 = arith.cmpf uno, %68, %68 : f32
          %72 = arith.select %71, %68, %70 : f32
          memref.store %72, %reinterpret_cast_3[%65] : memref<256xf32, #gpu.address_space<workgroup>>
        }
        gpu.barrier
        %50 = arith.cmpi ult, %35, %c8 : index
        %51 = arith.addi %40, %c8 : index
        %52 = arith.cmpi ult, %51, %dim_1 : index
        %53 = arith.andi %50, %52 : i1
        scf.if %53 {
          %64 = arith.addi %arg2, %c64 : index
          %65 = "disc_shape.linearize"(%arg2, %c256) {operand_segment_sizes = array<i32: 1, 1>} : (index, index) -> index
          %reinterpret_cast_3 = memref.reinterpret_cast %alloc_2 to offset: [0], sizes: [256], strides: [1] : memref<256xf32, #gpu.address_space<workgroup>> to memref<256xf32, #gpu.address_space<workgroup>>
          %66 = memref.load %reinterpret_cast_3[%65] : memref<256xf32, #gpu.address_space<workgroup>>
          %67 = "disc_shape.linearize"(%64, %c256) {operand_segment_sizes = array<i32: 1, 1>} : (index, index) -> index
          %68 = memref.load %reinterpret_cast_3[%67] : memref<256xf32, #gpu.address_space<workgroup>>
          %69 = arith.cmpf ugt, %66, %68 : f32
          %70 = arith.select %69, %66, %68 : f32
          %71 = arith.cmpf uno, %68, %68 : f32
          %72 = arith.select %71, %68, %70 : f32
          memref.store %72, %reinterpret_cast_3[%65] : memref<256xf32, #gpu.address_space<workgroup>>
        }
        gpu.barrier
        %54 = arith.cmpi ult, %35, %c4 : index
        %55 = arith.addi %40, %c4 : index
        %56 = arith.cmpi ult, %55, %dim_1 : index
        %57 = arith.andi %54, %56 : i1
        scf.if %57 {
          %64 = arith.addi %arg2, %c32 : index
          %65 = "disc_shape.linearize"(%arg2, %c256) {operand_segment_sizes = array<i32: 1, 1>} : (index, index) -> index
          %reinterpret_cast_3 = memref.reinterpret_cast %alloc_2 to offset: [0], sizes: [256], strides: [1] : memref<256xf32, #gpu.address_space<workgroup>> to memref<256xf32, #gpu.address_space<workgroup>>
          %66 = memref.load %reinterpret_cast_3[%65] : memref<256xf32, #gpu.address_space<workgroup>>
          %67 = "disc_shape.linearize"(%64, %c256) {operand_segment_sizes = array<i32: 1, 1>} : (index, index) -> index
          %68 = memref.load %reinterpret_cast_3[%67] : memref<256xf32, #gpu.address_space<workgroup>>
          %69 = arith.cmpf ugt, %66, %68 : f32
          %70 = arith.select %69, %66, %68 : f32
          %71 = arith.cmpf uno, %68, %68 : f32
          %72 = arith.select %71, %68, %70 : f32
          memref.store %72, %reinterpret_cast_3[%65] : memref<256xf32, #gpu.address_space<workgroup>>
        }
        gpu.barrier
        %58 = arith.cmpi ult, %35, %c2 : index
        %59 = arith.addi %40, %c2 : index
        %60 = arith.cmpi ult, %59, %dim_1 : index
        %61 = arith.andi %58, %60 : i1
        scf.if %61 {
          %64 = arith.addi %arg2, %c16 : index
          %65 = "disc_shape.linearize"(%arg2, %c256) {operand_segment_sizes = array<i32: 1, 1>} : (index, index) -> index
          %reinterpret_cast_3 = memref.reinterpret_cast %alloc_2 to offset: [0], sizes: [256], strides: [1] : memref<256xf32, #gpu.address_space<workgroup>> to memref<256xf32, #gpu.address_space<workgroup>>
          %66 = memref.load %reinterpret_cast_3[%65] : memref<256xf32, #gpu.address_space<workgroup>>
          %67 = "disc_shape.linearize"(%64, %c256) {operand_segment_sizes = array<i32: 1, 1>} : (index, index) -> index
          %68 = memref.load %reinterpret_cast_3[%67] : memref<256xf32, #gpu.address_space<workgroup>>
          %69 = arith.cmpf ugt, %66, %68 : f32
          %70 = arith.select %69, %66, %68 : f32
          %71 = arith.cmpf uno, %68, %68 : f32
          %72 = arith.select %71, %68, %70 : f32
          memref.store %72, %reinterpret_cast_3[%65] : memref<256xf32, #gpu.address_space<workgroup>>
        }
        gpu.barrier
        %62 = arith.cmpi eq, %35, %c0 : index
        %63 = arith.andi %62, %45 : i1
        scf.if %63 {
          %64 = arith.addi %arg2, %c8 : index
          %65 = "disc_shape.linearize"(%arg2, %c256) {operand_segment_sizes = array<i32: 1, 1>} : (index, index) -> index
          %reinterpret_cast_3 = memref.reinterpret_cast %alloc_2 to offset: [0], sizes: [256], strides: [1] : memref<256xf32, #gpu.address_space<workgroup>> to memref<256xf32, #gpu.address_space<workgroup>>
          %66 = memref.load %reinterpret_cast_3[%65] : memref<256xf32, #gpu.address_space<workgroup>>
          %67 = "disc_shape.linearize"(%64, %c256) {operand_segment_sizes = array<i32: 1, 1>} : (index, index) -> index
          %68 = memref.load %reinterpret_cast_3[%67] : memref<256xf32, #gpu.address_space<workgroup>>
          %69 = arith.cmpf ugt, %66, %68 : f32
          %70 = arith.select %69, %66, %68 : f32
          %71 = arith.cmpf uno, %68, %68 : f32
          %72 = arith.select %71, %68, %70 : f32
          %73 = memref.generic_atomic_rmw %alloc[%42] : memref<?xf32, "gpu"> {
          ^bb0(%arg3: f32):
            %74 = arith.cmpf ogt, %arg3, %72 : f32
            %75 = arith.select %74, %arg3, %72 : f32
            memref.atomic_yield %75 : f32
          }
        }
        scf.yield
      }
      "lmhlo.terminator"() : () -> ()
    }) {disc.device = "gpu", disc.fusion.name = "main_kColReduction_reduce__4_1_0", disc.fusion.tag = "8w32h", disc.fusion_type = "kColReduction", disc_col_reduction_schedule_hint = 1 : i32, disc_thread_per_block_hint = 256 : i32} : () -> ()
  } else {
    "lmhlo.fusion"() ({
      scf.parallel (%arg1) = (%c0) to (%5) step (%c1) {
        %reinterpret_cast_2 = memref.reinterpret_cast %alloc to offset: [%c0], sizes: [%5], strides: [%c1] : memref<?xf32, "gpu"> to memref<?xf32, "gpu">
        memref.store %cst, %reinterpret_cast_2[%arg1] : memref<?xf32, "gpu">
        scf.yield
      }
      %18 = arith.addi %5, %c-1 : index
      %19 = arith.divsi %18, %c8 : index
      %20 = arith.addi %19, %c1 : index
      %21 = arith.subi %c0, %5 : index
      %22 = arith.divsi %21, %c8 : index
      %23 = arith.subi %c0, %22 : index
      %24 = arith.cmpi sgt, %5, %c0 : index
      %25 = arith.select %24, %20, %23 : index
      %26 = arith.addi %dim_1, %c-1 : index
      %27 = arith.divsi %26, %c16 : index
      %28 = arith.addi %27, %c1 : index
      %29 = arith.subi %c0, %dim_1 : index
      %30 = arith.divsi %29, %c16 : index
      %31 = arith.subi %c0, %30 : index
      %32 = arith.cmpi sgt, %dim_1, %c0 : index
      %33 = arith.select %32, %28, %31 : index
      %34 = arith.muli %25, %33 : index
      scf.parallel (%arg1, %arg2) = (%c0, %c0) to (%34, %c128) step (%c1, %c1) {
        %35 = arith.divui %arg2, %c8 : index
        %36 = arith.remui %arg2, %c8 : index
        %37 = arith.divui %arg1, %25 : index
        %38 = arith.remui %arg1, %25 : index
        %39 = arith.muli %37, %c16 : index
        %40 = arith.addi %39, %35 : index
        %41 = arith.muli %38, %c8 : index
        %42 = arith.addi %41, %36 : index
        %alloc_2 = memref.alloc() : memref<128xf32, #gpu.address_space<workgroup>>
        %43 = arith.cmpi ult, %40, %dim_1 : index
        %44 = arith.cmpi ult, %42, %5 : index
        %45 = arith.andi %43, %44 : i1
        scf.if %45 {
          %60 = "disc_shape.linearize"(%40, %42, %dim_1, %5) {operand_segment_sizes = array<i32: 2, 2>} : (index, index, index, index) -> index
          %61 = arith.muli %dim_1, %dim_0 : index
          %62 = arith.muli %61, %dim : index
          %reinterpret_cast_3 = memref.reinterpret_cast %1 to offset: [%c0], sizes: [%62], strides: [%c1] : memref<?x?x?xf32, "gpu"> to memref<?xf32, "gpu">
          %63 = memref.load %reinterpret_cast_3[%60] : memref<?xf32, "gpu">
          %64 = math.absf %63 : f32
          %65 = "disc_shape.linearize"(%arg2, %c128) {operand_segment_sizes = array<i32: 1, 1>} : (index, index) -> index
          %reinterpret_cast_4 = memref.reinterpret_cast %alloc_2 to offset: [0], sizes: [128], strides: [1] : memref<128xf32, #gpu.address_space<workgroup>> to memref<128xf32, #gpu.address_space<workgroup>>
          memref.store %64, %reinterpret_cast_4[%65] : memref<128xf32, #gpu.address_space<workgroup>>
        } else {
          %60 = "disc_shape.linearize"(%arg2, %c128) {operand_segment_sizes = array<i32: 1, 1>} : (index, index) -> index
          %reinterpret_cast_3 = memref.reinterpret_cast %alloc_2 to offset: [0], sizes: [128], strides: [1] : memref<128xf32, #gpu.address_space<workgroup>> to memref<128xf32, #gpu.address_space<workgroup>>
          memref.store %cst, %reinterpret_cast_3[%60] : memref<128xf32, #gpu.address_space<workgroup>>
        }
        gpu.barrier
        %46 = arith.cmpi ult, %35, %c8 : index
        %47 = arith.addi %40, %c8 : index
        %48 = arith.cmpi ult, %47, %dim_1 : index
        %49 = arith.andi %46, %48 : i1
        scf.if %49 {
          %60 = arith.addi %arg2, %c64 : index
          %61 = "disc_shape.linearize"(%arg2, %c128) {operand_segment_sizes = array<i32: 1, 1>} : (index, index) -> index
          %reinterpret_cast_3 = memref.reinterpret_cast %alloc_2 to offset: [0], sizes: [128], strides: [1] : memref<128xf32, #gpu.address_space<workgroup>> to memref<128xf32, #gpu.address_space<workgroup>>
          %62 = memref.load %reinterpret_cast_3[%61] : memref<128xf32, #gpu.address_space<workgroup>>
          %63 = "disc_shape.linearize"(%60, %c128) {operand_segment_sizes = array<i32: 1, 1>} : (index, index) -> index
          %64 = memref.load %reinterpret_cast_3[%63] : memref<128xf32, #gpu.address_space<workgroup>>
          %65 = arith.cmpf ugt, %62, %64 : f32
          %66 = arith.select %65, %62, %64 : f32
          %67 = arith.cmpf uno, %64, %64 : f32
          %68 = arith.select %67, %64, %66 : f32
          memref.store %68, %reinterpret_cast_3[%61] : memref<128xf32, #gpu.address_space<workgroup>>
        }
        gpu.barrier
        %50 = arith.cmpi ult, %35, %c4 : index
        %51 = arith.addi %40, %c4 : index
        %52 = arith.cmpi ult, %51, %dim_1 : index
        %53 = arith.andi %50, %52 : i1
        scf.if %53 {
          %60 = arith.addi %arg2, %c32 : index
          %61 = "disc_shape.linearize"(%arg2, %c128) {operand_segment_sizes = array<i32: 1, 1>} : (index, index) -> index
          %reinterpret_cast_3 = memref.reinterpret_cast %alloc_2 to offset: [0], sizes: [128], strides: [1] : memref<128xf32, #gpu.address_space<workgroup>> to memref<128xf32, #gpu.address_space<workgroup>>
          %62 = memref.load %reinterpret_cast_3[%61] : memref<128xf32, #gpu.address_space<workgroup>>
          %63 = "disc_shape.linearize"(%60, %c128) {operand_segment_sizes = array<i32: 1, 1>} : (index, index) -> index
          %64 = memref.load %reinterpret_cast_3[%63] : memref<128xf32, #gpu.address_space<workgroup>>
          %65 = arith.cmpf ugt, %62, %64 : f32
          %66 = arith.select %65, %62, %64 : f32
          %67 = arith.cmpf uno, %64, %64 : f32
          %68 = arith.select %67, %64, %66 : f32
          memref.store %68, %reinterpret_cast_3[%61] : memref<128xf32, #gpu.address_space<workgroup>>
        }
        gpu.barrier
        %54 = arith.cmpi ult, %35, %c2 : index
        %55 = arith.addi %40, %c2 : index
        %56 = arith.cmpi ult, %55, %dim_1 : index
        %57 = arith.andi %54, %56 : i1
        scf.if %57 {
          %60 = arith.addi %arg2, %c16 : index
          %61 = "disc_shape.linearize"(%arg2, %c128) {operand_segment_sizes = array<i32: 1, 1>} : (index, index) -> index
          %reinterpret_cast_3 = memref.reinterpret_cast %alloc_2 to offset: [0], sizes: [128], strides: [1] : memref<128xf32, #gpu.address_space<workgroup>> to memref<128xf32, #gpu.address_space<workgroup>>
          %62 = memref.load %reinterpret_cast_3[%61] : memref<128xf32, #gpu.address_space<workgroup>>
          %63 = "disc_shape.linearize"(%60, %c128) {operand_segment_sizes = array<i32: 1, 1>} : (index, index) -> index
          %64 = memref.load %reinterpret_cast_3[%63] : memref<128xf32, #gpu.address_space<workgroup>>
          %65 = arith.cmpf ugt, %62, %64 : f32
          %66 = arith.select %65, %62, %64 : f32
          %67 = arith.cmpf uno, %64, %64 : f32
          %68 = arith.select %67, %64, %66 : f32
          memref.store %68, %reinterpret_cast_3[%61] : memref<128xf32, #gpu.address_space<workgroup>>
        }
        gpu.barrier
        %58 = arith.cmpi eq, %35, %c0 : index
        %59 = arith.andi %58, %45 : i1
        scf.if %59 {
          %60 = arith.addi %arg2, %c8 : index
          %61 = "disc_shape.linearize"(%arg2, %c128) {operand_segment_sizes = array<i32: 1, 1>} : (index, index) -> index
          %reinterpret_cast_3 = memref.reinterpret_cast %alloc_2 to offset: [0], sizes: [128], strides: [1] : memref<128xf32, #gpu.address_space<workgroup>> to memref<128xf32, #gpu.address_space<workgroup>>
          %62 = memref.load %reinterpret_cast_3[%61] : memref<128xf32, #gpu.address_space<workgroup>>
          %63 = "disc_shape.linearize"(%60, %c128) {operand_segment_sizes = array<i32: 1, 1>} : (index, index) -> index
          %64 = memref.load %reinterpret_cast_3[%63] : memref<128xf32, #gpu.address_space<workgroup>>
          %65 = arith.cmpf ugt, %62, %64 : f32
          %66 = arith.select %65, %62, %64 : f32
          %67 = arith.cmpf uno, %64, %64 : f32
          %68 = arith.select %67, %64, %66 : f32
          %69 = memref.generic_atomic_rmw %alloc[%42] : memref<?xf32, "gpu"> {
          ^bb0(%arg3: f32):
            %70 = arith.cmpf ogt, %arg3, %68 : f32
            %71 = arith.select %70, %arg3, %68 : f32
            memref.atomic_yield %71 : f32
          }
        }
        scf.yield
      }
      "lmhlo.terminator"() : () -> ()
    }) {disc.device = "gpu", disc.fusion.name = "main_kColReduction_reduce__4_1_0", disc.fusion.tag = "8w16h", disc.fusion_type = "kColReduction", disc_col_reduction_schedule_hint = 2 : i32, disc_thread_per_block_hint = 128 : i32} : () -> ()
  }
  %16 = llvm.inttoptr %0 : i32 to !llvm.ptr<i8>
  %alloca = memref.alloca() : memref<2xindex, "cpu">
  memref.store %dim_0, %alloca[%c0] : memref<2xindex, "cpu">
  memref.store %dim, %alloca[%c1] : memref<2xindex, "cpu">
  %17 = "disc_ral.dispatch"(%arg0, %16, %alloc, %alloca) {backend_config = "", call_target_name = "inc_ref", device = "gpu", has_side_effect = false} : (!disc_ral.context, !llvm.ptr<i8>, memref<?xf32, "gpu">, memref<2xindex, "cpu">) -> memref<?x?xf32, "gpu">
  %reinterpret_cast = memref.reinterpret_cast %17 to offset: [0], sizes: [%dim_0, %dim], strides: [%dim, 1] {kDiscSymbolicDimAttr = [@S1, @S2]} : memref<?x?xf32, "gpu"> to memref<?x?xf32, "gpu">
  memref.dealloc %alloc : memref<?xf32, "gpu">
  "disc_ral.dispatch"(%arg0, %c0, %reinterpret_cast) {backend_config = "", call_target_name = "ral_send_output", device = "cpu", has_side_effect = false} : (!disc_ral.context, index, memref<?x?xf32, "gpu">) -> ()
  return
}

// -----// IR Dump After ConvertShapeToStandardPass (disc-convert-shape-to-std) //----- //
func.func @main(%arg0: !disc_ral.context) attributes {tf.entry_function = {input_placements = "gpu", inputs = "input0", output_placements = "gpu", outputs = "output0"}} {
  %c-1 = arith.constant -1 : index
  %cst = arith.constant 0xFF800000 : f32
  %c64 = arith.constant 64 : index
  %c128 = arith.constant 128 : index
  %c4 = arith.constant 4 : index
  %c16 = arith.constant 16 : index
  %c32 = arith.constant 32 : index
  %c8 = arith.constant 8 : index
  %0 = llvm.mlir.constant(0 : i32) : i32
  %c0 = arith.constant 0 : index
  %c1 = arith.constant 1 : index
  %c2 = arith.constant 2 : index
  %c256 = arith.constant 256 : index
  %c108 = arith.constant 108 : index
  %1 = "disc_ral.dispatch"(%arg0, %c0) {backend_config = "", call_target_name = "ral_recv_input", device = "cpu", has_side_effect = false} : (!disc_ral.context, index) -> memref<?x?x?xf32, "gpu">
  %dim = memref.dim %1, %c2 : memref<?x?x?xf32, "gpu">
  %dim_0 = memref.dim %1, %c1 : memref<?x?x?xf32, "gpu">
  %dim_1 = memref.dim %1, %c0 : memref<?x?x?xf32, "gpu">
  %2 = arith.index_cast %dim_0 : index to i32
  %3 = arith.index_cast %dim : index to i32
  %4 = arith.muli %2, %3 : i32
  %5 = arith.index_cast %4 : i32 to index
  %alloc = memref.alloc(%5) {kDiscSymbolicDimAttr = [@S3]} : memref<?xf32, "gpu">
  %6 = arith.muli %dim_1, %5 : index
  %7 = arith.addi %6, %c-1 : index
  %8 = arith.divsi %7, %c256 : index
  %9 = arith.addi %8, %c1 : index
  %10 = arith.subi %c0, %6 : index
  %11 = arith.divsi %10, %c256 : index
  %12 = arith.subi %c0, %11 : index
  %13 = arith.cmpi sgt, %6, %c0 : index
  %14 = arith.select %13, %9, %12 : index
  %15 = arith.cmpi sgt, %14, %c108 : index
  scf.if %15 {
    "lmhlo.fusion"() ({
      scf.parallel (%arg1) = (%c0) to (%5) step (%c1) {
        %reinterpret_cast_2 = memref.reinterpret_cast %alloc to offset: [%c0], sizes: [%5], strides: [%c1] : memref<?xf32, "gpu"> to memref<?xf32, "gpu">
        memref.store %cst, %reinterpret_cast_2[%arg1] : memref<?xf32, "gpu">
        scf.yield
      }
      %18 = arith.addi %5, %c-1 : index
      %19 = arith.divsi %18, %c8 : index
      %20 = arith.addi %19, %c1 : index
      %21 = arith.subi %c0, %5 : index
      %22 = arith.divsi %21, %c8 : index
      %23 = arith.subi %c0, %22 : index
      %24 = arith.cmpi sgt, %5, %c0 : index
      %25 = arith.select %24, %20, %23 : index
      %26 = arith.addi %dim_1, %c-1 : index
      %27 = arith.divsi %26, %c32 : index
      %28 = arith.addi %27, %c1 : index
      %29 = arith.subi %c0, %dim_1 : index
      %30 = arith.divsi %29, %c32 : index
      %31 = arith.subi %c0, %30 : index
      %32 = arith.cmpi sgt, %dim_1, %c0 : index
      %33 = arith.select %32, %28, %31 : index
      %34 = arith.muli %25, %33 : index
      scf.parallel (%arg1, %arg2) = (%c0, %c0) to (%34, %c256) step (%c1, %c1) {
        %35 = arith.divui %arg2, %c8 : index
        %36 = arith.remui %arg2, %c8 : index
        %37 = arith.divui %arg1, %25 : index
        %38 = arith.remui %arg1, %25 : index
        %39 = arith.muli %37, %c32 : index
        %40 = arith.addi %39, %35 : index
        %41 = arith.muli %38, %c8 : index
        %42 = arith.addi %41, %36 : index
        %alloc_2 = memref.alloc() : memref<256xf32, #gpu.address_space<workgroup>>
        %43 = arith.cmpi ult, %40, %dim_1 : index
        %44 = arith.cmpi ult, %42, %5 : index
        %45 = arith.andi %43, %44 : i1
        scf.if %45 {
          %c0_3 = arith.constant 0 : index
          %64 = arith.muli %40, %5 : index
          %65 = arith.addi %64, %42 : index
          %66 = arith.muli %dim_1, %dim_0 : index
          %67 = arith.muli %66, %dim : index
          %reinterpret_cast_4 = memref.reinterpret_cast %1 to offset: [%c0], sizes: [%67], strides: [%c1] : memref<?x?x?xf32, "gpu"> to memref<?xf32, "gpu">
          %68 = memref.load %reinterpret_cast_4[%65] : memref<?xf32, "gpu">
          %69 = math.absf %68 : f32
          %c0_5 = arith.constant 0 : index
          %reinterpret_cast_6 = memref.reinterpret_cast %alloc_2 to offset: [0], sizes: [256], strides: [1] : memref<256xf32, #gpu.address_space<workgroup>> to memref<256xf32, #gpu.address_space<workgroup>>
          memref.store %69, %reinterpret_cast_6[%arg2] : memref<256xf32, #gpu.address_space<workgroup>>
        } else {
          %c0_3 = arith.constant 0 : index
          %reinterpret_cast_4 = memref.reinterpret_cast %alloc_2 to offset: [0], sizes: [256], strides: [1] : memref<256xf32, #gpu.address_space<workgroup>> to memref<256xf32, #gpu.address_space<workgroup>>
          memref.store %cst, %reinterpret_cast_4[%arg2] : memref<256xf32, #gpu.address_space<workgroup>>
        }
        gpu.barrier
        %46 = arith.cmpi ult, %35, %c16 : index
        %47 = arith.addi %40, %c16 : index
        %48 = arith.cmpi ult, %47, %dim_1 : index
        %49 = arith.andi %46, %48 : i1
        scf.if %49 {
          %64 = arith.addi %arg2, %c128 : index
          %c0_3 = arith.constant 0 : index
          %reinterpret_cast_4 = memref.reinterpret_cast %alloc_2 to offset: [0], sizes: [256], strides: [1] : memref<256xf32, #gpu.address_space<workgroup>> to memref<256xf32, #gpu.address_space<workgroup>>
          %65 = memref.load %reinterpret_cast_4[%arg2] : memref<256xf32, #gpu.address_space<workgroup>>
          %c0_5 = arith.constant 0 : index
          %66 = memref.load %reinterpret_cast_4[%64] : memref<256xf32, #gpu.address_space<workgroup>>
          %67 = arith.cmpf ugt, %65, %66 : f32
          %68 = arith.select %67, %65, %66 : f32
          %69 = arith.cmpf uno, %66, %66 : f32
          %70 = arith.select %69, %66, %68 : f32
          memref.store %70, %reinterpret_cast_4[%arg2] : memref<256xf32, #gpu.address_space<workgroup>>
        }
        gpu.barrier
        %50 = arith.cmpi ult, %35, %c8 : index
        %51 = arith.addi %40, %c8 : index
        %52 = arith.cmpi ult, %51, %dim_1 : index
        %53 = arith.andi %50, %52 : i1
        scf.if %53 {
          %64 = arith.addi %arg2, %c64 : index
          %c0_3 = arith.constant 0 : index
          %reinterpret_cast_4 = memref.reinterpret_cast %alloc_2 to offset: [0], sizes: [256], strides: [1] : memref<256xf32, #gpu.address_space<workgroup>> to memref<256xf32, #gpu.address_space<workgroup>>
          %65 = memref.load %reinterpret_cast_4[%arg2] : memref<256xf32, #gpu.address_space<workgroup>>
          %c0_5 = arith.constant 0 : index
          %66 = memref.load %reinterpret_cast_4[%64] : memref<256xf32, #gpu.address_space<workgroup>>
          %67 = arith.cmpf ugt, %65, %66 : f32
          %68 = arith.select %67, %65, %66 : f32
          %69 = arith.cmpf uno, %66, %66 : f32
          %70 = arith.select %69, %66, %68 : f32
          memref.store %70, %reinterpret_cast_4[%arg2] : memref<256xf32, #gpu.address_space<workgroup>>
        }
        gpu.barrier
        %54 = arith.cmpi ult, %35, %c4 : index
        %55 = arith.addi %40, %c4 : index
        %56 = arith.cmpi ult, %55, %dim_1 : index
        %57 = arith.andi %54, %56 : i1
        scf.if %57 {
          %64 = arith.addi %arg2, %c32 : index
          %c0_3 = arith.constant 0 : index
          %reinterpret_cast_4 = memref.reinterpret_cast %alloc_2 to offset: [0], sizes: [256], strides: [1] : memref<256xf32, #gpu.address_space<workgroup>> to memref<256xf32, #gpu.address_space<workgroup>>
          %65 = memref.load %reinterpret_cast_4[%arg2] : memref<256xf32, #gpu.address_space<workgroup>>
          %c0_5 = arith.constant 0 : index
          %66 = memref.load %reinterpret_cast_4[%64] : memref<256xf32, #gpu.address_space<workgroup>>
          %67 = arith.cmpf ugt, %65, %66 : f32
          %68 = arith.select %67, %65, %66 : f32
          %69 = arith.cmpf uno, %66, %66 : f32
          %70 = arith.select %69, %66, %68 : f32
          memref.store %70, %reinterpret_cast_4[%arg2] : memref<256xf32, #gpu.address_space<workgroup>>
        }
        gpu.barrier
        %58 = arith.cmpi ult, %35, %c2 : index
        %59 = arith.addi %40, %c2 : index
        %60 = arith.cmpi ult, %59, %dim_1 : index
        %61 = arith.andi %58, %60 : i1
        scf.if %61 {
          %64 = arith.addi %arg2, %c16 : index
          %c0_3 = arith.constant 0 : index
          %reinterpret_cast_4 = memref.reinterpret_cast %alloc_2 to offset: [0], sizes: [256], strides: [1] : memref<256xf32, #gpu.address_space<workgroup>> to memref<256xf32, #gpu.address_space<workgroup>>
          %65 = memref.load %reinterpret_cast_4[%arg2] : memref<256xf32, #gpu.address_space<workgroup>>
          %c0_5 = arith.constant 0 : index
          %66 = memref.load %reinterpret_cast_4[%64] : memref<256xf32, #gpu.address_space<workgroup>>
          %67 = arith.cmpf ugt, %65, %66 : f32
          %68 = arith.select %67, %65, %66 : f32
          %69 = arith.cmpf uno, %66, %66 : f32
          %70 = arith.select %69, %66, %68 : f32
          memref.store %70, %reinterpret_cast_4[%arg2] : memref<256xf32, #gpu.address_space<workgroup>>
        }
        gpu.barrier
        %62 = arith.cmpi eq, %35, %c0 : index
        %63 = arith.andi %62, %45 : i1
        scf.if %63 {
          %64 = arith.addi %arg2, %c8 : index
          %c0_3 = arith.constant 0 : index
          %reinterpret_cast_4 = memref.reinterpret_cast %alloc_2 to offset: [0], sizes: [256], strides: [1] : memref<256xf32, #gpu.address_space<workgroup>> to memref<256xf32, #gpu.address_space<workgroup>>
          %65 = memref.load %reinterpret_cast_4[%arg2] : memref<256xf32, #gpu.address_space<workgroup>>
          %c0_5 = arith.constant 0 : index
          %66 = memref.load %reinterpret_cast_4[%64] : memref<256xf32, #gpu.address_space<workgroup>>
          %67 = arith.cmpf ugt, %65, %66 : f32
          %68 = arith.select %67, %65, %66 : f32
          %69 = arith.cmpf uno, %66, %66 : f32
          %70 = arith.select %69, %66, %68 : f32
          %71 = memref.generic_atomic_rmw %alloc[%42] : memref<?xf32, "gpu"> {
          ^bb0(%arg3: f32):
            %72 = arith.cmpf ogt, %arg3, %70 : f32
            %73 = arith.select %72, %arg3, %70 : f32
            memref.atomic_yield %73 : f32
          }
        }
        scf.yield
      }
      "lmhlo.terminator"() : () -> ()
    }) {disc.device = "gpu", disc.fusion.name = "main_kColReduction_reduce__4_1_0", disc.fusion.tag = "8w32h", disc.fusion_type = "kColReduction", disc_col_reduction_schedule_hint = 1 : i32, disc_thread_per_block_hint = 256 : i32} : () -> ()
  } else {
    "lmhlo.fusion"() ({
      scf.parallel (%arg1) = (%c0) to (%5) step (%c1) {
        %reinterpret_cast_2 = memref.reinterpret_cast %alloc to offset: [%c0], sizes: [%5], strides: [%c1] : memref<?xf32, "gpu"> to memref<?xf32, "gpu">
        memref.store %cst, %reinterpret_cast_2[%arg1] : memref<?xf32, "gpu">
        scf.yield
      }
      %18 = arith.addi %5, %c-1 : index
      %19 = arith.divsi %18, %c8 : index
      %20 = arith.addi %19, %c1 : index
      %21 = arith.subi %c0, %5 : index
      %22 = arith.divsi %21, %c8 : index
      %23 = arith.subi %c0, %22 : index
      %24 = arith.cmpi sgt, %5, %c0 : index
      %25 = arith.select %24, %20, %23 : index
      %26 = arith.addi %dim_1, %c-1 : index
      %27 = arith.divsi %26, %c16 : index
      %28 = arith.addi %27, %c1 : index
      %29 = arith.subi %c0, %dim_1 : index
      %30 = arith.divsi %29, %c16 : index
      %31 = arith.subi %c0, %30 : index
      %32 = arith.cmpi sgt, %dim_1, %c0 : index
      %33 = arith.select %32, %28, %31 : index
      %34 = arith.muli %25, %33 : index
      scf.parallel (%arg1, %arg2) = (%c0, %c0) to (%34, %c128) step (%c1, %c1) {
        %35 = arith.divui %arg2, %c8 : index
        %36 = arith.remui %arg2, %c8 : index
        %37 = arith.divui %arg1, %25 : index
        %38 = arith.remui %arg1, %25 : index
        %39 = arith.muli %37, %c16 : index
        %40 = arith.addi %39, %35 : index
        %41 = arith.muli %38, %c8 : index
        %42 = arith.addi %41, %36 : index
        %alloc_2 = memref.alloc() : memref<128xf32, #gpu.address_space<workgroup>>
        %43 = arith.cmpi ult, %40, %dim_1 : index
        %44 = arith.cmpi ult, %42, %5 : index
        %45 = arith.andi %43, %44 : i1
        scf.if %45 {
          %c0_3 = arith.constant 0 : index
          %60 = arith.muli %40, %5 : index
          %61 = arith.addi %60, %42 : index
          %62 = arith.muli %dim_1, %dim_0 : index
          %63 = arith.muli %62, %dim : index
          %reinterpret_cast_4 = memref.reinterpret_cast %1 to offset: [%c0], sizes: [%63], strides: [%c1] : memref<?x?x?xf32, "gpu"> to memref<?xf32, "gpu">
          %64 = memref.load %reinterpret_cast_4[%61] : memref<?xf32, "gpu">
          %65 = math.absf %64 : f32
          %c0_5 = arith.constant 0 : index
          %reinterpret_cast_6 = memref.reinterpret_cast %alloc_2 to offset: [0], sizes: [128], strides: [1] : memref<128xf32, #gpu.address_space<workgroup>> to memref<128xf32, #gpu.address_space<workgroup>>
          memref.store %65, %reinterpret_cast_6[%arg2] : memref<128xf32, #gpu.address_space<workgroup>>
        } else {
          %c0_3 = arith.constant 0 : index
          %reinterpret_cast_4 = memref.reinterpret_cast %alloc_2 to offset: [0], sizes: [128], strides: [1] : memref<128xf32, #gpu.address_space<workgroup>> to memref<128xf32, #gpu.address_space<workgroup>>
          memref.store %cst, %reinterpret_cast_4[%arg2] : memref<128xf32, #gpu.address_space<workgroup>>
        }
        gpu.barrier
        %46 = arith.cmpi ult, %35, %c8 : index
        %47 = arith.addi %40, %c8 : index
        %48 = arith.cmpi ult, %47, %dim_1 : index
        %49 = arith.andi %46, %48 : i1
        scf.if %49 {
          %60 = arith.addi %arg2, %c64 : index
          %c0_3 = arith.constant 0 : index
          %reinterpret_cast_4 = memref.reinterpret_cast %alloc_2 to offset: [0], sizes: [128], strides: [1] : memref<128xf32, #gpu.address_space<workgroup>> to memref<128xf32, #gpu.address_space<workgroup>>
          %61 = memref.load %reinterpret_cast_4[%arg2] : memref<128xf32, #gpu.address_space<workgroup>>
          %c0_5 = arith.constant 0 : index
          %62 = memref.load %reinterpret_cast_4[%60] : memref<128xf32, #gpu.address_space<workgroup>>
          %63 = arith.cmpf ugt, %61, %62 : f32
          %64 = arith.select %63, %61, %62 : f32
          %65 = arith.cmpf uno, %62, %62 : f32
          %66 = arith.select %65, %62, %64 : f32
          memref.store %66, %reinterpret_cast_4[%arg2] : memref<128xf32, #gpu.address_space<workgroup>>
        }
        gpu.barrier
        %50 = arith.cmpi ult, %35, %c4 : index
        %51 = arith.addi %40, %c4 : index
        %52 = arith.cmpi ult, %51, %dim_1 : index
        %53 = arith.andi %50, %52 : i1
        scf.if %53 {
          %60 = arith.addi %arg2, %c32 : index
          %c0_3 = arith.constant 0 : index
          %reinterpret_cast_4 = memref.reinterpret_cast %alloc_2 to offset: [0], sizes: [128], strides: [1] : memref<128xf32, #gpu.address_space<workgroup>> to memref<128xf32, #gpu.address_space<workgroup>>
          %61 = memref.load %reinterpret_cast_4[%arg2] : memref<128xf32, #gpu.address_space<workgroup>>
          %c0_5 = arith.constant 0 : index
          %62 = memref.load %reinterpret_cast_4[%60] : memref<128xf32, #gpu.address_space<workgroup>>
          %63 = arith.cmpf ugt, %61, %62 : f32
          %64 = arith.select %63, %61, %62 : f32
          %65 = arith.cmpf uno, %62, %62 : f32
          %66 = arith.select %65, %62, %64 : f32
          memref.store %66, %reinterpret_cast_4[%arg2] : memref<128xf32, #gpu.address_space<workgroup>>
        }
        gpu.barrier
        %54 = arith.cmpi ult, %35, %c2 : index
        %55 = arith.addi %40, %c2 : index
        %56 = arith.cmpi ult, %55, %dim_1 : index
        %57 = arith.andi %54, %56 : i1
        scf.if %57 {
          %60 = arith.addi %arg2, %c16 : index
          %c0_3 = arith.constant 0 : index
          %reinterpret_cast_4 = memref.reinterpret_cast %alloc_2 to offset: [0], sizes: [128], strides: [1] : memref<128xf32, #gpu.address_space<workgroup>> to memref<128xf32, #gpu.address_space<workgroup>>
          %61 = memref.load %reinterpret_cast_4[%arg2] : memref<128xf32, #gpu.address_space<workgroup>>
          %c0_5 = arith.constant 0 : index
          %62 = memref.load %reinterpret_cast_4[%60] : memref<128xf32, #gpu.address_space<workgroup>>
          %63 = arith.cmpf ugt, %61, %62 : f32
          %64 = arith.select %63, %61, %62 : f32
          %65 = arith.cmpf uno, %62, %62 : f32
          %66 = arith.select %65, %62, %64 : f32
          memref.store %66, %reinterpret_cast_4[%arg2] : memref<128xf32, #gpu.address_space<workgroup>>
        }
        gpu.barrier
        %58 = arith.cmpi eq, %35, %c0 : index
        %59 = arith.andi %58, %45 : i1
        scf.if %59 {
          %60 = arith.addi %arg2, %c8 : index
          %c0_3 = arith.constant 0 : index
          %reinterpret_cast_4 = memref.reinterpret_cast %alloc_2 to offset: [0], sizes: [128], strides: [1] : memref<128xf32, #gpu.address_space<workgroup>> to memref<128xf32, #gpu.address_space<workgroup>>
          %61 = memref.load %reinterpret_cast_4[%arg2] : memref<128xf32, #gpu.address_space<workgroup>>
          %c0_5 = arith.constant 0 : index
          %62 = memref.load %reinterpret_cast_4[%60] : memref<128xf32, #gpu.address_space<workgroup>>
          %63 = arith.cmpf ugt, %61, %62 : f32
          %64 = arith.select %63, %61, %62 : f32
          %65 = arith.cmpf uno, %62, %62 : f32
          %66 = arith.select %65, %62, %64 : f32
          %67 = memref.generic_atomic_rmw %alloc[%42] : memref<?xf32, "gpu"> {
          ^bb0(%arg3: f32):
            %68 = arith.cmpf ogt, %arg3, %66 : f32
            %69 = arith.select %68, %arg3, %66 : f32
            memref.atomic_yield %69 : f32
          }
        }
        scf.yield
      }
      "lmhlo.terminator"() : () -> ()
    }) {disc.device = "gpu", disc.fusion.name = "main_kColReduction_reduce__4_1_0", disc.fusion.tag = "8w16h", disc.fusion_type = "kColReduction", disc_col_reduction_schedule_hint = 2 : i32, disc_thread_per_block_hint = 128 : i32} : () -> ()
  }
  %16 = llvm.inttoptr %0 : i32 to !llvm.ptr<i8>
  %alloca = memref.alloca() : memref<2xindex, "cpu">
  memref.store %dim_0, %alloca[%c0] : memref<2xindex, "cpu">
  memref.store %dim, %alloca[%c1] : memref<2xindex, "cpu">
  %17 = "disc_ral.dispatch"(%arg0, %16, %alloc, %alloca) {backend_config = "", call_target_name = "inc_ref", device = "gpu", has_side_effect = false} : (!disc_ral.context, !llvm.ptr<i8>, memref<?xf32, "gpu">, memref<2xindex, "cpu">) -> memref<?x?xf32, "gpu">
  %reinterpret_cast = memref.reinterpret_cast %17 to offset: [0], sizes: [%dim_0, %dim], strides: [%dim, 1] {kDiscSymbolicDimAttr = [@S1, @S2]} : memref<?x?xf32, "gpu"> to memref<?x?xf32, "gpu">
  memref.dealloc %alloc : memref<?xf32, "gpu">
  "disc_ral.dispatch"(%arg0, %c0, %reinterpret_cast) {backend_config = "", call_target_name = "ral_send_output", device = "cpu", has_side_effect = false} : (!disc_ral.context, index, memref<?x?xf32, "gpu">) -> ()
  return
}

// -----// IR Dump After Canonicalizer (disc-canonicalize) //----- //
func.func @main(%arg0: !disc_ral.context) attributes {tf.entry_function = {input_placements = "gpu", inputs = "input0", output_placements = "gpu", outputs = "output0"}} {
  %c-1 = arith.constant -1 : index
  %cst = arith.constant 0xFF800000 : f32
  %c64 = arith.constant 64 : index
  %c128 = arith.constant 128 : index
  %c4 = arith.constant 4 : index
  %c16 = arith.constant 16 : index
  %c32 = arith.constant 32 : index
  %c8 = arith.constant 8 : index
  %0 = llvm.mlir.constant(0 : i32) : i32
  %c0 = arith.constant 0 : index
  %c1 = arith.constant 1 : index
  %c2 = arith.constant 2 : index
  %c256 = arith.constant 256 : index
  %c108 = arith.constant 108 : index
  %1 = "disc_ral.dispatch"(%arg0, %c0) {backend_config = "", call_target_name = "ral_recv_input", device = "cpu", has_side_effect = false} : (!disc_ral.context, index) -> memref<?x?x?xf32, "gpu">
  %dim = memref.dim %1, %c2 : memref<?x?x?xf32, "gpu">
  %dim_0 = memref.dim %1, %c1 : memref<?x?x?xf32, "gpu">
  %dim_1 = memref.dim %1, %c0 : memref<?x?x?xf32, "gpu">
  %2 = arith.index_cast %dim_0 : index to i32
  %3 = arith.index_cast %dim : index to i32
  %4 = arith.muli %2, %3 : i32
  %5 = arith.index_cast %4 : i32 to index
  %alloc = memref.alloc(%5) {kDiscSymbolicDimAttr = [@S3]} : memref<?xf32, "gpu">
  %6 = arith.muli %dim_1, %5 : index
  %7 = arith.addi %6, %c-1 : index
  %8 = arith.divsi %7, %c256 : index
  %9 = arith.addi %8, %c1 : index
  %10 = arith.subi %c0, %6 : index
  %11 = arith.divsi %10, %c256 : index
  %12 = arith.subi %c0, %11 : index
  %13 = arith.cmpi sgt, %6, %c0 : index
  %14 = arith.select %13, %9, %12 : index
  %15 = arith.cmpi sgt, %14, %c108 : index
  scf.if %15 {
    "lmhlo.fusion"() ({
      scf.parallel (%arg1) = (%c0) to (%5) step (%c1) {
        %reinterpret_cast_2 = memref.reinterpret_cast %alloc to offset: [%c0], sizes: [%5], strides: [%c1] : memref<?xf32, "gpu"> to memref<?xf32, "gpu">
        memref.store %cst, %reinterpret_cast_2[%arg1] : memref<?xf32, "gpu">
        scf.yield
      }
      %18 = arith.addi %5, %c-1 : index
      %19 = arith.divsi %18, %c8 : index
      %20 = arith.addi %19, %c1 : index
      %21 = arith.subi %c0, %5 : index
      %22 = arith.divsi %21, %c8 : index
      %23 = arith.subi %c0, %22 : index
      %24 = arith.cmpi sgt, %5, %c0 : index
      %25 = arith.select %24, %20, %23 : index
      %26 = arith.addi %dim_1, %c-1 : index
      %27 = arith.divsi %26, %c32 : index
      %28 = arith.addi %27, %c1 : index
      %29 = arith.subi %c0, %dim_1 : index
      %30 = arith.divsi %29, %c32 : index
      %31 = arith.subi %c0, %30 : index
      %32 = arith.cmpi sgt, %dim_1, %c0 : index
      %33 = arith.select %32, %28, %31 : index
      %34 = arith.muli %25, %33 : index
      scf.parallel (%arg1, %arg2) = (%c0, %c0) to (%34, %c256) step (%c1, %c1) {
        %35 = arith.divui %arg2, %c8 : index
        %36 = arith.remui %arg2, %c8 : index
        %37 = arith.divui %arg1, %25 : index
        %38 = arith.remui %arg1, %25 : index
        %39 = arith.muli %37, %c32 : index
        %40 = arith.addi %39, %35 : index
        %41 = arith.muli %38, %c8 : index
        %42 = arith.addi %41, %36 : index
        %alloc_2 = memref.alloc() : memref<256xf32, #gpu.address_space<workgroup>>
        %43 = arith.cmpi ult, %40, %dim_1 : index
        %44 = arith.cmpi ult, %42, %5 : index
        %45 = arith.andi %43, %44 : i1
        scf.if %45 {
          %64 = arith.muli %40, %5 : index
          %65 = arith.addi %64, %42 : index
          %66 = arith.muli %dim_1, %dim_0 : index
          %67 = arith.muli %66, %dim : index
          %reinterpret_cast_3 = memref.reinterpret_cast %1 to offset: [%c0], sizes: [%67], strides: [%c1] : memref<?x?x?xf32, "gpu"> to memref<?xf32, "gpu">
          %68 = memref.load %reinterpret_cast_3[%65] : memref<?xf32, "gpu">
          %69 = math.absf %68 : f32
          %reinterpret_cast_4 = memref.reinterpret_cast %alloc_2 to offset: [0], sizes: [256], strides: [1] : memref<256xf32, #gpu.address_space<workgroup>> to memref<256xf32, #gpu.address_space<workgroup>>
          memref.store %69, %reinterpret_cast_4[%arg2] : memref<256xf32, #gpu.address_space<workgroup>>
        } else {
          %reinterpret_cast_3 = memref.reinterpret_cast %alloc_2 to offset: [0], sizes: [256], strides: [1] : memref<256xf32, #gpu.address_space<workgroup>> to memref<256xf32, #gpu.address_space<workgroup>>
          memref.store %cst, %reinterpret_cast_3[%arg2] : memref<256xf32, #gpu.address_space<workgroup>>
        }
        gpu.barrier
        %46 = arith.cmpi ult, %35, %c16 : index
        %47 = arith.addi %40, %c16 : index
        %48 = arith.cmpi ult, %47, %dim_1 : index
        %49 = arith.andi %46, %48 : i1
        scf.if %49 {
          %64 = arith.addi %arg2, %c128 : index
          %reinterpret_cast_3 = memref.reinterpret_cast %alloc_2 to offset: [0], sizes: [256], strides: [1] : memref<256xf32, #gpu.address_space<workgroup>> to memref<256xf32, #gpu.address_space<workgroup>>
          %65 = memref.load %reinterpret_cast_3[%arg2] : memref<256xf32, #gpu.address_space<workgroup>>
          %66 = memref.load %reinterpret_cast_3[%64] : memref<256xf32, #gpu.address_space<workgroup>>
          %67 = arith.cmpf ugt, %65, %66 : f32
          %68 = arith.select %67, %65, %66 : f32
          %69 = arith.cmpf uno, %66, %66 : f32
          %70 = arith.select %69, %66, %68 : f32
          memref.store %70, %reinterpret_cast_3[%arg2] : memref<256xf32, #gpu.address_space<workgroup>>
        }
        gpu.barrier
        %50 = arith.cmpi ult, %35, %c8 : index
        %51 = arith.addi %40, %c8 : index
        %52 = arith.cmpi ult, %51, %dim_1 : index
        %53 = arith.andi %50, %52 : i1
        scf.if %53 {
          %64 = arith.addi %arg2, %c64 : index
          %reinterpret_cast_3 = memref.reinterpret_cast %alloc_2 to offset: [0], sizes: [256], strides: [1] : memref<256xf32, #gpu.address_space<workgroup>> to memref<256xf32, #gpu.address_space<workgroup>>
          %65 = memref.load %reinterpret_cast_3[%arg2] : memref<256xf32, #gpu.address_space<workgroup>>
          %66 = memref.load %reinterpret_cast_3[%64] : memref<256xf32, #gpu.address_space<workgroup>>
          %67 = arith.cmpf ugt, %65, %66 : f32
          %68 = arith.select %67, %65, %66 : f32
          %69 = arith.cmpf uno, %66, %66 : f32
          %70 = arith.select %69, %66, %68 : f32
          memref.store %70, %reinterpret_cast_3[%arg2] : memref<256xf32, #gpu.address_space<workgroup>>
        }
        gpu.barrier
        %54 = arith.cmpi ult, %35, %c4 : index
        %55 = arith.addi %40, %c4 : index
        %56 = arith.cmpi ult, %55, %dim_1 : index
        %57 = arith.andi %54, %56 : i1
        scf.if %57 {
          %64 = arith.addi %arg2, %c32 : index
          %reinterpret_cast_3 = memref.reinterpret_cast %alloc_2 to offset: [0], sizes: [256], strides: [1] : memref<256xf32, #gpu.address_space<workgroup>> to memref<256xf32, #gpu.address_space<workgroup>>
          %65 = memref.load %reinterpret_cast_3[%arg2] : memref<256xf32, #gpu.address_space<workgroup>>
          %66 = memref.load %reinterpret_cast_3[%64] : memref<256xf32, #gpu.address_space<workgroup>>
          %67 = arith.cmpf ugt, %65, %66 : f32
          %68 = arith.select %67, %65, %66 : f32
          %69 = arith.cmpf uno, %66, %66 : f32
          %70 = arith.select %69, %66, %68 : f32
          memref.store %70, %reinterpret_cast_3[%arg2] : memref<256xf32, #gpu.address_space<workgroup>>
        }
        gpu.barrier
        %58 = arith.cmpi ult, %35, %c2 : index
        %59 = arith.addi %40, %c2 : index
        %60 = arith.cmpi ult, %59, %dim_1 : index
        %61 = arith.andi %58, %60 : i1
        scf.if %61 {
          %64 = arith.addi %arg2, %c16 : index
          %reinterpret_cast_3 = memref.reinterpret_cast %alloc_2 to offset: [0], sizes: [256], strides: [1] : memref<256xf32, #gpu.address_space<workgroup>> to memref<256xf32, #gpu.address_space<workgroup>>
          %65 = memref.load %reinterpret_cast_3[%arg2] : memref<256xf32, #gpu.address_space<workgroup>>
          %66 = memref.load %reinterpret_cast_3[%64] : memref<256xf32, #gpu.address_space<workgroup>>
          %67 = arith.cmpf ugt, %65, %66 : f32
          %68 = arith.select %67, %65, %66 : f32
          %69 = arith.cmpf uno, %66, %66 : f32
          %70 = arith.select %69, %66, %68 : f32
          memref.store %70, %reinterpret_cast_3[%arg2] : memref<256xf32, #gpu.address_space<workgroup>>
        }
        gpu.barrier
        %62 = arith.cmpi eq, %35, %c0 : index
        %63 = arith.andi %62, %45 : i1
        scf.if %63 {
          %64 = arith.addi %arg2, %c8 : index
          %reinterpret_cast_3 = memref.reinterpret_cast %alloc_2 to offset: [0], sizes: [256], strides: [1] : memref<256xf32, #gpu.address_space<workgroup>> to memref<256xf32, #gpu.address_space<workgroup>>
          %65 = memref.load %reinterpret_cast_3[%arg2] : memref<256xf32, #gpu.address_space<workgroup>>
          %66 = memref.load %reinterpret_cast_3[%64] : memref<256xf32, #gpu.address_space<workgroup>>
          %67 = arith.cmpf ugt, %65, %66 : f32
          %68 = arith.select %67, %65, %66 : f32
          %69 = arith.cmpf uno, %66, %66 : f32
          %70 = arith.select %69, %66, %68 : f32
          %71 = memref.generic_atomic_rmw %alloc[%42] : memref<?xf32, "gpu"> {
          ^bb0(%arg3: f32):
            %72 = arith.cmpf ogt, %arg3, %70 : f32
            %73 = arith.select %72, %arg3, %70 : f32
            memref.atomic_yield %73 : f32
          }
        }
        scf.yield
      }
      "lmhlo.terminator"() : () -> ()
    }) {disc.device = "gpu", disc.fusion.name = "main_kColReduction_reduce__4_1_0", disc.fusion.tag = "8w32h", disc.fusion_type = "kColReduction", disc_col_reduction_schedule_hint = 1 : i32, disc_thread_per_block_hint = 256 : i32} : () -> ()
  } else {
    "lmhlo.fusion"() ({
      scf.parallel (%arg1) = (%c0) to (%5) step (%c1) {
        %reinterpret_cast_2 = memref.reinterpret_cast %alloc to offset: [%c0], sizes: [%5], strides: [%c1] : memref<?xf32, "gpu"> to memref<?xf32, "gpu">
        memref.store %cst, %reinterpret_cast_2[%arg1] : memref<?xf32, "gpu">
        scf.yield
      }
      %18 = arith.addi %5, %c-1 : index
      %19 = arith.divsi %18, %c8 : index
      %20 = arith.addi %19, %c1 : index
      %21 = arith.subi %c0, %5 : index
      %22 = arith.divsi %21, %c8 : index
      %23 = arith.subi %c0, %22 : index
      %24 = arith.cmpi sgt, %5, %c0 : index
      %25 = arith.select %24, %20, %23 : index
      %26 = arith.addi %dim_1, %c-1 : index
      %27 = arith.divsi %26, %c16 : index
      %28 = arith.addi %27, %c1 : index
      %29 = arith.subi %c0, %dim_1 : index
      %30 = arith.divsi %29, %c16 : index
      %31 = arith.subi %c0, %30 : index
      %32 = arith.cmpi sgt, %dim_1, %c0 : index
      %33 = arith.select %32, %28, %31 : index
      %34 = arith.muli %25, %33 : index
      scf.parallel (%arg1, %arg2) = (%c0, %c0) to (%34, %c128) step (%c1, %c1) {
        %35 = arith.divui %arg2, %c8 : index
        %36 = arith.remui %arg2, %c8 : index
        %37 = arith.divui %arg1, %25 : index
        %38 = arith.remui %arg1, %25 : index
        %39 = arith.muli %37, %c16 : index
        %40 = arith.addi %39, %35 : index
        %41 = arith.muli %38, %c8 : index
        %42 = arith.addi %41, %36 : index
        %alloc_2 = memref.alloc() : memref<128xf32, #gpu.address_space<workgroup>>
        %43 = arith.cmpi ult, %40, %dim_1 : index
        %44 = arith.cmpi ult, %42, %5 : index
        %45 = arith.andi %43, %44 : i1
        scf.if %45 {
          %60 = arith.muli %40, %5 : index
          %61 = arith.addi %60, %42 : index
          %62 = arith.muli %dim_1, %dim_0 : index
          %63 = arith.muli %62, %dim : index
          %reinterpret_cast_3 = memref.reinterpret_cast %1 to offset: [%c0], sizes: [%63], strides: [%c1] : memref<?x?x?xf32, "gpu"> to memref<?xf32, "gpu">
          %64 = memref.load %reinterpret_cast_3[%61] : memref<?xf32, "gpu">
          %65 = math.absf %64 : f32
          %reinterpret_cast_4 = memref.reinterpret_cast %alloc_2 to offset: [0], sizes: [128], strides: [1] : memref<128xf32, #gpu.address_space<workgroup>> to memref<128xf32, #gpu.address_space<workgroup>>
          memref.store %65, %reinterpret_cast_4[%arg2] : memref<128xf32, #gpu.address_space<workgroup>>
        } else {
          %reinterpret_cast_3 = memref.reinterpret_cast %alloc_2 to offset: [0], sizes: [128], strides: [1] : memref<128xf32, #gpu.address_space<workgroup>> to memref<128xf32, #gpu.address_space<workgroup>>
          memref.store %cst, %reinterpret_cast_3[%arg2] : memref<128xf32, #gpu.address_space<workgroup>>
        }
        gpu.barrier
        %46 = arith.cmpi ult, %35, %c8 : index
        %47 = arith.addi %40, %c8 : index
        %48 = arith.cmpi ult, %47, %dim_1 : index
        %49 = arith.andi %46, %48 : i1
        scf.if %49 {
          %60 = arith.addi %arg2, %c64 : index
          %reinterpret_cast_3 = memref.reinterpret_cast %alloc_2 to offset: [0], sizes: [128], strides: [1] : memref<128xf32, #gpu.address_space<workgroup>> to memref<128xf32, #gpu.address_space<workgroup>>
          %61 = memref.load %reinterpret_cast_3[%arg2] : memref<128xf32, #gpu.address_space<workgroup>>
          %62 = memref.load %reinterpret_cast_3[%60] : memref<128xf32, #gpu.address_space<workgroup>>
          %63 = arith.cmpf ugt, %61, %62 : f32
          %64 = arith.select %63, %61, %62 : f32
          %65 = arith.cmpf uno, %62, %62 : f32
          %66 = arith.select %65, %62, %64 : f32
          memref.store %66, %reinterpret_cast_3[%arg2] : memref<128xf32, #gpu.address_space<workgroup>>
        }
        gpu.barrier
        %50 = arith.cmpi ult, %35, %c4 : index
        %51 = arith.addi %40, %c4 : index
        %52 = arith.cmpi ult, %51, %dim_1 : index
        %53 = arith.andi %50, %52 : i1
        scf.if %53 {
          %60 = arith.addi %arg2, %c32 : index
          %reinterpret_cast_3 = memref.reinterpret_cast %alloc_2 to offset: [0], sizes: [128], strides: [1] : memref<128xf32, #gpu.address_space<workgroup>> to memref<128xf32, #gpu.address_space<workgroup>>
          %61 = memref.load %reinterpret_cast_3[%arg2] : memref<128xf32, #gpu.address_space<workgroup>>
          %62 = memref.load %reinterpret_cast_3[%60] : memref<128xf32, #gpu.address_space<workgroup>>
          %63 = arith.cmpf ugt, %61, %62 : f32
          %64 = arith.select %63, %61, %62 : f32
          %65 = arith.cmpf uno, %62, %62 : f32
          %66 = arith.select %65, %62, %64 : f32
          memref.store %66, %reinterpret_cast_3[%arg2] : memref<128xf32, #gpu.address_space<workgroup>>
        }
        gpu.barrier
        %54 = arith.cmpi ult, %35, %c2 : index
        %55 = arith.addi %40, %c2 : index
        %56 = arith.cmpi ult, %55, %dim_1 : index
        %57 = arith.andi %54, %56 : i1
        scf.if %57 {
          %60 = arith.addi %arg2, %c16 : index
          %reinterpret_cast_3 = memref.reinterpret_cast %alloc_2 to offset: [0], sizes: [128], strides: [1] : memref<128xf32, #gpu.address_space<workgroup>> to memref<128xf32, #gpu.address_space<workgroup>>
          %61 = memref.load %reinterpret_cast_3[%arg2] : memref<128xf32, #gpu.address_space<workgroup>>
          %62 = memref.load %reinterpret_cast_3[%60] : memref<128xf32, #gpu.address_space<workgroup>>
          %63 = arith.cmpf ugt, %61, %62 : f32
          %64 = arith.select %63, %61, %62 : f32
          %65 = arith.cmpf uno, %62, %62 : f32
          %66 = arith.select %65, %62, %64 : f32
          memref.store %66, %reinterpret_cast_3[%arg2] : memref<128xf32, #gpu.address_space<workgroup>>
        }
        gpu.barrier
        %58 = arith.cmpi eq, %35, %c0 : index
        %59 = arith.andi %58, %45 : i1
        scf.if %59 {
          %60 = arith.addi %arg2, %c8 : index
          %reinterpret_cast_3 = memref.reinterpret_cast %alloc_2 to offset: [0], sizes: [128], strides: [1] : memref<128xf32, #gpu.address_space<workgroup>> to memref<128xf32, #gpu.address_space<workgroup>>
          %61 = memref.load %reinterpret_cast_3[%arg2] : memref<128xf32, #gpu.address_space<workgroup>>
          %62 = memref.load %reinterpret_cast_3[%60] : memref<128xf32, #gpu.address_space<workgroup>>
          %63 = arith.cmpf ugt, %61, %62 : f32
          %64 = arith.select %63, %61, %62 : f32
          %65 = arith.cmpf uno, %62, %62 : f32
          %66 = arith.select %65, %62, %64 : f32
          %67 = memref.generic_atomic_rmw %alloc[%42] : memref<?xf32, "gpu"> {
          ^bb0(%arg3: f32):
            %68 = arith.cmpf ogt, %arg3, %66 : f32
            %69 = arith.select %68, %arg3, %66 : f32
            memref.atomic_yield %69 : f32
          }
        }
        scf.yield
      }
      "lmhlo.terminator"() : () -> ()
    }) {disc.device = "gpu", disc.fusion.name = "main_kColReduction_reduce__4_1_0", disc.fusion.tag = "8w16h", disc.fusion_type = "kColReduction", disc_col_reduction_schedule_hint = 2 : i32, disc_thread_per_block_hint = 128 : i32} : () -> ()
  }
  %16 = llvm.inttoptr %0 : i32 to !llvm.ptr<i8>
  %alloca = memref.alloca() : memref<2xindex, "cpu">
  memref.store %dim_0, %alloca[%c0] : memref<2xindex, "cpu">
  memref.store %dim, %alloca[%c1] : memref<2xindex, "cpu">
  %17 = "disc_ral.dispatch"(%arg0, %16, %alloc, %alloca) {backend_config = "", call_target_name = "inc_ref", device = "gpu", has_side_effect = false} : (!disc_ral.context, !llvm.ptr<i8>, memref<?xf32, "gpu">, memref<2xindex, "cpu">) -> memref<?x?xf32, "gpu">
  %reinterpret_cast = memref.reinterpret_cast %17 to offset: [0], sizes: [%dim_0, %dim], strides: [%dim, 1] {kDiscSymbolicDimAttr = [@S1, @S2]} : memref<?x?xf32, "gpu"> to memref<?x?xf32, "gpu">
  memref.dealloc %alloc : memref<?xf32, "gpu">
  "disc_ral.dispatch"(%arg0, %c0, %reinterpret_cast) {backend_config = "", call_target_name = "ral_send_output", device = "cpu", has_side_effect = false} : (!disc_ral.context, index, memref<?x?xf32, "gpu">) -> ()
  return
}

// -----// IR Dump After ParallelLoopCollapsing (disc-parallel-loop-collapsing) //----- //
func.func @main(%arg0: !disc_ral.context) attributes {tf.entry_function = {input_placements = "gpu", inputs = "input0", output_placements = "gpu", outputs = "output0"}} {
  %c-1 = arith.constant -1 : index
  %cst = arith.constant 0xFF800000 : f32
  %c64 = arith.constant 64 : index
  %c128 = arith.constant 128 : index
  %c4 = arith.constant 4 : index
  %c16 = arith.constant 16 : index
  %c32 = arith.constant 32 : index
  %c8 = arith.constant 8 : index
  %0 = llvm.mlir.constant(0 : i32) : i32
  %c0 = arith.constant 0 : index
  %c1 = arith.constant 1 : index
  %c2 = arith.constant 2 : index
  %c256 = arith.constant 256 : index
  %c108 = arith.constant 108 : index
  %1 = "disc_ral.dispatch"(%arg0, %c0) {backend_config = "", call_target_name = "ral_recv_input", device = "cpu", has_side_effect = false} : (!disc_ral.context, index) -> memref<?x?x?xf32, "gpu">
  %dim = memref.dim %1, %c2 : memref<?x?x?xf32, "gpu">
  %dim_0 = memref.dim %1, %c1 : memref<?x?x?xf32, "gpu">
  %dim_1 = memref.dim %1, %c0 : memref<?x?x?xf32, "gpu">
  %2 = arith.index_cast %dim_0 : index to i32
  %3 = arith.index_cast %dim : index to i32
  %4 = arith.muli %2, %3 : i32
  %5 = arith.index_cast %4 : i32 to index
  %alloc = memref.alloc(%5) {kDiscSymbolicDimAttr = [@S3]} : memref<?xf32, "gpu">
  %6 = arith.muli %dim_1, %5 : index
  %7 = arith.addi %6, %c-1 : index
  %8 = arith.divsi %7, %c256 : index
  %9 = arith.addi %8, %c1 : index
  %10 = arith.subi %c0, %6 : index
  %11 = arith.divsi %10, %c256 : index
  %12 = arith.subi %c0, %11 : index
  %13 = arith.cmpi sgt, %6, %c0 : index
  %14 = arith.select %13, %9, %12 : index
  %15 = arith.cmpi sgt, %14, %c108 : index
  scf.if %15 {
    "lmhlo.fusion"() ({
      scf.parallel (%arg1) = (%c0) to (%5) step (%c1) {
        %reinterpret_cast_5 = memref.reinterpret_cast %alloc to offset: [%c0], sizes: [%5], strides: [%c1] : memref<?xf32, "gpu"> to memref<?xf32, "gpu">
        memref.store %cst, %reinterpret_cast_5[%arg1] : memref<?xf32, "gpu">
        scf.yield
      }
      %18 = arith.addi %5, %c-1 : index
      %19 = arith.divsi %18, %c8 : index
      %20 = arith.addi %19, %c1 : index
      %21 = arith.subi %c0, %5 : index
      %22 = arith.divsi %21, %c8 : index
      %23 = arith.subi %c0, %22 : index
      %24 = arith.cmpi sgt, %5, %c0 : index
      %25 = arith.select %24, %20, %23 : index
      %26 = arith.addi %dim_1, %c-1 : index
      %27 = arith.divsi %26, %c32 : index
      %28 = arith.addi %27, %c1 : index
      %29 = arith.subi %c0, %dim_1 : index
      %30 = arith.divsi %29, %c32 : index
      %31 = arith.subi %c0, %30 : index
      %32 = arith.cmpi sgt, %dim_1, %c0 : index
      %33 = arith.select %32, %28, %31 : index
      %34 = arith.muli %25, %33 : index
      %c0_2 = arith.constant 0 : index
      %c1_3 = arith.constant 1 : index
      %c1_4 = arith.constant 1 : index
      %35 = arith.muli %c1_4, %34 : index
      %36 = arith.muli %35, %c256 : index
      scf.parallel (%arg1) = (%c0_2) to (%36) step (%c1_3) {
        %37 = arith.remsi %arg1, %c256 : index
        %38 = arith.divsi %arg1, %c256 : index
        %39 = arith.divui %37, %c8 : index
        %40 = arith.remui %37, %c8 : index
        %41 = arith.divui %38, %25 : index
        %42 = arith.remui %38, %25 : index
        %43 = arith.muli %41, %c32 : index
        %44 = arith.addi %43, %39 : index
        %45 = arith.muli %42, %c8 : index
        %46 = arith.addi %45, %40 : index
        %alloc_5 = memref.alloc() : memref<256xf32, #gpu.address_space<workgroup>>
        %47 = arith.cmpi ult, %44, %dim_1 : index
        %48 = arith.cmpi ult, %46, %5 : index
        %49 = arith.andi %47, %48 : i1
        scf.if %49 {
          %68 = arith.muli %44, %5 : index
          %69 = arith.addi %68, %46 : index
          %70 = arith.muli %dim_1, %dim_0 : index
          %71 = arith.muli %70, %dim : index
          %reinterpret_cast_6 = memref.reinterpret_cast %1 to offset: [%c0], sizes: [%71], strides: [%c1] : memref<?x?x?xf32, "gpu"> to memref<?xf32, "gpu">
          %72 = memref.load %reinterpret_cast_6[%69] : memref<?xf32, "gpu">
          %73 = math.absf %72 : f32
          %reinterpret_cast_7 = memref.reinterpret_cast %alloc_5 to offset: [0], sizes: [256], strides: [1] : memref<256xf32, #gpu.address_space<workgroup>> to memref<256xf32, #gpu.address_space<workgroup>>
          memref.store %73, %reinterpret_cast_7[%37] : memref<256xf32, #gpu.address_space<workgroup>>
        } else {
          %reinterpret_cast_6 = memref.reinterpret_cast %alloc_5 to offset: [0], sizes: [256], strides: [1] : memref<256xf32, #gpu.address_space<workgroup>> to memref<256xf32, #gpu.address_space<workgroup>>
          memref.store %cst, %reinterpret_cast_6[%37] : memref<256xf32, #gpu.address_space<workgroup>>
        }
        gpu.barrier
        %50 = arith.cmpi ult, %39, %c16 : index
        %51 = arith.addi %44, %c16 : index
        %52 = arith.cmpi ult, %51, %dim_1 : index
        %53 = arith.andi %50, %52 : i1
        scf.if %53 {
          %68 = arith.addi %37, %c128 : index
          %reinterpret_cast_6 = memref.reinterpret_cast %alloc_5 to offset: [0], sizes: [256], strides: [1] : memref<256xf32, #gpu.address_space<workgroup>> to memref<256xf32, #gpu.address_space<workgroup>>
          %69 = memref.load %reinterpret_cast_6[%37] : memref<256xf32, #gpu.address_space<workgroup>>
          %70 = memref.load %reinterpret_cast_6[%68] : memref<256xf32, #gpu.address_space<workgroup>>
          %71 = arith.cmpf ugt, %69, %70 : f32
          %72 = arith.select %71, %69, %70 : f32
          %73 = arith.cmpf uno, %70, %70 : f32
          %74 = arith.select %73, %70, %72 : f32
          memref.store %74, %reinterpret_cast_6[%37] : memref<256xf32, #gpu.address_space<workgroup>>
        }
        gpu.barrier
        %54 = arith.cmpi ult, %39, %c8 : index
        %55 = arith.addi %44, %c8 : index
        %56 = arith.cmpi ult, %55, %dim_1 : index
        %57 = arith.andi %54, %56 : i1
        scf.if %57 {
          %68 = arith.addi %37, %c64 : index
          %reinterpret_cast_6 = memref.reinterpret_cast %alloc_5 to offset: [0], sizes: [256], strides: [1] : memref<256xf32, #gpu.address_space<workgroup>> to memref<256xf32, #gpu.address_space<workgroup>>
          %69 = memref.load %reinterpret_cast_6[%37] : memref<256xf32, #gpu.address_space<workgroup>>
          %70 = memref.load %reinterpret_cast_6[%68] : memref<256xf32, #gpu.address_space<workgroup>>
          %71 = arith.cmpf ugt, %69, %70 : f32
          %72 = arith.select %71, %69, %70 : f32
          %73 = arith.cmpf uno, %70, %70 : f32
          %74 = arith.select %73, %70, %72 : f32
          memref.store %74, %reinterpret_cast_6[%37] : memref<256xf32, #gpu.address_space<workgroup>>
        }
        gpu.barrier
        %58 = arith.cmpi ult, %39, %c4 : index
        %59 = arith.addi %44, %c4 : index
        %60 = arith.cmpi ult, %59, %dim_1 : index
        %61 = arith.andi %58, %60 : i1
        scf.if %61 {
          %68 = arith.addi %37, %c32 : index
          %reinterpret_cast_6 = memref.reinterpret_cast %alloc_5 to offset: [0], sizes: [256], strides: [1] : memref<256xf32, #gpu.address_space<workgroup>> to memref<256xf32, #gpu.address_space<workgroup>>
          %69 = memref.load %reinterpret_cast_6[%37] : memref<256xf32, #gpu.address_space<workgroup>>
          %70 = memref.load %reinterpret_cast_6[%68] : memref<256xf32, #gpu.address_space<workgroup>>
          %71 = arith.cmpf ugt, %69, %70 : f32
          %72 = arith.select %71, %69, %70 : f32
          %73 = arith.cmpf uno, %70, %70 : f32
          %74 = arith.select %73, %70, %72 : f32
          memref.store %74, %reinterpret_cast_6[%37] : memref<256xf32, #gpu.address_space<workgroup>>
        }
        gpu.barrier
        %62 = arith.cmpi ult, %39, %c2 : index
        %63 = arith.addi %44, %c2 : index
        %64 = arith.cmpi ult, %63, %dim_1 : index
        %65 = arith.andi %62, %64 : i1
        scf.if %65 {
          %68 = arith.addi %37, %c16 : index
          %reinterpret_cast_6 = memref.reinterpret_cast %alloc_5 to offset: [0], sizes: [256], strides: [1] : memref<256xf32, #gpu.address_space<workgroup>> to memref<256xf32, #gpu.address_space<workgroup>>
          %69 = memref.load %reinterpret_cast_6[%37] : memref<256xf32, #gpu.address_space<workgroup>>
          %70 = memref.load %reinterpret_cast_6[%68] : memref<256xf32, #gpu.address_space<workgroup>>
          %71 = arith.cmpf ugt, %69, %70 : f32
          %72 = arith.select %71, %69, %70 : f32
          %73 = arith.cmpf uno, %70, %70 : f32
          %74 = arith.select %73, %70, %72 : f32
          memref.store %74, %reinterpret_cast_6[%37] : memref<256xf32, #gpu.address_space<workgroup>>
        }
        gpu.barrier
        %66 = arith.cmpi eq, %39, %c0 : index
        %67 = arith.andi %66, %49 : i1
        scf.if %67 {
          %68 = arith.addi %37, %c8 : index
          %reinterpret_cast_6 = memref.reinterpret_cast %alloc_5 to offset: [0], sizes: [256], strides: [1] : memref<256xf32, #gpu.address_space<workgroup>> to memref<256xf32, #gpu.address_space<workgroup>>
          %69 = memref.load %reinterpret_cast_6[%37] : memref<256xf32, #gpu.address_space<workgroup>>
          %70 = memref.load %reinterpret_cast_6[%68] : memref<256xf32, #gpu.address_space<workgroup>>
          %71 = arith.cmpf ugt, %69, %70 : f32
          %72 = arith.select %71, %69, %70 : f32
          %73 = arith.cmpf uno, %70, %70 : f32
          %74 = arith.select %73, %70, %72 : f32
          %75 = memref.generic_atomic_rmw %alloc[%46] : memref<?xf32, "gpu"> {
          ^bb0(%arg2: f32):
            %76 = arith.cmpf ogt, %arg2, %74 : f32
            %77 = arith.select %76, %arg2, %74 : f32
            memref.atomic_yield %77 : f32
          }
        }
        scf.yield
      }
      "lmhlo.terminator"() : () -> ()
    }) {disc.device = "gpu", disc.fusion.name = "main_kColReduction_reduce__4_1_0", disc.fusion.tag = "8w32h", disc.fusion_type = "kColReduction", disc_col_reduction_schedule_hint = 1 : i32, disc_thread_per_block_hint = 256 : i32} : () -> ()
  } else {
    "lmhlo.fusion"() ({
      scf.parallel (%arg1) = (%c0) to (%5) step (%c1) {
        %reinterpret_cast_5 = memref.reinterpret_cast %alloc to offset: [%c0], sizes: [%5], strides: [%c1] : memref<?xf32, "gpu"> to memref<?xf32, "gpu">
        memref.store %cst, %reinterpret_cast_5[%arg1] : memref<?xf32, "gpu">
        scf.yield
      }
      %18 = arith.addi %5, %c-1 : index
      %19 = arith.divsi %18, %c8 : index
      %20 = arith.addi %19, %c1 : index
      %21 = arith.subi %c0, %5 : index
      %22 = arith.divsi %21, %c8 : index
      %23 = arith.subi %c0, %22 : index
      %24 = arith.cmpi sgt, %5, %c0 : index
      %25 = arith.select %24, %20, %23 : index
      %26 = arith.addi %dim_1, %c-1 : index
      %27 = arith.divsi %26, %c16 : index
      %28 = arith.addi %27, %c1 : index
      %29 = arith.subi %c0, %dim_1 : index
      %30 = arith.divsi %29, %c16 : index
      %31 = arith.subi %c0, %30 : index
      %32 = arith.cmpi sgt, %dim_1, %c0 : index
      %33 = arith.select %32, %28, %31 : index
      %34 = arith.muli %25, %33 : index
      %c0_2 = arith.constant 0 : index
      %c1_3 = arith.constant 1 : index
      %c1_4 = arith.constant 1 : index
      %35 = arith.muli %c1_4, %34 : index
      %36 = arith.muli %35, %c128 : index
      scf.parallel (%arg1) = (%c0_2) to (%36) step (%c1_3) {
        %37 = arith.remsi %arg1, %c128 : index
        %38 = arith.divsi %arg1, %c128 : index
        %39 = arith.divui %37, %c8 : index
        %40 = arith.remui %37, %c8 : index
        %41 = arith.divui %38, %25 : index
        %42 = arith.remui %38, %25 : index
        %43 = arith.muli %41, %c16 : index
        %44 = arith.addi %43, %39 : index
        %45 = arith.muli %42, %c8 : index
        %46 = arith.addi %45, %40 : index
        %alloc_5 = memref.alloc() : memref<128xf32, #gpu.address_space<workgroup>>
        %47 = arith.cmpi ult, %44, %dim_1 : index
        %48 = arith.cmpi ult, %46, %5 : index
        %49 = arith.andi %47, %48 : i1
        scf.if %49 {
          %64 = arith.muli %44, %5 : index
          %65 = arith.addi %64, %46 : index
          %66 = arith.muli %dim_1, %dim_0 : index
          %67 = arith.muli %66, %dim : index
          %reinterpret_cast_6 = memref.reinterpret_cast %1 to offset: [%c0], sizes: [%67], strides: [%c1] : memref<?x?x?xf32, "gpu"> to memref<?xf32, "gpu">
          %68 = memref.load %reinterpret_cast_6[%65] : memref<?xf32, "gpu">
          %69 = math.absf %68 : f32
          %reinterpret_cast_7 = memref.reinterpret_cast %alloc_5 to offset: [0], sizes: [128], strides: [1] : memref<128xf32, #gpu.address_space<workgroup>> to memref<128xf32, #gpu.address_space<workgroup>>
          memref.store %69, %reinterpret_cast_7[%37] : memref<128xf32, #gpu.address_space<workgroup>>
        } else {
          %reinterpret_cast_6 = memref.reinterpret_cast %alloc_5 to offset: [0], sizes: [128], strides: [1] : memref<128xf32, #gpu.address_space<workgroup>> to memref<128xf32, #gpu.address_space<workgroup>>
          memref.store %cst, %reinterpret_cast_6[%37] : memref<128xf32, #gpu.address_space<workgroup>>
        }
        gpu.barrier
        %50 = arith.cmpi ult, %39, %c8 : index
        %51 = arith.addi %44, %c8 : index
        %52 = arith.cmpi ult, %51, %dim_1 : index
        %53 = arith.andi %50, %52 : i1
        scf.if %53 {
          %64 = arith.addi %37, %c64 : index
          %reinterpret_cast_6 = memref.reinterpret_cast %alloc_5 to offset: [0], sizes: [128], strides: [1] : memref<128xf32, #gpu.address_space<workgroup>> to memref<128xf32, #gpu.address_space<workgroup>>
          %65 = memref.load %reinterpret_cast_6[%37] : memref<128xf32, #gpu.address_space<workgroup>>
          %66 = memref.load %reinterpret_cast_6[%64] : memref<128xf32, #gpu.address_space<workgroup>>
          %67 = arith.cmpf ugt, %65, %66 : f32
          %68 = arith.select %67, %65, %66 : f32
          %69 = arith.cmpf uno, %66, %66 : f32
          %70 = arith.select %69, %66, %68 : f32
          memref.store %70, %reinterpret_cast_6[%37] : memref<128xf32, #gpu.address_space<workgroup>>
        }
        gpu.barrier
        %54 = arith.cmpi ult, %39, %c4 : index
        %55 = arith.addi %44, %c4 : index
        %56 = arith.cmpi ult, %55, %dim_1 : index
        %57 = arith.andi %54, %56 : i1
        scf.if %57 {
          %64 = arith.addi %37, %c32 : index
          %reinterpret_cast_6 = memref.reinterpret_cast %alloc_5 to offset: [0], sizes: [128], strides: [1] : memref<128xf32, #gpu.address_space<workgroup>> to memref<128xf32, #gpu.address_space<workgroup>>
          %65 = memref.load %reinterpret_cast_6[%37] : memref<128xf32, #gpu.address_space<workgroup>>
          %66 = memref.load %reinterpret_cast_6[%64] : memref<128xf32, #gpu.address_space<workgroup>>
          %67 = arith.cmpf ugt, %65, %66 : f32
          %68 = arith.select %67, %65, %66 : f32
          %69 = arith.cmpf uno, %66, %66 : f32
          %70 = arith.select %69, %66, %68 : f32
          memref.store %70, %reinterpret_cast_6[%37] : memref<128xf32, #gpu.address_space<workgroup>>
        }
        gpu.barrier
        %58 = arith.cmpi ult, %39, %c2 : index
        %59 = arith.addi %44, %c2 : index
        %60 = arith.cmpi ult, %59, %dim_1 : index
        %61 = arith.andi %58, %60 : i1
        scf.if %61 {
          %64 = arith.addi %37, %c16 : index
          %reinterpret_cast_6 = memref.reinterpret_cast %alloc_5 to offset: [0], sizes: [128], strides: [1] : memref<128xf32, #gpu.address_space<workgroup>> to memref<128xf32, #gpu.address_space<workgroup>>
          %65 = memref.load %reinterpret_cast_6[%37] : memref<128xf32, #gpu.address_space<workgroup>>
          %66 = memref.load %reinterpret_cast_6[%64] : memref<128xf32, #gpu.address_space<workgroup>>
          %67 = arith.cmpf ugt, %65, %66 : f32
          %68 = arith.select %67, %65, %66 : f32
          %69 = arith.cmpf uno, %66, %66 : f32
          %70 = arith.select %69, %66, %68 : f32
          memref.store %70, %reinterpret_cast_6[%37] : memref<128xf32, #gpu.address_space<workgroup>>
        }
        gpu.barrier
        %62 = arith.cmpi eq, %39, %c0 : index
        %63 = arith.andi %62, %49 : i1
        scf.if %63 {
          %64 = arith.addi %37, %c8 : index
          %reinterpret_cast_6 = memref.reinterpret_cast %alloc_5 to offset: [0], sizes: [128], strides: [1] : memref<128xf32, #gpu.address_space<workgroup>> to memref<128xf32, #gpu.address_space<workgroup>>
          %65 = memref.load %reinterpret_cast_6[%37] : memref<128xf32, #gpu.address_space<workgroup>>
          %66 = memref.load %reinterpret_cast_6[%64] : memref<128xf32, #gpu.address_space<workgroup>>
          %67 = arith.cmpf ugt, %65, %66 : f32
          %68 = arith.select %67, %65, %66 : f32
          %69 = arith.cmpf uno, %66, %66 : f32
          %70 = arith.select %69, %66, %68 : f32
          %71 = memref.generic_atomic_rmw %alloc[%46] : memref<?xf32, "gpu"> {
          ^bb0(%arg2: f32):
            %72 = arith.cmpf ogt, %arg2, %70 : f32
            %73 = arith.select %72, %arg2, %70 : f32
            memref.atomic_yield %73 : f32
          }
        }
        scf.yield
      }
      "lmhlo.terminator"() : () -> ()
    }) {disc.device = "gpu", disc.fusion.name = "main_kColReduction_reduce__4_1_0", disc.fusion.tag = "8w16h", disc.fusion_type = "kColReduction", disc_col_reduction_schedule_hint = 2 : i32, disc_thread_per_block_hint = 128 : i32} : () -> ()
  }
  %16 = llvm.inttoptr %0 : i32 to !llvm.ptr<i8>
  %alloca = memref.alloca() : memref<2xindex, "cpu">
  memref.store %dim_0, %alloca[%c0] : memref<2xindex, "cpu">
  memref.store %dim, %alloca[%c1] : memref<2xindex, "cpu">
  %17 = "disc_ral.dispatch"(%arg0, %16, %alloc, %alloca) {backend_config = "", call_target_name = "inc_ref", device = "gpu", has_side_effect = false} : (!disc_ral.context, !llvm.ptr<i8>, memref<?xf32, "gpu">, memref<2xindex, "cpu">) -> memref<?x?xf32, "gpu">
  %reinterpret_cast = memref.reinterpret_cast %17 to offset: [0], sizes: [%dim_0, %dim], strides: [%dim, 1] {kDiscSymbolicDimAttr = [@S1, @S2]} : memref<?x?xf32, "gpu"> to memref<?x?xf32, "gpu">
  memref.dealloc %alloc : memref<?xf32, "gpu">
  "disc_ral.dispatch"(%arg0, %c0, %reinterpret_cast) {backend_config = "", call_target_name = "ral_send_output", device = "cpu", has_side_effect = false} : (!disc_ral.context, index, memref<?x?xf32, "gpu">) -> ()
  return
}

// -----// IR Dump After SCFParallelLoopTiling (disc-parallel-loop-tiling) //----- //
func.func @main(%arg0: !disc_ral.context) attributes {tf.entry_function = {input_placements = "gpu", inputs = "input0", output_placements = "gpu", outputs = "output0"}} {
  %c-1 = arith.constant -1 : index
  %cst = arith.constant 0xFF800000 : f32
  %c64 = arith.constant 64 : index
  %c128 = arith.constant 128 : index
  %c4 = arith.constant 4 : index
  %c16 = arith.constant 16 : index
  %c32 = arith.constant 32 : index
  %c8 = arith.constant 8 : index
  %0 = llvm.mlir.constant(0 : i32) : i32
  %c0 = arith.constant 0 : index
  %c1 = arith.constant 1 : index
  %c2 = arith.constant 2 : index
  %c256 = arith.constant 256 : index
  %c108 = arith.constant 108 : index
  %1 = "disc_ral.dispatch"(%arg0, %c0) {backend_config = "", call_target_name = "ral_recv_input", device = "cpu", has_side_effect = false} : (!disc_ral.context, index) -> memref<?x?x?xf32, "gpu">
  %dim = memref.dim %1, %c2 : memref<?x?x?xf32, "gpu">
  %dim_0 = memref.dim %1, %c1 : memref<?x?x?xf32, "gpu">
  %dim_1 = memref.dim %1, %c0 : memref<?x?x?xf32, "gpu">
  %2 = arith.index_cast %dim_0 : index to i32
  %3 = arith.index_cast %dim : index to i32
  %4 = arith.muli %2, %3 : i32
  %5 = arith.index_cast %4 : i32 to index
  %alloc = memref.alloc(%5) {kDiscSymbolicDimAttr = [@S3]} : memref<?xf32, "gpu">
  %6 = arith.muli %dim_1, %5 : index
  %7 = arith.addi %6, %c-1 : index
  %8 = arith.divsi %7, %c256 : index
  %9 = arith.addi %8, %c1 : index
  %10 = arith.subi %c0, %6 : index
  %11 = arith.divsi %10, %c256 : index
  %12 = arith.subi %c0, %11 : index
  %13 = arith.cmpi sgt, %6, %c0 : index
  %14 = arith.select %13, %9, %12 : index
  %15 = arith.cmpi sgt, %14, %c108 : index
  scf.if %15 {
    "lmhlo.fusion"() ({
      %c0_2 = arith.constant 0 : index
      %c256_3 = arith.constant 256 : index
      %18 = arith.muli %c1, %c256_3 : index
      scf.parallel (%arg1) = (%c0) to (%5) step (%18) {
        scf.parallel (%arg2) = (%c0_2) to (%18) step (%c1) {
          %39 = arith.addi %arg2, %arg1 : index
          %true = arith.constant true
          %40 = arith.muli %arg2, %c1 : index
          %41 = arith.addi %40, %arg1 : index
          %42 = arith.cmpi ult, %41, %5 : index
          %43 = arith.andi %true, %42 : i1
          scf.if %43 {
            %reinterpret_cast_9 = memref.reinterpret_cast %alloc to offset: [%c0], sizes: [%5], strides: [%c1] : memref<?xf32, "gpu"> to memref<?xf32, "gpu">
            memref.store %cst, %reinterpret_cast_9[%39] : memref<?xf32, "gpu">
          }
          scf.yield
        }
        scf.yield
      }
      %19 = arith.addi %5, %c-1 : index
      %20 = arith.divsi %19, %c8 : index
      %21 = arith.addi %20, %c1 : index
      %22 = arith.subi %c0, %5 : index
      %23 = arith.divsi %22, %c8 : index
      %24 = arith.subi %c0, %23 : index
      %25 = arith.cmpi sgt, %5, %c0 : index
      %26 = arith.select %25, %21, %24 : index
      %27 = arith.addi %dim_1, %c-1 : index
      %28 = arith.divsi %27, %c32 : index
      %29 = arith.addi %28, %c1 : index
      %30 = arith.subi %c0, %dim_1 : index
      %31 = arith.divsi %30, %c32 : index
      %32 = arith.subi %c0, %31 : index
      %33 = arith.cmpi sgt, %dim_1, %c0 : index
      %34 = arith.select %33, %29, %32 : index
      %35 = arith.muli %26, %34 : index
      %c0_4 = arith.constant 0 : index
      %c1_5 = arith.constant 1 : index
      %c1_6 = arith.constant 1 : index
      %36 = arith.muli %c1_6, %35 : index
      %37 = arith.muli %36, %c256 : index
      %c0_7 = arith.constant 0 : index
      %c256_8 = arith.constant 256 : index
      %38 = arith.muli %c1_5, %c256_8 : index
      scf.parallel (%arg1) = (%c0_4) to (%37) step (%38) {
        scf.parallel (%arg2) = (%c0_7) to (%38) step (%c1_5) {
          %39 = arith.addi %arg2, %arg1 : index
          %true = arith.constant true
          %40 = arith.muli %arg2, %c1_5 : index
          %41 = arith.addi %40, %arg1 : index
          %42 = arith.cmpi ult, %41, %37 : index
          %43 = arith.andi %true, %42 : i1
          scf.if %43 {
            %44 = arith.remsi %39, %c256 : index
            %45 = arith.divsi %39, %c256 : index
            %46 = arith.divui %44, %c8 : index
            %47 = arith.remui %44, %c8 : index
            %48 = arith.divui %45, %26 : index
            %49 = arith.remui %45, %26 : index
            %50 = arith.muli %48, %c32 : index
            %51 = arith.addi %50, %46 : index
            %52 = arith.muli %49, %c8 : index
            %53 = arith.addi %52, %47 : index
            %alloc_9 = memref.alloc() : memref<256xf32, #gpu.address_space<workgroup>>
            %54 = arith.cmpi ult, %51, %dim_1 : index
            %55 = arith.cmpi ult, %53, %5 : index
            %56 = arith.andi %54, %55 : i1
            scf.if %56 {
              %75 = arith.muli %51, %5 : index
              %76 = arith.addi %75, %53 : index
              %77 = arith.muli %dim_1, %dim_0 : index
              %78 = arith.muli %77, %dim : index
              %reinterpret_cast_10 = memref.reinterpret_cast %1 to offset: [%c0], sizes: [%78], strides: [%c1] : memref<?x?x?xf32, "gpu"> to memref<?xf32, "gpu">
              %79 = memref.load %reinterpret_cast_10[%76] : memref<?xf32, "gpu">
              %80 = math.absf %79 : f32
              %reinterpret_cast_11 = memref.reinterpret_cast %alloc_9 to offset: [0], sizes: [256], strides: [1] : memref<256xf32, #gpu.address_space<workgroup>> to memref<256xf32, #gpu.address_space<workgroup>>
              memref.store %80, %reinterpret_cast_11[%44] : memref<256xf32, #gpu.address_space<workgroup>>
            } else {
              %reinterpret_cast_10 = memref.reinterpret_cast %alloc_9 to offset: [0], sizes: [256], strides: [1] : memref<256xf32, #gpu.address_space<workgroup>> to memref<256xf32, #gpu.address_space<workgroup>>
              memref.store %cst, %reinterpret_cast_10[%44] : memref<256xf32, #gpu.address_space<workgroup>>
            }
            gpu.barrier
            %57 = arith.cmpi ult, %46, %c16 : index
            %58 = arith.addi %51, %c16 : index
            %59 = arith.cmpi ult, %58, %dim_1 : index
            %60 = arith.andi %57, %59 : i1
            scf.if %60 {
              %75 = arith.addi %44, %c128 : index
              %reinterpret_cast_10 = memref.reinterpret_cast %alloc_9 to offset: [0], sizes: [256], strides: [1] : memref<256xf32, #gpu.address_space<workgroup>> to memref<256xf32, #gpu.address_space<workgroup>>
              %76 = memref.load %reinterpret_cast_10[%44] : memref<256xf32, #gpu.address_space<workgroup>>
              %77 = memref.load %reinterpret_cast_10[%75] : memref<256xf32, #gpu.address_space<workgroup>>
              %78 = arith.cmpf ugt, %76, %77 : f32
              %79 = arith.select %78, %76, %77 : f32
              %80 = arith.cmpf uno, %77, %77 : f32
              %81 = arith.select %80, %77, %79 : f32
              memref.store %81, %reinterpret_cast_10[%44] : memref<256xf32, #gpu.address_space<workgroup>>
            }
            gpu.barrier
            %61 = arith.cmpi ult, %46, %c8 : index
            %62 = arith.addi %51, %c8 : index
            %63 = arith.cmpi ult, %62, %dim_1 : index
            %64 = arith.andi %61, %63 : i1
            scf.if %64 {
              %75 = arith.addi %44, %c64 : index
              %reinterpret_cast_10 = memref.reinterpret_cast %alloc_9 to offset: [0], sizes: [256], strides: [1] : memref<256xf32, #gpu.address_space<workgroup>> to memref<256xf32, #gpu.address_space<workgroup>>
              %76 = memref.load %reinterpret_cast_10[%44] : memref<256xf32, #gpu.address_space<workgroup>>
              %77 = memref.load %reinterpret_cast_10[%75] : memref<256xf32, #gpu.address_space<workgroup>>
              %78 = arith.cmpf ugt, %76, %77 : f32
              %79 = arith.select %78, %76, %77 : f32
              %80 = arith.cmpf uno, %77, %77 : f32
              %81 = arith.select %80, %77, %79 : f32
              memref.store %81, %reinterpret_cast_10[%44] : memref<256xf32, #gpu.address_space<workgroup>>
            }
            gpu.barrier
            %65 = arith.cmpi ult, %46, %c4 : index
            %66 = arith.addi %51, %c4 : index
            %67 = arith.cmpi ult, %66, %dim_1 : index
            %68 = arith.andi %65, %67 : i1
            scf.if %68 {
              %75 = arith.addi %44, %c32 : index
              %reinterpret_cast_10 = memref.reinterpret_cast %alloc_9 to offset: [0], sizes: [256], strides: [1] : memref<256xf32, #gpu.address_space<workgroup>> to memref<256xf32, #gpu.address_space<workgroup>>
              %76 = memref.load %reinterpret_cast_10[%44] : memref<256xf32, #gpu.address_space<workgroup>>
              %77 = memref.load %reinterpret_cast_10[%75] : memref<256xf32, #gpu.address_space<workgroup>>
              %78 = arith.cmpf ugt, %76, %77 : f32
              %79 = arith.select %78, %76, %77 : f32
              %80 = arith.cmpf uno, %77, %77 : f32
              %81 = arith.select %80, %77, %79 : f32
              memref.store %81, %reinterpret_cast_10[%44] : memref<256xf32, #gpu.address_space<workgroup>>
            }
            gpu.barrier
            %69 = arith.cmpi ult, %46, %c2 : index
            %70 = arith.addi %51, %c2 : index
            %71 = arith.cmpi ult, %70, %dim_1 : index
            %72 = arith.andi %69, %71 : i1
            scf.if %72 {
              %75 = arith.addi %44, %c16 : index
              %reinterpret_cast_10 = memref.reinterpret_cast %alloc_9 to offset: [0], sizes: [256], strides: [1] : memref<256xf32, #gpu.address_space<workgroup>> to memref<256xf32, #gpu.address_space<workgroup>>
              %76 = memref.load %reinterpret_cast_10[%44] : memref<256xf32, #gpu.address_space<workgroup>>
              %77 = memref.load %reinterpret_cast_10[%75] : memref<256xf32, #gpu.address_space<workgroup>>
              %78 = arith.cmpf ugt, %76, %77 : f32
              %79 = arith.select %78, %76, %77 : f32
              %80 = arith.cmpf uno, %77, %77 : f32
              %81 = arith.select %80, %77, %79 : f32
              memref.store %81, %reinterpret_cast_10[%44] : memref<256xf32, #gpu.address_space<workgroup>>
            }
            gpu.barrier
            %73 = arith.cmpi eq, %46, %c0 : index
            %74 = arith.andi %73, %56 : i1
            scf.if %74 {
              %75 = arith.addi %44, %c8 : index
              %reinterpret_cast_10 = memref.reinterpret_cast %alloc_9 to offset: [0], sizes: [256], strides: [1] : memref<256xf32, #gpu.address_space<workgroup>> to memref<256xf32, #gpu.address_space<workgroup>>
              %76 = memref.load %reinterpret_cast_10[%44] : memref<256xf32, #gpu.address_space<workgroup>>
              %77 = memref.load %reinterpret_cast_10[%75] : memref<256xf32, #gpu.address_space<workgroup>>
              %78 = arith.cmpf ugt, %76, %77 : f32
              %79 = arith.select %78, %76, %77 : f32
              %80 = arith.cmpf uno, %77, %77 : f32
              %81 = arith.select %80, %77, %79 : f32
              %82 = memref.generic_atomic_rmw %alloc[%53] : memref<?xf32, "gpu"> {
              ^bb0(%arg3: f32):
                %83 = arith.cmpf ogt, %arg3, %81 : f32
                %84 = arith.select %83, %arg3, %81 : f32
                memref.atomic_yield %84 : f32
              }
            }
          }
          scf.yield
        }
        scf.yield
      }
      "lmhlo.terminator"() : () -> ()
    }) {disc.device = "gpu", disc.fusion.name = "main_kColReduction_reduce__4_1_0", disc.fusion.tag = "8w32h", disc.fusion_type = "kColReduction", disc_col_reduction_schedule_hint = 1 : i32, disc_thread_per_block_hint = 256 : i32} : () -> ()
  } else {
    "lmhlo.fusion"() ({
      %c0_2 = arith.constant 0 : index
      %c128_3 = arith.constant 128 : index
      %18 = arith.muli %c1, %c128_3 : index
      scf.parallel (%arg1) = (%c0) to (%5) step (%18) {
        scf.parallel (%arg2) = (%c0_2) to (%18) step (%c1) {
          %39 = arith.addi %arg2, %arg1 : index
          %true = arith.constant true
          %40 = arith.muli %arg2, %c1 : index
          %41 = arith.addi %40, %arg1 : index
          %42 = arith.cmpi ult, %41, %5 : index
          %43 = arith.andi %true, %42 : i1
          scf.if %43 {
            %reinterpret_cast_9 = memref.reinterpret_cast %alloc to offset: [%c0], sizes: [%5], strides: [%c1] : memref<?xf32, "gpu"> to memref<?xf32, "gpu">
            memref.store %cst, %reinterpret_cast_9[%39] : memref<?xf32, "gpu">
          }
          scf.yield
        }
        scf.yield
      }
      %19 = arith.addi %5, %c-1 : index
      %20 = arith.divsi %19, %c8 : index
      %21 = arith.addi %20, %c1 : index
      %22 = arith.subi %c0, %5 : index
      %23 = arith.divsi %22, %c8 : index
      %24 = arith.subi %c0, %23 : index
      %25 = arith.cmpi sgt, %5, %c0 : index
      %26 = arith.select %25, %21, %24 : index
      %27 = arith.addi %dim_1, %c-1 : index
      %28 = arith.divsi %27, %c16 : index
      %29 = arith.addi %28, %c1 : index
      %30 = arith.subi %c0, %dim_1 : index
      %31 = arith.divsi %30, %c16 : index
      %32 = arith.subi %c0, %31 : index
      %33 = arith.cmpi sgt, %dim_1, %c0 : index
      %34 = arith.select %33, %29, %32 : index
      %35 = arith.muli %26, %34 : index
      %c0_4 = arith.constant 0 : index
      %c1_5 = arith.constant 1 : index
      %c1_6 = arith.constant 1 : index
      %36 = arith.muli %c1_6, %35 : index
      %37 = arith.muli %36, %c128 : index
      %c0_7 = arith.constant 0 : index
      %c128_8 = arith.constant 128 : index
      %38 = arith.muli %c1_5, %c128_8 : index
      scf.parallel (%arg1) = (%c0_4) to (%37) step (%38) {
        scf.parallel (%arg2) = (%c0_7) to (%38) step (%c1_5) {
          %39 = arith.addi %arg2, %arg1 : index
          %true = arith.constant true
          %40 = arith.muli %arg2, %c1_5 : index
          %41 = arith.addi %40, %arg1 : index
          %42 = arith.cmpi ult, %41, %37 : index
          %43 = arith.andi %true, %42 : i1
          scf.if %43 {
            %44 = arith.remsi %39, %c128 : index
            %45 = arith.divsi %39, %c128 : index
            %46 = arith.divui %44, %c8 : index
            %47 = arith.remui %44, %c8 : index
            %48 = arith.divui %45, %26 : index
            %49 = arith.remui %45, %26 : index
            %50 = arith.muli %48, %c16 : index
            %51 = arith.addi %50, %46 : index
            %52 = arith.muli %49, %c8 : index
            %53 = arith.addi %52, %47 : index
            %alloc_9 = memref.alloc() : memref<128xf32, #gpu.address_space<workgroup>>
            %54 = arith.cmpi ult, %51, %dim_1 : index
            %55 = arith.cmpi ult, %53, %5 : index
            %56 = arith.andi %54, %55 : i1
            scf.if %56 {
              %71 = arith.muli %51, %5 : index
              %72 = arith.addi %71, %53 : index
              %73 = arith.muli %dim_1, %dim_0 : index
              %74 = arith.muli %73, %dim : index
              %reinterpret_cast_10 = memref.reinterpret_cast %1 to offset: [%c0], sizes: [%74], strides: [%c1] : memref<?x?x?xf32, "gpu"> to memref<?xf32, "gpu">
              %75 = memref.load %reinterpret_cast_10[%72] : memref<?xf32, "gpu">
              %76 = math.absf %75 : f32
              %reinterpret_cast_11 = memref.reinterpret_cast %alloc_9 to offset: [0], sizes: [128], strides: [1] : memref<128xf32, #gpu.address_space<workgroup>> to memref<128xf32, #gpu.address_space<workgroup>>
              memref.store %76, %reinterpret_cast_11[%44] : memref<128xf32, #gpu.address_space<workgroup>>
            } else {
              %reinterpret_cast_10 = memref.reinterpret_cast %alloc_9 to offset: [0], sizes: [128], strides: [1] : memref<128xf32, #gpu.address_space<workgroup>> to memref<128xf32, #gpu.address_space<workgroup>>
              memref.store %cst, %reinterpret_cast_10[%44] : memref<128xf32, #gpu.address_space<workgroup>>
            }
            gpu.barrier
            %57 = arith.cmpi ult, %46, %c8 : index
            %58 = arith.addi %51, %c8 : index
            %59 = arith.cmpi ult, %58, %dim_1 : index
            %60 = arith.andi %57, %59 : i1
            scf.if %60 {
              %71 = arith.addi %44, %c64 : index
              %reinterpret_cast_10 = memref.reinterpret_cast %alloc_9 to offset: [0], sizes: [128], strides: [1] : memref<128xf32, #gpu.address_space<workgroup>> to memref<128xf32, #gpu.address_space<workgroup>>
              %72 = memref.load %reinterpret_cast_10[%44] : memref<128xf32, #gpu.address_space<workgroup>>
              %73 = memref.load %reinterpret_cast_10[%71] : memref<128xf32, #gpu.address_space<workgroup>>
              %74 = arith.cmpf ugt, %72, %73 : f32
              %75 = arith.select %74, %72, %73 : f32
              %76 = arith.cmpf uno, %73, %73 : f32
              %77 = arith.select %76, %73, %75 : f32
              memref.store %77, %reinterpret_cast_10[%44] : memref<128xf32, #gpu.address_space<workgroup>>
            }
            gpu.barrier
            %61 = arith.cmpi ult, %46, %c4 : index
            %62 = arith.addi %51, %c4 : index
            %63 = arith.cmpi ult, %62, %dim_1 : index
            %64 = arith.andi %61, %63 : i1
            scf.if %64 {
              %71 = arith.addi %44, %c32 : index
              %reinterpret_cast_10 = memref.reinterpret_cast %alloc_9 to offset: [0], sizes: [128], strides: [1] : memref<128xf32, #gpu.address_space<workgroup>> to memref<128xf32, #gpu.address_space<workgroup>>
              %72 = memref.load %reinterpret_cast_10[%44] : memref<128xf32, #gpu.address_space<workgroup>>
              %73 = memref.load %reinterpret_cast_10[%71] : memref<128xf32, #gpu.address_space<workgroup>>
              %74 = arith.cmpf ugt, %72, %73 : f32
              %75 = arith.select %74, %72, %73 : f32
              %76 = arith.cmpf uno, %73, %73 : f32
              %77 = arith.select %76, %73, %75 : f32
              memref.store %77, %reinterpret_cast_10[%44] : memref<128xf32, #gpu.address_space<workgroup>>
            }
            gpu.barrier
            %65 = arith.cmpi ult, %46, %c2 : index
            %66 = arith.addi %51, %c2 : index
            %67 = arith.cmpi ult, %66, %dim_1 : index
            %68 = arith.andi %65, %67 : i1
            scf.if %68 {
              %71 = arith.addi %44, %c16 : index
              %reinterpret_cast_10 = memref.reinterpret_cast %alloc_9 to offset: [0], sizes: [128], strides: [1] : memref<128xf32, #gpu.address_space<workgroup>> to memref<128xf32, #gpu.address_space<workgroup>>
              %72 = memref.load %reinterpret_cast_10[%44] : memref<128xf32, #gpu.address_space<workgroup>>
              %73 = memref.load %reinterpret_cast_10[%71] : memref<128xf32, #gpu.address_space<workgroup>>
              %74 = arith.cmpf ugt, %72, %73 : f32
              %75 = arith.select %74, %72, %73 : f32
              %76 = arith.cmpf uno, %73, %73 : f32
              %77 = arith.select %76, %73, %75 : f32
              memref.store %77, %reinterpret_cast_10[%44] : memref<128xf32, #gpu.address_space<workgroup>>
            }
            gpu.barrier
            %69 = arith.cmpi eq, %46, %c0 : index
            %70 = arith.andi %69, %56 : i1
            scf.if %70 {
              %71 = arith.addi %44, %c8 : index
              %reinterpret_cast_10 = memref.reinterpret_cast %alloc_9 to offset: [0], sizes: [128], strides: [1] : memref<128xf32, #gpu.address_space<workgroup>> to memref<128xf32, #gpu.address_space<workgroup>>
              %72 = memref.load %reinterpret_cast_10[%44] : memref<128xf32, #gpu.address_space<workgroup>>
              %73 = memref.load %reinterpret_cast_10[%71] : memref<128xf32, #gpu.address_space<workgroup>>
              %74 = arith.cmpf ugt, %72, %73 : f32
              %75 = arith.select %74, %72, %73 : f32
              %76 = arith.cmpf uno, %73, %73 : f32
              %77 = arith.select %76, %73, %75 : f32
              %78 = memref.generic_atomic_rmw %alloc[%53] : memref<?xf32, "gpu"> {
              ^bb0(%arg3: f32):
                %79 = arith.cmpf ogt, %arg3, %77 : f32
                %80 = arith.select %79, %arg3, %77 : f32
                memref.atomic_yield %80 : f32
              }
            }
          }
          scf.yield
        }
        scf.yield
      }
      "lmhlo.terminator"() : () -> ()
    }) {disc.device = "gpu", disc.fusion.name = "main_kColReduction_reduce__4_1_0", disc.fusion.tag = "8w16h", disc.fusion_type = "kColReduction", disc_col_reduction_schedule_hint = 2 : i32, disc_thread_per_block_hint = 128 : i32} : () -> ()
  }
  %16 = llvm.inttoptr %0 : i32 to !llvm.ptr<i8>
  %alloca = memref.alloca() : memref<2xindex, "cpu">
  memref.store %dim_0, %alloca[%c0] : memref<2xindex, "cpu">
  memref.store %dim, %alloca[%c1] : memref<2xindex, "cpu">
  %17 = "disc_ral.dispatch"(%arg0, %16, %alloc, %alloca) {backend_config = "", call_target_name = "inc_ref", device = "gpu", has_side_effect = false} : (!disc_ral.context, !llvm.ptr<i8>, memref<?xf32, "gpu">, memref<2xindex, "cpu">) -> memref<?x?xf32, "gpu">
  %reinterpret_cast = memref.reinterpret_cast %17 to offset: [0], sizes: [%dim_0, %dim], strides: [%dim, 1] {kDiscSymbolicDimAttr = [@S1, @S2]} : memref<?x?xf32, "gpu"> to memref<?x?xf32, "gpu">
  memref.dealloc %alloc : memref<?xf32, "gpu">
  "disc_ral.dispatch"(%arg0, %c0, %reinterpret_cast) {backend_config = "", call_target_name = "ral_send_output", device = "cpu", has_side_effect = false} : (!disc_ral.context, index, memref<?x?xf32, "gpu">) -> ()
  return
}

// -----// IR Dump After GpuMapParallelLoopsPass (gpu-map-parallel-loops) //----- //
func.func @main(%arg0: !disc_ral.context) attributes {tf.entry_function = {input_placements = "gpu", inputs = "input0", output_placements = "gpu", outputs = "output0"}} {
  %c-1 = arith.constant -1 : index
  %cst = arith.constant 0xFF800000 : f32
  %c64 = arith.constant 64 : index
  %c128 = arith.constant 128 : index
  %c4 = arith.constant 4 : index
  %c16 = arith.constant 16 : index
  %c32 = arith.constant 32 : index
  %c8 = arith.constant 8 : index
  %0 = llvm.mlir.constant(0 : i32) : i32
  %c0 = arith.constant 0 : index
  %c1 = arith.constant 1 : index
  %c2 = arith.constant 2 : index
  %c256 = arith.constant 256 : index
  %c108 = arith.constant 108 : index
  %1 = "disc_ral.dispatch"(%arg0, %c0) {backend_config = "", call_target_name = "ral_recv_input", device = "cpu", has_side_effect = false} : (!disc_ral.context, index) -> memref<?x?x?xf32, "gpu">
  %dim = memref.dim %1, %c2 : memref<?x?x?xf32, "gpu">
  %dim_0 = memref.dim %1, %c1 : memref<?x?x?xf32, "gpu">
  %dim_1 = memref.dim %1, %c0 : memref<?x?x?xf32, "gpu">
  %2 = arith.index_cast %dim_0 : index to i32
  %3 = arith.index_cast %dim : index to i32
  %4 = arith.muli %2, %3 : i32
  %5 = arith.index_cast %4 : i32 to index
  %alloc = memref.alloc(%5) {kDiscSymbolicDimAttr = [@S3]} : memref<?xf32, "gpu">
  %6 = arith.muli %dim_1, %5 : index
  %7 = arith.addi %6, %c-1 : index
  %8 = arith.divsi %7, %c256 : index
  %9 = arith.addi %8, %c1 : index
  %10 = arith.subi %c0, %6 : index
  %11 = arith.divsi %10, %c256 : index
  %12 = arith.subi %c0, %11 : index
  %13 = arith.cmpi sgt, %6, %c0 : index
  %14 = arith.select %13, %9, %12 : index
  %15 = arith.cmpi sgt, %14, %c108 : index
  scf.if %15 {
    "lmhlo.fusion"() ({
      %c0_2 = arith.constant 0 : index
      %c256_3 = arith.constant 256 : index
      %18 = arith.muli %c1, %c256_3 : index
      scf.parallel (%arg1) = (%c0) to (%5) step (%18) {
        scf.parallel (%arg2) = (%c0_2) to (%18) step (%c1) {
          %39 = arith.addi %arg2, %arg1 : index
          %true = arith.constant true
          %40 = arith.muli %arg2, %c1 : index
          %41 = arith.addi %40, %arg1 : index
          %42 = arith.cmpi ult, %41, %5 : index
          %43 = arith.andi %true, %42 : i1
          scf.if %43 {
            %reinterpret_cast_9 = memref.reinterpret_cast %alloc to offset: [%c0], sizes: [%5], strides: [%c1] : memref<?xf32, "gpu"> to memref<?xf32, "gpu">
            memref.store %cst, %reinterpret_cast_9[%39] : memref<?xf32, "gpu">
          }
          scf.yield
        } {mapping = [#gpu.loop_dim_map<processor = thread_x, map = (d0) -> (d0), bound = (d0) -> (d0)>]}
        scf.yield
      } {mapping = [#gpu.loop_dim_map<processor = block_x, map = (d0) -> (d0), bound = (d0) -> (d0)>]}
      %19 = arith.addi %5, %c-1 : index
      %20 = arith.divsi %19, %c8 : index
      %21 = arith.addi %20, %c1 : index
      %22 = arith.subi %c0, %5 : index
      %23 = arith.divsi %22, %c8 : index
      %24 = arith.subi %c0, %23 : index
      %25 = arith.cmpi sgt, %5, %c0 : index
      %26 = arith.select %25, %21, %24 : index
      %27 = arith.addi %dim_1, %c-1 : index
      %28 = arith.divsi %27, %c32 : index
      %29 = arith.addi %28, %c1 : index
      %30 = arith.subi %c0, %dim_1 : index
      %31 = arith.divsi %30, %c32 : index
      %32 = arith.subi %c0, %31 : index
      %33 = arith.cmpi sgt, %dim_1, %c0 : index
      %34 = arith.select %33, %29, %32 : index
      %35 = arith.muli %26, %34 : index
      %c0_4 = arith.constant 0 : index
      %c1_5 = arith.constant 1 : index
      %c1_6 = arith.constant 1 : index
      %36 = arith.muli %c1_6, %35 : index
      %37 = arith.muli %36, %c256 : index
      %c0_7 = arith.constant 0 : index
      %c256_8 = arith.constant 256 : index
      %38 = arith.muli %c1_5, %c256_8 : index
      scf.parallel (%arg1) = (%c0_4) to (%37) step (%38) {
        scf.parallel (%arg2) = (%c0_7) to (%38) step (%c1_5) {
          %39 = arith.addi %arg2, %arg1 : index
          %true = arith.constant true
          %40 = arith.muli %arg2, %c1_5 : index
          %41 = arith.addi %40, %arg1 : index
          %42 = arith.cmpi ult, %41, %37 : index
          %43 = arith.andi %true, %42 : i1
          scf.if %43 {
            %44 = arith.remsi %39, %c256 : index
            %45 = arith.divsi %39, %c256 : index
            %46 = arith.divui %44, %c8 : index
            %47 = arith.remui %44, %c8 : index
            %48 = arith.divui %45, %26 : index
            %49 = arith.remui %45, %26 : index
            %50 = arith.muli %48, %c32 : index
            %51 = arith.addi %50, %46 : index
            %52 = arith.muli %49, %c8 : index
            %53 = arith.addi %52, %47 : index
            %alloc_9 = memref.alloc() : memref<256xf32, #gpu.address_space<workgroup>>
            %54 = arith.cmpi ult, %51, %dim_1 : index
            %55 = arith.cmpi ult, %53, %5 : index
            %56 = arith.andi %54, %55 : i1
            scf.if %56 {
              %75 = arith.muli %51, %5 : index
              %76 = arith.addi %75, %53 : index
              %77 = arith.muli %dim_1, %dim_0 : index
              %78 = arith.muli %77, %dim : index
              %reinterpret_cast_10 = memref.reinterpret_cast %1 to offset: [%c0], sizes: [%78], strides: [%c1] : memref<?x?x?xf32, "gpu"> to memref<?xf32, "gpu">
              %79 = memref.load %reinterpret_cast_10[%76] : memref<?xf32, "gpu">
              %80 = math.absf %79 : f32
              %reinterpret_cast_11 = memref.reinterpret_cast %alloc_9 to offset: [0], sizes: [256], strides: [1] : memref<256xf32, #gpu.address_space<workgroup>> to memref<256xf32, #gpu.address_space<workgroup>>
              memref.store %80, %reinterpret_cast_11[%44] : memref<256xf32, #gpu.address_space<workgroup>>
            } else {
              %reinterpret_cast_10 = memref.reinterpret_cast %alloc_9 to offset: [0], sizes: [256], strides: [1] : memref<256xf32, #gpu.address_space<workgroup>> to memref<256xf32, #gpu.address_space<workgroup>>
              memref.store %cst, %reinterpret_cast_10[%44] : memref<256xf32, #gpu.address_space<workgroup>>
            }
            gpu.barrier
            %57 = arith.cmpi ult, %46, %c16 : index
            %58 = arith.addi %51, %c16 : index
            %59 = arith.cmpi ult, %58, %dim_1 : index
            %60 = arith.andi %57, %59 : i1
            scf.if %60 {
              %75 = arith.addi %44, %c128 : index
              %reinterpret_cast_10 = memref.reinterpret_cast %alloc_9 to offset: [0], sizes: [256], strides: [1] : memref<256xf32, #gpu.address_space<workgroup>> to memref<256xf32, #gpu.address_space<workgroup>>
              %76 = memref.load %reinterpret_cast_10[%44] : memref<256xf32, #gpu.address_space<workgroup>>
              %77 = memref.load %reinterpret_cast_10[%75] : memref<256xf32, #gpu.address_space<workgroup>>
              %78 = arith.cmpf ugt, %76, %77 : f32
              %79 = arith.select %78, %76, %77 : f32
              %80 = arith.cmpf uno, %77, %77 : f32
              %81 = arith.select %80, %77, %79 : f32
              memref.store %81, %reinterpret_cast_10[%44] : memref<256xf32, #gpu.address_space<workgroup>>
            }
            gpu.barrier
            %61 = arith.cmpi ult, %46, %c8 : index
            %62 = arith.addi %51, %c8 : index
            %63 = arith.cmpi ult, %62, %dim_1 : index
            %64 = arith.andi %61, %63 : i1
            scf.if %64 {
              %75 = arith.addi %44, %c64 : index
              %reinterpret_cast_10 = memref.reinterpret_cast %alloc_9 to offset: [0], sizes: [256], strides: [1] : memref<256xf32, #gpu.address_space<workgroup>> to memref<256xf32, #gpu.address_space<workgroup>>
              %76 = memref.load %reinterpret_cast_10[%44] : memref<256xf32, #gpu.address_space<workgroup>>
              %77 = memref.load %reinterpret_cast_10[%75] : memref<256xf32, #gpu.address_space<workgroup>>
              %78 = arith.cmpf ugt, %76, %77 : f32
              %79 = arith.select %78, %76, %77 : f32
              %80 = arith.cmpf uno, %77, %77 : f32
              %81 = arith.select %80, %77, %79 : f32
              memref.store %81, %reinterpret_cast_10[%44] : memref<256xf32, #gpu.address_space<workgroup>>
            }
            gpu.barrier
            %65 = arith.cmpi ult, %46, %c4 : index
            %66 = arith.addi %51, %c4 : index
            %67 = arith.cmpi ult, %66, %dim_1 : index
            %68 = arith.andi %65, %67 : i1
            scf.if %68 {
              %75 = arith.addi %44, %c32 : index
              %reinterpret_cast_10 = memref.reinterpret_cast %alloc_9 to offset: [0], sizes: [256], strides: [1] : memref<256xf32, #gpu.address_space<workgroup>> to memref<256xf32, #gpu.address_space<workgroup>>
              %76 = memref.load %reinterpret_cast_10[%44] : memref<256xf32, #gpu.address_space<workgroup>>
              %77 = memref.load %reinterpret_cast_10[%75] : memref<256xf32, #gpu.address_space<workgroup>>
              %78 = arith.cmpf ugt, %76, %77 : f32
              %79 = arith.select %78, %76, %77 : f32
              %80 = arith.cmpf uno, %77, %77 : f32
              %81 = arith.select %80, %77, %79 : f32
              memref.store %81, %reinterpret_cast_10[%44] : memref<256xf32, #gpu.address_space<workgroup>>
            }
            gpu.barrier
            %69 = arith.cmpi ult, %46, %c2 : index
            %70 = arith.addi %51, %c2 : index
            %71 = arith.cmpi ult, %70, %dim_1 : index
            %72 = arith.andi %69, %71 : i1
            scf.if %72 {
              %75 = arith.addi %44, %c16 : index
              %reinterpret_cast_10 = memref.reinterpret_cast %alloc_9 to offset: [0], sizes: [256], strides: [1] : memref<256xf32, #gpu.address_space<workgroup>> to memref<256xf32, #gpu.address_space<workgroup>>
              %76 = memref.load %reinterpret_cast_10[%44] : memref<256xf32, #gpu.address_space<workgroup>>
              %77 = memref.load %reinterpret_cast_10[%75] : memref<256xf32, #gpu.address_space<workgroup>>
              %78 = arith.cmpf ugt, %76, %77 : f32
              %79 = arith.select %78, %76, %77 : f32
              %80 = arith.cmpf uno, %77, %77 : f32
              %81 = arith.select %80, %77, %79 : f32
              memref.store %81, %reinterpret_cast_10[%44] : memref<256xf32, #gpu.address_space<workgroup>>
            }
            gpu.barrier
            %73 = arith.cmpi eq, %46, %c0 : index
            %74 = arith.andi %73, %56 : i1
            scf.if %74 {
              %75 = arith.addi %44, %c8 : index
              %reinterpret_cast_10 = memref.reinterpret_cast %alloc_9 to offset: [0], sizes: [256], strides: [1] : memref<256xf32, #gpu.address_space<workgroup>> to memref<256xf32, #gpu.address_space<workgroup>>
              %76 = memref.load %reinterpret_cast_10[%44] : memref<256xf32, #gpu.address_space<workgroup>>
              %77 = memref.load %reinterpret_cast_10[%75] : memref<256xf32, #gpu.address_space<workgroup>>
              %78 = arith.cmpf ugt, %76, %77 : f32
              %79 = arith.select %78, %76, %77 : f32
              %80 = arith.cmpf uno, %77, %77 : f32
              %81 = arith.select %80, %77, %79 : f32
              %82 = memref.generic_atomic_rmw %alloc[%53] : memref<?xf32, "gpu"> {
              ^bb0(%arg3: f32):
                %83 = arith.cmpf ogt, %arg3, %81 : f32
                %84 = arith.select %83, %arg3, %81 : f32
                memref.atomic_yield %84 : f32
              }
            }
          }
          scf.yield
        } {mapping = [#gpu.loop_dim_map<processor = thread_x, map = (d0) -> (d0), bound = (d0) -> (d0)>]}
        scf.yield
      } {mapping = [#gpu.loop_dim_map<processor = block_x, map = (d0) -> (d0), bound = (d0) -> (d0)>]}
      "lmhlo.terminator"() : () -> ()
    }) {disc.device = "gpu", disc.fusion.name = "main_kColReduction_reduce__4_1_0", disc.fusion.tag = "8w32h", disc.fusion_type = "kColReduction", disc_col_reduction_schedule_hint = 1 : i32, disc_thread_per_block_hint = 256 : i32} : () -> ()
  } else {
    "lmhlo.fusion"() ({
      %c0_2 = arith.constant 0 : index
      %c128_3 = arith.constant 128 : index
      %18 = arith.muli %c1, %c128_3 : index
      scf.parallel (%arg1) = (%c0) to (%5) step (%18) {
        scf.parallel (%arg2) = (%c0_2) to (%18) step (%c1) {
          %39 = arith.addi %arg2, %arg1 : index
          %true = arith.constant true
          %40 = arith.muli %arg2, %c1 : index
          %41 = arith.addi %40, %arg1 : index
          %42 = arith.cmpi ult, %41, %5 : index
          %43 = arith.andi %true, %42 : i1
          scf.if %43 {
            %reinterpret_cast_9 = memref.reinterpret_cast %alloc to offset: [%c0], sizes: [%5], strides: [%c1] : memref<?xf32, "gpu"> to memref<?xf32, "gpu">
            memref.store %cst, %reinterpret_cast_9[%39] : memref<?xf32, "gpu">
          }
          scf.yield
        } {mapping = [#gpu.loop_dim_map<processor = thread_x, map = (d0) -> (d0), bound = (d0) -> (d0)>]}
        scf.yield
      } {mapping = [#gpu.loop_dim_map<processor = block_x, map = (d0) -> (d0), bound = (d0) -> (d0)>]}
      %19 = arith.addi %5, %c-1 : index
      %20 = arith.divsi %19, %c8 : index
      %21 = arith.addi %20, %c1 : index
      %22 = arith.subi %c0, %5 : index
      %23 = arith.divsi %22, %c8 : index
      %24 = arith.subi %c0, %23 : index
      %25 = arith.cmpi sgt, %5, %c0 : index
      %26 = arith.select %25, %21, %24 : index
      %27 = arith.addi %dim_1, %c-1 : index
      %28 = arith.divsi %27, %c16 : index
      %29 = arith.addi %28, %c1 : index
      %30 = arith.subi %c0, %dim_1 : index
      %31 = arith.divsi %30, %c16 : index
      %32 = arith.subi %c0, %31 : index
      %33 = arith.cmpi sgt, %dim_1, %c0 : index
      %34 = arith.select %33, %29, %32 : index
      %35 = arith.muli %26, %34 : index
      %c0_4 = arith.constant 0 : index
      %c1_5 = arith.constant 1 : index
      %c1_6 = arith.constant 1 : index
      %36 = arith.muli %c1_6, %35 : index
      %37 = arith.muli %36, %c128 : index
      %c0_7 = arith.constant 0 : index
      %c128_8 = arith.constant 128 : index
      %38 = arith.muli %c1_5, %c128_8 : index
      scf.parallel (%arg1) = (%c0_4) to (%37) step (%38) {
        scf.parallel (%arg2) = (%c0_7) to (%38) step (%c1_5) {
          %39 = arith.addi %arg2, %arg1 : index
          %true = arith.constant true
          %40 = arith.muli %arg2, %c1_5 : index
          %41 = arith.addi %40, %arg1 : index
          %42 = arith.cmpi ult, %41, %37 : index
          %43 = arith.andi %true, %42 : i1
          scf.if %43 {
            %44 = arith.remsi %39, %c128 : index
            %45 = arith.divsi %39, %c128 : index
            %46 = arith.divui %44, %c8 : index
            %47 = arith.remui %44, %c8 : index
            %48 = arith.divui %45, %26 : index
            %49 = arith.remui %45, %26 : index
            %50 = arith.muli %48, %c16 : index
            %51 = arith.addi %50, %46 : index
            %52 = arith.muli %49, %c8 : index
            %53 = arith.addi %52, %47 : index
            %alloc_9 = memref.alloc() : memref<128xf32, #gpu.address_space<workgroup>>
            %54 = arith.cmpi ult, %51, %dim_1 : index
            %55 = arith.cmpi ult, %53, %5 : index
            %56 = arith.andi %54, %55 : i1
            scf.if %56 {
              %71 = arith.muli %51, %5 : index
              %72 = arith.addi %71, %53 : index
              %73 = arith.muli %dim_1, %dim_0 : index
              %74 = arith.muli %73, %dim : index
              %reinterpret_cast_10 = memref.reinterpret_cast %1 to offset: [%c0], sizes: [%74], strides: [%c1] : memref<?x?x?xf32, "gpu"> to memref<?xf32, "gpu">
              %75 = memref.load %reinterpret_cast_10[%72] : memref<?xf32, "gpu">
              %76 = math.absf %75 : f32
              %reinterpret_cast_11 = memref.reinterpret_cast %alloc_9 to offset: [0], sizes: [128], strides: [1] : memref<128xf32, #gpu.address_space<workgroup>> to memref<128xf32, #gpu.address_space<workgroup>>
              memref.store %76, %reinterpret_cast_11[%44] : memref<128xf32, #gpu.address_space<workgroup>>
            } else {
              %reinterpret_cast_10 = memref.reinterpret_cast %alloc_9 to offset: [0], sizes: [128], strides: [1] : memref<128xf32, #gpu.address_space<workgroup>> to memref<128xf32, #gpu.address_space<workgroup>>
              memref.store %cst, %reinterpret_cast_10[%44] : memref<128xf32, #gpu.address_space<workgroup>>
            }
            gpu.barrier
            %57 = arith.cmpi ult, %46, %c8 : index
            %58 = arith.addi %51, %c8 : index
            %59 = arith.cmpi ult, %58, %dim_1 : index
            %60 = arith.andi %57, %59 : i1
            scf.if %60 {
              %71 = arith.addi %44, %c64 : index
              %reinterpret_cast_10 = memref.reinterpret_cast %alloc_9 to offset: [0], sizes: [128], strides: [1] : memref<128xf32, #gpu.address_space<workgroup>> to memref<128xf32, #gpu.address_space<workgroup>>
              %72 = memref.load %reinterpret_cast_10[%44] : memref<128xf32, #gpu.address_space<workgroup>>
              %73 = memref.load %reinterpret_cast_10[%71] : memref<128xf32, #gpu.address_space<workgroup>>
              %74 = arith.cmpf ugt, %72, %73 : f32
              %75 = arith.select %74, %72, %73 : f32
              %76 = arith.cmpf uno, %73, %73 : f32
              %77 = arith.select %76, %73, %75 : f32
              memref.store %77, %reinterpret_cast_10[%44] : memref<128xf32, #gpu.address_space<workgroup>>
            }
            gpu.barrier
            %61 = arith.cmpi ult, %46, %c4 : index
            %62 = arith.addi %51, %c4 : index
            %63 = arith.cmpi ult, %62, %dim_1 : index
            %64 = arith.andi %61, %63 : i1
            scf.if %64 {
              %71 = arith.addi %44, %c32 : index
              %reinterpret_cast_10 = memref.reinterpret_cast %alloc_9 to offset: [0], sizes: [128], strides: [1] : memref<128xf32, #gpu.address_space<workgroup>> to memref<128xf32, #gpu.address_space<workgroup>>
              %72 = memref.load %reinterpret_cast_10[%44] : memref<128xf32, #gpu.address_space<workgroup>>
              %73 = memref.load %reinterpret_cast_10[%71] : memref<128xf32, #gpu.address_space<workgroup>>
              %74 = arith.cmpf ugt, %72, %73 : f32
              %75 = arith.select %74, %72, %73 : f32
              %76 = arith.cmpf uno, %73, %73 : f32
              %77 = arith.select %76, %73, %75 : f32
              memref.store %77, %reinterpret_cast_10[%44] : memref<128xf32, #gpu.address_space<workgroup>>
            }
            gpu.barrier
            %65 = arith.cmpi ult, %46, %c2 : index
            %66 = arith.addi %51, %c2 : index
            %67 = arith.cmpi ult, %66, %dim_1 : index
            %68 = arith.andi %65, %67 : i1
            scf.if %68 {
              %71 = arith.addi %44, %c16 : index
              %reinterpret_cast_10 = memref.reinterpret_cast %alloc_9 to offset: [0], sizes: [128], strides: [1] : memref<128xf32, #gpu.address_space<workgroup>> to memref<128xf32, #gpu.address_space<workgroup>>
              %72 = memref.load %reinterpret_cast_10[%44] : memref<128xf32, #gpu.address_space<workgroup>>
              %73 = memref.load %reinterpret_cast_10[%71] : memref<128xf32, #gpu.address_space<workgroup>>
              %74 = arith.cmpf ugt, %72, %73 : f32
              %75 = arith.select %74, %72, %73 : f32
              %76 = arith.cmpf uno, %73, %73 : f32
              %77 = arith.select %76, %73, %75 : f32
              memref.store %77, %reinterpret_cast_10[%44] : memref<128xf32, #gpu.address_space<workgroup>>
            }
            gpu.barrier
            %69 = arith.cmpi eq, %46, %c0 : index
            %70 = arith.andi %69, %56 : i1
            scf.if %70 {
              %71 = arith.addi %44, %c8 : index
              %reinterpret_cast_10 = memref.reinterpret_cast %alloc_9 to offset: [0], sizes: [128], strides: [1] : memref<128xf32, #gpu.address_space<workgroup>> to memref<128xf32, #gpu.address_space<workgroup>>
              %72 = memref.load %reinterpret_cast_10[%44] : memref<128xf32, #gpu.address_space<workgroup>>
              %73 = memref.load %reinterpret_cast_10[%71] : memref<128xf32, #gpu.address_space<workgroup>>
              %74 = arith.cmpf ugt, %72, %73 : f32
              %75 = arith.select %74, %72, %73 : f32
              %76 = arith.cmpf uno, %73, %73 : f32
              %77 = arith.select %76, %73, %75 : f32
              %78 = memref.generic_atomic_rmw %alloc[%53] : memref<?xf32, "gpu"> {
              ^bb0(%arg3: f32):
                %79 = arith.cmpf ogt, %arg3, %77 : f32
                %80 = arith.select %79, %arg3, %77 : f32
                memref.atomic_yield %80 : f32
              }
            }
          }
          scf.yield
        } {mapping = [#gpu.loop_dim_map<processor = thread_x, map = (d0) -> (d0), bound = (d0) -> (d0)>]}
        scf.yield
      } {mapping = [#gpu.loop_dim_map<processor = block_x, map = (d0) -> (d0), bound = (d0) -> (d0)>]}
      "lmhlo.terminator"() : () -> ()
    }) {disc.device = "gpu", disc.fusion.name = "main_kColReduction_reduce__4_1_0", disc.fusion.tag = "8w16h", disc.fusion_type = "kColReduction", disc_col_reduction_schedule_hint = 2 : i32, disc_thread_per_block_hint = 128 : i32} : () -> ()
  }
  %16 = llvm.inttoptr %0 : i32 to !llvm.ptr<i8>
  %alloca = memref.alloca() : memref<2xindex, "cpu">
  memref.store %dim_0, %alloca[%c0] : memref<2xindex, "cpu">
  memref.store %dim, %alloca[%c1] : memref<2xindex, "cpu">
  %17 = "disc_ral.dispatch"(%arg0, %16, %alloc, %alloca) {backend_config = "", call_target_name = "inc_ref", device = "gpu", has_side_effect = false} : (!disc_ral.context, !llvm.ptr<i8>, memref<?xf32, "gpu">, memref<2xindex, "cpu">) -> memref<?x?xf32, "gpu">
  %reinterpret_cast = memref.reinterpret_cast %17 to offset: [0], sizes: [%dim_0, %dim], strides: [%dim, 1] {kDiscSymbolicDimAttr = [@S1, @S2]} : memref<?x?xf32, "gpu"> to memref<?x?xf32, "gpu">
  memref.dealloc %alloc : memref<?xf32, "gpu">
  "disc_ral.dispatch"(%arg0, %c0, %reinterpret_cast) {backend_config = "", call_target_name = "ral_send_output", device = "cpu", has_side_effect = false} : (!disc_ral.context, index, memref<?x?xf32, "gpu">) -> ()
  return
}

// -----// IR Dump After ConvertParallelLoopToGpu (convert-parallel-loops-to-gpu) //----- //
func.func @main(%arg0: !disc_ral.context) attributes {tf.entry_function = {input_placements = "gpu", inputs = "input0", output_placements = "gpu", outputs = "output0"}} {
  %c-1 = arith.constant -1 : index
  %cst = arith.constant 0xFF800000 : f32
  %c64 = arith.constant 64 : index
  %c128 = arith.constant 128 : index
  %c4 = arith.constant 4 : index
  %c16 = arith.constant 16 : index
  %c32 = arith.constant 32 : index
  %c8 = arith.constant 8 : index
  %0 = llvm.mlir.constant(0 : i32) : i32
  %c0 = arith.constant 0 : index
  %c1 = arith.constant 1 : index
  %c2 = arith.constant 2 : index
  %c256 = arith.constant 256 : index
  %c108 = arith.constant 108 : index
  %1 = "disc_ral.dispatch"(%arg0, %c0) {backend_config = "", call_target_name = "ral_recv_input", device = "cpu", has_side_effect = false} : (!disc_ral.context, index) -> memref<?x?x?xf32, "gpu">
  %dim = memref.dim %1, %c2 : memref<?x?x?xf32, "gpu">
  %dim_0 = memref.dim %1, %c1 : memref<?x?x?xf32, "gpu">
  %dim_1 = memref.dim %1, %c0 : memref<?x?x?xf32, "gpu">
  %2 = arith.index_cast %dim_0 : index to i32
  %3 = arith.index_cast %dim : index to i32
  %4 = arith.muli %2, %3 : i32
  %5 = arith.index_cast %4 : i32 to index
  %alloc = memref.alloc(%5) {kDiscSymbolicDimAttr = [@S3]} : memref<?xf32, "gpu">
  %6 = arith.muli %dim_1, %5 : index
  %7 = arith.addi %6, %c-1 : index
  %8 = arith.divsi %7, %c256 : index
  %9 = arith.addi %8, %c1 : index
  %10 = arith.subi %c0, %6 : index
  %11 = arith.divsi %10, %c256 : index
  %12 = arith.subi %c0, %11 : index
  %13 = arith.cmpi sgt, %6, %c0 : index
  %14 = arith.select %13, %9, %12 : index
  %15 = arith.cmpi sgt, %14, %c108 : index
  scf.if %15 {
    "lmhlo.fusion"() ({
      %c0_2 = arith.constant 0 : index
      %c256_3 = arith.constant 256 : index
      %18 = arith.muli %c1, %c256_3 : index
      %c1_4 = arith.constant 1 : index
      %19 = affine.apply affine_map<(d0)[s0, s1] -> ((d0 - s0) ceildiv s1)>(%5)[%c0, %18]
      %20 = affine.apply affine_map<(d0)[s0, s1] -> ((d0 - s0) ceildiv s1)>(%18)[%c0_2, %c1]
      gpu.launch blocks(%arg1, %arg2, %arg3) in (%arg7 = %19, %arg8 = %c1_4, %arg9 = %c1_4) threads(%arg4, %arg5, %arg6) in (%arg10 = %20, %arg11 = %c1_4, %arg12 = %c1_4) {
        %43 = affine.apply affine_map<(d0)[s0, s1] -> (d0 * s0 + s1)>(%arg1)[%18, %c0]
        %44 = affine.apply affine_map<(d0)[s0, s1] -> (d0 * s0 + s1)>(%arg4)[%c1, %c0_2]
        %45 = arith.addi %44, %43 : index
        %true = arith.constant true
        %46 = arith.muli %44, %c1 : index
        %47 = arith.addi %46, %43 : index
        %48 = arith.cmpi ult, %47, %5 : index
        %49 = arith.andi %true, %48 : i1
        scf.if %49 {
          %reinterpret_cast_11 = memref.reinterpret_cast %alloc to offset: [%c0], sizes: [%5], strides: [%c1] : memref<?xf32, "gpu"> to memref<?xf32, "gpu">
          memref.store %cst, %reinterpret_cast_11[%45] : memref<?xf32, "gpu">
        }
        gpu.terminator
      } {SCFToGPU_visited}
      %21 = arith.addi %5, %c-1 : index
      %22 = arith.divsi %21, %c8 : index
      %23 = arith.addi %22, %c1 : index
      %24 = arith.subi %c0, %5 : index
      %25 = arith.divsi %24, %c8 : index
      %26 = arith.subi %c0, %25 : index
      %27 = arith.cmpi sgt, %5, %c0 : index
      %28 = arith.select %27, %23, %26 : index
      %29 = arith.addi %dim_1, %c-1 : index
      %30 = arith.divsi %29, %c32 : index
      %31 = arith.addi %30, %c1 : index
      %32 = arith.subi %c0, %dim_1 : index
      %33 = arith.divsi %32, %c32 : index
      %34 = arith.subi %c0, %33 : index
      %35 = arith.cmpi sgt, %dim_1, %c0 : index
      %36 = arith.select %35, %31, %34 : index
      %37 = arith.muli %28, %36 : index
      %c0_5 = arith.constant 0 : index
      %c1_6 = arith.constant 1 : index
      %c1_7 = arith.constant 1 : index
      %38 = arith.muli %c1_7, %37 : index
      %39 = arith.muli %38, %c256 : index
      %c0_8 = arith.constant 0 : index
      %c256_9 = arith.constant 256 : index
      %40 = arith.muli %c1_6, %c256_9 : index
      %c1_10 = arith.constant 1 : index
      %41 = affine.apply affine_map<(d0)[s0, s1] -> ((d0 - s0) ceildiv s1)>(%39)[%c0_5, %40]
      %42 = affine.apply affine_map<(d0)[s0, s1] -> ((d0 - s0) ceildiv s1)>(%40)[%c0_8, %c1_6]
      gpu.launch blocks(%arg1, %arg2, %arg3) in (%arg7 = %41, %arg8 = %c1_10, %arg9 = %c1_10) threads(%arg4, %arg5, %arg6) in (%arg10 = %42, %arg11 = %c1_10, %arg12 = %c1_10) {
        %43 = affine.apply affine_map<(d0)[s0, s1] -> (d0 * s0 + s1)>(%arg1)[%40, %c0_5]
        %44 = affine.apply affine_map<(d0)[s0, s1] -> (d0 * s0 + s1)>(%arg4)[%c1_6, %c0_8]
        %45 = arith.addi %44, %43 : index
        %true = arith.constant true
        %46 = arith.muli %44, %c1_6 : index
        %47 = arith.addi %46, %43 : index
        %48 = arith.cmpi ult, %47, %39 : index
        %49 = arith.andi %true, %48 : i1
        scf.if %49 {
          %50 = arith.remsi %45, %c256 : index
          %51 = arith.divsi %45, %c256 : index
          %52 = arith.divui %50, %c8 : index
          %53 = arith.remui %50, %c8 : index
          %54 = arith.divui %51, %28 : index
          %55 = arith.remui %51, %28 : index
          %56 = arith.muli %54, %c32 : index
          %57 = arith.addi %56, %52 : index
          %58 = arith.muli %55, %c8 : index
          %59 = arith.addi %58, %53 : index
          %alloc_11 = memref.alloc() : memref<256xf32, #gpu.address_space<workgroup>>
          %60 = arith.cmpi ult, %57, %dim_1 : index
          %61 = arith.cmpi ult, %59, %5 : index
          %62 = arith.andi %60, %61 : i1
          scf.if %62 {
            %81 = arith.muli %57, %5 : index
            %82 = arith.addi %81, %59 : index
            %83 = arith.muli %dim_1, %dim_0 : index
            %84 = arith.muli %83, %dim : index
            %reinterpret_cast_12 = memref.reinterpret_cast %1 to offset: [%c0], sizes: [%84], strides: [%c1] : memref<?x?x?xf32, "gpu"> to memref<?xf32, "gpu">
            %85 = memref.load %reinterpret_cast_12[%82] : memref<?xf32, "gpu">
            %86 = math.absf %85 : f32
            %reinterpret_cast_13 = memref.reinterpret_cast %alloc_11 to offset: [0], sizes: [256], strides: [1] : memref<256xf32, #gpu.address_space<workgroup>> to memref<256xf32, #gpu.address_space<workgroup>>
            memref.store %86, %reinterpret_cast_13[%50] : memref<256xf32, #gpu.address_space<workgroup>>
          } else {
            %reinterpret_cast_12 = memref.reinterpret_cast %alloc_11 to offset: [0], sizes: [256], strides: [1] : memref<256xf32, #gpu.address_space<workgroup>> to memref<256xf32, #gpu.address_space<workgroup>>
            memref.store %cst, %reinterpret_cast_12[%50] : memref<256xf32, #gpu.address_space<workgroup>>
          }
          gpu.barrier
          %63 = arith.cmpi ult, %52, %c16 : index
          %64 = arith.addi %57, %c16 : index
          %65 = arith.cmpi ult, %64, %dim_1 : index
          %66 = arith.andi %63, %65 : i1
          scf.if %66 {
            %81 = arith.addi %50, %c128 : index
            %reinterpret_cast_12 = memref.reinterpret_cast %alloc_11 to offset: [0], sizes: [256], strides: [1] : memref<256xf32, #gpu.address_space<workgroup>> to memref<256xf32, #gpu.address_space<workgroup>>
            %82 = memref.load %reinterpret_cast_12[%50] : memref<256xf32, #gpu.address_space<workgroup>>
            %83 = memref.load %reinterpret_cast_12[%81] : memref<256xf32, #gpu.address_space<workgroup>>
            %84 = arith.cmpf ugt, %82, %83 : f32
            %85 = arith.select %84, %82, %83 : f32
            %86 = arith.cmpf uno, %83, %83 : f32
            %87 = arith.select %86, %83, %85 : f32
            memref.store %87, %reinterpret_cast_12[%50] : memref<256xf32, #gpu.address_space<workgroup>>
          }
          gpu.barrier
          %67 = arith.cmpi ult, %52, %c8 : index
          %68 = arith.addi %57, %c8 : index
          %69 = arith.cmpi ult, %68, %dim_1 : index
          %70 = arith.andi %67, %69 : i1
          scf.if %70 {
            %81 = arith.addi %50, %c64 : index
            %reinterpret_cast_12 = memref.reinterpret_cast %alloc_11 to offset: [0], sizes: [256], strides: [1] : memref<256xf32, #gpu.address_space<workgroup>> to memref<256xf32, #gpu.address_space<workgroup>>
            %82 = memref.load %reinterpret_cast_12[%50] : memref<256xf32, #gpu.address_space<workgroup>>
            %83 = memref.load %reinterpret_cast_12[%81] : memref<256xf32, #gpu.address_space<workgroup>>
            %84 = arith.cmpf ugt, %82, %83 : f32
            %85 = arith.select %84, %82, %83 : f32
            %86 = arith.cmpf uno, %83, %83 : f32
            %87 = arith.select %86, %83, %85 : f32
            memref.store %87, %reinterpret_cast_12[%50] : memref<256xf32, #gpu.address_space<workgroup>>
          }
          gpu.barrier
          %71 = arith.cmpi ult, %52, %c4 : index
          %72 = arith.addi %57, %c4 : index
          %73 = arith.cmpi ult, %72, %dim_1 : index
          %74 = arith.andi %71, %73 : i1
          scf.if %74 {
            %81 = arith.addi %50, %c32 : index
            %reinterpret_cast_12 = memref.reinterpret_cast %alloc_11 to offset: [0], sizes: [256], strides: [1] : memref<256xf32, #gpu.address_space<workgroup>> to memref<256xf32, #gpu.address_space<workgroup>>
            %82 = memref.load %reinterpret_cast_12[%50] : memref<256xf32, #gpu.address_space<workgroup>>
            %83 = memref.load %reinterpret_cast_12[%81] : memref<256xf32, #gpu.address_space<workgroup>>
            %84 = arith.cmpf ugt, %82, %83 : f32
            %85 = arith.select %84, %82, %83 : f32
            %86 = arith.cmpf uno, %83, %83 : f32
            %87 = arith.select %86, %83, %85 : f32
            memref.store %87, %reinterpret_cast_12[%50] : memref<256xf32, #gpu.address_space<workgroup>>
          }
          gpu.barrier
          %75 = arith.cmpi ult, %52, %c2 : index
          %76 = arith.addi %57, %c2 : index
          %77 = arith.cmpi ult, %76, %dim_1 : index
          %78 = arith.andi %75, %77 : i1
          scf.if %78 {
            %81 = arith.addi %50, %c16 : index
            %reinterpret_cast_12 = memref.reinterpret_cast %alloc_11 to offset: [0], sizes: [256], strides: [1] : memref<256xf32, #gpu.address_space<workgroup>> to memref<256xf32, #gpu.address_space<workgroup>>
            %82 = memref.load %reinterpret_cast_12[%50] : memref<256xf32, #gpu.address_space<workgroup>>
            %83 = memref.load %reinterpret_cast_12[%81] : memref<256xf32, #gpu.address_space<workgroup>>
            %84 = arith.cmpf ugt, %82, %83 : f32
            %85 = arith.select %84, %82, %83 : f32
            %86 = arith.cmpf uno, %83, %83 : f32
            %87 = arith.select %86, %83, %85 : f32
            memref.store %87, %reinterpret_cast_12[%50] : memref<256xf32, #gpu.address_space<workgroup>>
          }
          gpu.barrier
          %79 = arith.cmpi eq, %52, %c0 : index
          %80 = arith.andi %79, %62 : i1
          scf.if %80 {
            %81 = arith.addi %50, %c8 : index
            %reinterpret_cast_12 = memref.reinterpret_cast %alloc_11 to offset: [0], sizes: [256], strides: [1] : memref<256xf32, #gpu.address_space<workgroup>> to memref<256xf32, #gpu.address_space<workgroup>>
            %82 = memref.load %reinterpret_cast_12[%50] : memref<256xf32, #gpu.address_space<workgroup>>
            %83 = memref.load %reinterpret_cast_12[%81] : memref<256xf32, #gpu.address_space<workgroup>>
            %84 = arith.cmpf ugt, %82, %83 : f32
            %85 = arith.select %84, %82, %83 : f32
            %86 = arith.cmpf uno, %83, %83 : f32
            %87 = arith.select %86, %83, %85 : f32
            %88 = memref.generic_atomic_rmw %alloc[%59] : memref<?xf32, "gpu"> {
            ^bb0(%arg13: f32):
              %89 = arith.cmpf ogt, %arg13, %87 : f32
              %90 = arith.select %89, %arg13, %87 : f32
              memref.atomic_yield %90 : f32
            }
          }
        }
        gpu.terminator
      } {SCFToGPU_visited}
      "lmhlo.terminator"() : () -> ()
    }) {disc.device = "gpu", disc.fusion.name = "main_kColReduction_reduce__4_1_0", disc.fusion.tag = "8w32h", disc.fusion_type = "kColReduction", disc_col_reduction_schedule_hint = 1 : i32, disc_thread_per_block_hint = 256 : i32} : () -> ()
  } else {
    "lmhlo.fusion"() ({
      %c0_2 = arith.constant 0 : index
      %c128_3 = arith.constant 128 : index
      %18 = arith.muli %c1, %c128_3 : index
      %c1_4 = arith.constant 1 : index
      %19 = affine.apply affine_map<(d0)[s0, s1] -> ((d0 - s0) ceildiv s1)>(%5)[%c0, %18]
      %20 = affine.apply affine_map<(d0)[s0, s1] -> ((d0 - s0) ceildiv s1)>(%18)[%c0_2, %c1]
      gpu.launch blocks(%arg1, %arg2, %arg3) in (%arg7 = %19, %arg8 = %c1_4, %arg9 = %c1_4) threads(%arg4, %arg5, %arg6) in (%arg10 = %20, %arg11 = %c1_4, %arg12 = %c1_4) {
        %43 = affine.apply affine_map<(d0)[s0, s1] -> (d0 * s0 + s1)>(%arg1)[%18, %c0]
        %44 = affine.apply affine_map<(d0)[s0, s1] -> (d0 * s0 + s1)>(%arg4)[%c1, %c0_2]
        %45 = arith.addi %44, %43 : index
        %true = arith.constant true
        %46 = arith.muli %44, %c1 : index
        %47 = arith.addi %46, %43 : index
        %48 = arith.cmpi ult, %47, %5 : index
        %49 = arith.andi %true, %48 : i1
        scf.if %49 {
          %reinterpret_cast_11 = memref.reinterpret_cast %alloc to offset: [%c0], sizes: [%5], strides: [%c1] : memref<?xf32, "gpu"> to memref<?xf32, "gpu">
          memref.store %cst, %reinterpret_cast_11[%45] : memref<?xf32, "gpu">
        }
        gpu.terminator
      } {SCFToGPU_visited}
      %21 = arith.addi %5, %c-1 : index
      %22 = arith.divsi %21, %c8 : index
      %23 = arith.addi %22, %c1 : index
      %24 = arith.subi %c0, %5 : index
      %25 = arith.divsi %24, %c8 : index
      %26 = arith.subi %c0, %25 : index
      %27 = arith.cmpi sgt, %5, %c0 : index
      %28 = arith.select %27, %23, %26 : index
      %29 = arith.addi %dim_1, %c-1 : index
      %30 = arith.divsi %29, %c16 : index
      %31 = arith.addi %30, %c1 : index
      %32 = arith.subi %c0, %dim_1 : index
      %33 = arith.divsi %32, %c16 : index
      %34 = arith.subi %c0, %33 : index
      %35 = arith.cmpi sgt, %dim_1, %c0 : index
      %36 = arith.select %35, %31, %34 : index
      %37 = arith.muli %28, %36 : index
      %c0_5 = arith.constant 0 : index
      %c1_6 = arith.constant 1 : index
      %c1_7 = arith.constant 1 : index
      %38 = arith.muli %c1_7, %37 : index
      %39 = arith.muli %38, %c128 : index
      %c0_8 = arith.constant 0 : index
      %c128_9 = arith.constant 128 : index
      %40 = arith.muli %c1_6, %c128_9 : index
      %c1_10 = arith.constant 1 : index
      %41 = affine.apply affine_map<(d0)[s0, s1] -> ((d0 - s0) ceildiv s1)>(%39)[%c0_5, %40]
      %42 = affine.apply affine_map<(d0)[s0, s1] -> ((d0 - s0) ceildiv s1)>(%40)[%c0_8, %c1_6]
      gpu.launch blocks(%arg1, %arg2, %arg3) in (%arg7 = %41, %arg8 = %c1_10, %arg9 = %c1_10) threads(%arg4, %arg5, %arg6) in (%arg10 = %42, %arg11 = %c1_10, %arg12 = %c1_10) {
        %43 = affine.apply affine_map<(d0)[s0, s1] -> (d0 * s0 + s1)>(%arg1)[%40, %c0_5]
        %44 = affine.apply affine_map<(d0)[s0, s1] -> (d0 * s0 + s1)>(%arg4)[%c1_6, %c0_8]
        %45 = arith.addi %44, %43 : index
        %true = arith.constant true
        %46 = arith.muli %44, %c1_6 : index
        %47 = arith.addi %46, %43 : index
        %48 = arith.cmpi ult, %47, %39 : index
        %49 = arith.andi %true, %48 : i1
        scf.if %49 {
          %50 = arith.remsi %45, %c128 : index
          %51 = arith.divsi %45, %c128 : index
          %52 = arith.divui %50, %c8 : index
          %53 = arith.remui %50, %c8 : index
          %54 = arith.divui %51, %28 : index
          %55 = arith.remui %51, %28 : index
          %56 = arith.muli %54, %c16 : index
          %57 = arith.addi %56, %52 : index
          %58 = arith.muli %55, %c8 : index
          %59 = arith.addi %58, %53 : index
          %alloc_11 = memref.alloc() : memref<128xf32, #gpu.address_space<workgroup>>
          %60 = arith.cmpi ult, %57, %dim_1 : index
          %61 = arith.cmpi ult, %59, %5 : index
          %62 = arith.andi %60, %61 : i1
          scf.if %62 {
            %77 = arith.muli %57, %5 : index
            %78 = arith.addi %77, %59 : index
            %79 = arith.muli %dim_1, %dim_0 : index
            %80 = arith.muli %79, %dim : index
            %reinterpret_cast_12 = memref.reinterpret_cast %1 to offset: [%c0], sizes: [%80], strides: [%c1] : memref<?x?x?xf32, "gpu"> to memref<?xf32, "gpu">
            %81 = memref.load %reinterpret_cast_12[%78] : memref<?xf32, "gpu">
            %82 = math.absf %81 : f32
            %reinterpret_cast_13 = memref.reinterpret_cast %alloc_11 to offset: [0], sizes: [128], strides: [1] : memref<128xf32, #gpu.address_space<workgroup>> to memref<128xf32, #gpu.address_space<workgroup>>
            memref.store %82, %reinterpret_cast_13[%50] : memref<128xf32, #gpu.address_space<workgroup>>
          } else {
            %reinterpret_cast_12 = memref.reinterpret_cast %alloc_11 to offset: [0], sizes: [128], strides: [1] : memref<128xf32, #gpu.address_space<workgroup>> to memref<128xf32, #gpu.address_space<workgroup>>
            memref.store %cst, %reinterpret_cast_12[%50] : memref<128xf32, #gpu.address_space<workgroup>>
          }
          gpu.barrier
          %63 = arith.cmpi ult, %52, %c8 : index
          %64 = arith.addi %57, %c8 : index
          %65 = arith.cmpi ult, %64, %dim_1 : index
          %66 = arith.andi %63, %65 : i1
          scf.if %66 {
            %77 = arith.addi %50, %c64 : index
            %reinterpret_cast_12 = memref.reinterpret_cast %alloc_11 to offset: [0], sizes: [128], strides: [1] : memref<128xf32, #gpu.address_space<workgroup>> to memref<128xf32, #gpu.address_space<workgroup>>
            %78 = memref.load %reinterpret_cast_12[%50] : memref<128xf32, #gpu.address_space<workgroup>>
            %79 = memref.load %reinterpret_cast_12[%77] : memref<128xf32, #gpu.address_space<workgroup>>
            %80 = arith.cmpf ugt, %78, %79 : f32
            %81 = arith.select %80, %78, %79 : f32
            %82 = arith.cmpf uno, %79, %79 : f32
            %83 = arith.select %82, %79, %81 : f32
            memref.store %83, %reinterpret_cast_12[%50] : memref<128xf32, #gpu.address_space<workgroup>>
          }
          gpu.barrier
          %67 = arith.cmpi ult, %52, %c4 : index
          %68 = arith.addi %57, %c4 : index
          %69 = arith.cmpi ult, %68, %dim_1 : index
          %70 = arith.andi %67, %69 : i1
          scf.if %70 {
            %77 = arith.addi %50, %c32 : index
            %reinterpret_cast_12 = memref.reinterpret_cast %alloc_11 to offset: [0], sizes: [128], strides: [1] : memref<128xf32, #gpu.address_space<workgroup>> to memref<128xf32, #gpu.address_space<workgroup>>
            %78 = memref.load %reinterpret_cast_12[%50] : memref<128xf32, #gpu.address_space<workgroup>>
            %79 = memref.load %reinterpret_cast_12[%77] : memref<128xf32, #gpu.address_space<workgroup>>
            %80 = arith.cmpf ugt, %78, %79 : f32
            %81 = arith.select %80, %78, %79 : f32
            %82 = arith.cmpf uno, %79, %79 : f32
            %83 = arith.select %82, %79, %81 : f32
            memref.store %83, %reinterpret_cast_12[%50] : memref<128xf32, #gpu.address_space<workgroup>>
          }
          gpu.barrier
          %71 = arith.cmpi ult, %52, %c2 : index
          %72 = arith.addi %57, %c2 : index
          %73 = arith.cmpi ult, %72, %dim_1 : index
          %74 = arith.andi %71, %73 : i1
          scf.if %74 {
            %77 = arith.addi %50, %c16 : index
            %reinterpret_cast_12 = memref.reinterpret_cast %alloc_11 to offset: [0], sizes: [128], strides: [1] : memref<128xf32, #gpu.address_space<workgroup>> to memref<128xf32, #gpu.address_space<workgroup>>
            %78 = memref.load %reinterpret_cast_12[%50] : memref<128xf32, #gpu.address_space<workgroup>>
            %79 = memref.load %reinterpret_cast_12[%77] : memref<128xf32, #gpu.address_space<workgroup>>
            %80 = arith.cmpf ugt, %78, %79 : f32
            %81 = arith.select %80, %78, %79 : f32
            %82 = arith.cmpf uno, %79, %79 : f32
            %83 = arith.select %82, %79, %81 : f32
            memref.store %83, %reinterpret_cast_12[%50] : memref<128xf32, #gpu.address_space<workgroup>>
          }
          gpu.barrier
          %75 = arith.cmpi eq, %52, %c0 : index
          %76 = arith.andi %75, %62 : i1
          scf.if %76 {
            %77 = arith.addi %50, %c8 : index
            %reinterpret_cast_12 = memref.reinterpret_cast %alloc_11 to offset: [0], sizes: [128], strides: [1] : memref<128xf32, #gpu.address_space<workgroup>> to memref<128xf32, #gpu.address_space<workgroup>>
            %78 = memref.load %reinterpret_cast_12[%50] : memref<128xf32, #gpu.address_space<workgroup>>
            %79 = memref.load %reinterpret_cast_12[%77] : memref<128xf32, #gpu.address_space<workgroup>>
            %80 = arith.cmpf ugt, %78, %79 : f32
            %81 = arith.select %80, %78, %79 : f32
            %82 = arith.cmpf uno, %79, %79 : f32
            %83 = arith.select %82, %79, %81 : f32
            %84 = memref.generic_atomic_rmw %alloc[%59] : memref<?xf32, "gpu"> {
            ^bb0(%arg13: f32):
              %85 = arith.cmpf ogt, %arg13, %83 : f32
              %86 = arith.select %85, %arg13, %83 : f32
              memref.atomic_yield %86 : f32
            }
          }
        }
        gpu.terminator
      } {SCFToGPU_visited}
      "lmhlo.terminator"() : () -> ()
    }) {disc.device = "gpu", disc.fusion.name = "main_kColReduction_reduce__4_1_0", disc.fusion.tag = "8w16h", disc.fusion_type = "kColReduction", disc_col_reduction_schedule_hint = 2 : i32, disc_thread_per_block_hint = 128 : i32} : () -> ()
  }
  %16 = llvm.inttoptr %0 : i32 to !llvm.ptr<i8>
  %alloca = memref.alloca() : memref<2xindex, "cpu">
  memref.store %dim_0, %alloca[%c0] : memref<2xindex, "cpu">
  memref.store %dim, %alloca[%c1] : memref<2xindex, "cpu">
  %17 = "disc_ral.dispatch"(%arg0, %16, %alloc, %alloca) {backend_config = "", call_target_name = "inc_ref", device = "gpu", has_side_effect = false} : (!disc_ral.context, !llvm.ptr<i8>, memref<?xf32, "gpu">, memref<2xindex, "cpu">) -> memref<?x?xf32, "gpu">
  %reinterpret_cast = memref.reinterpret_cast %17 to offset: [0], sizes: [%dim_0, %dim], strides: [%dim, 1] {kDiscSymbolicDimAttr = [@S1, @S2]} : memref<?x?xf32, "gpu"> to memref<?x?xf32, "gpu">
  memref.dealloc %alloc : memref<?xf32, "gpu">
  "disc_ral.dispatch"(%arg0, %c0, %reinterpret_cast) {backend_config = "", call_target_name = "ral_send_output", device = "cpu", has_side_effect = false} : (!disc_ral.context, index, memref<?x?xf32, "gpu">) -> ()
  return
}

// -----// IR Dump After GpuLaunchSinkIndexComputations (gpu-launch-sink-index-computations) //----- //
#map = affine_map<(d0)[s0, s1] -> ((d0 - s0) ceildiv s1)>
#map1 = affine_map<(d0)[s0, s1] -> (d0 * s0 + s1)>
module {
  func.func @main(%arg0: !disc_ral.context) attributes {tf.entry_function = {input_placements = "gpu", inputs = "input0", output_placements = "gpu", outputs = "output0"}} {
    %c-1 = arith.constant -1 : index
    %cst = arith.constant 0xFF800000 : f32
    %c64 = arith.constant 64 : index
    %c128 = arith.constant 128 : index
    %c4 = arith.constant 4 : index
    %c16 = arith.constant 16 : index
    %c32 = arith.constant 32 : index
    %c8 = arith.constant 8 : index
    %0 = llvm.mlir.constant(0 : i32) : i32
    %c0 = arith.constant 0 : index
    %c1 = arith.constant 1 : index
    %c2 = arith.constant 2 : index
    %c256 = arith.constant 256 : index
    %c108 = arith.constant 108 : index
    %1 = "disc_ral.dispatch"(%arg0, %c0) {backend_config = "", call_target_name = "ral_recv_input", device = "cpu", has_side_effect = false} : (!disc_ral.context, index) -> memref<?x?x?xf32, "gpu">
    %dim = memref.dim %1, %c2 : memref<?x?x?xf32, "gpu">
    %dim_0 = memref.dim %1, %c1 : memref<?x?x?xf32, "gpu">
    %dim_1 = memref.dim %1, %c0 : memref<?x?x?xf32, "gpu">
    %2 = arith.index_cast %dim_0 : index to i32
    %3 = arith.index_cast %dim : index to i32
    %4 = arith.muli %2, %3 : i32
    %5 = arith.index_cast %4 : i32 to index
    %alloc = memref.alloc(%5) {kDiscSymbolicDimAttr = [@S3]} : memref<?xf32, "gpu">
    %6 = arith.muli %dim_1, %5 : index
    %7 = arith.addi %6, %c-1 : index
    %8 = arith.divsi %7, %c256 : index
    %9 = arith.addi %8, %c1 : index
    %10 = arith.subi %c0, %6 : index
    %11 = arith.divsi %10, %c256 : index
    %12 = arith.subi %c0, %11 : index
    %13 = arith.cmpi sgt, %6, %c0 : index
    %14 = arith.select %13, %9, %12 : index
    %15 = arith.cmpi sgt, %14, %c108 : index
    scf.if %15 {
      "lmhlo.fusion"() ({
        %c0_2 = arith.constant 0 : index
        %c256_3 = arith.constant 256 : index
        %18 = arith.muli %c1, %c256_3 : index
        %c1_4 = arith.constant 1 : index
        %19 = affine.apply #map(%5)[%c0, %18]
        %20 = affine.apply #map(%18)[%c0_2, %c1]
        gpu.launch blocks(%arg1, %arg2, %arg3) in (%arg7 = %19, %arg8 = %c1_4, %arg9 = %c1_4) threads(%arg4, %arg5, %arg6) in (%arg10 = %20, %arg11 = %c1_4, %arg12 = %c1_4) {
          %c0_11 = arith.constant 0 : index
          %c1_12 = arith.constant 1 : index
          %c0_13 = arith.constant 0 : index
          %cst_14 = arith.constant 0xFF800000 : f32
          %43 = affine.apply #map1(%arg1)[%18, %c0_11]
          %44 = affine.apply #map1(%arg4)[%c1_12, %c0_13]
          %45 = arith.addi %44, %43 : index
          %true = arith.constant true
          %46 = arith.muli %44, %c1_12 : index
          %47 = arith.addi %46, %43 : index
          %48 = arith.cmpi ult, %47, %5 : index
          %49 = arith.andi %true, %48 : i1
          scf.if %49 {
            %reinterpret_cast_15 = memref.reinterpret_cast %alloc to offset: [%c0_11], sizes: [%5], strides: [%c1_12] : memref<?xf32, "gpu"> to memref<?xf32, "gpu">
            memref.store %cst_14, %reinterpret_cast_15[%45] : memref<?xf32, "gpu">
          }
          gpu.terminator
        } {SCFToGPU_visited}
        %21 = arith.addi %5, %c-1 : index
        %22 = arith.divsi %21, %c8 : index
        %23 = arith.addi %22, %c1 : index
        %24 = arith.subi %c0, %5 : index
        %25 = arith.divsi %24, %c8 : index
        %26 = arith.subi %c0, %25 : index
        %27 = arith.cmpi sgt, %5, %c0 : index
        %28 = arith.select %27, %23, %26 : index
        %29 = arith.addi %dim_1, %c-1 : index
        %30 = arith.divsi %29, %c32 : index
        %31 = arith.addi %30, %c1 : index
        %32 = arith.subi %c0, %dim_1 : index
        %33 = arith.divsi %32, %c32 : index
        %34 = arith.subi %c0, %33 : index
        %35 = arith.cmpi sgt, %dim_1, %c0 : index
        %36 = arith.select %35, %31, %34 : index
        %37 = arith.muli %28, %36 : index
        %c0_5 = arith.constant 0 : index
        %c1_6 = arith.constant 1 : index
        %c1_7 = arith.constant 1 : index
        %38 = arith.muli %c1_7, %37 : index
        %39 = arith.muli %38, %c256 : index
        %c0_8 = arith.constant 0 : index
        %c256_9 = arith.constant 256 : index
        %40 = arith.muli %c1_6, %c256_9 : index
        %c1_10 = arith.constant 1 : index
        %41 = affine.apply #map(%39)[%c0_5, %40]
        %42 = affine.apply #map(%40)[%c0_8, %c1_6]
        gpu.launch blocks(%arg1, %arg2, %arg3) in (%arg7 = %41, %arg8 = %c1_10, %arg9 = %c1_10) threads(%arg4, %arg5, %arg6) in (%arg10 = %42, %arg11 = %c1_10, %arg12 = %c1_10) {
          %c0_11 = arith.constant 0 : index
          %c1_12 = arith.constant 1 : index
          %c0_13 = arith.constant 0 : index
          %c256_14 = arith.constant 256 : index
          %c8_15 = arith.constant 8 : index
          %c0_16 = arith.constant 0 : index
          %43 = arith.cmpi sgt, %5, %c0_16 : index
          %c32_17 = arith.constant 32 : index
          %dim_18 = memref.dim %1, %c0_16 : memref<?x?x?xf32, "gpu">
          %c1_19 = arith.constant 1 : index
          %dim_20 = memref.dim %1, %c1_19 : memref<?x?x?xf32, "gpu">
          %c2_21 = arith.constant 2 : index
          %dim_22 = memref.dim %1, %c2_21 : memref<?x?x?xf32, "gpu">
          %cst_23 = arith.constant 0xFF800000 : f32
          %c16_24 = arith.constant 16 : index
          %c128_25 = arith.constant 128 : index
          %c64_26 = arith.constant 64 : index
          %c4_27 = arith.constant 4 : index
          %44 = affine.apply #map1(%arg1)[%40, %c0_11]
          %45 = affine.apply #map1(%arg4)[%c1_12, %c0_13]
          %46 = arith.addi %45, %44 : index
          %true = arith.constant true
          %47 = arith.muli %45, %c1_12 : index
          %48 = arith.addi %47, %44 : index
          %49 = arith.cmpi ult, %48, %39 : index
          %50 = arith.andi %true, %49 : i1
          scf.if %50 {
            %51 = arith.remsi %46, %c256_14 : index
            %52 = arith.divsi %46, %c256_14 : index
            %53 = arith.divui %51, %c8_15 : index
            %54 = arith.remui %51, %c8_15 : index
            %55 = arith.divui %52, %28 : index
            %56 = arith.remui %52, %28 : index
            %57 = arith.muli %55, %c32_17 : index
            %58 = arith.addi %57, %53 : index
            %59 = arith.muli %56, %c8_15 : index
            %60 = arith.addi %59, %54 : index
            %alloc_28 = memref.alloc() : memref<256xf32, #gpu.address_space<workgroup>>
            %61 = arith.cmpi ult, %58, %dim_18 : index
            %62 = arith.cmpi ult, %60, %5 : index
            %63 = arith.andi %61, %62 : i1
            scf.if %63 {
              %82 = arith.muli %58, %5 : index
              %83 = arith.addi %82, %60 : index
              %84 = arith.muli %dim_18, %dim_20 : index
              %85 = arith.muli %84, %dim_22 : index
              %reinterpret_cast_29 = memref.reinterpret_cast %1 to offset: [%c0_16], sizes: [%85], strides: [%c1_19] : memref<?x?x?xf32, "gpu"> to memref<?xf32, "gpu">
              %86 = memref.load %reinterpret_cast_29[%83] : memref<?xf32, "gpu">
              %87 = math.absf %86 : f32
              %reinterpret_cast_30 = memref.reinterpret_cast %alloc_28 to offset: [0], sizes: [256], strides: [1] : memref<256xf32, #gpu.address_space<workgroup>> to memref<256xf32, #gpu.address_space<workgroup>>
              memref.store %87, %reinterpret_cast_30[%51] : memref<256xf32, #gpu.address_space<workgroup>>
            } else {
              %reinterpret_cast_29 = memref.reinterpret_cast %alloc_28 to offset: [0], sizes: [256], strides: [1] : memref<256xf32, #gpu.address_space<workgroup>> to memref<256xf32, #gpu.address_space<workgroup>>
              memref.store %cst_23, %reinterpret_cast_29[%51] : memref<256xf32, #gpu.address_space<workgroup>>
            }
            gpu.barrier
            %64 = arith.cmpi ult, %53, %c16_24 : index
            %65 = arith.addi %58, %c16_24 : index
            %66 = arith.cmpi ult, %65, %dim_18 : index
            %67 = arith.andi %64, %66 : i1
            scf.if %67 {
              %82 = arith.addi %51, %c128_25 : index
              %reinterpret_cast_29 = memref.reinterpret_cast %alloc_28 to offset: [0], sizes: [256], strides: [1] : memref<256xf32, #gpu.address_space<workgroup>> to memref<256xf32, #gpu.address_space<workgroup>>
              %83 = memref.load %reinterpret_cast_29[%51] : memref<256xf32, #gpu.address_space<workgroup>>
              %84 = memref.load %reinterpret_cast_29[%82] : memref<256xf32, #gpu.address_space<workgroup>>
              %85 = arith.cmpf ugt, %83, %84 : f32
              %86 = arith.select %85, %83, %84 : f32
              %87 = arith.cmpf uno, %84, %84 : f32
              %88 = arith.select %87, %84, %86 : f32
              memref.store %88, %reinterpret_cast_29[%51] : memref<256xf32, #gpu.address_space<workgroup>>
            }
            gpu.barrier
            %68 = arith.cmpi ult, %53, %c8_15 : index
            %69 = arith.addi %58, %c8_15 : index
            %70 = arith.cmpi ult, %69, %dim_18 : index
            %71 = arith.andi %68, %70 : i1
            scf.if %71 {
              %82 = arith.addi %51, %c64_26 : index
              %reinterpret_cast_29 = memref.reinterpret_cast %alloc_28 to offset: [0], sizes: [256], strides: [1] : memref<256xf32, #gpu.address_space<workgroup>> to memref<256xf32, #gpu.address_space<workgroup>>
              %83 = memref.load %reinterpret_cast_29[%51] : memref<256xf32, #gpu.address_space<workgroup>>
              %84 = memref.load %reinterpret_cast_29[%82] : memref<256xf32, #gpu.address_space<workgroup>>
              %85 = arith.cmpf ugt, %83, %84 : f32
              %86 = arith.select %85, %83, %84 : f32
              %87 = arith.cmpf uno, %84, %84 : f32
              %88 = arith.select %87, %84, %86 : f32
              memref.store %88, %reinterpret_cast_29[%51] : memref<256xf32, #gpu.address_space<workgroup>>
            }
            gpu.barrier
            %72 = arith.cmpi ult, %53, %c4_27 : index
            %73 = arith.addi %58, %c4_27 : index
            %74 = arith.cmpi ult, %73, %dim_18 : index
            %75 = arith.andi %72, %74 : i1
            scf.if %75 {
              %82 = arith.addi %51, %c32_17 : index
              %reinterpret_cast_29 = memref.reinterpret_cast %alloc_28 to offset: [0], sizes: [256], strides: [1] : memref<256xf32, #gpu.address_space<workgroup>> to memref<256xf32, #gpu.address_space<workgroup>>
              %83 = memref.load %reinterpret_cast_29[%51] : memref<256xf32, #gpu.address_space<workgroup>>
              %84 = memref.load %reinterpret_cast_29[%82] : memref<256xf32, #gpu.address_space<workgroup>>
              %85 = arith.cmpf ugt, %83, %84 : f32
              %86 = arith.select %85, %83, %84 : f32
              %87 = arith.cmpf uno, %84, %84 : f32
              %88 = arith.select %87, %84, %86 : f32
              memref.store %88, %reinterpret_cast_29[%51] : memref<256xf32, #gpu.address_space<workgroup>>
            }
            gpu.barrier
            %76 = arith.cmpi ult, %53, %c2_21 : index
            %77 = arith.addi %58, %c2_21 : index
            %78 = arith.cmpi ult, %77, %dim_18 : index
            %79 = arith.andi %76, %78 : i1
            scf.if %79 {
              %82 = arith.addi %51, %c16_24 : index
              %reinterpret_cast_29 = memref.reinterpret_cast %alloc_28 to offset: [0], sizes: [256], strides: [1] : memref<256xf32, #gpu.address_space<workgroup>> to memref<256xf32, #gpu.address_space<workgroup>>
              %83 = memref.load %reinterpret_cast_29[%51] : memref<256xf32, #gpu.address_space<workgroup>>
              %84 = memref.load %reinterpret_cast_29[%82] : memref<256xf32, #gpu.address_space<workgroup>>
              %85 = arith.cmpf ugt, %83, %84 : f32
              %86 = arith.select %85, %83, %84 : f32
              %87 = arith.cmpf uno, %84, %84 : f32
              %88 = arith.select %87, %84, %86 : f32
              memref.store %88, %reinterpret_cast_29[%51] : memref<256xf32, #gpu.address_space<workgroup>>
            }
            gpu.barrier
            %80 = arith.cmpi eq, %53, %c0_16 : index
            %81 = arith.andi %80, %63 : i1
            scf.if %81 {
              %82 = arith.addi %51, %c8_15 : index
              %reinterpret_cast_29 = memref.reinterpret_cast %alloc_28 to offset: [0], sizes: [256], strides: [1] : memref<256xf32, #gpu.address_space<workgroup>> to memref<256xf32, #gpu.address_space<workgroup>>
              %83 = memref.load %reinterpret_cast_29[%51] : memref<256xf32, #gpu.address_space<workgroup>>
              %84 = memref.load %reinterpret_cast_29[%82] : memref<256xf32, #gpu.address_space<workgroup>>
              %85 = arith.cmpf ugt, %83, %84 : f32
              %86 = arith.select %85, %83, %84 : f32
              %87 = arith.cmpf uno, %84, %84 : f32
              %88 = arith.select %87, %84, %86 : f32
              %89 = memref.generic_atomic_rmw %alloc[%60] : memref<?xf32, "gpu"> {
              ^bb0(%arg13: f32):
                %90 = arith.cmpf ogt, %arg13, %88 : f32
                %91 = arith.select %90, %arg13, %88 : f32
                memref.atomic_yield %91 : f32
              }
            }
          }
          gpu.terminator
        } {SCFToGPU_visited}
        "lmhlo.terminator"() : () -> ()
      }) {disc.device = "gpu", disc.fusion.name = "main_kColReduction_reduce__4_1_0", disc.fusion.tag = "8w32h", disc.fusion_type = "kColReduction", disc_col_reduction_schedule_hint = 1 : i32, disc_thread_per_block_hint = 256 : i32} : () -> ()
    } else {
      "lmhlo.fusion"() ({
        %c0_2 = arith.constant 0 : index
        %c128_3 = arith.constant 128 : index
        %18 = arith.muli %c1, %c128_3 : index
        %c1_4 = arith.constant 1 : index
        %19 = affine.apply #map(%5)[%c0, %18]
        %20 = affine.apply #map(%18)[%c0_2, %c1]
        gpu.launch blocks(%arg1, %arg2, %arg3) in (%arg7 = %19, %arg8 = %c1_4, %arg9 = %c1_4) threads(%arg4, %arg5, %arg6) in (%arg10 = %20, %arg11 = %c1_4, %arg12 = %c1_4) {
          %c0_11 = arith.constant 0 : index
          %c1_12 = arith.constant 1 : index
          %c0_13 = arith.constant 0 : index
          %cst_14 = arith.constant 0xFF800000 : f32
          %43 = affine.apply #map1(%arg1)[%18, %c0_11]
          %44 = affine.apply #map1(%arg4)[%c1_12, %c0_13]
          %45 = arith.addi %44, %43 : index
          %true = arith.constant true
          %46 = arith.muli %44, %c1_12 : index
          %47 = arith.addi %46, %43 : index
          %48 = arith.cmpi ult, %47, %5 : index
          %49 = arith.andi %true, %48 : i1
          scf.if %49 {
            %reinterpret_cast_15 = memref.reinterpret_cast %alloc to offset: [%c0_11], sizes: [%5], strides: [%c1_12] : memref<?xf32, "gpu"> to memref<?xf32, "gpu">
            memref.store %cst_14, %reinterpret_cast_15[%45] : memref<?xf32, "gpu">
          }
          gpu.terminator
        } {SCFToGPU_visited}
        %21 = arith.addi %5, %c-1 : index
        %22 = arith.divsi %21, %c8 : index
        %23 = arith.addi %22, %c1 : index
        %24 = arith.subi %c0, %5 : index
        %25 = arith.divsi %24, %c8 : index
        %26 = arith.subi %c0, %25 : index
        %27 = arith.cmpi sgt, %5, %c0 : index
        %28 = arith.select %27, %23, %26 : index
        %29 = arith.addi %dim_1, %c-1 : index
        %30 = arith.divsi %29, %c16 : index
        %31 = arith.addi %30, %c1 : index
        %32 = arith.subi %c0, %dim_1 : index
        %33 = arith.divsi %32, %c16 : index
        %34 = arith.subi %c0, %33 : index
        %35 = arith.cmpi sgt, %dim_1, %c0 : index
        %36 = arith.select %35, %31, %34 : index
        %37 = arith.muli %28, %36 : index
        %c0_5 = arith.constant 0 : index
        %c1_6 = arith.constant 1 : index
        %c1_7 = arith.constant 1 : index
        %38 = arith.muli %c1_7, %37 : index
        %39 = arith.muli %38, %c128 : index
        %c0_8 = arith.constant 0 : index
        %c128_9 = arith.constant 128 : index
        %40 = arith.muli %c1_6, %c128_9 : index
        %c1_10 = arith.constant 1 : index
        %41 = affine.apply #map(%39)[%c0_5, %40]
        %42 = affine.apply #map(%40)[%c0_8, %c1_6]
        gpu.launch blocks(%arg1, %arg2, %arg3) in (%arg7 = %41, %arg8 = %c1_10, %arg9 = %c1_10) threads(%arg4, %arg5, %arg6) in (%arg10 = %42, %arg11 = %c1_10, %arg12 = %c1_10) {
          %c0_11 = arith.constant 0 : index
          %c1_12 = arith.constant 1 : index
          %c0_13 = arith.constant 0 : index
          %c128_14 = arith.constant 128 : index
          %c8_15 = arith.constant 8 : index
          %c0_16 = arith.constant 0 : index
          %43 = arith.cmpi sgt, %5, %c0_16 : index
          %c16_17 = arith.constant 16 : index
          %dim_18 = memref.dim %1, %c0_16 : memref<?x?x?xf32, "gpu">
          %c1_19 = arith.constant 1 : index
          %dim_20 = memref.dim %1, %c1_19 : memref<?x?x?xf32, "gpu">
          %c2_21 = arith.constant 2 : index
          %dim_22 = memref.dim %1, %c2_21 : memref<?x?x?xf32, "gpu">
          %cst_23 = arith.constant 0xFF800000 : f32
          %c64_24 = arith.constant 64 : index
          %c4_25 = arith.constant 4 : index
          %c32_26 = arith.constant 32 : index
          %44 = affine.apply #map1(%arg1)[%40, %c0_11]
          %45 = affine.apply #map1(%arg4)[%c1_12, %c0_13]
          %46 = arith.addi %45, %44 : index
          %true = arith.constant true
          %47 = arith.muli %45, %c1_12 : index
          %48 = arith.addi %47, %44 : index
          %49 = arith.cmpi ult, %48, %39 : index
          %50 = arith.andi %true, %49 : i1
          scf.if %50 {
            %51 = arith.remsi %46, %c128_14 : index
            %52 = arith.divsi %46, %c128_14 : index
            %53 = arith.divui %51, %c8_15 : index
            %54 = arith.remui %51, %c8_15 : index
            %55 = arith.divui %52, %28 : index
            %56 = arith.remui %52, %28 : index
            %57 = arith.muli %55, %c16_17 : index
            %58 = arith.addi %57, %53 : index
            %59 = arith.muli %56, %c8_15 : index
            %60 = arith.addi %59, %54 : index
            %alloc_27 = memref.alloc() : memref<128xf32, #gpu.address_space<workgroup>>
            %61 = arith.cmpi ult, %58, %dim_18 : index
            %62 = arith.cmpi ult, %60, %5 : index
            %63 = arith.andi %61, %62 : i1
            scf.if %63 {
              %78 = arith.muli %58, %5 : index
              %79 = arith.addi %78, %60 : index
              %80 = arith.muli %dim_18, %dim_20 : index
              %81 = arith.muli %80, %dim_22 : index
              %reinterpret_cast_28 = memref.reinterpret_cast %1 to offset: [%c0_16], sizes: [%81], strides: [%c1_19] : memref<?x?x?xf32, "gpu"> to memref<?xf32, "gpu">
              %82 = memref.load %reinterpret_cast_28[%79] : memref<?xf32, "gpu">
              %83 = math.absf %82 : f32
              %reinterpret_cast_29 = memref.reinterpret_cast %alloc_27 to offset: [0], sizes: [128], strides: [1] : memref<128xf32, #gpu.address_space<workgroup>> to memref<128xf32, #gpu.address_space<workgroup>>
              memref.store %83, %reinterpret_cast_29[%51] : memref<128xf32, #gpu.address_space<workgroup>>
            } else {
              %reinterpret_cast_28 = memref.reinterpret_cast %alloc_27 to offset: [0], sizes: [128], strides: [1] : memref<128xf32, #gpu.address_space<workgroup>> to memref<128xf32, #gpu.address_space<workgroup>>
              memref.store %cst_23, %reinterpret_cast_28[%51] : memref<128xf32, #gpu.address_space<workgroup>>
            }
            gpu.barrier
            %64 = arith.cmpi ult, %53, %c8_15 : index
            %65 = arith.addi %58, %c8_15 : index
            %66 = arith.cmpi ult, %65, %dim_18 : index
            %67 = arith.andi %64, %66 : i1
            scf.if %67 {
              %78 = arith.addi %51, %c64_24 : index
              %reinterpret_cast_28 = memref.reinterpret_cast %alloc_27 to offset: [0], sizes: [128], strides: [1] : memref<128xf32, #gpu.address_space<workgroup>> to memref<128xf32, #gpu.address_space<workgroup>>
              %79 = memref.load %reinterpret_cast_28[%51] : memref<128xf32, #gpu.address_space<workgroup>>
              %80 = memref.load %reinterpret_cast_28[%78] : memref<128xf32, #gpu.address_space<workgroup>>
              %81 = arith.cmpf ugt, %79, %80 : f32
              %82 = arith.select %81, %79, %80 : f32
              %83 = arith.cmpf uno, %80, %80 : f32
              %84 = arith.select %83, %80, %82 : f32
              memref.store %84, %reinterpret_cast_28[%51] : memref<128xf32, #gpu.address_space<workgroup>>
            }
            gpu.barrier
            %68 = arith.cmpi ult, %53, %c4_25 : index
            %69 = arith.addi %58, %c4_25 : index
            %70 = arith.cmpi ult, %69, %dim_18 : index
            %71 = arith.andi %68, %70 : i1
            scf.if %71 {
              %78 = arith.addi %51, %c32_26 : index
              %reinterpret_cast_28 = memref.reinterpret_cast %alloc_27 to offset: [0], sizes: [128], strides: [1] : memref<128xf32, #gpu.address_space<workgroup>> to memref<128xf32, #gpu.address_space<workgroup>>
              %79 = memref.load %reinterpret_cast_28[%51] : memref<128xf32, #gpu.address_space<workgroup>>
              %80 = memref.load %reinterpret_cast_28[%78] : memref<128xf32, #gpu.address_space<workgroup>>
              %81 = arith.cmpf ugt, %79, %80 : f32
              %82 = arith.select %81, %79, %80 : f32
              %83 = arith.cmpf uno, %80, %80 : f32
              %84 = arith.select %83, %80, %82 : f32
              memref.store %84, %reinterpret_cast_28[%51] : memref<128xf32, #gpu.address_space<workgroup>>
            }
            gpu.barrier
            %72 = arith.cmpi ult, %53, %c2_21 : index
            %73 = arith.addi %58, %c2_21 : index
            %74 = arith.cmpi ult, %73, %dim_18 : index
            %75 = arith.andi %72, %74 : i1
            scf.if %75 {
              %78 = arith.addi %51, %c16_17 : index
              %reinterpret_cast_28 = memref.reinterpret_cast %alloc_27 to offset: [0], sizes: [128], strides: [1] : memref<128xf32, #gpu.address_space<workgroup>> to memref<128xf32, #gpu.address_space<workgroup>>
              %79 = memref.load %reinterpret_cast_28[%51] : memref<128xf32, #gpu.address_space<workgroup>>
              %80 = memref.load %reinterpret_cast_28[%78] : memref<128xf32, #gpu.address_space<workgroup>>
              %81 = arith.cmpf ugt, %79, %80 : f32
              %82 = arith.select %81, %79, %80 : f32
              %83 = arith.cmpf uno, %80, %80 : f32
              %84 = arith.select %83, %80, %82 : f32
              memref.store %84, %reinterpret_cast_28[%51] : memref<128xf32, #gpu.address_space<workgroup>>
            }
            gpu.barrier
            %76 = arith.cmpi eq, %53, %c0_16 : index
            %77 = arith.andi %76, %63 : i1
            scf.if %77 {
              %78 = arith.addi %51, %c8_15 : index
              %reinterpret_cast_28 = memref.reinterpret_cast %alloc_27 to offset: [0], sizes: [128], strides: [1] : memref<128xf32, #gpu.address_space<workgroup>> to memref<128xf32, #gpu.address_space<workgroup>>
              %79 = memref.load %reinterpret_cast_28[%51] : memref<128xf32, #gpu.address_space<workgroup>>
              %80 = memref.load %reinterpret_cast_28[%78] : memref<128xf32, #gpu.address_space<workgroup>>
              %81 = arith.cmpf ugt, %79, %80 : f32
              %82 = arith.select %81, %79, %80 : f32
              %83 = arith.cmpf uno, %80, %80 : f32
              %84 = arith.select %83, %80, %82 : f32
              %85 = memref.generic_atomic_rmw %alloc[%60] : memref<?xf32, "gpu"> {
              ^bb0(%arg13: f32):
                %86 = arith.cmpf ogt, %arg13, %84 : f32
                %87 = arith.select %86, %arg13, %84 : f32
                memref.atomic_yield %87 : f32
              }
            }
          }
          gpu.terminator
        } {SCFToGPU_visited}
        "lmhlo.terminator"() : () -> ()
      }) {disc.device = "gpu", disc.fusion.name = "main_kColReduction_reduce__4_1_0", disc.fusion.tag = "8w16h", disc.fusion_type = "kColReduction", disc_col_reduction_schedule_hint = 2 : i32, disc_thread_per_block_hint = 128 : i32} : () -> ()
    }
    %16 = llvm.inttoptr %0 : i32 to !llvm.ptr<i8>
    %alloca = memref.alloca() : memref<2xindex, "cpu">
    memref.store %dim_0, %alloca[%c0] : memref<2xindex, "cpu">
    memref.store %dim, %alloca[%c1] : memref<2xindex, "cpu">
    %17 = "disc_ral.dispatch"(%arg0, %16, %alloc, %alloca) {backend_config = "", call_target_name = "inc_ref", device = "gpu", has_side_effect = false} : (!disc_ral.context, !llvm.ptr<i8>, memref<?xf32, "gpu">, memref<2xindex, "cpu">) -> memref<?x?xf32, "gpu">
    %reinterpret_cast = memref.reinterpret_cast %17 to offset: [0], sizes: [%dim_0, %dim], strides: [%dim, 1] {kDiscSymbolicDimAttr = [@S1, @S2]} : memref<?x?xf32, "gpu"> to memref<?x?xf32, "gpu">
    memref.dealloc %alloc : memref<?xf32, "gpu">
    "disc_ral.dispatch"(%arg0, %c0, %reinterpret_cast) {backend_config = "", call_target_name = "ral_send_output", device = "cpu", has_side_effect = false} : (!disc_ral.context, index, memref<?x?xf32, "gpu">) -> ()
    return
  }
  "disc_shape.SymbolicDim"() {knownNegativeOne = false, knownNonNegative = true, knownNonSizeOne = false, knownNonSizeZero = false, sym_name = "S0", value = -9223372036854775808 : i64} : () -> ()
  "disc_shape.SymbolicDim"() {knownNegativeOne = false, knownNonNegative = true, knownNonSizeOne = false, knownNonSizeZero = false, sym_name = "S1", value = -9223372036854775808 : i64} : () -> ()
  "disc_shape.SymbolicDim"() {knownNegativeOne = false, knownNonNegative = true, knownNonSizeOne = false, knownNonSizeZero = false, sym_name = "S2", value = -9223372036854775808 : i64} : () -> ()
  "disc_shape.SymbolicDim"() {knownNegativeOne = false, knownNonNegative = true, knownNonSizeOne = false, knownNonSizeZero = false, sym_name = "S3", value = -9223372036854775808 : i64} : () -> ()
  func.func @shape_constraint_graph() {
    %0 = "disc_shape.dim"() {name = @S3} : () -> index
    %1 = "disc_shape.dim"() {name = @S1} : () -> index
    %2 = "disc_shape.dim"() {name = @S2} : () -> index
    "disc_shape.tie_product_equal"(%0, %1, %2) {operand_segment_sizes = array<i32: 1, 2>} : (index, index, index) -> ()
    return
  }
}


// -----// IR Dump After GpuKernelOutlining (gpu-kernel-outlining) //----- //
#map = affine_map<(d0)[s0, s1] -> ((d0 - s0) ceildiv s1)>
#map1 = affine_map<(d0)[s0, s1] -> (d0 * s0 + s1)>
module attributes {gpu.container_module} {
  func.func @main(%arg0: !disc_ral.context) attributes {tf.entry_function = {input_placements = "gpu", inputs = "input0", output_placements = "gpu", outputs = "output0"}} {
    %c-1 = arith.constant -1 : index
    %cst = arith.constant 0xFF800000 : f32
    %c64 = arith.constant 64 : index
    %c128 = arith.constant 128 : index
    %c4 = arith.constant 4 : index
    %c16 = arith.constant 16 : index
    %c32 = arith.constant 32 : index
    %c8 = arith.constant 8 : index
    %0 = llvm.mlir.constant(0 : i32) : i32
    %c0 = arith.constant 0 : index
    %c1 = arith.constant 1 : index
    %c2 = arith.constant 2 : index
    %c256 = arith.constant 256 : index
    %c108 = arith.constant 108 : index
    %1 = "disc_ral.dispatch"(%arg0, %c0) {backend_config = "", call_target_name = "ral_recv_input", device = "cpu", has_side_effect = false} : (!disc_ral.context, index) -> memref<?x?x?xf32, "gpu">
    %dim = memref.dim %1, %c2 : memref<?x?x?xf32, "gpu">
    %dim_0 = memref.dim %1, %c1 : memref<?x?x?xf32, "gpu">
    %dim_1 = memref.dim %1, %c0 : memref<?x?x?xf32, "gpu">
    %2 = arith.index_cast %dim_0 : index to i32
    %3 = arith.index_cast %dim : index to i32
    %4 = arith.muli %2, %3 : i32
    %5 = arith.index_cast %4 : i32 to index
    %alloc = memref.alloc(%5) {kDiscSymbolicDimAttr = [@S3]} : memref<?xf32, "gpu">
    %6 = arith.muli %dim_1, %5 : index
    %7 = arith.addi %6, %c-1 : index
    %8 = arith.divsi %7, %c256 : index
    %9 = arith.addi %8, %c1 : index
    %10 = arith.subi %c0, %6 : index
    %11 = arith.divsi %10, %c256 : index
    %12 = arith.subi %c0, %11 : index
    %13 = arith.cmpi sgt, %6, %c0 : index
    %14 = arith.select %13, %9, %12 : index
    %15 = arith.cmpi sgt, %14, %c108 : index
    scf.if %15 {
      "lmhlo.fusion"() ({
        %c0_2 = arith.constant 0 : index
        %c256_3 = arith.constant 256 : index
        %18 = arith.muli %c1, %c256_3 : index
        %c1_4 = arith.constant 1 : index
        %19 = affine.apply #map(%5)[%c0, %18]
        %20 = affine.apply #map(%18)[%c0_2, %c1]
        gpu.launch_func  @main_kernel::@main_kernel blocks in (%19, %c1_4, %c1_4) threads in (%20, %c1_4, %c1_4) args(%18 : index, %5 : index, %alloc : memref<?xf32, "gpu">)
        %21 = arith.addi %5, %c-1 : index
        %22 = arith.divsi %21, %c8 : index
        %23 = arith.addi %22, %c1 : index
        %24 = arith.subi %c0, %5 : index
        %25 = arith.divsi %24, %c8 : index
        %26 = arith.subi %c0, %25 : index
        %27 = arith.cmpi sgt, %5, %c0 : index
        %28 = arith.select %27, %23, %26 : index
        %29 = arith.addi %dim_1, %c-1 : index
        %30 = arith.divsi %29, %c32 : index
        %31 = arith.addi %30, %c1 : index
        %32 = arith.subi %c0, %dim_1 : index
        %33 = arith.divsi %32, %c32 : index
        %34 = arith.subi %c0, %33 : index
        %35 = arith.cmpi sgt, %dim_1, %c0 : index
        %36 = arith.select %35, %31, %34 : index
        %37 = arith.muli %28, %36 : index
        %c0_5 = arith.constant 0 : index
        %c1_6 = arith.constant 1 : index
        %c1_7 = arith.constant 1 : index
        %38 = arith.muli %c1_7, %37 : index
        %39 = arith.muli %38, %c256 : index
        %c0_8 = arith.constant 0 : index
        %c256_9 = arith.constant 256 : index
        %40 = arith.muli %c1_6, %c256_9 : index
        %c1_10 = arith.constant 1 : index
        %41 = affine.apply #map(%39)[%c0_5, %40]
        %42 = affine.apply #map(%40)[%c0_8, %c1_6]
        gpu.launch_func  @main_kernel_0::@main_kernel blocks in (%41, %c1_10, %c1_10) threads in (%42, %c1_10, %c1_10) args(%5 : index, %1 : memref<?x?x?xf32, "gpu">, %40 : index, %39 : index, %28 : index, %alloc : memref<?xf32, "gpu">)
        "lmhlo.terminator"() : () -> ()
      }) {disc.device = "gpu", disc.fusion.name = "main_kColReduction_reduce__4_1_0", disc.fusion.tag = "8w32h", disc.fusion_type = "kColReduction", disc_col_reduction_schedule_hint = 1 : i32, disc_thread_per_block_hint = 256 : i32} : () -> ()
    } else {
      "lmhlo.fusion"() ({
        %c0_2 = arith.constant 0 : index
        %c128_3 = arith.constant 128 : index
        %18 = arith.muli %c1, %c128_3 : index
        %c1_4 = arith.constant 1 : index
        %19 = affine.apply #map(%5)[%c0, %18]
        %20 = affine.apply #map(%18)[%c0_2, %c1]
        gpu.launch_func  @main_kernel_1::@main_kernel blocks in (%19, %c1_4, %c1_4) threads in (%20, %c1_4, %c1_4) args(%18 : index, %5 : index, %alloc : memref<?xf32, "gpu">)
        %21 = arith.addi %5, %c-1 : index
        %22 = arith.divsi %21, %c8 : index
        %23 = arith.addi %22, %c1 : index
        %24 = arith.subi %c0, %5 : index
        %25 = arith.divsi %24, %c8 : index
        %26 = arith.subi %c0, %25 : index
        %27 = arith.cmpi sgt, %5, %c0 : index
        %28 = arith.select %27, %23, %26 : index
        %29 = arith.addi %dim_1, %c-1 : index
        %30 = arith.divsi %29, %c16 : index
        %31 = arith.addi %30, %c1 : index
        %32 = arith.subi %c0, %dim_1 : index
        %33 = arith.divsi %32, %c16 : index
        %34 = arith.subi %c0, %33 : index
        %35 = arith.cmpi sgt, %dim_1, %c0 : index
        %36 = arith.select %35, %31, %34 : index
        %37 = arith.muli %28, %36 : index
        %c0_5 = arith.constant 0 : index
        %c1_6 = arith.constant 1 : index
        %c1_7 = arith.constant 1 : index
        %38 = arith.muli %c1_7, %37 : index
        %39 = arith.muli %38, %c128 : index
        %c0_8 = arith.constant 0 : index
        %c128_9 = arith.constant 128 : index
        %40 = arith.muli %c1_6, %c128_9 : index
        %c1_10 = arith.constant 1 : index
        %41 = affine.apply #map(%39)[%c0_5, %40]
        %42 = affine.apply #map(%40)[%c0_8, %c1_6]
        gpu.launch_func  @main_kernel_2::@main_kernel blocks in (%41, %c1_10, %c1_10) threads in (%42, %c1_10, %c1_10) args(%5 : index, %1 : memref<?x?x?xf32, "gpu">, %40 : index, %39 : index, %28 : index, %alloc : memref<?xf32, "gpu">)
        "lmhlo.terminator"() : () -> ()
      }) {disc.device = "gpu", disc.fusion.name = "main_kColReduction_reduce__4_1_0", disc.fusion.tag = "8w16h", disc.fusion_type = "kColReduction", disc_col_reduction_schedule_hint = 2 : i32, disc_thread_per_block_hint = 128 : i32} : () -> ()
    }
    %16 = llvm.inttoptr %0 : i32 to !llvm.ptr<i8>
    %alloca = memref.alloca() : memref<2xindex, "cpu">
    memref.store %dim_0, %alloca[%c0] : memref<2xindex, "cpu">
    memref.store %dim, %alloca[%c1] : memref<2xindex, "cpu">
    %17 = "disc_ral.dispatch"(%arg0, %16, %alloc, %alloca) {backend_config = "", call_target_name = "inc_ref", device = "gpu", has_side_effect = false} : (!disc_ral.context, !llvm.ptr<i8>, memref<?xf32, "gpu">, memref<2xindex, "cpu">) -> memref<?x?xf32, "gpu">
    %reinterpret_cast = memref.reinterpret_cast %17 to offset: [0], sizes: [%dim_0, %dim], strides: [%dim, 1] {kDiscSymbolicDimAttr = [@S1, @S2]} : memref<?x?xf32, "gpu"> to memref<?x?xf32, "gpu">
    memref.dealloc %alloc : memref<?xf32, "gpu">
    "disc_ral.dispatch"(%arg0, %c0, %reinterpret_cast) {backend_config = "", call_target_name = "ral_send_output", device = "cpu", has_side_effect = false} : (!disc_ral.context, index, memref<?x?xf32, "gpu">) -> ()
    return
  }
  gpu.module @main_kernel {
    gpu.func @main_kernel(%arg0: index, %arg1: index, %arg2: memref<?xf32, "gpu">) kernel {
      %0 = gpu.block_id  x
      %1 = gpu.block_id  y
      %2 = gpu.block_id  z
      %3 = gpu.thread_id  x
      %4 = gpu.thread_id  y
      %5 = gpu.thread_id  z
      %6 = gpu.grid_dim  x
      %7 = gpu.grid_dim  y
      %8 = gpu.grid_dim  z
      %9 = gpu.block_dim  x
      %10 = gpu.block_dim  y
      %11 = gpu.block_dim  z
      cf.br ^bb1
    ^bb1:  // pred: ^bb0
      %c0 = arith.constant 0 : index
      %c1 = arith.constant 1 : index
      %c0_0 = arith.constant 0 : index
      %cst = arith.constant 0xFF800000 : f32
      %12 = affine.apply #map1(%0)[%arg0, %c0]
      %13 = affine.apply #map1(%3)[%c1, %c0_0]
      %14 = arith.addi %13, %12 : index
      %true = arith.constant true
      %15 = arith.muli %13, %c1 : index
      %16 = arith.addi %15, %12 : index
      %17 = arith.cmpi ult, %16, %arg1 : index
      %18 = arith.andi %true, %17 : i1
      scf.if %18 {
        %reinterpret_cast = memref.reinterpret_cast %arg2 to offset: [%c0], sizes: [%arg1], strides: [%c1] : memref<?xf32, "gpu"> to memref<?xf32, "gpu">
        memref.store %cst, %reinterpret_cast[%14] : memref<?xf32, "gpu">
      }
      gpu.return
    }
  }
  gpu.module @main_kernel_0 {
    gpu.func @main_kernel(%arg0: index, %arg1: memref<?x?x?xf32, "gpu">, %arg2: index, %arg3: index, %arg4: index, %arg5: memref<?xf32, "gpu">) kernel {
      %0 = gpu.block_id  x
      %1 = gpu.block_id  y
      %2 = gpu.block_id  z
      %3 = gpu.thread_id  x
      %4 = gpu.thread_id  y
      %5 = gpu.thread_id  z
      %6 = gpu.grid_dim  x
      %7 = gpu.grid_dim  y
      %8 = gpu.grid_dim  z
      %9 = gpu.block_dim  x
      %10 = gpu.block_dim  y
      %11 = gpu.block_dim  z
      cf.br ^bb1
    ^bb1:  // pred: ^bb0
      %c0 = arith.constant 0 : index
      %c1 = arith.constant 1 : index
      %c0_0 = arith.constant 0 : index
      %c256 = arith.constant 256 : index
      %c8 = arith.constant 8 : index
      %c0_1 = arith.constant 0 : index
      %12 = arith.cmpi sgt, %arg0, %c0_1 : index
      %c32 = arith.constant 32 : index
      %dim = memref.dim %arg1, %c0_1 : memref<?x?x?xf32, "gpu">
      %c1_2 = arith.constant 1 : index
      %dim_3 = memref.dim %arg1, %c1_2 : memref<?x?x?xf32, "gpu">
      %c2 = arith.constant 2 : index
      %dim_4 = memref.dim %arg1, %c2 : memref<?x?x?xf32, "gpu">
      %cst = arith.constant 0xFF800000 : f32
      %c16 = arith.constant 16 : index
      %c128 = arith.constant 128 : index
      %c64 = arith.constant 64 : index
      %c4 = arith.constant 4 : index
      %13 = affine.apply #map1(%0)[%arg2, %c0]
      %14 = affine.apply #map1(%3)[%c1, %c0_0]
      %15 = arith.addi %14, %13 : index
      %true = arith.constant true
      %16 = arith.muli %14, %c1 : index
      %17 = arith.addi %16, %13 : index
      %18 = arith.cmpi ult, %17, %arg3 : index
      %19 = arith.andi %true, %18 : i1
      scf.if %19 {
        %20 = arith.remsi %15, %c256 : index
        %21 = arith.divsi %15, %c256 : index
        %22 = arith.divui %20, %c8 : index
        %23 = arith.remui %20, %c8 : index
        %24 = arith.divui %21, %arg4 : index
        %25 = arith.remui %21, %arg4 : index
        %26 = arith.muli %24, %c32 : index
        %27 = arith.addi %26, %22 : index
        %28 = arith.muli %25, %c8 : index
        %29 = arith.addi %28, %23 : index
        %alloc = memref.alloc() : memref<256xf32, #gpu.address_space<workgroup>>
        %30 = arith.cmpi ult, %27, %dim : index
        %31 = arith.cmpi ult, %29, %arg0 : index
        %32 = arith.andi %30, %31 : i1
        scf.if %32 {
          %51 = arith.muli %27, %arg0 : index
          %52 = arith.addi %51, %29 : index
          %53 = arith.muli %dim, %dim_3 : index
          %54 = arith.muli %53, %dim_4 : index
          %reinterpret_cast = memref.reinterpret_cast %arg1 to offset: [%c0_1], sizes: [%54], strides: [%c1_2] : memref<?x?x?xf32, "gpu"> to memref<?xf32, "gpu">
          %55 = memref.load %reinterpret_cast[%52] : memref<?xf32, "gpu">
          %56 = math.absf %55 : f32
          %reinterpret_cast_5 = memref.reinterpret_cast %alloc to offset: [0], sizes: [256], strides: [1] : memref<256xf32, #gpu.address_space<workgroup>> to memref<256xf32, #gpu.address_space<workgroup>>
          memref.store %56, %reinterpret_cast_5[%20] : memref<256xf32, #gpu.address_space<workgroup>>
        } else {
          %reinterpret_cast = memref.reinterpret_cast %alloc to offset: [0], sizes: [256], strides: [1] : memref<256xf32, #gpu.address_space<workgroup>> to memref<256xf32, #gpu.address_space<workgroup>>
          memref.store %cst, %reinterpret_cast[%20] : memref<256xf32, #gpu.address_space<workgroup>>
        }
        gpu.barrier
        %33 = arith.cmpi ult, %22, %c16 : index
        %34 = arith.addi %27, %c16 : index
        %35 = arith.cmpi ult, %34, %dim : index
        %36 = arith.andi %33, %35 : i1
        scf.if %36 {
          %51 = arith.addi %20, %c128 : index
          %reinterpret_cast = memref.reinterpret_cast %alloc to offset: [0], sizes: [256], strides: [1] : memref<256xf32, #gpu.address_space<workgroup>> to memref<256xf32, #gpu.address_space<workgroup>>
          %52 = memref.load %reinterpret_cast[%20] : memref<256xf32, #gpu.address_space<workgroup>>
          %53 = memref.load %reinterpret_cast[%51] : memref<256xf32, #gpu.address_space<workgroup>>
          %54 = arith.cmpf ugt, %52, %53 : f32
          %55 = arith.select %54, %52, %53 : f32
          %56 = arith.cmpf uno, %53, %53 : f32
          %57 = arith.select %56, %53, %55 : f32
          memref.store %57, %reinterpret_cast[%20] : memref<256xf32, #gpu.address_space<workgroup>>
        }
        gpu.barrier
        %37 = arith.cmpi ult, %22, %c8 : index
        %38 = arith.addi %27, %c8 : index
        %39 = arith.cmpi ult, %38, %dim : index
        %40 = arith.andi %37, %39 : i1
        scf.if %40 {
          %51 = arith.addi %20, %c64 : index
          %reinterpret_cast = memref.reinterpret_cast %alloc to offset: [0], sizes: [256], strides: [1] : memref<256xf32, #gpu.address_space<workgroup>> to memref<256xf32, #gpu.address_space<workgroup>>
          %52 = memref.load %reinterpret_cast[%20] : memref<256xf32, #gpu.address_space<workgroup>>
          %53 = memref.load %reinterpret_cast[%51] : memref<256xf32, #gpu.address_space<workgroup>>
          %54 = arith.cmpf ugt, %52, %53 : f32
          %55 = arith.select %54, %52, %53 : f32
          %56 = arith.cmpf uno, %53, %53 : f32
          %57 = arith.select %56, %53, %55 : f32
          memref.store %57, %reinterpret_cast[%20] : memref<256xf32, #gpu.address_space<workgroup>>
        }
        gpu.barrier
        %41 = arith.cmpi ult, %22, %c4 : index
        %42 = arith.addi %27, %c4 : index
        %43 = arith.cmpi ult, %42, %dim : index
        %44 = arith.andi %41, %43 : i1
        scf.if %44 {
          %51 = arith.addi %20, %c32 : index
          %reinterpret_cast = memref.reinterpret_cast %alloc to offset: [0], sizes: [256], strides: [1] : memref<256xf32, #gpu.address_space<workgroup>> to memref<256xf32, #gpu.address_space<workgroup>>
          %52 = memref.load %reinterpret_cast[%20] : memref<256xf32, #gpu.address_space<workgroup>>
          %53 = memref.load %reinterpret_cast[%51] : memref<256xf32, #gpu.address_space<workgroup>>
          %54 = arith.cmpf ugt, %52, %53 : f32
          %55 = arith.select %54, %52, %53 : f32
          %56 = arith.cmpf uno, %53, %53 : f32
          %57 = arith.select %56, %53, %55 : f32
          memref.store %57, %reinterpret_cast[%20] : memref<256xf32, #gpu.address_space<workgroup>>
        }
        gpu.barrier
        %45 = arith.cmpi ult, %22, %c2 : index
        %46 = arith.addi %27, %c2 : index
        %47 = arith.cmpi ult, %46, %dim : index
        %48 = arith.andi %45, %47 : i1
        scf.if %48 {
          %51 = arith.addi %20, %c16 : index
          %reinterpret_cast = memref.reinterpret_cast %alloc to offset: [0], sizes: [256], strides: [1] : memref<256xf32, #gpu.address_space<workgroup>> to memref<256xf32, #gpu.address_space<workgroup>>
          %52 = memref.load %reinterpret_cast[%20] : memref<256xf32, #gpu.address_space<workgroup>>
          %53 = memref.load %reinterpret_cast[%51] : memref<256xf32, #gpu.address_space<workgroup>>
          %54 = arith.cmpf ugt, %52, %53 : f32
          %55 = arith.select %54, %52, %53 : f32
          %56 = arith.cmpf uno, %53, %53 : f32
          %57 = arith.select %56, %53, %55 : f32
          memref.store %57, %reinterpret_cast[%20] : memref<256xf32, #gpu.address_space<workgroup>>
        }
        gpu.barrier
        %49 = arith.cmpi eq, %22, %c0_1 : index
        %50 = arith.andi %49, %32 : i1
        scf.if %50 {
          %51 = arith.addi %20, %c8 : index
          %reinterpret_cast = memref.reinterpret_cast %alloc to offset: [0], sizes: [256], strides: [1] : memref<256xf32, #gpu.address_space<workgroup>> to memref<256xf32, #gpu.address_space<workgroup>>
          %52 = memref.load %reinterpret_cast[%20] : memref<256xf32, #gpu.address_space<workgroup>>
          %53 = memref.load %reinterpret_cast[%51] : memref<256xf32, #gpu.address_space<workgroup>>
          %54 = arith.cmpf ugt, %52, %53 : f32
          %55 = arith.select %54, %52, %53 : f32
          %56 = arith.cmpf uno, %53, %53 : f32
          %57 = arith.select %56, %53, %55 : f32
          %58 = memref.generic_atomic_rmw %arg5[%29] : memref<?xf32, "gpu"> {
          ^bb0(%arg6: f32):
            %59 = arith.cmpf ogt, %arg6, %57 : f32
            %60 = arith.select %59, %arg6, %57 : f32
            memref.atomic_yield %60 : f32
          }
        }
      }
      gpu.return
    }
  }
  gpu.module @main_kernel_1 {
    gpu.func @main_kernel(%arg0: index, %arg1: index, %arg2: memref<?xf32, "gpu">) kernel {
      %0 = gpu.block_id  x
      %1 = gpu.block_id  y
      %2 = gpu.block_id  z
      %3 = gpu.thread_id  x
      %4 = gpu.thread_id  y
      %5 = gpu.thread_id  z
      %6 = gpu.grid_dim  x
      %7 = gpu.grid_dim  y
      %8 = gpu.grid_dim  z
      %9 = gpu.block_dim  x
      %10 = gpu.block_dim  y
      %11 = gpu.block_dim  z
      cf.br ^bb1
    ^bb1:  // pred: ^bb0
      %c0 = arith.constant 0 : index
      %c1 = arith.constant 1 : index
      %c0_0 = arith.constant 0 : index
      %cst = arith.constant 0xFF800000 : f32
      %12 = affine.apply #map1(%0)[%arg0, %c0]
      %13 = affine.apply #map1(%3)[%c1, %c0_0]
      %14 = arith.addi %13, %12 : index
      %true = arith.constant true
      %15 = arith.muli %13, %c1 : index
      %16 = arith.addi %15, %12 : index
      %17 = arith.cmpi ult, %16, %arg1 : index
      %18 = arith.andi %true, %17 : i1
      scf.if %18 {
        %reinterpret_cast = memref.reinterpret_cast %arg2 to offset: [%c0], sizes: [%arg1], strides: [%c1] : memref<?xf32, "gpu"> to memref<?xf32, "gpu">
        memref.store %cst, %reinterpret_cast[%14] : memref<?xf32, "gpu">
      }
      gpu.return
    }
  }
  gpu.module @main_kernel_2 {
    gpu.func @main_kernel(%arg0: index, %arg1: memref<?x?x?xf32, "gpu">, %arg2: index, %arg3: index, %arg4: index, %arg5: memref<?xf32, "gpu">) kernel {
      %0 = gpu.block_id  x
      %1 = gpu.block_id  y
      %2 = gpu.block_id  z
      %3 = gpu.thread_id  x
      %4 = gpu.thread_id  y
      %5 = gpu.thread_id  z
      %6 = gpu.grid_dim  x
      %7 = gpu.grid_dim  y
      %8 = gpu.grid_dim  z
      %9 = gpu.block_dim  x
      %10 = gpu.block_dim  y
      %11 = gpu.block_dim  z
      cf.br ^bb1
    ^bb1:  // pred: ^bb0
      %c0 = arith.constant 0 : index
      %c1 = arith.constant 1 : index
      %c0_0 = arith.constant 0 : index
      %c128 = arith.constant 128 : index
      %c8 = arith.constant 8 : index
      %c0_1 = arith.constant 0 : index
      %12 = arith.cmpi sgt, %arg0, %c0_1 : index
      %c16 = arith.constant 16 : index
      %dim = memref.dim %arg1, %c0_1 : memref<?x?x?xf32, "gpu">
      %c1_2 = arith.constant 1 : index
      %dim_3 = memref.dim %arg1, %c1_2 : memref<?x?x?xf32, "gpu">
      %c2 = arith.constant 2 : index
      %dim_4 = memref.dim %arg1, %c2 : memref<?x?x?xf32, "gpu">
      %cst = arith.constant 0xFF800000 : f32
      %c64 = arith.constant 64 : index
      %c4 = arith.constant 4 : index
      %c32 = arith.constant 32 : index
      %13 = affine.apply #map1(%0)[%arg2, %c0]
      %14 = affine.apply #map1(%3)[%c1, %c0_0]
      %15 = arith.addi %14, %13 : index
      %true = arith.constant true
      %16 = arith.muli %14, %c1 : index
      %17 = arith.addi %16, %13 : index
      %18 = arith.cmpi ult, %17, %arg3 : index
      %19 = arith.andi %true, %18 : i1
      scf.if %19 {
        %20 = arith.remsi %15, %c128 : index
        %21 = arith.divsi %15, %c128 : index
        %22 = arith.divui %20, %c8 : index
        %23 = arith.remui %20, %c8 : index
        %24 = arith.divui %21, %arg4 : index
        %25 = arith.remui %21, %arg4 : index
        %26 = arith.muli %24, %c16 : index
        %27 = arith.addi %26, %22 : index
        %28 = arith.muli %25, %c8 : index
        %29 = arith.addi %28, %23 : index
        %alloc = memref.alloc() : memref<128xf32, #gpu.address_space<workgroup>>
        %30 = arith.cmpi ult, %27, %dim : index
        %31 = arith.cmpi ult, %29, %arg0 : index
        %32 = arith.andi %30, %31 : i1
        scf.if %32 {
          %47 = arith.muli %27, %arg0 : index
          %48 = arith.addi %47, %29 : index
          %49 = arith.muli %dim, %dim_3 : index
          %50 = arith.muli %49, %dim_4 : index
          %reinterpret_cast = memref.reinterpret_cast %arg1 to offset: [%c0_1], sizes: [%50], strides: [%c1_2] : memref<?x?x?xf32, "gpu"> to memref<?xf32, "gpu">
          %51 = memref.load %reinterpret_cast[%48] : memref<?xf32, "gpu">
          %52 = math.absf %51 : f32
          %reinterpret_cast_5 = memref.reinterpret_cast %alloc to offset: [0], sizes: [128], strides: [1] : memref<128xf32, #gpu.address_space<workgroup>> to memref<128xf32, #gpu.address_space<workgroup>>
          memref.store %52, %reinterpret_cast_5[%20] : memref<128xf32, #gpu.address_space<workgroup>>
        } else {
          %reinterpret_cast = memref.reinterpret_cast %alloc to offset: [0], sizes: [128], strides: [1] : memref<128xf32, #gpu.address_space<workgroup>> to memref<128xf32, #gpu.address_space<workgroup>>
          memref.store %cst, %reinterpret_cast[%20] : memref<128xf32, #gpu.address_space<workgroup>>
        }
        gpu.barrier
        %33 = arith.cmpi ult, %22, %c8 : index
        %34 = arith.addi %27, %c8 : index
        %35 = arith.cmpi ult, %34, %dim : index
        %36 = arith.andi %33, %35 : i1
        scf.if %36 {
          %47 = arith.addi %20, %c64 : index
          %reinterpret_cast = memref.reinterpret_cast %alloc to offset: [0], sizes: [128], strides: [1] : memref<128xf32, #gpu.address_space<workgroup>> to memref<128xf32, #gpu.address_space<workgroup>>
          %48 = memref.load %reinterpret_cast[%20] : memref<128xf32, #gpu.address_space<workgroup>>
          %49 = memref.load %reinterpret_cast[%47] : memref<128xf32, #gpu.address_space<workgroup>>
          %50 = arith.cmpf ugt, %48, %49 : f32
          %51 = arith.select %50, %48, %49 : f32
          %52 = arith.cmpf uno, %49, %49 : f32
          %53 = arith.select %52, %49, %51 : f32
          memref.store %53, %reinterpret_cast[%20] : memref<128xf32, #gpu.address_space<workgroup>>
        }
        gpu.barrier
        %37 = arith.cmpi ult, %22, %c4 : index
        %38 = arith.addi %27, %c4 : index
        %39 = arith.cmpi ult, %38, %dim : index
        %40 = arith.andi %37, %39 : i1
        scf.if %40 {
          %47 = arith.addi %20, %c32 : index
          %reinterpret_cast = memref.reinterpret_cast %alloc to offset: [0], sizes: [128], strides: [1] : memref<128xf32, #gpu.address_space<workgroup>> to memref<128xf32, #gpu.address_space<workgroup>>
          %48 = memref.load %reinterpret_cast[%20] : memref<128xf32, #gpu.address_space<workgroup>>
          %49 = memref.load %reinterpret_cast[%47] : memref<128xf32, #gpu.address_space<workgroup>>
          %50 = arith.cmpf ugt, %48, %49 : f32
          %51 = arith.select %50, %48, %49 : f32
          %52 = arith.cmpf uno, %49, %49 : f32
          %53 = arith.select %52, %49, %51 : f32
          memref.store %53, %reinterpret_cast[%20] : memref<128xf32, #gpu.address_space<workgroup>>
        }
        gpu.barrier
        %41 = arith.cmpi ult, %22, %c2 : index
        %42 = arith.addi %27, %c2 : index
        %43 = arith.cmpi ult, %42, %dim : index
        %44 = arith.andi %41, %43 : i1
        scf.if %44 {
          %47 = arith.addi %20, %c16 : index
          %reinterpret_cast = memref.reinterpret_cast %alloc to offset: [0], sizes: [128], strides: [1] : memref<128xf32, #gpu.address_space<workgroup>> to memref<128xf32, #gpu.address_space<workgroup>>
          %48 = memref.load %reinterpret_cast[%20] : memref<128xf32, #gpu.address_space<workgroup>>
          %49 = memref.load %reinterpret_cast[%47] : memref<128xf32, #gpu.address_space<workgroup>>
          %50 = arith.cmpf ugt, %48, %49 : f32
          %51 = arith.select %50, %48, %49 : f32
          %52 = arith.cmpf uno, %49, %49 : f32
          %53 = arith.select %52, %49, %51 : f32
          memref.store %53, %reinterpret_cast[%20] : memref<128xf32, #gpu.address_space<workgroup>>
        }
        gpu.barrier
        %45 = arith.cmpi eq, %22, %c0_1 : index
        %46 = arith.andi %45, %32 : i1
        scf.if %46 {
          %47 = arith.addi %20, %c8 : index
          %reinterpret_cast = memref.reinterpret_cast %alloc to offset: [0], sizes: [128], strides: [1] : memref<128xf32, #gpu.address_space<workgroup>> to memref<128xf32, #gpu.address_space<workgroup>>
          %48 = memref.load %reinterpret_cast[%20] : memref<128xf32, #gpu.address_space<workgroup>>
          %49 = memref.load %reinterpret_cast[%47] : memref<128xf32, #gpu.address_space<workgroup>>
          %50 = arith.cmpf ugt, %48, %49 : f32
          %51 = arith.select %50, %48, %49 : f32
          %52 = arith.cmpf uno, %49, %49 : f32
          %53 = arith.select %52, %49, %51 : f32
          %54 = memref.generic_atomic_rmw %arg5[%29] : memref<?xf32, "gpu"> {
          ^bb0(%arg6: f32):
            %55 = arith.cmpf ogt, %arg6, %53 : f32
            %56 = arith.select %55, %arg6, %53 : f32
            memref.atomic_yield %56 : f32
          }
        }
      }
      gpu.return
    }
  }
  "disc_shape.SymbolicDim"() {knownNegativeOne = false, knownNonNegative = true, knownNonSizeOne = false, knownNonSizeZero = false, sym_name = "S0", value = -9223372036854775808 : i64} : () -> ()
  "disc_shape.SymbolicDim"() {knownNegativeOne = false, knownNonNegative = true, knownNonSizeOne = false, knownNonSizeZero = false, sym_name = "S1", value = -9223372036854775808 : i64} : () -> ()
  "disc_shape.SymbolicDim"() {knownNegativeOne = false, knownNonNegative = true, knownNonSizeOne = false, knownNonSizeZero = false, sym_name = "S2", value = -9223372036854775808 : i64} : () -> ()
  "disc_shape.SymbolicDim"() {knownNegativeOne = false, knownNonNegative = true, knownNonSizeOne = false, knownNonSizeZero = false, sym_name = "S3", value = -9223372036854775808 : i64} : () -> ()
  func.func @shape_constraint_graph() {
    %0 = "disc_shape.dim"() {name = @S3} : () -> index
    %1 = "disc_shape.dim"() {name = @S1} : () -> index
    %2 = "disc_shape.dim"() {name = @S2} : () -> index
    "disc_shape.tie_product_equal"(%0, %1, %2) {operand_segment_sizes = array<i32: 1, 2>} : (index, index, index) -> ()
    return
  }
}


// -----// IR Dump After AssignKernelNamePass (disc-assign-kernel-name) //----- //
#map = affine_map<(d0)[s0, s1] -> ((d0 - s0) ceildiv s1)>
#map1 = affine_map<(d0)[s0, s1] -> (d0 * s0 + s1)>
module attributes {gpu.container_module} {
  func.func @main(%arg0: !disc_ral.context) attributes {tf.entry_function = {input_placements = "gpu", inputs = "input0", output_placements = "gpu", outputs = "output0"}} {
    %c-1 = arith.constant -1 : index
    %cst = arith.constant 0xFF800000 : f32
    %c64 = arith.constant 64 : index
    %c128 = arith.constant 128 : index
    %c4 = arith.constant 4 : index
    %c16 = arith.constant 16 : index
    %c32 = arith.constant 32 : index
    %c8 = arith.constant 8 : index
    %0 = llvm.mlir.constant(0 : i32) : i32
    %c0 = arith.constant 0 : index
    %c1 = arith.constant 1 : index
    %c2 = arith.constant 2 : index
    %c256 = arith.constant 256 : index
    %c108 = arith.constant 108 : index
    %1 = "disc_ral.dispatch"(%arg0, %c0) {backend_config = "", call_target_name = "ral_recv_input", device = "cpu", has_side_effect = false} : (!disc_ral.context, index) -> memref<?x?x?xf32, "gpu">
    %dim = memref.dim %1, %c2 : memref<?x?x?xf32, "gpu">
    %dim_0 = memref.dim %1, %c1 : memref<?x?x?xf32, "gpu">
    %dim_1 = memref.dim %1, %c0 : memref<?x?x?xf32, "gpu">
    %2 = arith.index_cast %dim_0 : index to i32
    %3 = arith.index_cast %dim : index to i32
    %4 = arith.muli %2, %3 : i32
    %5 = arith.index_cast %4 : i32 to index
    %alloc = memref.alloc(%5) {kDiscSymbolicDimAttr = [@S3]} : memref<?xf32, "gpu">
    %6 = arith.muli %dim_1, %5 : index
    %7 = arith.addi %6, %c-1 : index
    %8 = arith.divsi %7, %c256 : index
    %9 = arith.addi %8, %c1 : index
    %10 = arith.subi %c0, %6 : index
    %11 = arith.divsi %10, %c256 : index
    %12 = arith.subi %c0, %11 : index
    %13 = arith.cmpi sgt, %6, %c0 : index
    %14 = arith.select %13, %9, %12 : index
    %15 = arith.cmpi sgt, %14, %c108 : index
    scf.if %15 {
      "lmhlo.fusion"() ({
        %c0_2 = arith.constant 0 : index
        %c256_3 = arith.constant 256 : index
        %18 = arith.muli %c1, %c256_3 : index
        %c1_4 = arith.constant 1 : index
        %19 = affine.apply #map(%5)[%c0, %18]
        %20 = affine.apply #map(%18)[%c0_2, %c1]
        gpu.launch_func  @main_kernel::@main_kColReduction_reduce__4_1_0___8w32h blocks in (%19, %c1_4, %c1_4) threads in (%20, %c1_4, %c1_4) args(%18 : index, %5 : index, %alloc : memref<?xf32, "gpu">)
        %21 = arith.addi %5, %c-1 : index
        %22 = arith.divsi %21, %c8 : index
        %23 = arith.addi %22, %c1 : index
        %24 = arith.subi %c0, %5 : index
        %25 = arith.divsi %24, %c8 : index
        %26 = arith.subi %c0, %25 : index
        %27 = arith.cmpi sgt, %5, %c0 : index
        %28 = arith.select %27, %23, %26 : index
        %29 = arith.addi %dim_1, %c-1 : index
        %30 = arith.divsi %29, %c32 : index
        %31 = arith.addi %30, %c1 : index
        %32 = arith.subi %c0, %dim_1 : index
        %33 = arith.divsi %32, %c32 : index
        %34 = arith.subi %c0, %33 : index
        %35 = arith.cmpi sgt, %dim_1, %c0 : index
        %36 = arith.select %35, %31, %34 : index
        %37 = arith.muli %28, %36 : index
        %c0_5 = arith.constant 0 : index
        %c1_6 = arith.constant 1 : index
        %c1_7 = arith.constant 1 : index
        %38 = arith.muli %c1_7, %37 : index
        %39 = arith.muli %38, %c256 : index
        %c0_8 = arith.constant 0 : index
        %c256_9 = arith.constant 256 : index
        %40 = arith.muli %c1_6, %c256_9 : index
        %c1_10 = arith.constant 1 : index
        %41 = affine.apply #map(%39)[%c0_5, %40]
        %42 = affine.apply #map(%40)[%c0_8, %c1_6]
        gpu.launch_func  @main_kernel_0::@main_kColReduction_reduce__4_1_0___8w32h_1 blocks in (%41, %c1_10, %c1_10) threads in (%42, %c1_10, %c1_10) args(%5 : index, %1 : memref<?x?x?xf32, "gpu">, %40 : index, %39 : index, %28 : index, %alloc : memref<?xf32, "gpu">)
        "lmhlo.terminator"() : () -> ()
      }) {disc.device = "gpu", disc.fusion.name = "main_kColReduction_reduce__4_1_0", disc.fusion.tag = "8w32h", disc.fusion_type = "kColReduction", disc_col_reduction_schedule_hint = 1 : i32, disc_thread_per_block_hint = 256 : i32} : () -> ()
    } else {
      "lmhlo.fusion"() ({
        %c0_2 = arith.constant 0 : index
        %c128_3 = arith.constant 128 : index
        %18 = arith.muli %c1, %c128_3 : index
        %c1_4 = arith.constant 1 : index
        %19 = affine.apply #map(%5)[%c0, %18]
        %20 = affine.apply #map(%18)[%c0_2, %c1]
        gpu.launch_func  @main_kernel_1::@main_kColReduction_reduce__4_1_0___8w16h blocks in (%19, %c1_4, %c1_4) threads in (%20, %c1_4, %c1_4) args(%18 : index, %5 : index, %alloc : memref<?xf32, "gpu">)
        %21 = arith.addi %5, %c-1 : index
        %22 = arith.divsi %21, %c8 : index
        %23 = arith.addi %22, %c1 : index
        %24 = arith.subi %c0, %5 : index
        %25 = arith.divsi %24, %c8 : index
        %26 = arith.subi %c0, %25 : index
        %27 = arith.cmpi sgt, %5, %c0 : index
        %28 = arith.select %27, %23, %26 : index
        %29 = arith.addi %dim_1, %c-1 : index
        %30 = arith.divsi %29, %c16 : index
        %31 = arith.addi %30, %c1 : index
        %32 = arith.subi %c0, %dim_1 : index
        %33 = arith.divsi %32, %c16 : index
        %34 = arith.subi %c0, %33 : index
        %35 = arith.cmpi sgt, %dim_1, %c0 : index
        %36 = arith.select %35, %31, %34 : index
        %37 = arith.muli %28, %36 : index
        %c0_5 = arith.constant 0 : index
        %c1_6 = arith.constant 1 : index
        %c1_7 = arith.constant 1 : index
        %38 = arith.muli %c1_7, %37 : index
        %39 = arith.muli %38, %c128 : index
        %c0_8 = arith.constant 0 : index
        %c128_9 = arith.constant 128 : index
        %40 = arith.muli %c1_6, %c128_9 : index
        %c1_10 = arith.constant 1 : index
        %41 = affine.apply #map(%39)[%c0_5, %40]
        %42 = affine.apply #map(%40)[%c0_8, %c1_6]
        gpu.launch_func  @main_kernel_2::@main_kColReduction_reduce__4_1_0___8w16h_1 blocks in (%41, %c1_10, %c1_10) threads in (%42, %c1_10, %c1_10) args(%5 : index, %1 : memref<?x?x?xf32, "gpu">, %40 : index, %39 : index, %28 : index, %alloc : memref<?xf32, "gpu">)
        "lmhlo.terminator"() : () -> ()
      }) {disc.device = "gpu", disc.fusion.name = "main_kColReduction_reduce__4_1_0", disc.fusion.tag = "8w16h", disc.fusion_type = "kColReduction", disc_col_reduction_schedule_hint = 2 : i32, disc_thread_per_block_hint = 128 : i32} : () -> ()
    }
    %16 = llvm.inttoptr %0 : i32 to !llvm.ptr<i8>
    %alloca = memref.alloca() : memref<2xindex, "cpu">
    memref.store %dim_0, %alloca[%c0] : memref<2xindex, "cpu">
    memref.store %dim, %alloca[%c1] : memref<2xindex, "cpu">
    %17 = "disc_ral.dispatch"(%arg0, %16, %alloc, %alloca) {backend_config = "", call_target_name = "inc_ref", device = "gpu", has_side_effect = false} : (!disc_ral.context, !llvm.ptr<i8>, memref<?xf32, "gpu">, memref<2xindex, "cpu">) -> memref<?x?xf32, "gpu">
    %reinterpret_cast = memref.reinterpret_cast %17 to offset: [0], sizes: [%dim_0, %dim], strides: [%dim, 1] {kDiscSymbolicDimAttr = [@S1, @S2]} : memref<?x?xf32, "gpu"> to memref<?x?xf32, "gpu">
    memref.dealloc %alloc : memref<?xf32, "gpu">
    "disc_ral.dispatch"(%arg0, %c0, %reinterpret_cast) {backend_config = "", call_target_name = "ral_send_output", device = "cpu", has_side_effect = false} : (!disc_ral.context, index, memref<?x?xf32, "gpu">) -> ()
    return
  }
  gpu.module @main_kernel {
    gpu.func @main_kColReduction_reduce__4_1_0___8w32h(%arg0: index, %arg1: index, %arg2: memref<?xf32, "gpu">) kernel {
      %0 = gpu.block_id  x
      %1 = gpu.block_id  y
      %2 = gpu.block_id  z
      %3 = gpu.thread_id  x
      %4 = gpu.thread_id  y
      %5 = gpu.thread_id  z
      %6 = gpu.grid_dim  x
      %7 = gpu.grid_dim  y
      %8 = gpu.grid_dim  z
      %9 = gpu.block_dim  x
      %10 = gpu.block_dim  y
      %11 = gpu.block_dim  z
      cf.br ^bb1
    ^bb1:  // pred: ^bb0
      %c0 = arith.constant 0 : index
      %c1 = arith.constant 1 : index
      %c0_0 = arith.constant 0 : index
      %cst = arith.constant 0xFF800000 : f32
      %12 = affine.apply #map1(%0)[%arg0, %c0]
      %13 = affine.apply #map1(%3)[%c1, %c0_0]
      %14 = arith.addi %13, %12 : index
      %true = arith.constant true
      %15 = arith.muli %13, %c1 : index
      %16 = arith.addi %15, %12 : index
      %17 = arith.cmpi ult, %16, %arg1 : index
      %18 = arith.andi %true, %17 : i1
      scf.if %18 {
        %reinterpret_cast = memref.reinterpret_cast %arg2 to offset: [%c0], sizes: [%arg1], strides: [%c1] : memref<?xf32, "gpu"> to memref<?xf32, "gpu">
        memref.store %cst, %reinterpret_cast[%14] : memref<?xf32, "gpu">
      }
      gpu.return
    }
  }
  gpu.module @main_kernel_0 {
    gpu.func @main_kColReduction_reduce__4_1_0___8w32h_1(%arg0: index, %arg1: memref<?x?x?xf32, "gpu">, %arg2: index, %arg3: index, %arg4: index, %arg5: memref<?xf32, "gpu">) kernel {
      %0 = gpu.block_id  x
      %1 = gpu.block_id  y
      %2 = gpu.block_id  z
      %3 = gpu.thread_id  x
      %4 = gpu.thread_id  y
      %5 = gpu.thread_id  z
      %6 = gpu.grid_dim  x
      %7 = gpu.grid_dim  y
      %8 = gpu.grid_dim  z
      %9 = gpu.block_dim  x
      %10 = gpu.block_dim  y
      %11 = gpu.block_dim  z
      cf.br ^bb1
    ^bb1:  // pred: ^bb0
      %c0 = arith.constant 0 : index
      %c1 = arith.constant 1 : index
      %c0_0 = arith.constant 0 : index
      %c256 = arith.constant 256 : index
      %c8 = arith.constant 8 : index
      %c0_1 = arith.constant 0 : index
      %12 = arith.cmpi sgt, %arg0, %c0_1 : index
      %c32 = arith.constant 32 : index
      %dim = memref.dim %arg1, %c0_1 : memref<?x?x?xf32, "gpu">
      %c1_2 = arith.constant 1 : index
      %dim_3 = memref.dim %arg1, %c1_2 : memref<?x?x?xf32, "gpu">
      %c2 = arith.constant 2 : index
      %dim_4 = memref.dim %arg1, %c2 : memref<?x?x?xf32, "gpu">
      %cst = arith.constant 0xFF800000 : f32
      %c16 = arith.constant 16 : index
      %c128 = arith.constant 128 : index
      %c64 = arith.constant 64 : index
      %c4 = arith.constant 4 : index
      %13 = affine.apply #map1(%0)[%arg2, %c0]
      %14 = affine.apply #map1(%3)[%c1, %c0_0]
      %15 = arith.addi %14, %13 : index
      %true = arith.constant true
      %16 = arith.muli %14, %c1 : index
      %17 = arith.addi %16, %13 : index
      %18 = arith.cmpi ult, %17, %arg3 : index
      %19 = arith.andi %true, %18 : i1
      scf.if %19 {
        %20 = arith.remsi %15, %c256 : index
        %21 = arith.divsi %15, %c256 : index
        %22 = arith.divui %20, %c8 : index
        %23 = arith.remui %20, %c8 : index
        %24 = arith.divui %21, %arg4 : index
        %25 = arith.remui %21, %arg4 : index
        %26 = arith.muli %24, %c32 : index
        %27 = arith.addi %26, %22 : index
        %28 = arith.muli %25, %c8 : index
        %29 = arith.addi %28, %23 : index
        %alloc = memref.alloc() : memref<256xf32, #gpu.address_space<workgroup>>
        %30 = arith.cmpi ult, %27, %dim : index
        %31 = arith.cmpi ult, %29, %arg0 : index
        %32 = arith.andi %30, %31 : i1
        scf.if %32 {
          %51 = arith.muli %27, %arg0 : index
          %52 = arith.addi %51, %29 : index
          %53 = arith.muli %dim, %dim_3 : index
          %54 = arith.muli %53, %dim_4 : index
          %reinterpret_cast = memref.reinterpret_cast %arg1 to offset: [%c0_1], sizes: [%54], strides: [%c1_2] : memref<?x?x?xf32, "gpu"> to memref<?xf32, "gpu">
          %55 = memref.load %reinterpret_cast[%52] : memref<?xf32, "gpu">
          %56 = math.absf %55 : f32
          %reinterpret_cast_5 = memref.reinterpret_cast %alloc to offset: [0], sizes: [256], strides: [1] : memref<256xf32, #gpu.address_space<workgroup>> to memref<256xf32, #gpu.address_space<workgroup>>
          memref.store %56, %reinterpret_cast_5[%20] : memref<256xf32, #gpu.address_space<workgroup>>
        } else {
          %reinterpret_cast = memref.reinterpret_cast %alloc to offset: [0], sizes: [256], strides: [1] : memref<256xf32, #gpu.address_space<workgroup>> to memref<256xf32, #gpu.address_space<workgroup>>
          memref.store %cst, %reinterpret_cast[%20] : memref<256xf32, #gpu.address_space<workgroup>>
        }
        gpu.barrier
        %33 = arith.cmpi ult, %22, %c16 : index
        %34 = arith.addi %27, %c16 : index
        %35 = arith.cmpi ult, %34, %dim : index
        %36 = arith.andi %33, %35 : i1
        scf.if %36 {
          %51 = arith.addi %20, %c128 : index
          %reinterpret_cast = memref.reinterpret_cast %alloc to offset: [0], sizes: [256], strides: [1] : memref<256xf32, #gpu.address_space<workgroup>> to memref<256xf32, #gpu.address_space<workgroup>>
          %52 = memref.load %reinterpret_cast[%20] : memref<256xf32, #gpu.address_space<workgroup>>
          %53 = memref.load %reinterpret_cast[%51] : memref<256xf32, #gpu.address_space<workgroup>>
          %54 = arith.cmpf ugt, %52, %53 : f32
          %55 = arith.select %54, %52, %53 : f32
          %56 = arith.cmpf uno, %53, %53 : f32
          %57 = arith.select %56, %53, %55 : f32
          memref.store %57, %reinterpret_cast[%20] : memref<256xf32, #gpu.address_space<workgroup>>
        }
        gpu.barrier
        %37 = arith.cmpi ult, %22, %c8 : index
        %38 = arith.addi %27, %c8 : index
        %39 = arith.cmpi ult, %38, %dim : index
        %40 = arith.andi %37, %39 : i1
        scf.if %40 {
          %51 = arith.addi %20, %c64 : index
          %reinterpret_cast = memref.reinterpret_cast %alloc to offset: [0], sizes: [256], strides: [1] : memref<256xf32, #gpu.address_space<workgroup>> to memref<256xf32, #gpu.address_space<workgroup>>
          %52 = memref.load %reinterpret_cast[%20] : memref<256xf32, #gpu.address_space<workgroup>>
          %53 = memref.load %reinterpret_cast[%51] : memref<256xf32, #gpu.address_space<workgroup>>
          %54 = arith.cmpf ugt, %52, %53 : f32
          %55 = arith.select %54, %52, %53 : f32
          %56 = arith.cmpf uno, %53, %53 : f32
          %57 = arith.select %56, %53, %55 : f32
          memref.store %57, %reinterpret_cast[%20] : memref<256xf32, #gpu.address_space<workgroup>>
        }
        gpu.barrier
        %41 = arith.cmpi ult, %22, %c4 : index
        %42 = arith.addi %27, %c4 : index
        %43 = arith.cmpi ult, %42, %dim : index
        %44 = arith.andi %41, %43 : i1
        scf.if %44 {
          %51 = arith.addi %20, %c32 : index
          %reinterpret_cast = memref.reinterpret_cast %alloc to offset: [0], sizes: [256], strides: [1] : memref<256xf32, #gpu.address_space<workgroup>> to memref<256xf32, #gpu.address_space<workgroup>>
          %52 = memref.load %reinterpret_cast[%20] : memref<256xf32, #gpu.address_space<workgroup>>
          %53 = memref.load %reinterpret_cast[%51] : memref<256xf32, #gpu.address_space<workgroup>>
          %54 = arith.cmpf ugt, %52, %53 : f32
          %55 = arith.select %54, %52, %53 : f32
          %56 = arith.cmpf uno, %53, %53 : f32
          %57 = arith.select %56, %53, %55 : f32
          memref.store %57, %reinterpret_cast[%20] : memref<256xf32, #gpu.address_space<workgroup>>
        }
        gpu.barrier
        %45 = arith.cmpi ult, %22, %c2 : index
        %46 = arith.addi %27, %c2 : index
        %47 = arith.cmpi ult, %46, %dim : index
        %48 = arith.andi %45, %47 : i1
        scf.if %48 {
          %51 = arith.addi %20, %c16 : index
          %reinterpret_cast = memref.reinterpret_cast %alloc to offset: [0], sizes: [256], strides: [1] : memref<256xf32, #gpu.address_space<workgroup>> to memref<256xf32, #gpu.address_space<workgroup>>
          %52 = memref.load %reinterpret_cast[%20] : memref<256xf32, #gpu.address_space<workgroup>>
          %53 = memref.load %reinterpret_cast[%51] : memref<256xf32, #gpu.address_space<workgroup>>
          %54 = arith.cmpf ugt, %52, %53 : f32
          %55 = arith.select %54, %52, %53 : f32
          %56 = arith.cmpf uno, %53, %53 : f32
          %57 = arith.select %56, %53, %55 : f32
          memref.store %57, %reinterpret_cast[%20] : memref<256xf32, #gpu.address_space<workgroup>>
        }
        gpu.barrier
        %49 = arith.cmpi eq, %22, %c0_1 : index
        %50 = arith.andi %49, %32 : i1
        scf.if %50 {
          %51 = arith.addi %20, %c8 : index
          %reinterpret_cast = memref.reinterpret_cast %alloc to offset: [0], sizes: [256], strides: [1] : memref<256xf32, #gpu.address_space<workgroup>> to memref<256xf32, #gpu.address_space<workgroup>>
          %52 = memref.load %reinterpret_cast[%20] : memref<256xf32, #gpu.address_space<workgroup>>
          %53 = memref.load %reinterpret_cast[%51] : memref<256xf32, #gpu.address_space<workgroup>>
          %54 = arith.cmpf ugt, %52, %53 : f32
          %55 = arith.select %54, %52, %53 : f32
          %56 = arith.cmpf uno, %53, %53 : f32
          %57 = arith.select %56, %53, %55 : f32
          %58 = memref.generic_atomic_rmw %arg5[%29] : memref<?xf32, "gpu"> {
          ^bb0(%arg6: f32):
            %59 = arith.cmpf ogt, %arg6, %57 : f32
            %60 = arith.select %59, %arg6, %57 : f32
            memref.atomic_yield %60 : f32
          }
        }
      }
      gpu.return
    }
  }
  gpu.module @main_kernel_1 {
    gpu.func @main_kColReduction_reduce__4_1_0___8w16h(%arg0: index, %arg1: index, %arg2: memref<?xf32, "gpu">) kernel {
      %0 = gpu.block_id  x
      %1 = gpu.block_id  y
      %2 = gpu.block_id  z
      %3 = gpu.thread_id  x
      %4 = gpu.thread_id  y
      %5 = gpu.thread_id  z
      %6 = gpu.grid_dim  x
      %7 = gpu.grid_dim  y
      %8 = gpu.grid_dim  z
      %9 = gpu.block_dim  x
      %10 = gpu.block_dim  y
      %11 = gpu.block_dim  z
      cf.br ^bb1
    ^bb1:  // pred: ^bb0
      %c0 = arith.constant 0 : index
      %c1 = arith.constant 1 : index
      %c0_0 = arith.constant 0 : index
      %cst = arith.constant 0xFF800000 : f32
      %12 = affine.apply #map1(%0)[%arg0, %c0]
      %13 = affine.apply #map1(%3)[%c1, %c0_0]
      %14 = arith.addi %13, %12 : index
      %true = arith.constant true
      %15 = arith.muli %13, %c1 : index
      %16 = arith.addi %15, %12 : index
      %17 = arith.cmpi ult, %16, %arg1 : index
      %18 = arith.andi %true, %17 : i1
      scf.if %18 {
        %reinterpret_cast = memref.reinterpret_cast %arg2 to offset: [%c0], sizes: [%arg1], strides: [%c1] : memref<?xf32, "gpu"> to memref<?xf32, "gpu">
        memref.store %cst, %reinterpret_cast[%14] : memref<?xf32, "gpu">
      }
      gpu.return
    }
  }
  gpu.module @main_kernel_2 {
    gpu.func @main_kColReduction_reduce__4_1_0___8w16h_1(%arg0: index, %arg1: memref<?x?x?xf32, "gpu">, %arg2: index, %arg3: index, %arg4: index, %arg5: memref<?xf32, "gpu">) kernel {
      %0 = gpu.block_id  x
      %1 = gpu.block_id  y
      %2 = gpu.block_id  z
      %3 = gpu.thread_id  x
      %4 = gpu.thread_id  y
      %5 = gpu.thread_id  z
      %6 = gpu.grid_dim  x
      %7 = gpu.grid_dim  y
      %8 = gpu.grid_dim  z
      %9 = gpu.block_dim  x
      %10 = gpu.block_dim  y
      %11 = gpu.block_dim  z
      cf.br ^bb1
    ^bb1:  // pred: ^bb0
      %c0 = arith.constant 0 : index
      %c1 = arith.constant 1 : index
      %c0_0 = arith.constant 0 : index
      %c128 = arith.constant 128 : index
      %c8 = arith.constant 8 : index
      %c0_1 = arith.constant 0 : index
      %12 = arith.cmpi sgt, %arg0, %c0_1 : index
      %c16 = arith.constant 16 : index
      %dim = memref.dim %arg1, %c0_1 : memref<?x?x?xf32, "gpu">
      %c1_2 = arith.constant 1 : index
      %dim_3 = memref.dim %arg1, %c1_2 : memref<?x?x?xf32, "gpu">
      %c2 = arith.constant 2 : index
      %dim_4 = memref.dim %arg1, %c2 : memref<?x?x?xf32, "gpu">
      %cst = arith.constant 0xFF800000 : f32
      %c64 = arith.constant 64 : index
      %c4 = arith.constant 4 : index
      %c32 = arith.constant 32 : index
      %13 = affine.apply #map1(%0)[%arg2, %c0]
      %14 = affine.apply #map1(%3)[%c1, %c0_0]
      %15 = arith.addi %14, %13 : index
      %true = arith.constant true
      %16 = arith.muli %14, %c1 : index
      %17 = arith.addi %16, %13 : index
      %18 = arith.cmpi ult, %17, %arg3 : index
      %19 = arith.andi %true, %18 : i1
      scf.if %19 {
        %20 = arith.remsi %15, %c128 : index
        %21 = arith.divsi %15, %c128 : index
        %22 = arith.divui %20, %c8 : index
        %23 = arith.remui %20, %c8 : index
        %24 = arith.divui %21, %arg4 : index
        %25 = arith.remui %21, %arg4 : index
        %26 = arith.muli %24, %c16 : index
        %27 = arith.addi %26, %22 : index
        %28 = arith.muli %25, %c8 : index
        %29 = arith.addi %28, %23 : index
        %alloc = memref.alloc() : memref<128xf32, #gpu.address_space<workgroup>>
        %30 = arith.cmpi ult, %27, %dim : index
        %31 = arith.cmpi ult, %29, %arg0 : index
        %32 = arith.andi %30, %31 : i1
        scf.if %32 {
          %47 = arith.muli %27, %arg0 : index
          %48 = arith.addi %47, %29 : index
          %49 = arith.muli %dim, %dim_3 : index
          %50 = arith.muli %49, %dim_4 : index
          %reinterpret_cast = memref.reinterpret_cast %arg1 to offset: [%c0_1], sizes: [%50], strides: [%c1_2] : memref<?x?x?xf32, "gpu"> to memref<?xf32, "gpu">
          %51 = memref.load %reinterpret_cast[%48] : memref<?xf32, "gpu">
          %52 = math.absf %51 : f32
          %reinterpret_cast_5 = memref.reinterpret_cast %alloc to offset: [0], sizes: [128], strides: [1] : memref<128xf32, #gpu.address_space<workgroup>> to memref<128xf32, #gpu.address_space<workgroup>>
          memref.store %52, %reinterpret_cast_5[%20] : memref<128xf32, #gpu.address_space<workgroup>>
        } else {
          %reinterpret_cast = memref.reinterpret_cast %alloc to offset: [0], sizes: [128], strides: [1] : memref<128xf32, #gpu.address_space<workgroup>> to memref<128xf32, #gpu.address_space<workgroup>>
          memref.store %cst, %reinterpret_cast[%20] : memref<128xf32, #gpu.address_space<workgroup>>
        }
        gpu.barrier
        %33 = arith.cmpi ult, %22, %c8 : index
        %34 = arith.addi %27, %c8 : index
        %35 = arith.cmpi ult, %34, %dim : index
        %36 = arith.andi %33, %35 : i1
        scf.if %36 {
          %47 = arith.addi %20, %c64 : index
          %reinterpret_cast = memref.reinterpret_cast %alloc to offset: [0], sizes: [128], strides: [1] : memref<128xf32, #gpu.address_space<workgroup>> to memref<128xf32, #gpu.address_space<workgroup>>
          %48 = memref.load %reinterpret_cast[%20] : memref<128xf32, #gpu.address_space<workgroup>>
          %49 = memref.load %reinterpret_cast[%47] : memref<128xf32, #gpu.address_space<workgroup>>
          %50 = arith.cmpf ugt, %48, %49 : f32
          %51 = arith.select %50, %48, %49 : f32
          %52 = arith.cmpf uno, %49, %49 : f32
          %53 = arith.select %52, %49, %51 : f32
          memref.store %53, %reinterpret_cast[%20] : memref<128xf32, #gpu.address_space<workgroup>>
        }
        gpu.barrier
        %37 = arith.cmpi ult, %22, %c4 : index
        %38 = arith.addi %27, %c4 : index
        %39 = arith.cmpi ult, %38, %dim : index
        %40 = arith.andi %37, %39 : i1
        scf.if %40 {
          %47 = arith.addi %20, %c32 : index
          %reinterpret_cast = memref.reinterpret_cast %alloc to offset: [0], sizes: [128], strides: [1] : memref<128xf32, #gpu.address_space<workgroup>> to memref<128xf32, #gpu.address_space<workgroup>>
          %48 = memref.load %reinterpret_cast[%20] : memref<128xf32, #gpu.address_space<workgroup>>
          %49 = memref.load %reinterpret_cast[%47] : memref<128xf32, #gpu.address_space<workgroup>>
          %50 = arith.cmpf ugt, %48, %49 : f32
          %51 = arith.select %50, %48, %49 : f32
          %52 = arith.cmpf uno, %49, %49 : f32
          %53 = arith.select %52, %49, %51 : f32
          memref.store %53, %reinterpret_cast[%20] : memref<128xf32, #gpu.address_space<workgroup>>
        }
        gpu.barrier
        %41 = arith.cmpi ult, %22, %c2 : index
        %42 = arith.addi %27, %c2 : index
        %43 = arith.cmpi ult, %42, %dim : index
        %44 = arith.andi %41, %43 : i1
        scf.if %44 {
          %47 = arith.addi %20, %c16 : index
          %reinterpret_cast = memref.reinterpret_cast %alloc to offset: [0], sizes: [128], strides: [1] : memref<128xf32, #gpu.address_space<workgroup>> to memref<128xf32, #gpu.address_space<workgroup>>
          %48 = memref.load %reinterpret_cast[%20] : memref<128xf32, #gpu.address_space<workgroup>>
          %49 = memref.load %reinterpret_cast[%47] : memref<128xf32, #gpu.address_space<workgroup>>
          %50 = arith.cmpf ugt, %48, %49 : f32
          %51 = arith.select %50, %48, %49 : f32
          %52 = arith.cmpf uno, %49, %49 : f32
          %53 = arith.select %52, %49, %51 : f32
          memref.store %53, %reinterpret_cast[%20] : memref<128xf32, #gpu.address_space<workgroup>>
        }
        gpu.barrier
        %45 = arith.cmpi eq, %22, %c0_1 : index
        %46 = arith.andi %45, %32 : i1
        scf.if %46 {
          %47 = arith.addi %20, %c8 : index
          %reinterpret_cast = memref.reinterpret_cast %alloc to offset: [0], sizes: [128], strides: [1] : memref<128xf32, #gpu.address_space<workgroup>> to memref<128xf32, #gpu.address_space<workgroup>>
          %48 = memref.load %reinterpret_cast[%20] : memref<128xf32, #gpu.address_space<workgroup>>
          %49 = memref.load %reinterpret_cast[%47] : memref<128xf32, #gpu.address_space<workgroup>>
          %50 = arith.cmpf ugt, %48, %49 : f32
          %51 = arith.select %50, %48, %49 : f32
          %52 = arith.cmpf uno, %49, %49 : f32
          %53 = arith.select %52, %49, %51 : f32
          %54 = memref.generic_atomic_rmw %arg5[%29] : memref<?xf32, "gpu"> {
          ^bb0(%arg6: f32):
            %55 = arith.cmpf ogt, %arg6, %53 : f32
            %56 = arith.select %55, %arg6, %53 : f32
            memref.atomic_yield %56 : f32
          }
        }
      }
      gpu.return
    }
  }
  "disc_shape.SymbolicDim"() {knownNegativeOne = false, knownNonNegative = true, knownNonSizeOne = false, knownNonSizeZero = false, sym_name = "S0", value = -9223372036854775808 : i64} : () -> ()
  "disc_shape.SymbolicDim"() {knownNegativeOne = false, knownNonNegative = true, knownNonSizeOne = false, knownNonSizeZero = false, sym_name = "S1", value = -9223372036854775808 : i64} : () -> ()
  "disc_shape.SymbolicDim"() {knownNegativeOne = false, knownNonNegative = true, knownNonSizeOne = false, knownNonSizeZero = false, sym_name = "S2", value = -9223372036854775808 : i64} : () -> ()
  "disc_shape.SymbolicDim"() {knownNegativeOne = false, knownNonNegative = true, knownNonSizeOne = false, knownNonSizeZero = false, sym_name = "S3", value = -9223372036854775808 : i64} : () -> ()
  func.func @shape_constraint_graph() {
    %0 = "disc_shape.dim"() {name = @S3} : () -> index
    %1 = "disc_shape.dim"() {name = @S1} : () -> index
    %2 = "disc_shape.dim"() {name = @S2} : () -> index
    "disc_shape.tie_product_equal"(%0, %1, %2) {operand_segment_sizes = array<i32: 1, 2>} : (index, index, index) -> ()
    return
  }
}


// -----// IR Dump After LhloFusionInlinerPass (lhlo-fusion-inliner) //----- //
func.func @main(%arg0: !disc_ral.context) attributes {tf.entry_function = {input_placements = "gpu", inputs = "input0", output_placements = "gpu", outputs = "output0"}} {
  %c-1 = arith.constant -1 : index
  %cst = arith.constant 0xFF800000 : f32
  %c64 = arith.constant 64 : index
  %c128 = arith.constant 128 : index
  %c4 = arith.constant 4 : index
  %c16 = arith.constant 16 : index
  %c32 = arith.constant 32 : index
  %c8 = arith.constant 8 : index
  %0 = llvm.mlir.constant(0 : i32) : i32
  %c0 = arith.constant 0 : index
  %c1 = arith.constant 1 : index
  %c2 = arith.constant 2 : index
  %c256 = arith.constant 256 : index
  %c108 = arith.constant 108 : index
  %1 = "disc_ral.dispatch"(%arg0, %c0) {backend_config = "", call_target_name = "ral_recv_input", device = "cpu", has_side_effect = false} : (!disc_ral.context, index) -> memref<?x?x?xf32, "gpu">
  %dim = memref.dim %1, %c2 : memref<?x?x?xf32, "gpu">
  %dim_0 = memref.dim %1, %c1 : memref<?x?x?xf32, "gpu">
  %dim_1 = memref.dim %1, %c0 : memref<?x?x?xf32, "gpu">
  %2 = arith.index_cast %dim_0 : index to i32
  %3 = arith.index_cast %dim : index to i32
  %4 = arith.muli %2, %3 : i32
  %5 = arith.index_cast %4 : i32 to index
  %alloc = memref.alloc(%5) {kDiscSymbolicDimAttr = [@S3]} : memref<?xf32, "gpu">
  %6 = arith.muli %dim_1, %5 : index
  %7 = arith.addi %6, %c-1 : index
  %8 = arith.divsi %7, %c256 : index
  %9 = arith.addi %8, %c1 : index
  %10 = arith.subi %c0, %6 : index
  %11 = arith.divsi %10, %c256 : index
  %12 = arith.subi %c0, %11 : index
  %13 = arith.cmpi sgt, %6, %c0 : index
  %14 = arith.select %13, %9, %12 : index
  %15 = arith.cmpi sgt, %14, %c108 : index
  scf.if %15 {
    %c0_2 = arith.constant 0 : index
    %c256_3 = arith.constant 256 : index
    %18 = arith.muli %c1, %c256_3 : index
    %c1_4 = arith.constant 1 : index
    %19 = affine.apply affine_map<(d0)[s0, s1] -> ((d0 - s0) ceildiv s1)>(%5)[%c0, %18]
    %20 = affine.apply affine_map<(d0)[s0, s1] -> ((d0 - s0) ceildiv s1)>(%18)[%c0_2, %c1]
    gpu.launch_func  @main_kernel::@main_kColReduction_reduce__4_1_0___8w32h blocks in (%19, %c1_4, %c1_4) threads in (%20, %c1_4, %c1_4) args(%18 : index, %5 : index, %alloc : memref<?xf32, "gpu">)
    %21 = arith.addi %5, %c-1 : index
    %22 = arith.divsi %21, %c8 : index
    %23 = arith.addi %22, %c1 : index
    %24 = arith.subi %c0, %5 : index
    %25 = arith.divsi %24, %c8 : index
    %26 = arith.subi %c0, %25 : index
    %27 = arith.cmpi sgt, %5, %c0 : index
    %28 = arith.select %27, %23, %26 : index
    %29 = arith.addi %dim_1, %c-1 : index
    %30 = arith.divsi %29, %c32 : index
    %31 = arith.addi %30, %c1 : index
    %32 = arith.subi %c0, %dim_1 : index
    %33 = arith.divsi %32, %c32 : index
    %34 = arith.subi %c0, %33 : index
    %35 = arith.cmpi sgt, %dim_1, %c0 : index
    %36 = arith.select %35, %31, %34 : index
    %37 = arith.muli %28, %36 : index
    %c0_5 = arith.constant 0 : index
    %c1_6 = arith.constant 1 : index
    %c1_7 = arith.constant 1 : index
    %38 = arith.muli %c1_7, %37 : index
    %39 = arith.muli %38, %c256 : index
    %c0_8 = arith.constant 0 : index
    %c256_9 = arith.constant 256 : index
    %40 = arith.muli %c1_6, %c256_9 : index
    %c1_10 = arith.constant 1 : index
    %41 = affine.apply affine_map<(d0)[s0, s1] -> ((d0 - s0) ceildiv s1)>(%39)[%c0_5, %40]
    %42 = affine.apply affine_map<(d0)[s0, s1] -> ((d0 - s0) ceildiv s1)>(%40)[%c0_8, %c1_6]
    gpu.launch_func  @main_kernel_0::@main_kColReduction_reduce__4_1_0___8w32h_1 blocks in (%41, %c1_10, %c1_10) threads in (%42, %c1_10, %c1_10) args(%5 : index, %1 : memref<?x?x?xf32, "gpu">, %40 : index, %39 : index, %28 : index, %alloc : memref<?xf32, "gpu">)
  } else {
    %c0_2 = arith.constant 0 : index
    %c128_3 = arith.constant 128 : index
    %18 = arith.muli %c1, %c128_3 : index
    %c1_4 = arith.constant 1 : index
    %19 = affine.apply affine_map<(d0)[s0, s1] -> ((d0 - s0) ceildiv s1)>(%5)[%c0, %18]
    %20 = affine.apply affine_map<(d0)[s0, s1] -> ((d0 - s0) ceildiv s1)>(%18)[%c0_2, %c1]
    gpu.launch_func  @main_kernel_1::@main_kColReduction_reduce__4_1_0___8w16h blocks in (%19, %c1_4, %c1_4) threads in (%20, %c1_4, %c1_4) args(%18 : index, %5 : index, %alloc : memref<?xf32, "gpu">)
    %21 = arith.addi %5, %c-1 : index
    %22 = arith.divsi %21, %c8 : index
    %23 = arith.addi %22, %c1 : index
    %24 = arith.subi %c0, %5 : index
    %25 = arith.divsi %24, %c8 : index
    %26 = arith.subi %c0, %25 : index
    %27 = arith.cmpi sgt, %5, %c0 : index
    %28 = arith.select %27, %23, %26 : index
    %29 = arith.addi %dim_1, %c-1 : index
    %30 = arith.divsi %29, %c16 : index
    %31 = arith.addi %30, %c1 : index
    %32 = arith.subi %c0, %dim_1 : index
    %33 = arith.divsi %32, %c16 : index
    %34 = arith.subi %c0, %33 : index
    %35 = arith.cmpi sgt, %dim_1, %c0 : index
    %36 = arith.select %35, %31, %34 : index
    %37 = arith.muli %28, %36 : index
    %c0_5 = arith.constant 0 : index
    %c1_6 = arith.constant 1 : index
    %c1_7 = arith.constant 1 : index
    %38 = arith.muli %c1_7, %37 : index
    %39 = arith.muli %38, %c128 : index
    %c0_8 = arith.constant 0 : index
    %c128_9 = arith.constant 128 : index
    %40 = arith.muli %c1_6, %c128_9 : index
    %c1_10 = arith.constant 1 : index
    %41 = affine.apply affine_map<(d0)[s0, s1] -> ((d0 - s0) ceildiv s1)>(%39)[%c0_5, %40]
    %42 = affine.apply affine_map<(d0)[s0, s1] -> ((d0 - s0) ceildiv s1)>(%40)[%c0_8, %c1_6]
    gpu.launch_func  @main_kernel_2::@main_kColReduction_reduce__4_1_0___8w16h_1 blocks in (%41, %c1_10, %c1_10) threads in (%42, %c1_10, %c1_10) args(%5 : index, %1 : memref<?x?x?xf32, "gpu">, %40 : index, %39 : index, %28 : index, %alloc : memref<?xf32, "gpu">)
  }
  %16 = llvm.inttoptr %0 : i32 to !llvm.ptr<i8>
  %alloca = memref.alloca() : memref<2xindex, "cpu">
  memref.store %dim_0, %alloca[%c0] : memref<2xindex, "cpu">
  memref.store %dim, %alloca[%c1] : memref<2xindex, "cpu">
  %17 = "disc_ral.dispatch"(%arg0, %16, %alloc, %alloca) {backend_config = "", call_target_name = "inc_ref", device = "gpu", has_side_effect = false} : (!disc_ral.context, !llvm.ptr<i8>, memref<?xf32, "gpu">, memref<2xindex, "cpu">) -> memref<?x?xf32, "gpu">
  %reinterpret_cast = memref.reinterpret_cast %17 to offset: [0], sizes: [%dim_0, %dim], strides: [%dim, 1] {kDiscSymbolicDimAttr = [@S1, @S2]} : memref<?x?xf32, "gpu"> to memref<?x?xf32, "gpu">
  memref.dealloc %alloc : memref<?xf32, "gpu">
  "disc_ral.dispatch"(%arg0, %c0, %reinterpret_cast) {backend_config = "", call_target_name = "ral_send_output", device = "cpu", has_side_effect = false} : (!disc_ral.context, index, memref<?x?xf32, "gpu">) -> ()
  return
}

// -----// IR Dump After ReviseGpuKernelOutliningPass (disc-revise-gpu-kernel-outlining) //----- //
#map = affine_map<(d0)[s0, s1] -> ((d0 - s0) ceildiv s1)>
#map1 = affine_map<(d0)[s0, s1] -> (d0 * s0 + s1)>
module attributes {gpu.container_module} {
  func.func @main(%arg0: !disc_ral.context) attributes {tf.entry_function = {input_placements = "gpu", inputs = "input0", output_placements = "gpu", outputs = "output0"}} {
    %c-1 = arith.constant -1 : index
    %cst = arith.constant 0xFF800000 : f32
    %c64 = arith.constant 64 : index
    %c128 = arith.constant 128 : index
    %c4 = arith.constant 4 : index
    %c16 = arith.constant 16 : index
    %c32 = arith.constant 32 : index
    %c8 = arith.constant 8 : index
    %0 = llvm.mlir.constant(0 : i32) : i32
    %c0 = arith.constant 0 : index
    %c1 = arith.constant 1 : index
    %c2 = arith.constant 2 : index
    %c256 = arith.constant 256 : index
    %c108 = arith.constant 108 : index
    %1 = "disc_ral.dispatch"(%arg0, %c0) {backend_config = "", call_target_name = "ral_recv_input", device = "cpu", has_side_effect = false} : (!disc_ral.context, index) -> memref<?x?x?xf32, "gpu">
    %dim = memref.dim %1, %c2 : memref<?x?x?xf32, "gpu">
    %dim_0 = memref.dim %1, %c1 : memref<?x?x?xf32, "gpu">
    %dim_1 = memref.dim %1, %c0 : memref<?x?x?xf32, "gpu">
    %2 = arith.index_cast %dim_0 : index to i32
    %3 = arith.index_cast %dim : index to i32
    %4 = arith.muli %2, %3 : i32
    %5 = arith.index_cast %4 : i32 to index
    %alloc = memref.alloc(%5) {kDiscSymbolicDimAttr = [@S3]} : memref<?xf32, "gpu">
    %6 = arith.muli %dim_1, %5 : index
    %7 = arith.addi %6, %c-1 : index
    %8 = arith.divsi %7, %c256 : index
    %9 = arith.addi %8, %c1 : index
    %10 = arith.subi %c0, %6 : index
    %11 = arith.divsi %10, %c256 : index
    %12 = arith.subi %c0, %11 : index
    %13 = arith.cmpi sgt, %6, %c0 : index
    %14 = arith.select %13, %9, %12 : index
    %15 = arith.cmpi sgt, %14, %c108 : index
    scf.if %15 {
      %c0_2 = arith.constant 0 : index
      %c256_3 = arith.constant 256 : index
      %18 = arith.muli %c1, %c256_3 : index
      %c1_4 = arith.constant 1 : index
      %19 = affine.apply #map(%5)[%c0, %18]
      %20 = affine.apply #map(%18)[%c0_2, %c1]
      gpu.launch_func  @main_kernel::@main_kColReduction_reduce__4_1_0___8w32h blocks in (%19, %c1_4, %c1_4) threads in (%20, %c1_4, %c1_4) args(%18 : index, %5 : index, %alloc : memref<?xf32, "gpu">)
      %21 = arith.addi %5, %c-1 : index
      %22 = arith.divsi %21, %c8 : index
      %23 = arith.addi %22, %c1 : index
      %24 = arith.subi %c0, %5 : index
      %25 = arith.divsi %24, %c8 : index
      %26 = arith.subi %c0, %25 : index
      %27 = arith.cmpi sgt, %5, %c0 : index
      %28 = arith.select %27, %23, %26 : index
      %29 = arith.addi %dim_1, %c-1 : index
      %30 = arith.divsi %29, %c32 : index
      %31 = arith.addi %30, %c1 : index
      %32 = arith.subi %c0, %dim_1 : index
      %33 = arith.divsi %32, %c32 : index
      %34 = arith.subi %c0, %33 : index
      %35 = arith.cmpi sgt, %dim_1, %c0 : index
      %36 = arith.select %35, %31, %34 : index
      %37 = arith.muli %28, %36 : index
      %c0_5 = arith.constant 0 : index
      %c1_6 = arith.constant 1 : index
      %c1_7 = arith.constant 1 : index
      %38 = arith.muli %c1_7, %37 : index
      %39 = arith.muli %38, %c256 : index
      %c0_8 = arith.constant 0 : index
      %c256_9 = arith.constant 256 : index
      %40 = arith.muli %c1_6, %c256_9 : index
      %c1_10 = arith.constant 1 : index
      %41 = affine.apply #map(%39)[%c0_5, %40]
      %42 = affine.apply #map(%40)[%c0_8, %c1_6]
      gpu.launch_func  @main_kernel_0::@main_kColReduction_reduce__4_1_0___8w32h_1 blocks in (%41, %c1_10, %c1_10) threads in (%42, %c1_10, %c1_10) args(%5 : index, %1 : memref<?x?x?xf32, "gpu">, %40 : index, %39 : index, %28 : index, %alloc : memref<?xf32, "gpu">)
    } else {
      %c0_2 = arith.constant 0 : index
      %c128_3 = arith.constant 128 : index
      %18 = arith.muli %c1, %c128_3 : index
      %c1_4 = arith.constant 1 : index
      %19 = affine.apply #map(%5)[%c0, %18]
      %20 = affine.apply #map(%18)[%c0_2, %c1]
      gpu.launch_func  @main_kernel_1::@main_kColReduction_reduce__4_1_0___8w16h blocks in (%19, %c1_4, %c1_4) threads in (%20, %c1_4, %c1_4) args(%18 : index, %5 : index, %alloc : memref<?xf32, "gpu">)
      %21 = arith.addi %5, %c-1 : index
      %22 = arith.divsi %21, %c8 : index
      %23 = arith.addi %22, %c1 : index
      %24 = arith.subi %c0, %5 : index
      %25 = arith.divsi %24, %c8 : index
      %26 = arith.subi %c0, %25 : index
      %27 = arith.cmpi sgt, %5, %c0 : index
      %28 = arith.select %27, %23, %26 : index
      %29 = arith.addi %dim_1, %c-1 : index
      %30 = arith.divsi %29, %c16 : index
      %31 = arith.addi %30, %c1 : index
      %32 = arith.subi %c0, %dim_1 : index
      %33 = arith.divsi %32, %c16 : index
      %34 = arith.subi %c0, %33 : index
      %35 = arith.cmpi sgt, %dim_1, %c0 : index
      %36 = arith.select %35, %31, %34 : index
      %37 = arith.muli %28, %36 : index
      %c0_5 = arith.constant 0 : index
      %c1_6 = arith.constant 1 : index
      %c1_7 = arith.constant 1 : index
      %38 = arith.muli %c1_7, %37 : index
      %39 = arith.muli %38, %c128 : index
      %c0_8 = arith.constant 0 : index
      %c128_9 = arith.constant 128 : index
      %40 = arith.muli %c1_6, %c128_9 : index
      %c1_10 = arith.constant 1 : index
      %41 = affine.apply #map(%39)[%c0_5, %40]
      %42 = affine.apply #map(%40)[%c0_8, %c1_6]
      gpu.launch_func  @main_kernel_2::@main_kColReduction_reduce__4_1_0___8w16h_1 blocks in (%41, %c1_10, %c1_10) threads in (%42, %c1_10, %c1_10) args(%5 : index, %1 : memref<?x?x?xf32, "gpu">, %40 : index, %39 : index, %28 : index, %alloc : memref<?xf32, "gpu">)
    }
    %16 = llvm.inttoptr %0 : i32 to !llvm.ptr<i8>
    %alloca = memref.alloca() : memref<2xindex, "cpu">
    memref.store %dim_0, %alloca[%c0] : memref<2xindex, "cpu">
    memref.store %dim, %alloca[%c1] : memref<2xindex, "cpu">
    %17 = "disc_ral.dispatch"(%arg0, %16, %alloc, %alloca) {backend_config = "", call_target_name = "inc_ref", device = "gpu", has_side_effect = false} : (!disc_ral.context, !llvm.ptr<i8>, memref<?xf32, "gpu">, memref<2xindex, "cpu">) -> memref<?x?xf32, "gpu">
    %reinterpret_cast = memref.reinterpret_cast %17 to offset: [0], sizes: [%dim_0, %dim], strides: [%dim, 1] {kDiscSymbolicDimAttr = [@S1, @S2]} : memref<?x?xf32, "gpu"> to memref<?x?xf32, "gpu">
    memref.dealloc %alloc : memref<?xf32, "gpu">
    "disc_ral.dispatch"(%arg0, %c0, %reinterpret_cast) {backend_config = "", call_target_name = "ral_send_output", device = "cpu", has_side_effect = false} : (!disc_ral.context, index, memref<?x?xf32, "gpu">) -> ()
    return
  }
  gpu.module @main_kernel {
    gpu.func @main_kColReduction_reduce__4_1_0___8w32h(%arg0: index, %arg1: index, %arg2: memref<?xf32, "gpu">) kernel {
      %0 = gpu.block_id  x
      %1 = gpu.block_id  y
      %2 = gpu.block_id  z
      %3 = gpu.thread_id  x
      %4 = gpu.thread_id  y
      %5 = gpu.thread_id  z
      %6 = gpu.grid_dim  x
      %7 = gpu.grid_dim  y
      %8 = gpu.grid_dim  z
      %9 = gpu.block_dim  x
      %10 = gpu.block_dim  y
      %11 = gpu.block_dim  z
      cf.br ^bb1
    ^bb1:  // pred: ^bb0
      %c0 = arith.constant 0 : index
      %c1 = arith.constant 1 : index
      %c0_0 = arith.constant 0 : index
      %cst = arith.constant 0xFF800000 : f32
      %12 = affine.apply #map1(%0)[%arg0, %c0]
      %13 = affine.apply #map1(%3)[%c1, %c0_0]
      %14 = arith.addi %13, %12 : index
      %true = arith.constant true
      %15 = arith.muli %13, %c1 : index
      %16 = arith.addi %15, %12 : index
      %17 = arith.cmpi ult, %16, %arg1 : index
      %18 = arith.andi %true, %17 : i1
      scf.if %18 {
        %reinterpret_cast = memref.reinterpret_cast %arg2 to offset: [%c0], sizes: [%arg1], strides: [%c1] : memref<?xf32, "gpu"> to memref<?xf32, "gpu">
        memref.store %cst, %reinterpret_cast[%14] : memref<?xf32, "gpu">
      }
      gpu.return
    }
  }
  gpu.module @main_kernel_0 {
    gpu.func @main_kColReduction_reduce__4_1_0___8w32h_1(%arg0: index, %arg1: memref<?x?x?xf32, "gpu">, %arg2: index, %arg3: index, %arg4: index, %arg5: memref<?xf32, "gpu">) workgroup(%arg6 : memref<256xf32, #gpu.address_space<workgroup>>) kernel {
      %0 = gpu.block_id  x
      %1 = gpu.block_id  y
      %2 = gpu.block_id  z
      %3 = gpu.thread_id  x
      %4 = gpu.thread_id  y
      %5 = gpu.thread_id  z
      %6 = gpu.grid_dim  x
      %7 = gpu.grid_dim  y
      %8 = gpu.grid_dim  z
      %9 = gpu.block_dim  x
      %10 = gpu.block_dim  y
      %11 = gpu.block_dim  z
      cf.br ^bb1
    ^bb1:  // pred: ^bb0
      %c0 = arith.constant 0 : index
      %c1 = arith.constant 1 : index
      %c0_0 = arith.constant 0 : index
      %c256 = arith.constant 256 : index
      %c8 = arith.constant 8 : index
      %c0_1 = arith.constant 0 : index
      %12 = arith.cmpi sgt, %arg0, %c0_1 : index
      %c32 = arith.constant 32 : index
      %dim = memref.dim %arg1, %c0_1 : memref<?x?x?xf32, "gpu">
      %c1_2 = arith.constant 1 : index
      %dim_3 = memref.dim %arg1, %c1_2 : memref<?x?x?xf32, "gpu">
      %c2 = arith.constant 2 : index
      %dim_4 = memref.dim %arg1, %c2 : memref<?x?x?xf32, "gpu">
      %cst = arith.constant 0xFF800000 : f32
      %c16 = arith.constant 16 : index
      %c128 = arith.constant 128 : index
      %c64 = arith.constant 64 : index
      %c4 = arith.constant 4 : index
      %13 = affine.apply #map1(%0)[%arg2, %c0]
      %14 = affine.apply #map1(%3)[%c1, %c0_0]
      %15 = arith.addi %14, %13 : index
      %true = arith.constant true
      %16 = arith.muli %14, %c1 : index
      %17 = arith.addi %16, %13 : index
      %18 = arith.cmpi ult, %17, %arg3 : index
      %19 = arith.andi %true, %18 : i1
      scf.if %19 {
        %20 = arith.remsi %15, %c256 : index
        %21 = arith.divsi %15, %c256 : index
        %22 = arith.divui %20, %c8 : index
        %23 = arith.remui %20, %c8 : index
        %24 = arith.divui %21, %arg4 : index
        %25 = arith.remui %21, %arg4 : index
        %26 = arith.muli %24, %c32 : index
        %27 = arith.addi %26, %22 : index
        %28 = arith.muli %25, %c8 : index
        %29 = arith.addi %28, %23 : index
        %30 = arith.cmpi ult, %27, %dim : index
        %31 = arith.cmpi ult, %29, %arg0 : index
        %32 = arith.andi %30, %31 : i1
        scf.if %32 {
          %51 = arith.muli %27, %arg0 : index
          %52 = arith.addi %51, %29 : index
          %53 = arith.muli %dim, %dim_3 : index
          %54 = arith.muli %53, %dim_4 : index
          %reinterpret_cast = memref.reinterpret_cast %arg1 to offset: [%c0_1], sizes: [%54], strides: [%c1_2] : memref<?x?x?xf32, "gpu"> to memref<?xf32, "gpu">
          %55 = memref.load %reinterpret_cast[%52] : memref<?xf32, "gpu">
          %56 = math.absf %55 : f32
          %reinterpret_cast_5 = memref.reinterpret_cast %arg6 to offset: [0], sizes: [256], strides: [1] : memref<256xf32, #gpu.address_space<workgroup>> to memref<256xf32, #gpu.address_space<workgroup>>
          memref.store %56, %reinterpret_cast_5[%20] : memref<256xf32, #gpu.address_space<workgroup>>
        } else {
          %reinterpret_cast = memref.reinterpret_cast %arg6 to offset: [0], sizes: [256], strides: [1] : memref<256xf32, #gpu.address_space<workgroup>> to memref<256xf32, #gpu.address_space<workgroup>>
          memref.store %cst, %reinterpret_cast[%20] : memref<256xf32, #gpu.address_space<workgroup>>
        }
        gpu.barrier
        %33 = arith.cmpi ult, %22, %c16 : index
        %34 = arith.addi %27, %c16 : index
        %35 = arith.cmpi ult, %34, %dim : index
        %36 = arith.andi %33, %35 : i1
        scf.if %36 {
          %51 = arith.addi %20, %c128 : index
          %reinterpret_cast = memref.reinterpret_cast %arg6 to offset: [0], sizes: [256], strides: [1] : memref<256xf32, #gpu.address_space<workgroup>> to memref<256xf32, #gpu.address_space<workgroup>>
          %52 = memref.load %reinterpret_cast[%20] : memref<256xf32, #gpu.address_space<workgroup>>
          %53 = memref.load %reinterpret_cast[%51] : memref<256xf32, #gpu.address_space<workgroup>>
          %54 = arith.cmpf ugt, %52, %53 : f32
          %55 = arith.select %54, %52, %53 : f32
          %56 = arith.cmpf uno, %53, %53 : f32
          %57 = arith.select %56, %53, %55 : f32
          memref.store %57, %reinterpret_cast[%20] : memref<256xf32, #gpu.address_space<workgroup>>
        }
        gpu.barrier
        %37 = arith.cmpi ult, %22, %c8 : index
        %38 = arith.addi %27, %c8 : index
        %39 = arith.cmpi ult, %38, %dim : index
        %40 = arith.andi %37, %39 : i1
        scf.if %40 {
          %51 = arith.addi %20, %c64 : index
          %reinterpret_cast = memref.reinterpret_cast %arg6 to offset: [0], sizes: [256], strides: [1] : memref<256xf32, #gpu.address_space<workgroup>> to memref<256xf32, #gpu.address_space<workgroup>>
          %52 = memref.load %reinterpret_cast[%20] : memref<256xf32, #gpu.address_space<workgroup>>
          %53 = memref.load %reinterpret_cast[%51] : memref<256xf32, #gpu.address_space<workgroup>>
          %54 = arith.cmpf ugt, %52, %53 : f32
          %55 = arith.select %54, %52, %53 : f32
          %56 = arith.cmpf uno, %53, %53 : f32
          %57 = arith.select %56, %53, %55 : f32
          memref.store %57, %reinterpret_cast[%20] : memref<256xf32, #gpu.address_space<workgroup>>
        }
        gpu.barrier
        %41 = arith.cmpi ult, %22, %c4 : index
        %42 = arith.addi %27, %c4 : index
        %43 = arith.cmpi ult, %42, %dim : index
        %44 = arith.andi %41, %43 : i1
        scf.if %44 {
          %51 = arith.addi %20, %c32 : index
          %reinterpret_cast = memref.reinterpret_cast %arg6 to offset: [0], sizes: [256], strides: [1] : memref<256xf32, #gpu.address_space<workgroup>> to memref<256xf32, #gpu.address_space<workgroup>>
          %52 = memref.load %reinterpret_cast[%20] : memref<256xf32, #gpu.address_space<workgroup>>
          %53 = memref.load %reinterpret_cast[%51] : memref<256xf32, #gpu.address_space<workgroup>>
          %54 = arith.cmpf ugt, %52, %53 : f32
          %55 = arith.select %54, %52, %53 : f32
          %56 = arith.cmpf uno, %53, %53 : f32
          %57 = arith.select %56, %53, %55 : f32
          memref.store %57, %reinterpret_cast[%20] : memref<256xf32, #gpu.address_space<workgroup>>
        }
        gpu.barrier
        %45 = arith.cmpi ult, %22, %c2 : index
        %46 = arith.addi %27, %c2 : index
        %47 = arith.cmpi ult, %46, %dim : index
        %48 = arith.andi %45, %47 : i1
        scf.if %48 {
          %51 = arith.addi %20, %c16 : index
          %reinterpret_cast = memref.reinterpret_cast %arg6 to offset: [0], sizes: [256], strides: [1] : memref<256xf32, #gpu.address_space<workgroup>> to memref<256xf32, #gpu.address_space<workgroup>>
          %52 = memref.load %reinterpret_cast[%20] : memref<256xf32, #gpu.address_space<workgroup>>
          %53 = memref.load %reinterpret_cast[%51] : memref<256xf32, #gpu.address_space<workgroup>>
          %54 = arith.cmpf ugt, %52, %53 : f32
          %55 = arith.select %54, %52, %53 : f32
          %56 = arith.cmpf uno, %53, %53 : f32
          %57 = arith.select %56, %53, %55 : f32
          memref.store %57, %reinterpret_cast[%20] : memref<256xf32, #gpu.address_space<workgroup>>
        }
        gpu.barrier
        %49 = arith.cmpi eq, %22, %c0_1 : index
        %50 = arith.andi %49, %32 : i1
        scf.if %50 {
          %51 = arith.addi %20, %c8 : index
          %reinterpret_cast = memref.reinterpret_cast %arg6 to offset: [0], sizes: [256], strides: [1] : memref<256xf32, #gpu.address_space<workgroup>> to memref<256xf32, #gpu.address_space<workgroup>>
          %52 = memref.load %reinterpret_cast[%20] : memref<256xf32, #gpu.address_space<workgroup>>
          %53 = memref.load %reinterpret_cast[%51] : memref<256xf32, #gpu.address_space<workgroup>>
          %54 = arith.cmpf ugt, %52, %53 : f32
          %55 = arith.select %54, %52, %53 : f32
          %56 = arith.cmpf uno, %53, %53 : f32
          %57 = arith.select %56, %53, %55 : f32
          %58 = memref.generic_atomic_rmw %arg5[%29] : memref<?xf32, "gpu"> {
          ^bb0(%arg7: f32):
            %59 = arith.cmpf ogt, %arg7, %57 : f32
            %60 = arith.select %59, %arg7, %57 : f32
            memref.atomic_yield %60 : f32
          }
        }
      }
      gpu.return
    }
  }
  gpu.module @main_kernel_1 {
    gpu.func @main_kColReduction_reduce__4_1_0___8w16h(%arg0: index, %arg1: index, %arg2: memref<?xf32, "gpu">) kernel {
      %0 = gpu.block_id  x
      %1 = gpu.block_id  y
      %2 = gpu.block_id  z
      %3 = gpu.thread_id  x
      %4 = gpu.thread_id  y
      %5 = gpu.thread_id  z
      %6 = gpu.grid_dim  x
      %7 = gpu.grid_dim  y
      %8 = gpu.grid_dim  z
      %9 = gpu.block_dim  x
      %10 = gpu.block_dim  y
      %11 = gpu.block_dim  z
      cf.br ^bb1
    ^bb1:  // pred: ^bb0
      %c0 = arith.constant 0 : index
      %c1 = arith.constant 1 : index
      %c0_0 = arith.constant 0 : index
      %cst = arith.constant 0xFF800000 : f32
      %12 = affine.apply #map1(%0)[%arg0, %c0]
      %13 = affine.apply #map1(%3)[%c1, %c0_0]
      %14 = arith.addi %13, %12 : index
      %true = arith.constant true
      %15 = arith.muli %13, %c1 : index
      %16 = arith.addi %15, %12 : index
      %17 = arith.cmpi ult, %16, %arg1 : index
      %18 = arith.andi %true, %17 : i1
      scf.if %18 {
        %reinterpret_cast = memref.reinterpret_cast %arg2 to offset: [%c0], sizes: [%arg1], strides: [%c1] : memref<?xf32, "gpu"> to memref<?xf32, "gpu">
        memref.store %cst, %reinterpret_cast[%14] : memref<?xf32, "gpu">
      }
      gpu.return
    }
  }
  gpu.module @main_kernel_2 {
    gpu.func @main_kColReduction_reduce__4_1_0___8w16h_1(%arg0: index, %arg1: memref<?x?x?xf32, "gpu">, %arg2: index, %arg3: index, %arg4: index, %arg5: memref<?xf32, "gpu">) workgroup(%arg6 : memref<128xf32, #gpu.address_space<workgroup>>) kernel {
      %0 = gpu.block_id  x
      %1 = gpu.block_id  y
      %2 = gpu.block_id  z
      %3 = gpu.thread_id  x
      %4 = gpu.thread_id  y
      %5 = gpu.thread_id  z
      %6 = gpu.grid_dim  x
      %7 = gpu.grid_dim  y
      %8 = gpu.grid_dim  z
      %9 = gpu.block_dim  x
      %10 = gpu.block_dim  y
      %11 = gpu.block_dim  z
      cf.br ^bb1
    ^bb1:  // pred: ^bb0
      %c0 = arith.constant 0 : index
      %c1 = arith.constant 1 : index
      %c0_0 = arith.constant 0 : index
      %c128 = arith.constant 128 : index
      %c8 = arith.constant 8 : index
      %c0_1 = arith.constant 0 : index
      %12 = arith.cmpi sgt, %arg0, %c0_1 : index
      %c16 = arith.constant 16 : index
      %dim = memref.dim %arg1, %c0_1 : memref<?x?x?xf32, "gpu">
      %c1_2 = arith.constant 1 : index
      %dim_3 = memref.dim %arg1, %c1_2 : memref<?x?x?xf32, "gpu">
      %c2 = arith.constant 2 : index
      %dim_4 = memref.dim %arg1, %c2 : memref<?x?x?xf32, "gpu">
      %cst = arith.constant 0xFF800000 : f32
      %c64 = arith.constant 64 : index
      %c4 = arith.constant 4 : index
      %c32 = arith.constant 32 : index
      %13 = affine.apply #map1(%0)[%arg2, %c0]
      %14 = affine.apply #map1(%3)[%c1, %c0_0]
      %15 = arith.addi %14, %13 : index
      %true = arith.constant true
      %16 = arith.muli %14, %c1 : index
      %17 = arith.addi %16, %13 : index
      %18 = arith.cmpi ult, %17, %arg3 : index
      %19 = arith.andi %true, %18 : i1
      scf.if %19 {
        %20 = arith.remsi %15, %c128 : index
        %21 = arith.divsi %15, %c128 : index
        %22 = arith.divui %20, %c8 : index
        %23 = arith.remui %20, %c8 : index
        %24 = arith.divui %21, %arg4 : index
        %25 = arith.remui %21, %arg4 : index
        %26 = arith.muli %24, %c16 : index
        %27 = arith.addi %26, %22 : index
        %28 = arith.muli %25, %c8 : index
        %29 = arith.addi %28, %23 : index
        %30 = arith.cmpi ult, %27, %dim : index
        %31 = arith.cmpi ult, %29, %arg0 : index
        %32 = arith.andi %30, %31 : i1
        scf.if %32 {
          %47 = arith.muli %27, %arg0 : index
          %48 = arith.addi %47, %29 : index
          %49 = arith.muli %dim, %dim_3 : index
          %50 = arith.muli %49, %dim_4 : index
          %reinterpret_cast = memref.reinterpret_cast %arg1 to offset: [%c0_1], sizes: [%50], strides: [%c1_2] : memref<?x?x?xf32, "gpu"> to memref<?xf32, "gpu">
          %51 = memref.load %reinterpret_cast[%48] : memref<?xf32, "gpu">
          %52 = math.absf %51 : f32
          %reinterpret_cast_5 = memref.reinterpret_cast %arg6 to offset: [0], sizes: [128], strides: [1] : memref<128xf32, #gpu.address_space<workgroup>> to memref<128xf32, #gpu.address_space<workgroup>>
          memref.store %52, %reinterpret_cast_5[%20] : memref<128xf32, #gpu.address_space<workgroup>>
        } else {
          %reinterpret_cast = memref.reinterpret_cast %arg6 to offset: [0], sizes: [128], strides: [1] : memref<128xf32, #gpu.address_space<workgroup>> to memref<128xf32, #gpu.address_space<workgroup>>
          memref.store %cst, %reinterpret_cast[%20] : memref<128xf32, #gpu.address_space<workgroup>>
        }
        gpu.barrier
        %33 = arith.cmpi ult, %22, %c8 : index
        %34 = arith.addi %27, %c8 : index
        %35 = arith.cmpi ult, %34, %dim : index
        %36 = arith.andi %33, %35 : i1
        scf.if %36 {
          %47 = arith.addi %20, %c64 : index
          %reinterpret_cast = memref.reinterpret_cast %arg6 to offset: [0], sizes: [128], strides: [1] : memref<128xf32, #gpu.address_space<workgroup>> to memref<128xf32, #gpu.address_space<workgroup>>
          %48 = memref.load %reinterpret_cast[%20] : memref<128xf32, #gpu.address_space<workgroup>>
          %49 = memref.load %reinterpret_cast[%47] : memref<128xf32, #gpu.address_space<workgroup>>
          %50 = arith.cmpf ugt, %48, %49 : f32
          %51 = arith.select %50, %48, %49 : f32
          %52 = arith.cmpf uno, %49, %49 : f32
          %53 = arith.select %52, %49, %51 : f32
          memref.store %53, %reinterpret_cast[%20] : memref<128xf32, #gpu.address_space<workgroup>>
        }
        gpu.barrier
        %37 = arith.cmpi ult, %22, %c4 : index
        %38 = arith.addi %27, %c4 : index
        %39 = arith.cmpi ult, %38, %dim : index
        %40 = arith.andi %37, %39 : i1
        scf.if %40 {
          %47 = arith.addi %20, %c32 : index
          %reinterpret_cast = memref.reinterpret_cast %arg6 to offset: [0], sizes: [128], strides: [1] : memref<128xf32, #gpu.address_space<workgroup>> to memref<128xf32, #gpu.address_space<workgroup>>
          %48 = memref.load %reinterpret_cast[%20] : memref<128xf32, #gpu.address_space<workgroup>>
          %49 = memref.load %reinterpret_cast[%47] : memref<128xf32, #gpu.address_space<workgroup>>
          %50 = arith.cmpf ugt, %48, %49 : f32
          %51 = arith.select %50, %48, %49 : f32
          %52 = arith.cmpf uno, %49, %49 : f32
          %53 = arith.select %52, %49, %51 : f32
          memref.store %53, %reinterpret_cast[%20] : memref<128xf32, #gpu.address_space<workgroup>>
        }
        gpu.barrier
        %41 = arith.cmpi ult, %22, %c2 : index
        %42 = arith.addi %27, %c2 : index
        %43 = arith.cmpi ult, %42, %dim : index
        %44 = arith.andi %41, %43 : i1
        scf.if %44 {
          %47 = arith.addi %20, %c16 : index
          %reinterpret_cast = memref.reinterpret_cast %arg6 to offset: [0], sizes: [128], strides: [1] : memref<128xf32, #gpu.address_space<workgroup>> to memref<128xf32, #gpu.address_space<workgroup>>
          %48 = memref.load %reinterpret_cast[%20] : memref<128xf32, #gpu.address_space<workgroup>>
          %49 = memref.load %reinterpret_cast[%47] : memref<128xf32, #gpu.address_space<workgroup>>
          %50 = arith.cmpf ugt, %48, %49 : f32
          %51 = arith.select %50, %48, %49 : f32
          %52 = arith.cmpf uno, %49, %49 : f32
          %53 = arith.select %52, %49, %51 : f32
          memref.store %53, %reinterpret_cast[%20] : memref<128xf32, #gpu.address_space<workgroup>>
        }
        gpu.barrier
        %45 = arith.cmpi eq, %22, %c0_1 : index
        %46 = arith.andi %45, %32 : i1
        scf.if %46 {
          %47 = arith.addi %20, %c8 : index
          %reinterpret_cast = memref.reinterpret_cast %arg6 to offset: [0], sizes: [128], strides: [1] : memref<128xf32, #gpu.address_space<workgroup>> to memref<128xf32, #gpu.address_space<workgroup>>
          %48 = memref.load %reinterpret_cast[%20] : memref<128xf32, #gpu.address_space<workgroup>>
          %49 = memref.load %reinterpret_cast[%47] : memref<128xf32, #gpu.address_space<workgroup>>
          %50 = arith.cmpf ugt, %48, %49 : f32
          %51 = arith.select %50, %48, %49 : f32
          %52 = arith.cmpf uno, %49, %49 : f32
          %53 = arith.select %52, %49, %51 : f32
          %54 = memref.generic_atomic_rmw %arg5[%29] : memref<?xf32, "gpu"> {
          ^bb0(%arg7: f32):
            %55 = arith.cmpf ogt, %arg7, %53 : f32
            %56 = arith.select %55, %arg7, %53 : f32
            memref.atomic_yield %56 : f32
          }
        }
      }
      gpu.return
    }
  }
  "disc_shape.SymbolicDim"() {knownNegativeOne = false, knownNonNegative = true, knownNonSizeOne = false, knownNonSizeZero = false, sym_name = "S0", value = -9223372036854775808 : i64} : () -> ()
  "disc_shape.SymbolicDim"() {knownNegativeOne = false, knownNonNegative = true, knownNonSizeOne = false, knownNonSizeZero = false, sym_name = "S1", value = -9223372036854775808 : i64} : () -> ()
  "disc_shape.SymbolicDim"() {knownNegativeOne = false, knownNonNegative = true, knownNonSizeOne = false, knownNonSizeZero = false, sym_name = "S2", value = -9223372036854775808 : i64} : () -> ()
  "disc_shape.SymbolicDim"() {knownNegativeOne = false, knownNonNegative = true, knownNonSizeOne = false, knownNonSizeZero = false, sym_name = "S3", value = -9223372036854775808 : i64} : () -> ()
  func.func @shape_constraint_graph() {
    %0 = "disc_shape.dim"() {name = @S3} : () -> index
    %1 = "disc_shape.dim"() {name = @S1} : () -> index
    %2 = "disc_shape.dim"() {name = @S2} : () -> index
    "disc_shape.tie_product_equal"(%0, %1, %2) {operand_segment_sizes = array<i32: 1, 2>} : (index, index, index) -> ()
    return
  }
}


// -----// IR Dump After SideEffectLoopInvariantCodeMotionPass (disc-side-effect-loop-invariant-code-motion) //----- //
gpu.func @main_kColReduction_reduce__4_1_0___8w32h(%arg0: index, %arg1: index, %arg2: memref<?xf32, "gpu">) kernel {
  %cst = arith.constant 0xFF800000 : f32
  %c1 = arith.constant 1 : index
  %c0 = arith.constant 0 : index
  %0 = gpu.block_id  x
  %1 = gpu.thread_id  x
  cf.br ^bb1
^bb1:  // pred: ^bb0
  %2 = affine.apply affine_map<(d0)[s0, s1] -> (d0 * s0 + s1)>(%0)[%arg0, %c0]
  %3 = affine.apply affine_map<(d0)[s0, s1] -> (d0 * s0 + s1)>(%1)[%c1, %c0]
  %4 = arith.addi %3, %2 : index
  %5 = arith.addi %3, %2 : index
  %6 = arith.cmpi ult, %5, %arg1 : index
  scf.if %6 {
    %reinterpret_cast = memref.reinterpret_cast %arg2 to offset: [%c0], sizes: [%arg1], strides: [%c1] : memref<?xf32, "gpu"> to memref<?xf32, "gpu">
    memref.store %cst, %reinterpret_cast[%4] : memref<?xf32, "gpu">
  }
  gpu.return
}

// -----// IR Dump After CSE (cse) //----- //
gpu.module @main_kernel {
  gpu.func @main_kColReduction_reduce__4_1_0___8w32h(%arg0: index, %arg1: index, %arg2: memref<?xf32, "gpu">) kernel {
    %cst = arith.constant 0xFF800000 : f32
    %c1 = arith.constant 1 : index
    %c0 = arith.constant 0 : index
    %0 = gpu.block_id  x
    %1 = gpu.thread_id  x
    cf.br ^bb1
  ^bb1:  // pred: ^bb0
    %2 = affine.apply affine_map<(d0)[s0, s1] -> (d0 * s0 + s1)>(%0)[%arg0, %c0]
    %3 = affine.apply affine_map<(d0)[s0, s1] -> (d0 * s0 + s1)>(%1)[%c1, %c0]
    %4 = arith.addi %3, %2 : index
    %5 = arith.cmpi ult, %4, %arg1 : index
    scf.if %5 {
      %reinterpret_cast = memref.reinterpret_cast %arg2 to offset: [%c0], sizes: [%arg1], strides: [%c1] : memref<?xf32, "gpu"> to memref<?xf32, "gpu">
      memref.store %cst, %reinterpret_cast[%4] : memref<?xf32, "gpu">
    }
    gpu.return
  }
}

// -----// IR Dump After SCFToControlFlow (convert-scf-to-cf) //----- //
gpu.module @main_kernel {
  gpu.func @main_kColReduction_reduce__4_1_0___8w32h(%arg0: index, %arg1: index, %arg2: memref<?xf32, "gpu">) kernel {
    %cst = arith.constant 0xFF800000 : f32
    %c1 = arith.constant 1 : index
    %c0 = arith.constant 0 : index
    %0 = gpu.block_id  x
    %1 = gpu.thread_id  x
    cf.br ^bb1
  ^bb1:  // pred: ^bb0
    %2 = affine.apply affine_map<(d0)[s0, s1] -> (d0 * s0 + s1)>(%0)[%arg0, %c0]
    %3 = affine.apply affine_map<(d0)[s0, s1] -> (d0 * s0 + s1)>(%1)[%c1, %c0]
    %4 = arith.addi %3, %2 : index
    %5 = arith.cmpi ult, %4, %arg1 : index
    cf.cond_br %5, ^bb2, ^bb3
  ^bb2:  // pred: ^bb1
    %reinterpret_cast = memref.reinterpret_cast %arg2 to offset: [%c0], sizes: [%arg1], strides: [%c1] : memref<?xf32, "gpu"> to memref<?xf32, "gpu">
    memref.store %cst, %reinterpret_cast[%4] : memref<?xf32, "gpu">
    cf.br ^bb3
  ^bb3:  // 2 preds: ^bb1, ^bb2
    gpu.return
  }
}

// -----// IR Dump After ConvertAffineToStandard (lower-affine) //----- //
gpu.module @main_kernel {
  gpu.func @main_kColReduction_reduce__4_1_0___8w32h(%arg0: index, %arg1: index, %arg2: memref<?xf32, "gpu">) kernel {
    %cst = arith.constant 0xFF800000 : f32
    %c1 = arith.constant 1 : index
    %c0 = arith.constant 0 : index
    %0 = gpu.block_id  x
    %1 = gpu.thread_id  x
    cf.br ^bb1
  ^bb1:  // pred: ^bb0
    %2 = arith.muli %0, %arg0 : index
    %3 = arith.addi %2, %c0 : index
    %4 = arith.muli %1, %c1 : index
    %5 = arith.addi %4, %c0 : index
    %6 = arith.addi %5, %3 : index
    %7 = arith.cmpi ult, %6, %arg1 : index
    cf.cond_br %7, ^bb2, ^bb3
  ^bb2:  // pred: ^bb1
    %reinterpret_cast = memref.reinterpret_cast %arg2 to offset: [%c0], sizes: [%arg1], strides: [%c1] : memref<?xf32, "gpu"> to memref<?xf32, "gpu">
    memref.store %cst, %reinterpret_cast[%6] : memref<?xf32, "gpu">
    cf.br ^bb3
  ^bb3:  // 2 preds: ^bb1, ^bb2
    gpu.return
  }
}

// -----// IR Dump After StripDebugInfo (strip-debuginfo) //----- //
gpu.module @main_kernel {
  gpu.func @main_kColReduction_reduce__4_1_0___8w32h(%arg0: index, %arg1: index, %arg2: memref<?xf32, "gpu">) kernel {
    %cst = arith.constant 0xFF800000 : f32
    %c1 = arith.constant 1 : index
    %c0 = arith.constant 0 : index
    %0 = gpu.block_id  x
    %1 = gpu.thread_id  x
    cf.br ^bb1
  ^bb1:  // pred: ^bb0
    %2 = arith.muli %0, %arg0 : index
    %3 = arith.addi %2, %c0 : index
    %4 = arith.muli %1, %c1 : index
    %5 = arith.addi %4, %c0 : index
    %6 = arith.addi %5, %3 : index
    %7 = arith.cmpi ult, %6, %arg1 : index
    cf.cond_br %7, ^bb2, ^bb3
  ^bb2:  // pred: ^bb1
    %reinterpret_cast = memref.reinterpret_cast %arg2 to offset: [%c0], sizes: [%arg1], strides: [%c1] : memref<?xf32, "gpu"> to memref<?xf32, "gpu">
    memref.store %cst, %reinterpret_cast[%6] : memref<?xf32, "gpu">
    cf.br ^bb3
  ^bb3:  // 2 preds: ^bb1, ^bb2
    gpu.return
  }
}

// -----// IR Dump After DiscLowerGpuOpsToNVVMOpsPass (disc-convert-gpu-to-nvvm) //----- //
gpu.module @main_kernel {
  llvm.func @main_kColReduction_reduce__4_1_0___8w32h(%arg0: i32, %arg1: i32, %arg2: !llvm.ptr<f32>, %arg3: !llvm.ptr<f32>, %arg4: i32, %arg5: i32, %arg6: i32) attributes {gpu.kernel, nvvm.kernel} {
    %0 = llvm.mlir.undef : !llvm.struct<(ptr<f32>, ptr<f32>, i32, array<1 x i32>, array<1 x i32>)>
    %1 = llvm.insertvalue %arg2, %0[0] : !llvm.struct<(ptr<f32>, ptr<f32>, i32, array<1 x i32>, array<1 x i32>)> 
    %2 = llvm.insertvalue %arg3, %1[1] : !llvm.struct<(ptr<f32>, ptr<f32>, i32, array<1 x i32>, array<1 x i32>)> 
    %3 = llvm.insertvalue %arg4, %2[2] : !llvm.struct<(ptr<f32>, ptr<f32>, i32, array<1 x i32>, array<1 x i32>)> 
    %4 = llvm.insertvalue %arg5, %3[3, 0] : !llvm.struct<(ptr<f32>, ptr<f32>, i32, array<1 x i32>, array<1 x i32>)> 
    %5 = llvm.insertvalue %arg6, %4[4, 0] : !llvm.struct<(ptr<f32>, ptr<f32>, i32, array<1 x i32>, array<1 x i32>)> 
    %6 = llvm.mlir.constant(0xFF800000 : f32) : f32
    %7 = llvm.mlir.constant(1 : index) : i32
    %8 = llvm.mlir.constant(0 : index) : i32
    %9 = nvvm.read.ptx.sreg.ctaid.x : i32
    %10 = nvvm.read.ptx.sreg.tid.x : i32
    llvm.br ^bb1
  ^bb1:  // pred: ^bb0
    %11 = llvm.mul %9, %arg0  : i32
    %12 = llvm.add %10, %11  : i32
    %13 = llvm.icmp "ult" %12, %arg1 : i32
    llvm.cond_br %13, ^bb2, ^bb3
  ^bb2:  // pred: ^bb1
    %14 = llvm.mlir.undef : !llvm.struct<(ptr<f32>, ptr<f32>, i32, array<1 x i32>, array<1 x i32>)>
    %15 = llvm.extractvalue %5[0] : !llvm.struct<(ptr<f32>, ptr<f32>, i32, array<1 x i32>, array<1 x i32>)> 
    %16 = llvm.extractvalue %5[1] : !llvm.struct<(ptr<f32>, ptr<f32>, i32, array<1 x i32>, array<1 x i32>)> 
    %17 = llvm.insertvalue %15, %14[0] : !llvm.struct<(ptr<f32>, ptr<f32>, i32, array<1 x i32>, array<1 x i32>)> 
    %18 = llvm.insertvalue %16, %17[1] : !llvm.struct<(ptr<f32>, ptr<f32>, i32, array<1 x i32>, array<1 x i32>)> 
    %19 = llvm.insertvalue %8, %18[2] : !llvm.struct<(ptr<f32>, ptr<f32>, i32, array<1 x i32>, array<1 x i32>)> 
    %20 = llvm.insertvalue %arg1, %19[3, 0] : !llvm.struct<(ptr<f32>, ptr<f32>, i32, array<1 x i32>, array<1 x i32>)> 
    %21 = llvm.insertvalue %7, %20[4, 0] : !llvm.struct<(ptr<f32>, ptr<f32>, i32, array<1 x i32>, array<1 x i32>)> 
    %22 = llvm.extractvalue %21[1] : !llvm.struct<(ptr<f32>, ptr<f32>, i32, array<1 x i32>, array<1 x i32>)> 
    %23 = llvm.getelementptr %22[%12] : (!llvm.ptr<f32>, i32) -> !llvm.ptr<f32>
    llvm.store %6, %23 : !llvm.ptr<f32>
    llvm.br ^bb3
  ^bb3:  // 2 preds: ^bb1, ^bb2
    llvm.return
  }
}

// -----// IR Dump After LLVMInsertValueSimplifierPass (disc-llvm-insert-value-simplifier) //----- //
llvm.func @main_kColReduction_reduce__4_1_0___8w32h(%arg0: i32, %arg1: i32, %arg2: !llvm.ptr<f32>, %arg3: !llvm.ptr<f32>, %arg4: i32, %arg5: i32, %arg6: i32) attributes {gpu.kernel, nvvm.kernel} {
  %0 = llvm.mlir.constant(0xFF800000 : f32) : f32
  %1 = nvvm.read.ptx.sreg.ctaid.x : i32
  %2 = nvvm.read.ptx.sreg.tid.x : i32
  llvm.br ^bb1
^bb1:  // pred: ^bb0
  %3 = llvm.mul %1, %arg0  : i32
  %4 = llvm.add %2, %3  : i32
  %5 = llvm.icmp "ult" %4, %arg1 : i32
  llvm.cond_br %5, ^bb2, ^bb3
^bb2:  // pred: ^bb1
  %6 = llvm.getelementptr %arg3[%4] : (!llvm.ptr<f32>, i32) -> !llvm.ptr<f32>
  llvm.store %0, %6 : !llvm.ptr<f32>
  llvm.br ^bb3
^bb3:  // 2 preds: ^bb1, ^bb2
  llvm.return
}

// -----// IR Dump After FunctionDeadArgumentEliminationPass (disc-function-dead-argument-elimination) //----- //
gpu.module @main_kernel {
  llvm.func @main_kColReduction_reduce__4_1_0___8w32h(%arg0: i32, %arg1: i32, %arg2: !llvm.ptr<f32>) attributes {disc.elimargs = [2 : index, 4 : index, 5 : index, 6 : index], gpu.kernel, nvvm.kernel} {
    %0 = llvm.mlir.constant(0xFF800000 : f32) : f32
    %1 = nvvm.read.ptx.sreg.ctaid.x : i32
    %2 = nvvm.read.ptx.sreg.tid.x : i32
    llvm.br ^bb1
  ^bb1:  // pred: ^bb0
    %3 = llvm.mul %1, %arg0  : i32
    %4 = llvm.add %2, %3  : i32
    %5 = llvm.icmp "ult" %4, %arg1 : i32
    llvm.cond_br %5, ^bb2, ^bb3
  ^bb2:  // pred: ^bb1
    %6 = llvm.getelementptr %arg2[%4] : (!llvm.ptr<f32>, i32) -> !llvm.ptr<f32>
    llvm.store %0, %6 : !llvm.ptr<f32>
    llvm.br ^bb3
  ^bb3:  // 2 preds: ^bb1, ^bb2
    llvm.return
  }
}

// -----// IR Dump After GpuKernelToBlobPass (disc-gpu-kernel-to-blob) //----- //
gpu.module @main_kernel attributes {gpu.binary = "P\EDU\BA\01\00\10\008\04\00\00\00\00\00\00\02\00\01\01@\00\00\00\F8\03\00\00\00\00\00\00\F8\03\00\00\00\00\00\00\07\00\01\00P\00\00\00\00\00\00\00\00\00\00\00\11 \00\00\00\00\00\00\00\00\00\00\00\00\00\00\E8\0B\00\00\00\00\00\00\A2\7FELF\02\01\013\07\00\01\00f\02\00\BE\00q\00\01\00!@\0B\07\001\00\80\08\07\00\F5\0E\00P\05P\00@\008\00\03\00@\00\0B\00\01\00\00.shstrtab\00.\08\00'ym\08\00\F0\1B_shndx\00.nv.info\00.text.main_kColReduction_r\0A\00\F5\01e__4_1_0___8w32h8\00\0F2\00\1Boshared4\00\1B\9Fconstant07\00\18\FA\01debug_frame\00.rel\11\00!nv\14\00\11aC\00\0F+\01 \0F\88\00\15\0FT\01\BAo_param[\01\1C\0F\01\00\06\8C[\00\00\00\03\00\0A\00\01\00\11\F0\18\00,\09\00\01\00 .\01\18\00,\04\00\01\00\11L\18\00,\07\00\01\00f2\00\00\00\12\10`\00!\80\01\07\00v\00\FF\FF\FF\FF(\00\0C\00\00\01\00@\03\00\04|\08\00\D1\0F\0C\81\80\80(\00\08\FF\81\80(\08\0B\00\06(\00/0\00\01\00\03\13pX\00\10\04\9B\00R\04\14\00\00\00E\00\01\0B\00\00\13\00p/\08\00\05\00\00\00\A7\03\22\04#\0C\00\00\01\00'\04\12\0C\00\17\11\0C\00!7\04\E8\03\F3\08\015\00\00\04\0A\08\00\02\00\00\00`\01\10\00\03\19\10\00\04\17\0C\0C\04U\08\00\00\F0!\10\00\10\01\18\01%\F0\11\10\00\01\01\00\F2\02\F0\11\00\03\1B\FF\00\04\1C\08\00P\00\00\00\B0\00\01\00#K\00\01\00s\02\02\08\10\0A/\22\9B\00\00\07\00\03\08\00\13\10\08\00\13\18\08\00\13 \08\00\13(\08\00\130\08\00\228\08|\00\13\00\08\00\13\08\08\00\13\10\08\00\13\18\08\00\13 \08\00\13(\08\00\130\08\00\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\A2\00\00\00\14,\00\00\00H\00\01\00\000\01/\05\00\01\00\FF\B0A\02z\01\00\1F\04\B1\0F\00\00\00\C4\0F\00\19y\02\00\01\00\10%\8B\02Q\0E\00\19y\03\0F\00\F5\1A\00!\00\00\00$\0E\00$z\02\02\00X\00\00\03\02\8E\07\00\CA\1F\00\0Cz\00\02\00Y\00\00p`\F0\03\00\DA\0F\00MS\04\A0\80\03\00\EA\0F\005t\03\FF\B3\03\10\FF\C0\03P\E2\0F\00\02x6\02B\80\FF\00\0F\10\00r\B9z\04\00\00F\00\84\00\94\D0\0F\00%v\02\02\00Z`\00`\0F\00\86y\00\022\00@\04\19\10\0C0\009My\00`\00PGy\00\00\F09\04\A6\FF\83\03\00\C0\0F\00\18y\00\01\00\0F\10\00\90\0F\01\00-\00W\01.\03\00\01\00\22@\00\01\00=+\01\000\00\08\01\00\1F\0B@\00\04\13k)\00\1F[@\00\0C\13\13\0C\04\0C\01\00\13\C8\15\00&\90\000\04#\04\00\85\04\00\F6\04\12\00\01\00\1F\FET\00\00\00\01\00\13X\95\00/p\00\80\00\0B\1F)'\00\03#\00\C8@\00\04P\06\04\E4\00*\04\00\01\00\1Fa@\00\04\13\F81\00&\\\00@\00\1F\0A@\00\00!\1C\01D\01\0D@\00\13X)\00*\D8\00\01\00\1B\08\08\00?\0B\01\00\86\07\00Q\00\000\05\00\01\00&\10\00\80\00\17\048\00\04\18\00\13\C7\14\01\0C\84\01*@\058\07\1F\00\C0\00\04\132@\00+\06\00\01\00\1A\07\D0\07\12\03\F0\05:\08\80\00\01\00\13\06\08\06\04(\0B\0C\01\00*\A8\00\08\00\04\F8\00\14\018\00/\05\00\01\00\029@\03\00\08\00\088\00/\06\00\01\00\17\80\08\00\00\00\00\00\00\00"} {
  llvm.func @main_kColReduction_reduce__4_1_0___8w32h(%arg0: i32, %arg1: i32, %arg2: !llvm.ptr<f32>) attributes {disc.elimargs = [2 : index, 4 : index, 5 : index, 6 : index], gpu.kernel, nvvm.kernel} {
    %0 = llvm.mlir.constant(0xFF800000 : f32) : f32
    %1 = nvvm.read.ptx.sreg.ctaid.x : i32
    %2 = nvvm.read.ptx.sreg.tid.x : i32
    llvm.br ^bb1
  ^bb1:  // pred: ^bb0
    %3 = llvm.mul %1, %arg0  : i32
    %4 = llvm.add %2, %3  : i32
    %5 = llvm.icmp "ult" %4, %arg1 : i32
    llvm.cond_br %5, ^bb2, ^bb3
  ^bb2:  // pred: ^bb1
    %6 = llvm.getelementptr %arg2[%4] : (!llvm.ptr<f32>, i32) -> !llvm.ptr<f32>
    llvm.store %0, %6 : !llvm.ptr<f32>
    llvm.br ^bb3
  ^bb3:  // 2 preds: ^bb1, ^bb2
    llvm.return
  }
}

// -----// IR Dump After SideEffectLoopInvariantCodeMotionPass (disc-side-effect-loop-invariant-code-motion) //----- //
gpu.func @main_kColReduction_reduce__4_1_0___8w32h_1(%arg0: index, %arg1: memref<?x?x?xf32, "gpu">, %arg2: index, %arg3: index, %arg4: index, %arg5: memref<?xf32, "gpu">) workgroup(%arg6 : memref<256xf32, #gpu.address_space<workgroup>>) kernel {
  %c4 = arith.constant 4 : index
  %c64 = arith.constant 64 : index
  %c128 = arith.constant 128 : index
  %c16 = arith.constant 16 : index
  %cst = arith.constant 0xFF800000 : f32
  %c2 = arith.constant 2 : index
  %c32 = arith.constant 32 : index
  %c8 = arith.constant 8 : index
  %c256 = arith.constant 256 : index
  %c1 = arith.constant 1 : index
  %c0 = arith.constant 0 : index
  %0 = gpu.block_id  x
  %1 = gpu.thread_id  x
  cf.br ^bb1
^bb1:  // pred: ^bb0
  %dim = memref.dim %arg1, %c0 : memref<?x?x?xf32, "gpu">
  %dim_0 = memref.dim %arg1, %c1 : memref<?x?x?xf32, "gpu">
  %dim_1 = memref.dim %arg1, %c2 : memref<?x?x?xf32, "gpu">
  %2 = affine.apply affine_map<(d0)[s0, s1] -> (d0 * s0 + s1)>(%0)[%arg2, %c0]
  %3 = affine.apply affine_map<(d0)[s0, s1] -> (d0 * s0 + s1)>(%1)[%c1, %c0]
  %4 = arith.addi %3, %2 : index
  %5 = arith.addi %3, %2 : index
  %6 = arith.cmpi ult, %5, %arg3 : index
  scf.if %6 {
    %7 = arith.remsi %4, %c256 : index
    %8 = arith.divsi %4, %c256 : index
    %9 = arith.divui %7, %c8 : index
    %10 = arith.remui %7, %c8 : index
    %11 = arith.divui %8, %arg4 : index
    %12 = arith.remui %8, %arg4 : index
    %13 = arith.muli %11, %c32 : index
    %14 = arith.addi %13, %9 : index
    %15 = arith.muli %12, %c8 : index
    %16 = arith.addi %15, %10 : index
    %17 = arith.cmpi ult, %14, %dim : index
    %18 = arith.cmpi ult, %16, %arg0 : index
    %19 = arith.andi %17, %18 : i1
    scf.if %19 {
      %38 = arith.muli %14, %arg0 : index
      %39 = arith.addi %38, %16 : index
      %40 = arith.muli %dim, %dim_0 : index
      %41 = arith.muli %40, %dim_1 : index
      %reinterpret_cast = memref.reinterpret_cast %arg1 to offset: [%c0], sizes: [%41], strides: [%c1] : memref<?x?x?xf32, "gpu"> to memref<?xf32, "gpu">
      %42 = memref.load %reinterpret_cast[%39] : memref<?xf32, "gpu">
      %43 = math.absf %42 : f32
      %reinterpret_cast_2 = memref.reinterpret_cast %arg6 to offset: [0], sizes: [256], strides: [1] : memref<256xf32, #gpu.address_space<workgroup>> to memref<256xf32, #gpu.address_space<workgroup>>
      memref.store %43, %reinterpret_cast_2[%7] : memref<256xf32, #gpu.address_space<workgroup>>
    } else {
      %reinterpret_cast = memref.reinterpret_cast %arg6 to offset: [0], sizes: [256], strides: [1] : memref<256xf32, #gpu.address_space<workgroup>> to memref<256xf32, #gpu.address_space<workgroup>>
      memref.store %cst, %reinterpret_cast[%7] : memref<256xf32, #gpu.address_space<workgroup>>
    }
    gpu.barrier
    %20 = arith.cmpi ult, %9, %c16 : index
    %21 = arith.addi %14, %c16 : index
    %22 = arith.cmpi ult, %21, %dim : index
    %23 = arith.andi %20, %22 : i1
    scf.if %23 {
      %38 = arith.addi %7, %c128 : index
      %reinterpret_cast = memref.reinterpret_cast %arg6 to offset: [0], sizes: [256], strides: [1] : memref<256xf32, #gpu.address_space<workgroup>> to memref<256xf32, #gpu.address_space<workgroup>>
      %39 = memref.load %reinterpret_cast[%7] : memref<256xf32, #gpu.address_space<workgroup>>
      %40 = memref.load %reinterpret_cast[%38] : memref<256xf32, #gpu.address_space<workgroup>>
      %41 = arith.cmpf ugt, %39, %40 : f32
      %42 = arith.select %41, %39, %40 : f32
      %43 = arith.cmpf uno, %40, %40 : f32
      %44 = arith.select %43, %40, %42 : f32
      memref.store %44, %reinterpret_cast[%7] : memref<256xf32, #gpu.address_space<workgroup>>
    }
    gpu.barrier
    %24 = arith.cmpi ult, %9, %c8 : index
    %25 = arith.addi %14, %c8 : index
    %26 = arith.cmpi ult, %25, %dim : index
    %27 = arith.andi %24, %26 : i1
    scf.if %27 {
      %38 = arith.addi %7, %c64 : index
      %reinterpret_cast = memref.reinterpret_cast %arg6 to offset: [0], sizes: [256], strides: [1] : memref<256xf32, #gpu.address_space<workgroup>> to memref<256xf32, #gpu.address_space<workgroup>>
      %39 = memref.load %reinterpret_cast[%7] : memref<256xf32, #gpu.address_space<workgroup>>
      %40 = memref.load %reinterpret_cast[%38] : memref<256xf32, #gpu.address_space<workgroup>>
      %41 = arith.cmpf ugt, %39, %40 : f32
      %42 = arith.select %41, %39, %40 : f32
      %43 = arith.cmpf uno, %40, %40 : f32
      %44 = arith.select %43, %40, %42 : f32
      memref.store %44, %reinterpret_cast[%7] : memref<256xf32, #gpu.address_space<workgroup>>
    }
    gpu.barrier
    %28 = arith.cmpi ult, %9, %c4 : index
    %29 = arith.addi %14, %c4 : index
    %30 = arith.cmpi ult, %29, %dim : index
    %31 = arith.andi %28, %30 : i1
    scf.if %31 {
      %38 = arith.addi %7, %c32 : index
      %reinterpret_cast = memref.reinterpret_cast %arg6 to offset: [0], sizes: [256], strides: [1] : memref<256xf32, #gpu.address_space<workgroup>> to memref<256xf32, #gpu.address_space<workgroup>>
      %39 = memref.load %reinterpret_cast[%7] : memref<256xf32, #gpu.address_space<workgroup>>
      %40 = memref.load %reinterpret_cast[%38] : memref<256xf32, #gpu.address_space<workgroup>>
      %41 = arith.cmpf ugt, %39, %40 : f32
      %42 = arith.select %41, %39, %40 : f32
      %43 = arith.cmpf uno, %40, %40 : f32
      %44 = arith.select %43, %40, %42 : f32
      memref.store %44, %reinterpret_cast[%7] : memref<256xf32, #gpu.address_space<workgroup>>
    }
    gpu.barrier
    %32 = arith.cmpi ult, %9, %c2 : index
    %33 = arith.addi %14, %c2 : index
    %34 = arith.cmpi ult, %33, %dim : index
    %35 = arith.andi %32, %34 : i1
    scf.if %35 {
      %38 = arith.addi %7, %c16 : index
      %reinterpret_cast = memref.reinterpret_cast %arg6 to offset: [0], sizes: [256], strides: [1] : memref<256xf32, #gpu.address_space<workgroup>> to memref<256xf32, #gpu.address_space<workgroup>>
      %39 = memref.load %reinterpret_cast[%7] : memref<256xf32, #gpu.address_space<workgroup>>
      %40 = memref.load %reinterpret_cast[%38] : memref<256xf32, #gpu.address_space<workgroup>>
      %41 = arith.cmpf ugt, %39, %40 : f32
      %42 = arith.select %41, %39, %40 : f32
      %43 = arith.cmpf uno, %40, %40 : f32
      %44 = arith.select %43, %40, %42 : f32
      memref.store %44, %reinterpret_cast[%7] : memref<256xf32, #gpu.address_space<workgroup>>
    }
    gpu.barrier
    %36 = arith.cmpi eq, %9, %c0 : index
    %37 = arith.andi %36, %19 : i1
    scf.if %37 {
      %38 = arith.addi %7, %c8 : index
      %reinterpret_cast = memref.reinterpret_cast %arg6 to offset: [0], sizes: [256], strides: [1] : memref<256xf32, #gpu.address_space<workgroup>> to memref<256xf32, #gpu.address_space<workgroup>>
      %39 = memref.load %reinterpret_cast[%7] : memref<256xf32, #gpu.address_space<workgroup>>
      %40 = memref.load %reinterpret_cast[%38] : memref<256xf32, #gpu.address_space<workgroup>>
      %41 = arith.cmpf ugt, %39, %40 : f32
      %42 = arith.select %41, %39, %40 : f32
      %43 = arith.cmpf uno, %40, %40 : f32
      %44 = arith.select %43, %40, %42 : f32
      %45 = memref.generic_atomic_rmw %arg5[%16] : memref<?xf32, "gpu"> {
      ^bb0(%arg7: f32):
        %46 = arith.cmpf ogt, %arg7, %44 : f32
        %47 = arith.select %46, %arg7, %44 : f32
        memref.atomic_yield %47 : f32
      }
    }
  }
  gpu.return
}

// -----// IR Dump After CSE (cse) //----- //
gpu.module @main_kernel_0 {
  gpu.func @main_kColReduction_reduce__4_1_0___8w32h_1(%arg0: index, %arg1: memref<?x?x?xf32, "gpu">, %arg2: index, %arg3: index, %arg4: index, %arg5: memref<?xf32, "gpu">) workgroup(%arg6 : memref<256xf32, #gpu.address_space<workgroup>>) kernel {
    %c4 = arith.constant 4 : index
    %c64 = arith.constant 64 : index
    %c128 = arith.constant 128 : index
    %c16 = arith.constant 16 : index
    %cst = arith.constant 0xFF800000 : f32
    %c2 = arith.constant 2 : index
    %c32 = arith.constant 32 : index
    %c8 = arith.constant 8 : index
    %c256 = arith.constant 256 : index
    %c1 = arith.constant 1 : index
    %c0 = arith.constant 0 : index
    %0 = gpu.block_id  x
    %1 = gpu.thread_id  x
    cf.br ^bb1
  ^bb1:  // pred: ^bb0
    %dim = memref.dim %arg1, %c0 : memref<?x?x?xf32, "gpu">
    %dim_0 = memref.dim %arg1, %c1 : memref<?x?x?xf32, "gpu">
    %dim_1 = memref.dim %arg1, %c2 : memref<?x?x?xf32, "gpu">
    %2 = affine.apply affine_map<(d0)[s0, s1] -> (d0 * s0 + s1)>(%0)[%arg2, %c0]
    %3 = affine.apply affine_map<(d0)[s0, s1] -> (d0 * s0 + s1)>(%1)[%c1, %c0]
    %4 = arith.addi %3, %2 : index
    %5 = arith.cmpi ult, %4, %arg3 : index
    scf.if %5 {
      %6 = arith.remsi %4, %c256 : index
      %7 = arith.divsi %4, %c256 : index
      %8 = arith.divui %6, %c8 : index
      %9 = arith.remui %6, %c8 : index
      %10 = arith.divui %7, %arg4 : index
      %11 = arith.remui %7, %arg4 : index
      %12 = arith.muli %10, %c32 : index
      %13 = arith.addi %12, %8 : index
      %14 = arith.muli %11, %c8 : index
      %15 = arith.addi %14, %9 : index
      %16 = arith.cmpi ult, %13, %dim : index
      %17 = arith.cmpi ult, %15, %arg0 : index
      %18 = arith.andi %16, %17 : i1
      scf.if %18 {
        %37 = arith.muli %13, %arg0 : index
        %38 = arith.addi %37, %15 : index
        %39 = arith.muli %dim, %dim_0 : index
        %40 = arith.muli %39, %dim_1 : index
        %reinterpret_cast = memref.reinterpret_cast %arg1 to offset: [%c0], sizes: [%40], strides: [%c1] : memref<?x?x?xf32, "gpu"> to memref<?xf32, "gpu">
        %41 = memref.load %reinterpret_cast[%38] : memref<?xf32, "gpu">
        %42 = math.absf %41 : f32
        %reinterpret_cast_2 = memref.reinterpret_cast %arg6 to offset: [0], sizes: [256], strides: [1] : memref<256xf32, #gpu.address_space<workgroup>> to memref<256xf32, #gpu.address_space<workgroup>>
        memref.store %42, %reinterpret_cast_2[%6] : memref<256xf32, #gpu.address_space<workgroup>>
      } else {
        %reinterpret_cast = memref.reinterpret_cast %arg6 to offset: [0], sizes: [256], strides: [1] : memref<256xf32, #gpu.address_space<workgroup>> to memref<256xf32, #gpu.address_space<workgroup>>
        memref.store %cst, %reinterpret_cast[%6] : memref<256xf32, #gpu.address_space<workgroup>>
      }
      gpu.barrier
      %19 = arith.cmpi ult, %8, %c16 : index
      %20 = arith.addi %13, %c16 : index
      %21 = arith.cmpi ult, %20, %dim : index
      %22 = arith.andi %19, %21 : i1
      scf.if %22 {
        %37 = arith.addi %6, %c128 : index
        %reinterpret_cast = memref.reinterpret_cast %arg6 to offset: [0], sizes: [256], strides: [1] : memref<256xf32, #gpu.address_space<workgroup>> to memref<256xf32, #gpu.address_space<workgroup>>
        %38 = memref.load %reinterpret_cast[%6] : memref<256xf32, #gpu.address_space<workgroup>>
        %39 = memref.load %reinterpret_cast[%37] : memref<256xf32, #gpu.address_space<workgroup>>
        %40 = arith.cmpf ugt, %38, %39 : f32
        %41 = arith.select %40, %38, %39 : f32
        %42 = arith.cmpf uno, %39, %39 : f32
        %43 = arith.select %42, %39, %41 : f32
        memref.store %43, %reinterpret_cast[%6] : memref<256xf32, #gpu.address_space<workgroup>>
      }
      gpu.barrier
      %23 = arith.cmpi ult, %8, %c8 : index
      %24 = arith.addi %13, %c8 : index
      %25 = arith.cmpi ult, %24, %dim : index
      %26 = arith.andi %23, %25 : i1
      scf.if %26 {
        %37 = arith.addi %6, %c64 : index
        %reinterpret_cast = memref.reinterpret_cast %arg6 to offset: [0], sizes: [256], strides: [1] : memref<256xf32, #gpu.address_space<workgroup>> to memref<256xf32, #gpu.address_space<workgroup>>
        %38 = memref.load %reinterpret_cast[%6] : memref<256xf32, #gpu.address_space<workgroup>>
        %39 = memref.load %reinterpret_cast[%37] : memref<256xf32, #gpu.address_space<workgroup>>
        %40 = arith.cmpf ugt, %38, %39 : f32
        %41 = arith.select %40, %38, %39 : f32
        %42 = arith.cmpf uno, %39, %39 : f32
        %43 = arith.select %42, %39, %41 : f32
        memref.store %43, %reinterpret_cast[%6] : memref<256xf32, #gpu.address_space<workgroup>>
      }
      gpu.barrier
      %27 = arith.cmpi ult, %8, %c4 : index
      %28 = arith.addi %13, %c4 : index
      %29 = arith.cmpi ult, %28, %dim : index
      %30 = arith.andi %27, %29 : i1
      scf.if %30 {
        %37 = arith.addi %6, %c32 : index
        %reinterpret_cast = memref.reinterpret_cast %arg6 to offset: [0], sizes: [256], strides: [1] : memref<256xf32, #gpu.address_space<workgroup>> to memref<256xf32, #gpu.address_space<workgroup>>
        %38 = memref.load %reinterpret_cast[%6] : memref<256xf32, #gpu.address_space<workgroup>>
        %39 = memref.load %reinterpret_cast[%37] : memref<256xf32, #gpu.address_space<workgroup>>
        %40 = arith.cmpf ugt, %38, %39 : f32
        %41 = arith.select %40, %38, %39 : f32
        %42 = arith.cmpf uno, %39, %39 : f32
        %43 = arith.select %42, %39, %41 : f32
        memref.store %43, %reinterpret_cast[%6] : memref<256xf32, #gpu.address_space<workgroup>>
      }
      gpu.barrier
      %31 = arith.cmpi ult, %8, %c2 : index
      %32 = arith.addi %13, %c2 : index
      %33 = arith.cmpi ult, %32, %dim : index
      %34 = arith.andi %31, %33 : i1
      scf.if %34 {
        %37 = arith.addi %6, %c16 : index
        %reinterpret_cast = memref.reinterpret_cast %arg6 to offset: [0], sizes: [256], strides: [1] : memref<256xf32, #gpu.address_space<workgroup>> to memref<256xf32, #gpu.address_space<workgroup>>
        %38 = memref.load %reinterpret_cast[%6] : memref<256xf32, #gpu.address_space<workgroup>>
        %39 = memref.load %reinterpret_cast[%37] : memref<256xf32, #gpu.address_space<workgroup>>
        %40 = arith.cmpf ugt, %38, %39 : f32
        %41 = arith.select %40, %38, %39 : f32
        %42 = arith.cmpf uno, %39, %39 : f32
        %43 = arith.select %42, %39, %41 : f32
        memref.store %43, %reinterpret_cast[%6] : memref<256xf32, #gpu.address_space<workgroup>>
      }
      gpu.barrier
      %35 = arith.cmpi eq, %8, %c0 : index
      %36 = arith.andi %35, %18 : i1
      scf.if %36 {
        %37 = arith.addi %6, %c8 : index
        %reinterpret_cast = memref.reinterpret_cast %arg6 to offset: [0], sizes: [256], strides: [1] : memref<256xf32, #gpu.address_space<workgroup>> to memref<256xf32, #gpu.address_space<workgroup>>
        %38 = memref.load %reinterpret_cast[%6] : memref<256xf32, #gpu.address_space<workgroup>>
        %39 = memref.load %reinterpret_cast[%37] : memref<256xf32, #gpu.address_space<workgroup>>
        %40 = arith.cmpf ugt, %38, %39 : f32
        %41 = arith.select %40, %38, %39 : f32
        %42 = arith.cmpf uno, %39, %39 : f32
        %43 = arith.select %42, %39, %41 : f32
        %44 = memref.generic_atomic_rmw %arg5[%15] : memref<?xf32, "gpu"> {
        ^bb0(%arg7: f32):
          %45 = arith.cmpf ogt, %arg7, %43 : f32
          %46 = arith.select %45, %arg7, %43 : f32
          memref.atomic_yield %46 : f32
        }
      }
    }
    gpu.return
  }
}

// -----// IR Dump After SCFToControlFlow (convert-scf-to-cf) //----- //
gpu.module @main_kernel_0 {
  gpu.func @main_kColReduction_reduce__4_1_0___8w32h_1(%arg0: index, %arg1: memref<?x?x?xf32, "gpu">, %arg2: index, %arg3: index, %arg4: index, %arg5: memref<?xf32, "gpu">) workgroup(%arg6 : memref<256xf32, #gpu.address_space<workgroup>>) kernel {
    %c4 = arith.constant 4 : index
    %c64 = arith.constant 64 : index
    %c128 = arith.constant 128 : index
    %c16 = arith.constant 16 : index
    %cst = arith.constant 0xFF800000 : f32
    %c2 = arith.constant 2 : index
    %c32 = arith.constant 32 : index
    %c8 = arith.constant 8 : index
    %c256 = arith.constant 256 : index
    %c1 = arith.constant 1 : index
    %c0 = arith.constant 0 : index
    %0 = gpu.block_id  x
    %1 = gpu.thread_id  x
    cf.br ^bb1
  ^bb1:  // pred: ^bb0
    %dim = memref.dim %arg1, %c0 : memref<?x?x?xf32, "gpu">
    %dim_0 = memref.dim %arg1, %c1 : memref<?x?x?xf32, "gpu">
    %dim_1 = memref.dim %arg1, %c2 : memref<?x?x?xf32, "gpu">
    %2 = affine.apply affine_map<(d0)[s0, s1] -> (d0 * s0 + s1)>(%0)[%arg2, %c0]
    %3 = affine.apply affine_map<(d0)[s0, s1] -> (d0 * s0 + s1)>(%1)[%c1, %c0]
    %4 = arith.addi %3, %2 : index
    %5 = arith.cmpi ult, %4, %arg3 : index
    cf.cond_br %5, ^bb2, ^bb16
  ^bb2:  // pred: ^bb1
    %6 = arith.remsi %4, %c256 : index
    %7 = arith.divsi %4, %c256 : index
    %8 = arith.divui %6, %c8 : index
    %9 = arith.remui %6, %c8 : index
    %10 = arith.divui %7, %arg4 : index
    %11 = arith.remui %7, %arg4 : index
    %12 = arith.muli %10, %c32 : index
    %13 = arith.addi %12, %8 : index
    %14 = arith.muli %11, %c8 : index
    %15 = arith.addi %14, %9 : index
    %16 = arith.cmpi ult, %13, %dim : index
    %17 = arith.cmpi ult, %15, %arg0 : index
    %18 = arith.andi %16, %17 : i1
    cf.cond_br %18, ^bb3, ^bb4
  ^bb3:  // pred: ^bb2
    %19 = arith.muli %13, %arg0 : index
    %20 = arith.addi %19, %15 : index
    %21 = arith.muli %dim, %dim_0 : index
    %22 = arith.muli %21, %dim_1 : index
    %reinterpret_cast = memref.reinterpret_cast %arg1 to offset: [%c0], sizes: [%22], strides: [%c1] : memref<?x?x?xf32, "gpu"> to memref<?xf32, "gpu">
    %23 = memref.load %reinterpret_cast[%20] : memref<?xf32, "gpu">
    %24 = math.absf %23 : f32
    %reinterpret_cast_2 = memref.reinterpret_cast %arg6 to offset: [0], sizes: [256], strides: [1] : memref<256xf32, #gpu.address_space<workgroup>> to memref<256xf32, #gpu.address_space<workgroup>>
    memref.store %24, %reinterpret_cast_2[%6] : memref<256xf32, #gpu.address_space<workgroup>>
    cf.br ^bb5
  ^bb4:  // pred: ^bb2
    %reinterpret_cast_3 = memref.reinterpret_cast %arg6 to offset: [0], sizes: [256], strides: [1] : memref<256xf32, #gpu.address_space<workgroup>> to memref<256xf32, #gpu.address_space<workgroup>>
    memref.store %cst, %reinterpret_cast_3[%6] : memref<256xf32, #gpu.address_space<workgroup>>
    cf.br ^bb5
  ^bb5:  // 2 preds: ^bb3, ^bb4
    gpu.barrier
    %25 = arith.cmpi ult, %8, %c16 : index
    %26 = arith.addi %13, %c16 : index
    %27 = arith.cmpi ult, %26, %dim : index
    %28 = arith.andi %25, %27 : i1
    cf.cond_br %28, ^bb6, ^bb7
  ^bb6:  // pred: ^bb5
    %29 = arith.addi %6, %c128 : index
    %reinterpret_cast_4 = memref.reinterpret_cast %arg6 to offset: [0], sizes: [256], strides: [1] : memref<256xf32, #gpu.address_space<workgroup>> to memref<256xf32, #gpu.address_space<workgroup>>
    %30 = memref.load %reinterpret_cast_4[%6] : memref<256xf32, #gpu.address_space<workgroup>>
    %31 = memref.load %reinterpret_cast_4[%29] : memref<256xf32, #gpu.address_space<workgroup>>
    %32 = arith.cmpf ugt, %30, %31 : f32
    %33 = arith.select %32, %30, %31 : f32
    %34 = arith.cmpf uno, %31, %31 : f32
    %35 = arith.select %34, %31, %33 : f32
    memref.store %35, %reinterpret_cast_4[%6] : memref<256xf32, #gpu.address_space<workgroup>>
    cf.br ^bb7
  ^bb7:  // 2 preds: ^bb5, ^bb6
    gpu.barrier
    %36 = arith.cmpi ult, %8, %c8 : index
    %37 = arith.addi %13, %c8 : index
    %38 = arith.cmpi ult, %37, %dim : index
    %39 = arith.andi %36, %38 : i1
    cf.cond_br %39, ^bb8, ^bb9
  ^bb8:  // pred: ^bb7
    %40 = arith.addi %6, %c64 : index
    %reinterpret_cast_5 = memref.reinterpret_cast %arg6 to offset: [0], sizes: [256], strides: [1] : memref<256xf32, #gpu.address_space<workgroup>> to memref<256xf32, #gpu.address_space<workgroup>>
    %41 = memref.load %reinterpret_cast_5[%6] : memref<256xf32, #gpu.address_space<workgroup>>
    %42 = memref.load %reinterpret_cast_5[%40] : memref<256xf32, #gpu.address_space<workgroup>>
    %43 = arith.cmpf ugt, %41, %42 : f32
    %44 = arith.select %43, %41, %42 : f32
    %45 = arith.cmpf uno, %42, %42 : f32
    %46 = arith.select %45, %42, %44 : f32
    memref.store %46, %reinterpret_cast_5[%6] : memref<256xf32, #gpu.address_space<workgroup>>
    cf.br ^bb9
  ^bb9:  // 2 preds: ^bb7, ^bb8
    gpu.barrier
    %47 = arith.cmpi ult, %8, %c4 : index
    %48 = arith.addi %13, %c4 : index
    %49 = arith.cmpi ult, %48, %dim : index
    %50 = arith.andi %47, %49 : i1
    cf.cond_br %50, ^bb10, ^bb11
  ^bb10:  // pred: ^bb9
    %51 = arith.addi %6, %c32 : index
    %reinterpret_cast_6 = memref.reinterpret_cast %arg6 to offset: [0], sizes: [256], strides: [1] : memref<256xf32, #gpu.address_space<workgroup>> to memref<256xf32, #gpu.address_space<workgroup>>
    %52 = memref.load %reinterpret_cast_6[%6] : memref<256xf32, #gpu.address_space<workgroup>>
    %53 = memref.load %reinterpret_cast_6[%51] : memref<256xf32, #gpu.address_space<workgroup>>
    %54 = arith.cmpf ugt, %52, %53 : f32
    %55 = arith.select %54, %52, %53 : f32
    %56 = arith.cmpf uno, %53, %53 : f32
    %57 = arith.select %56, %53, %55 : f32
    memref.store %57, %reinterpret_cast_6[%6] : memref<256xf32, #gpu.address_space<workgroup>>
    cf.br ^bb11
  ^bb11:  // 2 preds: ^bb9, ^bb10
    gpu.barrier
    %58 = arith.cmpi ult, %8, %c2 : index
    %59 = arith.addi %13, %c2 : index
    %60 = arith.cmpi ult, %59, %dim : index
    %61 = arith.andi %58, %60 : i1
    cf.cond_br %61, ^bb12, ^bb13
  ^bb12:  // pred: ^bb11
    %62 = arith.addi %6, %c16 : index
    %reinterpret_cast_7 = memref.reinterpret_cast %arg6 to offset: [0], sizes: [256], strides: [1] : memref<256xf32, #gpu.address_space<workgroup>> to memref<256xf32, #gpu.address_space<workgroup>>
    %63 = memref.load %reinterpret_cast_7[%6] : memref<256xf32, #gpu.address_space<workgroup>>
    %64 = memref.load %reinterpret_cast_7[%62] : memref<256xf32, #gpu.address_space<workgroup>>
    %65 = arith.cmpf ugt, %63, %64 : f32
    %66 = arith.select %65, %63, %64 : f32
    %67 = arith.cmpf uno, %64, %64 : f32
    %68 = arith.select %67, %64, %66 : f32
    memref.store %68, %reinterpret_cast_7[%6] : memref<256xf32, #gpu.address_space<workgroup>>
    cf.br ^bb13
  ^bb13:  // 2 preds: ^bb11, ^bb12
    gpu.barrier
    %69 = arith.cmpi eq, %8, %c0 : index
    %70 = arith.andi %69, %18 : i1
    cf.cond_br %70, ^bb14, ^bb15
  ^bb14:  // pred: ^bb13
    %71 = arith.addi %6, %c8 : index
    %reinterpret_cast_8 = memref.reinterpret_cast %arg6 to offset: [0], sizes: [256], strides: [1] : memref<256xf32, #gpu.address_space<workgroup>> to memref<256xf32, #gpu.address_space<workgroup>>
    %72 = memref.load %reinterpret_cast_8[%6] : memref<256xf32, #gpu.address_space<workgroup>>
    %73 = memref.load %reinterpret_cast_8[%71] : memref<256xf32, #gpu.address_space<workgroup>>
    %74 = arith.cmpf ugt, %72, %73 : f32
    %75 = arith.select %74, %72, %73 : f32
    %76 = arith.cmpf uno, %73, %73 : f32
    %77 = arith.select %76, %73, %75 : f32
    %78 = memref.generic_atomic_rmw %arg5[%15] : memref<?xf32, "gpu"> {
    ^bb0(%arg7: f32):
      %79 = arith.cmpf ogt, %arg7, %77 : f32
      %80 = arith.select %79, %arg7, %77 : f32
      memref.atomic_yield %80 : f32
    }
    cf.br ^bb15
  ^bb15:  // 2 preds: ^bb13, ^bb14
    cf.br ^bb16
  ^bb16:  // 2 preds: ^bb1, ^bb15
    gpu.return
  }
}

// -----// IR Dump After ConvertAffineToStandard (lower-affine) //----- //
gpu.module @main_kernel_0 {
  gpu.func @main_kColReduction_reduce__4_1_0___8w32h_1(%arg0: index, %arg1: memref<?x?x?xf32, "gpu">, %arg2: index, %arg3: index, %arg4: index, %arg5: memref<?xf32, "gpu">) workgroup(%arg6 : memref<256xf32, #gpu.address_space<workgroup>>) kernel {
    %c4 = arith.constant 4 : index
    %c64 = arith.constant 64 : index
    %c128 = arith.constant 128 : index
    %c16 = arith.constant 16 : index
    %cst = arith.constant 0xFF800000 : f32
    %c2 = arith.constant 2 : index
    %c32 = arith.constant 32 : index
    %c8 = arith.constant 8 : index
    %c256 = arith.constant 256 : index
    %c1 = arith.constant 1 : index
    %c0 = arith.constant 0 : index
    %0 = gpu.block_id  x
    %1 = gpu.thread_id  x
    cf.br ^bb1
  ^bb1:  // pred: ^bb0
    %dim = memref.dim %arg1, %c0 : memref<?x?x?xf32, "gpu">
    %dim_0 = memref.dim %arg1, %c1 : memref<?x?x?xf32, "gpu">
    %dim_1 = memref.dim %arg1, %c2 : memref<?x?x?xf32, "gpu">
    %2 = arith.muli %0, %arg2 : index
    %3 = arith.addi %2, %c0 : index
    %4 = arith.muli %1, %c1 : index
    %5 = arith.addi %4, %c0 : index
    %6 = arith.addi %5, %3 : index
    %7 = arith.cmpi ult, %6, %arg3 : index
    cf.cond_br %7, ^bb2, ^bb16
  ^bb2:  // pred: ^bb1
    %8 = arith.remsi %6, %c256 : index
    %9 = arith.divsi %6, %c256 : index
    %10 = arith.divui %8, %c8 : index
    %11 = arith.remui %8, %c8 : index
    %12 = arith.divui %9, %arg4 : index
    %13 = arith.remui %9, %arg4 : index
    %14 = arith.muli %12, %c32 : index
    %15 = arith.addi %14, %10 : index
    %16 = arith.muli %13, %c8 : index
    %17 = arith.addi %16, %11 : index
    %18 = arith.cmpi ult, %15, %dim : index
    %19 = arith.cmpi ult, %17, %arg0 : index
    %20 = arith.andi %18, %19 : i1
    cf.cond_br %20, ^bb3, ^bb4
  ^bb3:  // pred: ^bb2
    %21 = arith.muli %15, %arg0 : index
    %22 = arith.addi %21, %17 : index
    %23 = arith.muli %dim, %dim_0 : index
    %24 = arith.muli %23, %dim_1 : index
    %reinterpret_cast = memref.reinterpret_cast %arg1 to offset: [%c0], sizes: [%24], strides: [%c1] : memref<?x?x?xf32, "gpu"> to memref<?xf32, "gpu">
    %25 = memref.load %reinterpret_cast[%22] : memref<?xf32, "gpu">
    %26 = math.absf %25 : f32
    %reinterpret_cast_2 = memref.reinterpret_cast %arg6 to offset: [0], sizes: [256], strides: [1] : memref<256xf32, #gpu.address_space<workgroup>> to memref<256xf32, #gpu.address_space<workgroup>>
    memref.store %26, %reinterpret_cast_2[%8] : memref<256xf32, #gpu.address_space<workgroup>>
    cf.br ^bb5
  ^bb4:  // pred: ^bb2
    %reinterpret_cast_3 = memref.reinterpret_cast %arg6 to offset: [0], sizes: [256], strides: [1] : memref<256xf32, #gpu.address_space<workgroup>> to memref<256xf32, #gpu.address_space<workgroup>>
    memref.store %cst, %reinterpret_cast_3[%8] : memref<256xf32, #gpu.address_space<workgroup>>
    cf.br ^bb5
  ^bb5:  // 2 preds: ^bb3, ^bb4
    gpu.barrier
    %27 = arith.cmpi ult, %10, %c16 : index
    %28 = arith.addi %15, %c16 : index
    %29 = arith.cmpi ult, %28, %dim : index
    %30 = arith.andi %27, %29 : i1
    cf.cond_br %30, ^bb6, ^bb7
  ^bb6:  // pred: ^bb5
    %31 = arith.addi %8, %c128 : index
    %reinterpret_cast_4 = memref.reinterpret_cast %arg6 to offset: [0], sizes: [256], strides: [1] : memref<256xf32, #gpu.address_space<workgroup>> to memref<256xf32, #gpu.address_space<workgroup>>
    %32 = memref.load %reinterpret_cast_4[%8] : memref<256xf32, #gpu.address_space<workgroup>>
    %33 = memref.load %reinterpret_cast_4[%31] : memref<256xf32, #gpu.address_space<workgroup>>
    %34 = arith.cmpf ugt, %32, %33 : f32
    %35 = arith.select %34, %32, %33 : f32
    %36 = arith.cmpf uno, %33, %33 : f32
    %37 = arith.select %36, %33, %35 : f32
    memref.store %37, %reinterpret_cast_4[%8] : memref<256xf32, #gpu.address_space<workgroup>>
    cf.br ^bb7
  ^bb7:  // 2 preds: ^bb5, ^bb6
    gpu.barrier
    %38 = arith.cmpi ult, %10, %c8 : index
    %39 = arith.addi %15, %c8 : index
    %40 = arith.cmpi ult, %39, %dim : index
    %41 = arith.andi %38, %40 : i1
    cf.cond_br %41, ^bb8, ^bb9
  ^bb8:  // pred: ^bb7
    %42 = arith.addi %8, %c64 : index
    %reinterpret_cast_5 = memref.reinterpret_cast %arg6 to offset: [0], sizes: [256], strides: [1] : memref<256xf32, #gpu.address_space<workgroup>> to memref<256xf32, #gpu.address_space<workgroup>>
    %43 = memref.load %reinterpret_cast_5[%8] : memref<256xf32, #gpu.address_space<workgroup>>
    %44 = memref.load %reinterpret_cast_5[%42] : memref<256xf32, #gpu.address_space<workgroup>>
    %45 = arith.cmpf ugt, %43, %44 : f32
    %46 = arith.select %45, %43, %44 : f32
    %47 = arith.cmpf uno, %44, %44 : f32
    %48 = arith.select %47, %44, %46 : f32
    memref.store %48, %reinterpret_cast_5[%8] : memref<256xf32, #gpu.address_space<workgroup>>
    cf.br ^bb9
  ^bb9:  // 2 preds: ^bb7, ^bb8
    gpu.barrier
    %49 = arith.cmpi ult, %10, %c4 : index
    %50 = arith.addi %15, %c4 : index
    %51 = arith.cmpi ult, %50, %dim : index
    %52 = arith.andi %49, %51 : i1
    cf.cond_br %52, ^bb10, ^bb11
  ^bb10:  // pred: ^bb9
    %53 = arith.addi %8, %c32 : index
    %reinterpret_cast_6 = memref.reinterpret_cast %arg6 to offset: [0], sizes: [256], strides: [1] : memref<256xf32, #gpu.address_space<workgroup>> to memref<256xf32, #gpu.address_space<workgroup>>
    %54 = memref.load %reinterpret_cast_6[%8] : memref<256xf32, #gpu.address_space<workgroup>>
    %55 = memref.load %reinterpret_cast_6[%53] : memref<256xf32, #gpu.address_space<workgroup>>
    %56 = arith.cmpf ugt, %54, %55 : f32
    %57 = arith.select %56, %54, %55 : f32
    %58 = arith.cmpf uno, %55, %55 : f32
    %59 = arith.select %58, %55, %57 : f32
    memref.store %59, %reinterpret_cast_6[%8] : memref<256xf32, #gpu.address_space<workgroup>>
    cf.br ^bb11
  ^bb11:  // 2 preds: ^bb9, ^bb10
    gpu.barrier
    %60 = arith.cmpi ult, %10, %c2 : index
    %61 = arith.addi %15, %c2 : index
    %62 = arith.cmpi ult, %61, %dim : index
    %63 = arith.andi %60, %62 : i1
    cf.cond_br %63, ^bb12, ^bb13
  ^bb12:  // pred: ^bb11
    %64 = arith.addi %8, %c16 : index
    %reinterpret_cast_7 = memref.reinterpret_cast %arg6 to offset: [0], sizes: [256], strides: [1] : memref<256xf32, #gpu.address_space<workgroup>> to memref<256xf32, #gpu.address_space<workgroup>>
    %65 = memref.load %reinterpret_cast_7[%8] : memref<256xf32, #gpu.address_space<workgroup>>
    %66 = memref.load %reinterpret_cast_7[%64] : memref<256xf32, #gpu.address_space<workgroup>>
    %67 = arith.cmpf ugt, %65, %66 : f32
    %68 = arith.select %67, %65, %66 : f32
    %69 = arith.cmpf uno, %66, %66 : f32
    %70 = arith.select %69, %66, %68 : f32
    memref.store %70, %reinterpret_cast_7[%8] : memref<256xf32, #gpu.address_space<workgroup>>
    cf.br ^bb13
  ^bb13:  // 2 preds: ^bb11, ^bb12
    gpu.barrier
    %71 = arith.cmpi eq, %10, %c0 : index
    %72 = arith.andi %71, %20 : i1
    cf.cond_br %72, ^bb14, ^bb15
  ^bb14:  // pred: ^bb13
    %73 = arith.addi %8, %c8 : index
    %reinterpret_cast_8 = memref.reinterpret_cast %arg6 to offset: [0], sizes: [256], strides: [1] : memref<256xf32, #gpu.address_space<workgroup>> to memref<256xf32, #gpu.address_space<workgroup>>
    %74 = memref.load %reinterpret_cast_8[%8] : memref<256xf32, #gpu.address_space<workgroup>>
    %75 = memref.load %reinterpret_cast_8[%73] : memref<256xf32, #gpu.address_space<workgroup>>
    %76 = arith.cmpf ugt, %74, %75 : f32
    %77 = arith.select %76, %74, %75 : f32
    %78 = arith.cmpf uno, %75, %75 : f32
    %79 = arith.select %78, %75, %77 : f32
    %80 = memref.generic_atomic_rmw %arg5[%17] : memref<?xf32, "gpu"> {
    ^bb0(%arg7: f32):
      %81 = arith.cmpf ogt, %arg7, %79 : f32
      %82 = arith.select %81, %arg7, %79 : f32
      memref.atomic_yield %82 : f32
    }
    cf.br ^bb15
  ^bb15:  // 2 preds: ^bb13, ^bb14
    cf.br ^bb16
  ^bb16:  // 2 preds: ^bb1, ^bb15
    gpu.return
  }
}

// -----// IR Dump After StripDebugInfo (strip-debuginfo) //----- //
gpu.module @main_kernel_0 {
  gpu.func @main_kColReduction_reduce__4_1_0___8w32h_1(%arg0: index, %arg1: memref<?x?x?xf32, "gpu">, %arg2: index, %arg3: index, %arg4: index, %arg5: memref<?xf32, "gpu">) workgroup(%arg6 : memref<256xf32, #gpu.address_space<workgroup>>) kernel {
    %c4 = arith.constant 4 : index
    %c64 = arith.constant 64 : index
    %c128 = arith.constant 128 : index
    %c16 = arith.constant 16 : index
    %cst = arith.constant 0xFF800000 : f32
    %c2 = arith.constant 2 : index
    %c32 = arith.constant 32 : index
    %c8 = arith.constant 8 : index
    %c256 = arith.constant 256 : index
    %c1 = arith.constant 1 : index
    %c0 = arith.constant 0 : index
    %0 = gpu.block_id  x
    %1 = gpu.thread_id  x
    cf.br ^bb1
  ^bb1:  // pred: ^bb0
    %dim = memref.dim %arg1, %c0 : memref<?x?x?xf32, "gpu">
    %dim_0 = memref.dim %arg1, %c1 : memref<?x?x?xf32, "gpu">
    %dim_1 = memref.dim %arg1, %c2 : memref<?x?x?xf32, "gpu">
    %2 = arith.muli %0, %arg2 : index
    %3 = arith.addi %2, %c0 : index
    %4 = arith.muli %1, %c1 : index
    %5 = arith.addi %4, %c0 : index
    %6 = arith.addi %5, %3 : index
    %7 = arith.cmpi ult, %6, %arg3 : index
    cf.cond_br %7, ^bb2, ^bb16
  ^bb2:  // pred: ^bb1
    %8 = arith.remsi %6, %c256 : index
    %9 = arith.divsi %6, %c256 : index
    %10 = arith.divui %8, %c8 : index
    %11 = arith.remui %8, %c8 : index
    %12 = arith.divui %9, %arg4 : index
    %13 = arith.remui %9, %arg4 : index
    %14 = arith.muli %12, %c32 : index
    %15 = arith.addi %14, %10 : index
    %16 = arith.muli %13, %c8 : index
    %17 = arith.addi %16, %11 : index
    %18 = arith.cmpi ult, %15, %dim : index
    %19 = arith.cmpi ult, %17, %arg0 : index
    %20 = arith.andi %18, %19 : i1
    cf.cond_br %20, ^bb3, ^bb4
  ^bb3:  // pred: ^bb2
    %21 = arith.muli %15, %arg0 : index
    %22 = arith.addi %21, %17 : index
    %23 = arith.muli %dim, %dim_0 : index
    %24 = arith.muli %23, %dim_1 : index
    %reinterpret_cast = memref.reinterpret_cast %arg1 to offset: [%c0], sizes: [%24], strides: [%c1] : memref<?x?x?xf32, "gpu"> to memref<?xf32, "gpu">
    %25 = memref.load %reinterpret_cast[%22] : memref<?xf32, "gpu">
    %26 = math.absf %25 : f32
    %reinterpret_cast_2 = memref.reinterpret_cast %arg6 to offset: [0], sizes: [256], strides: [1] : memref<256xf32, #gpu.address_space<workgroup>> to memref<256xf32, #gpu.address_space<workgroup>>
    memref.store %26, %reinterpret_cast_2[%8] : memref<256xf32, #gpu.address_space<workgroup>>
    cf.br ^bb5
  ^bb4:  // pred: ^bb2
    %reinterpret_cast_3 = memref.reinterpret_cast %arg6 to offset: [0], sizes: [256], strides: [1] : memref<256xf32, #gpu.address_space<workgroup>> to memref<256xf32, #gpu.address_space<workgroup>>
    memref.store %cst, %reinterpret_cast_3[%8] : memref<256xf32, #gpu.address_space<workgroup>>
    cf.br ^bb5
  ^bb5:  // 2 preds: ^bb3, ^bb4
    gpu.barrier
    %27 = arith.cmpi ult, %10, %c16 : index
    %28 = arith.addi %15, %c16 : index
    %29 = arith.cmpi ult, %28, %dim : index
    %30 = arith.andi %27, %29 : i1
    cf.cond_br %30, ^bb6, ^bb7
  ^bb6:  // pred: ^bb5
    %31 = arith.addi %8, %c128 : index
    %reinterpret_cast_4 = memref.reinterpret_cast %arg6 to offset: [0], sizes: [256], strides: [1] : memref<256xf32, #gpu.address_space<workgroup>> to memref<256xf32, #gpu.address_space<workgroup>>
    %32 = memref.load %reinterpret_cast_4[%8] : memref<256xf32, #gpu.address_space<workgroup>>
    %33 = memref.load %reinterpret_cast_4[%31] : memref<256xf32, #gpu.address_space<workgroup>>
    %34 = arith.cmpf ugt, %32, %33 : f32
    %35 = arith.select %34, %32, %33 : f32
    %36 = arith.cmpf uno, %33, %33 : f32
    %37 = arith.select %36, %33, %35 : f32
    memref.store %37, %reinterpret_cast_4[%8] : memref<256xf32, #gpu.address_space<workgroup>>
    cf.br ^bb7
  ^bb7:  // 2 preds: ^bb5, ^bb6
    gpu.barrier
    %38 = arith.cmpi ult, %10, %c8 : index
    %39 = arith.addi %15, %c8 : index
    %40 = arith.cmpi ult, %39, %dim : index
    %41 = arith.andi %38, %40 : i1
    cf.cond_br %41, ^bb8, ^bb9
  ^bb8:  // pred: ^bb7
    %42 = arith.addi %8, %c64 : index
    %reinterpret_cast_5 = memref.reinterpret_cast %arg6 to offset: [0], sizes: [256], strides: [1] : memref<256xf32, #gpu.address_space<workgroup>> to memref<256xf32, #gpu.address_space<workgroup>>
    %43 = memref.load %reinterpret_cast_5[%8] : memref<256xf32, #gpu.address_space<workgroup>>
    %44 = memref.load %reinterpret_cast_5[%42] : memref<256xf32, #gpu.address_space<workgroup>>
    %45 = arith.cmpf ugt, %43, %44 : f32
    %46 = arith.select %45, %43, %44 : f32
    %47 = arith.cmpf uno, %44, %44 : f32
    %48 = arith.select %47, %44, %46 : f32
    memref.store %48, %reinterpret_cast_5[%8] : memref<256xf32, #gpu.address_space<workgroup>>
    cf.br ^bb9
  ^bb9:  // 2 preds: ^bb7, ^bb8
    gpu.barrier
    %49 = arith.cmpi ult, %10, %c4 : index
    %50 = arith.addi %15, %c4 : index
    %51 = arith.cmpi ult, %50, %dim : index
    %52 = arith.andi %49, %51 : i1
    cf.cond_br %52, ^bb10, ^bb11
  ^bb10:  // pred: ^bb9
    %53 = arith.addi %8, %c32 : index
    %reinterpret_cast_6 = memref.reinterpret_cast %arg6 to offset: [0], sizes: [256], strides: [1] : memref<256xf32, #gpu.address_space<workgroup>> to memref<256xf32, #gpu.address_space<workgroup>>
    %54 = memref.load %reinterpret_cast_6[%8] : memref<256xf32, #gpu.address_space<workgroup>>
    %55 = memref.load %reinterpret_cast_6[%53] : memref<256xf32, #gpu.address_space<workgroup>>
    %56 = arith.cmpf ugt, %54, %55 : f32
    %57 = arith.select %56, %54, %55 : f32
    %58 = arith.cmpf uno, %55, %55 : f32
    %59 = arith.select %58, %55, %57 : f32
    memref.store %59, %reinterpret_cast_6[%8] : memref<256xf32, #gpu.address_space<workgroup>>
    cf.br ^bb11
  ^bb11:  // 2 preds: ^bb9, ^bb10
    gpu.barrier
    %60 = arith.cmpi ult, %10, %c2 : index
    %61 = arith.addi %15, %c2 : index
    %62 = arith.cmpi ult, %61, %dim : index
    %63 = arith.andi %60, %62 : i1
    cf.cond_br %63, ^bb12, ^bb13
  ^bb12:  // pred: ^bb11
    %64 = arith.addi %8, %c16 : index
    %reinterpret_cast_7 = memref.reinterpret_cast %arg6 to offset: [0], sizes: [256], strides: [1] : memref<256xf32, #gpu.address_space<workgroup>> to memref<256xf32, #gpu.address_space<workgroup>>
    %65 = memref.load %reinterpret_cast_7[%8] : memref<256xf32, #gpu.address_space<workgroup>>
    %66 = memref.load %reinterpret_cast_7[%64] : memref<256xf32, #gpu.address_space<workgroup>>
    %67 = arith.cmpf ugt, %65, %66 : f32
    %68 = arith.select %67, %65, %66 : f32
    %69 = arith.cmpf uno, %66, %66 : f32
    %70 = arith.select %69, %66, %68 : f32
    memref.store %70, %reinterpret_cast_7[%8] : memref<256xf32, #gpu.address_space<workgroup>>
    cf.br ^bb13
  ^bb13:  // 2 preds: ^bb11, ^bb12
    gpu.barrier
    %71 = arith.cmpi eq, %10, %c0 : index
    %72 = arith.andi %71, %20 : i1
    cf.cond_br %72, ^bb14, ^bb15
  ^bb14:  // pred: ^bb13
    %73 = arith.addi %8, %c8 : index
    %reinterpret_cast_8 = memref.reinterpret_cast %arg6 to offset: [0], sizes: [256], strides: [1] : memref<256xf32, #gpu.address_space<workgroup>> to memref<256xf32, #gpu.address_space<workgroup>>
    %74 = memref.load %reinterpret_cast_8[%8] : memref<256xf32, #gpu.address_space<workgroup>>
    %75 = memref.load %reinterpret_cast_8[%73] : memref<256xf32, #gpu.address_space<workgroup>>
    %76 = arith.cmpf ugt, %74, %75 : f32
    %77 = arith.select %76, %74, %75 : f32
    %78 = arith.cmpf uno, %75, %75 : f32
    %79 = arith.select %78, %75, %77 : f32
    %80 = memref.generic_atomic_rmw %arg5[%17] : memref<?xf32, "gpu"> {
    ^bb0(%arg7: f32):
      %81 = arith.cmpf ogt, %arg7, %79 : f32
      %82 = arith.select %81, %arg7, %79 : f32
      memref.atomic_yield %82 : f32
    }
    cf.br ^bb15
  ^bb15:  // 2 preds: ^bb13, ^bb14
    cf.br ^bb16
  ^bb16:  // 2 preds: ^bb1, ^bb15
    gpu.return
  }
}

// -----// IR Dump After DiscLowerGpuOpsToNVVMOpsPass (disc-convert-gpu-to-nvvm) //----- //
gpu.module @main_kernel_0 {
  llvm.mlir.global internal @__wg_main_kColReduction_reduce__4_1_0___8w32h_1_0() {addr_space = 3 : i32} : !llvm.array<256 x f32>
  llvm.func @__nv_fabsf(f32) -> f32
  llvm.func @main_kColReduction_reduce__4_1_0___8w32h_1(%arg0: i32, %arg1: !llvm.ptr<f32>, %arg2: !llvm.ptr<f32>, %arg3: i32, %arg4: i32, %arg5: i32, %arg6: i32, %arg7: i32, %arg8: i32, %arg9: i32, %arg10: i32, %arg11: i32, %arg12: i32, %arg13: !llvm.ptr<f32>, %arg14: !llvm.ptr<f32>, %arg15: i32, %arg16: i32, %arg17: i32) attributes {gpu.kernel, nvvm.kernel} {
    %0 = llvm.mlir.undef : !llvm.struct<(ptr<f32>, ptr<f32>, i32, array<3 x i32>, array<3 x i32>)>
    %1 = llvm.insertvalue %arg1, %0[0] : !llvm.struct<(ptr<f32>, ptr<f32>, i32, array<3 x i32>, array<3 x i32>)> 
    %2 = llvm.insertvalue %arg2, %1[1] : !llvm.struct<(ptr<f32>, ptr<f32>, i32, array<3 x i32>, array<3 x i32>)> 
    %3 = llvm.insertvalue %arg3, %2[2] : !llvm.struct<(ptr<f32>, ptr<f32>, i32, array<3 x i32>, array<3 x i32>)> 
    %4 = llvm.insertvalue %arg4, %3[3, 0] : !llvm.struct<(ptr<f32>, ptr<f32>, i32, array<3 x i32>, array<3 x i32>)> 
    %5 = llvm.insertvalue %arg7, %4[4, 0] : !llvm.struct<(ptr<f32>, ptr<f32>, i32, array<3 x i32>, array<3 x i32>)> 
    %6 = llvm.insertvalue %arg5, %5[3, 1] : !llvm.struct<(ptr<f32>, ptr<f32>, i32, array<3 x i32>, array<3 x i32>)> 
    %7 = llvm.insertvalue %arg8, %6[4, 1] : !llvm.struct<(ptr<f32>, ptr<f32>, i32, array<3 x i32>, array<3 x i32>)> 
    %8 = llvm.insertvalue %arg6, %7[3, 2] : !llvm.struct<(ptr<f32>, ptr<f32>, i32, array<3 x i32>, array<3 x i32>)> 
    %9 = llvm.insertvalue %arg9, %8[4, 2] : !llvm.struct<(ptr<f32>, ptr<f32>, i32, array<3 x i32>, array<3 x i32>)> 
    %10 = llvm.mlir.undef : !llvm.struct<(ptr<f32>, ptr<f32>, i32, array<1 x i32>, array<1 x i32>)>
    %11 = llvm.insertvalue %arg13, %10[0] : !llvm.struct<(ptr<f32>, ptr<f32>, i32, array<1 x i32>, array<1 x i32>)> 
    %12 = llvm.insertvalue %arg14, %11[1] : !llvm.struct<(ptr<f32>, ptr<f32>, i32, array<1 x i32>, array<1 x i32>)> 
    %13 = llvm.insertvalue %arg15, %12[2] : !llvm.struct<(ptr<f32>, ptr<f32>, i32, array<1 x i32>, array<1 x i32>)> 
    %14 = llvm.insertvalue %arg16, %13[3, 0] : !llvm.struct<(ptr<f32>, ptr<f32>, i32, array<1 x i32>, array<1 x i32>)> 
    %15 = llvm.insertvalue %arg17, %14[4, 0] : !llvm.struct<(ptr<f32>, ptr<f32>, i32, array<1 x i32>, array<1 x i32>)> 
    %16 = llvm.mlir.addressof @__wg_main_kColReduction_reduce__4_1_0___8w32h_1_0 : !llvm.ptr<array<256 x f32>, 3>
    %17 = llvm.getelementptr %16[0, 0] : (!llvm.ptr<array<256 x f32>, 3>) -> !llvm.ptr<f32, 3>
    %18 = llvm.mlir.undef : !llvm.struct<(ptr<f32, 3>, ptr<f32, 3>, i32, array<1 x i32>, array<1 x i32>)>
    %19 = llvm.insertvalue %17, %18[0] : !llvm.struct<(ptr<f32, 3>, ptr<f32, 3>, i32, array<1 x i32>, array<1 x i32>)> 
    %20 = llvm.insertvalue %17, %19[1] : !llvm.struct<(ptr<f32, 3>, ptr<f32, 3>, i32, array<1 x i32>, array<1 x i32>)> 
    %21 = llvm.mlir.constant(0 : index) : i32
    %22 = llvm.insertvalue %21, %20[2] : !llvm.struct<(ptr<f32, 3>, ptr<f32, 3>, i32, array<1 x i32>, array<1 x i32>)> 
    %23 = llvm.mlir.constant(256 : index) : i32
    %24 = llvm.insertvalue %23, %22[3, 0] : !llvm.struct<(ptr<f32, 3>, ptr<f32, 3>, i32, array<1 x i32>, array<1 x i32>)> 
    %25 = llvm.mlir.constant(1 : index) : i32
    %26 = llvm.insertvalue %25, %24[4, 0] : !llvm.struct<(ptr<f32, 3>, ptr<f32, 3>, i32, array<1 x i32>, array<1 x i32>)> 
    %27 = llvm.mlir.constant(4 : index) : i32
    %28 = llvm.mlir.constant(64 : index) : i32
    %29 = llvm.mlir.constant(128 : index) : i32
    %30 = llvm.mlir.constant(16 : index) : i32
    %31 = llvm.mlir.constant(0xFF800000 : f32) : f32
    %32 = llvm.mlir.constant(2 : index) : i32
    %33 = llvm.mlir.constant(32 : index) : i32
    %34 = llvm.mlir.constant(8 : index) : i32
    %35 = llvm.mlir.constant(256 : index) : i32
    %36 = llvm.mlir.constant(1 : index) : i32
    %37 = llvm.mlir.constant(0 : index) : i32
    %38 = nvvm.read.ptx.sreg.ctaid.x : i32
    %39 = nvvm.read.ptx.sreg.tid.x : i32
    llvm.br ^bb1
  ^bb1:  // pred: ^bb0
    %40 = llvm.extractvalue %9[3, 0] : !llvm.struct<(ptr<f32>, ptr<f32>, i32, array<3 x i32>, array<3 x i32>)> 
    %41 = llvm.extractvalue %9[3, 1] : !llvm.struct<(ptr<f32>, ptr<f32>, i32, array<3 x i32>, array<3 x i32>)> 
    %42 = llvm.extractvalue %9[3, 2] : !llvm.struct<(ptr<f32>, ptr<f32>, i32, array<3 x i32>, array<3 x i32>)> 
    %43 = llvm.mul %38, %arg10  : i32
    %44 = llvm.add %39, %43  : i32
    %45 = llvm.icmp "ult" %44, %arg11 : i32
    llvm.cond_br %45, ^bb2, ^bb18
  ^bb2:  // pred: ^bb1
    %46 = llvm.srem %44, %35  : i32
    %47 = llvm.sdiv %44, %35  : i32
    %48 = llvm.udiv %46, %34  : i32
    %49 = llvm.urem %46, %34  : i32
    %50 = llvm.udiv %47, %arg12  : i32
    %51 = llvm.urem %47, %arg12  : i32
    %52 = llvm.mul %50, %33  : i32
    %53 = llvm.add %52, %48  : i32
    %54 = llvm.mul %51, %34  : i32
    %55 = llvm.add %54, %49  : i32
    %56 = llvm.icmp "ult" %53, %40 : i32
    %57 = llvm.icmp "ult" %55, %arg0 : i32
    %58 = llvm.and %56, %57  : i1
    llvm.cond_br %58, ^bb3, ^bb4
  ^bb3:  // pred: ^bb2
    %59 = llvm.mul %53, %arg0  : i32
    %60 = llvm.add %59, %55  : i32
    %61 = llvm.mul %40, %41  : i32
    %62 = llvm.mul %61, %42  : i32
    %63 = llvm.mlir.undef : !llvm.struct<(ptr<f32>, ptr<f32>, i32, array<1 x i32>, array<1 x i32>)>
    %64 = llvm.extractvalue %9[0] : !llvm.struct<(ptr<f32>, ptr<f32>, i32, array<3 x i32>, array<3 x i32>)> 
    %65 = llvm.extractvalue %9[1] : !llvm.struct<(ptr<f32>, ptr<f32>, i32, array<3 x i32>, array<3 x i32>)> 
    %66 = llvm.insertvalue %64, %63[0] : !llvm.struct<(ptr<f32>, ptr<f32>, i32, array<1 x i32>, array<1 x i32>)> 
    %67 = llvm.insertvalue %65, %66[1] : !llvm.struct<(ptr<f32>, ptr<f32>, i32, array<1 x i32>, array<1 x i32>)> 
    %68 = llvm.insertvalue %37, %67[2] : !llvm.struct<(ptr<f32>, ptr<f32>, i32, array<1 x i32>, array<1 x i32>)> 
    %69 = llvm.insertvalue %62, %68[3, 0] : !llvm.struct<(ptr<f32>, ptr<f32>, i32, array<1 x i32>, array<1 x i32>)> 
    %70 = llvm.insertvalue %36, %69[4, 0] : !llvm.struct<(ptr<f32>, ptr<f32>, i32, array<1 x i32>, array<1 x i32>)> 
    %71 = llvm.extractvalue %70[1] : !llvm.struct<(ptr<f32>, ptr<f32>, i32, array<1 x i32>, array<1 x i32>)> 
    %72 = llvm.getelementptr %71[%60] : (!llvm.ptr<f32>, i32) -> !llvm.ptr<f32>
    %73 = llvm.load %72 : !llvm.ptr<f32>
    %74 = llvm.call @__nv_fabsf(%73) : (f32) -> f32
    %75 = llvm.mlir.undef : !llvm.struct<(ptr<f32, 3>, ptr<f32, 3>, i32, array<1 x i32>, array<1 x i32>)>
    %76 = llvm.extractvalue %26[0] : !llvm.struct<(ptr<f32, 3>, ptr<f32, 3>, i32, array<1 x i32>, array<1 x i32>)> 
    %77 = llvm.extractvalue %26[1] : !llvm.struct<(ptr<f32, 3>, ptr<f32, 3>, i32, array<1 x i32>, array<1 x i32>)> 
    %78 = llvm.insertvalue %76, %75[0] : !llvm.struct<(ptr<f32, 3>, ptr<f32, 3>, i32, array<1 x i32>, array<1 x i32>)> 
    %79 = llvm.insertvalue %77, %78[1] : !llvm.struct<(ptr<f32, 3>, ptr<f32, 3>, i32, array<1 x i32>, array<1 x i32>)> 
    %80 = llvm.mlir.constant(0 : index) : i32
    %81 = llvm.insertvalue %80, %79[2] : !llvm.struct<(ptr<f32, 3>, ptr<f32, 3>, i32, array<1 x i32>, array<1 x i32>)> 
    %82 = llvm.mlir.constant(256 : index) : i32
    %83 = llvm.insertvalue %82, %81[3, 0] : !llvm.struct<(ptr<f32, 3>, ptr<f32, 3>, i32, array<1 x i32>, array<1 x i32>)> 
    %84 = llvm.mlir.constant(1 : index) : i32
    %85 = llvm.insertvalue %84, %83[4, 0] : !llvm.struct<(ptr<f32, 3>, ptr<f32, 3>, i32, array<1 x i32>, array<1 x i32>)> 
    %86 = llvm.extractvalue %85[1] : !llvm.struct<(ptr<f32, 3>, ptr<f32, 3>, i32, array<1 x i32>, array<1 x i32>)> 
    %87 = llvm.getelementptr %86[%46] : (!llvm.ptr<f32, 3>, i32) -> !llvm.ptr<f32, 3>
    llvm.store %74, %87 : !llvm.ptr<f32, 3>
    llvm.br ^bb5
  ^bb4:  // pred: ^bb2
    %88 = llvm.mlir.undef : !llvm.struct<(ptr<f32, 3>, ptr<f32, 3>, i32, array<1 x i32>, array<1 x i32>)>
    %89 = llvm.extractvalue %26[0] : !llvm.struct<(ptr<f32, 3>, ptr<f32, 3>, i32, array<1 x i32>, array<1 x i32>)> 
    %90 = llvm.extractvalue %26[1] : !llvm.struct<(ptr<f32, 3>, ptr<f32, 3>, i32, array<1 x i32>, array<1 x i32>)> 
    %91 = llvm.insertvalue %89, %88[0] : !llvm.struct<(ptr<f32, 3>, ptr<f32, 3>, i32, array<1 x i32>, array<1 x i32>)> 
    %92 = llvm.insertvalue %90, %91[1] : !llvm.struct<(ptr<f32, 3>, ptr<f32, 3>, i32, array<1 x i32>, array<1 x i32>)> 
    %93 = llvm.mlir.constant(0 : index) : i32
    %94 = llvm.insertvalue %93, %92[2] : !llvm.struct<(ptr<f32, 3>, ptr<f32, 3>, i32, array<1 x i32>, array<1 x i32>)> 
    %95 = llvm.mlir.constant(256 : index) : i32
    %96 = llvm.insertvalue %95, %94[3, 0] : !llvm.struct<(ptr<f32, 3>, ptr<f32, 3>, i32, array<1 x i32>, array<1 x i32>)> 
    %97 = llvm.mlir.constant(1 : index) : i32
    %98 = llvm.insertvalue %97, %96[4, 0] : !llvm.struct<(ptr<f32, 3>, ptr<f32, 3>, i32, array<1 x i32>, array<1 x i32>)> 
    %99 = llvm.extractvalue %98[1] : !llvm.struct<(ptr<f32, 3>, ptr<f32, 3>, i32, array<1 x i32>, array<1 x i32>)> 
    %100 = llvm.getelementptr %99[%46] : (!llvm.ptr<f32, 3>, i32) -> !llvm.ptr<f32, 3>
    llvm.store %31, %100 : !llvm.ptr<f32, 3>
    llvm.br ^bb5
  ^bb5:  // 2 preds: ^bb3, ^bb4
    nvvm.barrier0
    %101 = llvm.icmp "ult" %48, %30 : i32
    %102 = llvm.add %53, %30  : i32
    %103 = llvm.icmp "ult" %102, %40 : i32
    %104 = llvm.and %101, %103  : i1
    llvm.cond_br %104, ^bb6, ^bb7
  ^bb6:  // pred: ^bb5
    %105 = llvm.add %46, %29  : i32
    %106 = llvm.mlir.undef : !llvm.struct<(ptr<f32, 3>, ptr<f32, 3>, i32, array<1 x i32>, array<1 x i32>)>
    %107 = llvm.extractvalue %26[0] : !llvm.struct<(ptr<f32, 3>, ptr<f32, 3>, i32, array<1 x i32>, array<1 x i32>)> 
    %108 = llvm.extractvalue %26[1] : !llvm.struct<(ptr<f32, 3>, ptr<f32, 3>, i32, array<1 x i32>, array<1 x i32>)> 
    %109 = llvm.insertvalue %107, %106[0] : !llvm.struct<(ptr<f32, 3>, ptr<f32, 3>, i32, array<1 x i32>, array<1 x i32>)> 
    %110 = llvm.insertvalue %108, %109[1] : !llvm.struct<(ptr<f32, 3>, ptr<f32, 3>, i32, array<1 x i32>, array<1 x i32>)> 
    %111 = llvm.mlir.constant(0 : index) : i32
    %112 = llvm.insertvalue %111, %110[2] : !llvm.struct<(ptr<f32, 3>, ptr<f32, 3>, i32, array<1 x i32>, array<1 x i32>)> 
    %113 = llvm.mlir.constant(256 : index) : i32
    %114 = llvm.insertvalue %113, %112[3, 0] : !llvm.struct<(ptr<f32, 3>, ptr<f32, 3>, i32, array<1 x i32>, array<1 x i32>)> 
    %115 = llvm.mlir.constant(1 : index) : i32
    %116 = llvm.insertvalue %115, %114[4, 0] : !llvm.struct<(ptr<f32, 3>, ptr<f32, 3>, i32, array<1 x i32>, array<1 x i32>)> 
    %117 = llvm.extractvalue %116[1] : !llvm.struct<(ptr<f32, 3>, ptr<f32, 3>, i32, array<1 x i32>, array<1 x i32>)> 
    %118 = llvm.getelementptr %117[%46] : (!llvm.ptr<f32, 3>, i32) -> !llvm.ptr<f32, 3>
    %119 = llvm.load %118 : !llvm.ptr<f32, 3>
    %120 = llvm.extractvalue %116[1] : !llvm.struct<(ptr<f32, 3>, ptr<f32, 3>, i32, array<1 x i32>, array<1 x i32>)> 
    %121 = llvm.getelementptr %120[%105] : (!llvm.ptr<f32, 3>, i32) -> !llvm.ptr<f32, 3>
    %122 = llvm.load %121 : !llvm.ptr<f32, 3>
    %123 = llvm.fcmp "ugt" %119, %122 : f32
    %124 = llvm.select %123, %119, %122 : i1, f32
    %125 = llvm.fcmp "uno" %122, %122 : f32
    %126 = llvm.select %125, %122, %124 : i1, f32
    %127 = llvm.extractvalue %116[1] : !llvm.struct<(ptr<f32, 3>, ptr<f32, 3>, i32, array<1 x i32>, array<1 x i32>)> 
    %128 = llvm.getelementptr %127[%46] : (!llvm.ptr<f32, 3>, i32) -> !llvm.ptr<f32, 3>
    llvm.store %126, %128 : !llvm.ptr<f32, 3>
    llvm.br ^bb7
  ^bb7:  // 2 preds: ^bb5, ^bb6
    nvvm.barrier0
    %129 = llvm.icmp "ult" %48, %34 : i32
    %130 = llvm.add %53, %34  : i32
    %131 = llvm.icmp "ult" %130, %40 : i32
    %132 = llvm.and %129, %131  : i1
    llvm.cond_br %132, ^bb8, ^bb9
  ^bb8:  // pred: ^bb7
    %133 = llvm.add %46, %28  : i32
    %134 = llvm.mlir.undef : !llvm.struct<(ptr<f32, 3>, ptr<f32, 3>, i32, array<1 x i32>, array<1 x i32>)>
    %135 = llvm.extractvalue %26[0] : !llvm.struct<(ptr<f32, 3>, ptr<f32, 3>, i32, array<1 x i32>, array<1 x i32>)> 
    %136 = llvm.extractvalue %26[1] : !llvm.struct<(ptr<f32, 3>, ptr<f32, 3>, i32, array<1 x i32>, array<1 x i32>)> 
    %137 = llvm.insertvalue %135, %134[0] : !llvm.struct<(ptr<f32, 3>, ptr<f32, 3>, i32, array<1 x i32>, array<1 x i32>)> 
    %138 = llvm.insertvalue %136, %137[1] : !llvm.struct<(ptr<f32, 3>, ptr<f32, 3>, i32, array<1 x i32>, array<1 x i32>)> 
    %139 = llvm.mlir.constant(0 : index) : i32
    %140 = llvm.insertvalue %139, %138[2] : !llvm.struct<(ptr<f32, 3>, ptr<f32, 3>, i32, array<1 x i32>, array<1 x i32>)> 
    %141 = llvm.mlir.constant(256 : index) : i32
    %142 = llvm.insertvalue %141, %140[3, 0] : !llvm.struct<(ptr<f32, 3>, ptr<f32, 3>, i32, array<1 x i32>, array<1 x i32>)> 
    %143 = llvm.mlir.constant(1 : index) : i32
    %144 = llvm.insertvalue %143, %142[4, 0] : !llvm.struct<(ptr<f32, 3>, ptr<f32, 3>, i32, array<1 x i32>, array<1 x i32>)> 
    %145 = llvm.extractvalue %144[1] : !llvm.struct<(ptr<f32, 3>, ptr<f32, 3>, i32, array<1 x i32>, array<1 x i32>)> 
    %146 = llvm.getelementptr %145[%46] : (!llvm.ptr<f32, 3>, i32) -> !llvm.ptr<f32, 3>
    %147 = llvm.load %146 : !llvm.ptr<f32, 3>
    %148 = llvm.extractvalue %144[1] : !llvm.struct<(ptr<f32, 3>, ptr<f32, 3>, i32, array<1 x i32>, array<1 x i32>)> 
    %149 = llvm.getelementptr %148[%133] : (!llvm.ptr<f32, 3>, i32) -> !llvm.ptr<f32, 3>
    %150 = llvm.load %149 : !llvm.ptr<f32, 3>
    %151 = llvm.fcmp "ugt" %147, %150 : f32
    %152 = llvm.select %151, %147, %150 : i1, f32
    %153 = llvm.fcmp "uno" %150, %150 : f32
    %154 = llvm.select %153, %150, %152 : i1, f32
    %155 = llvm.extractvalue %144[1] : !llvm.struct<(ptr<f32, 3>, ptr<f32, 3>, i32, array<1 x i32>, array<1 x i32>)> 
    %156 = llvm.getelementptr %155[%46] : (!llvm.ptr<f32, 3>, i32) -> !llvm.ptr<f32, 3>
    llvm.store %154, %156 : !llvm.ptr<f32, 3>
    llvm.br ^bb9
  ^bb9:  // 2 preds: ^bb7, ^bb8
    nvvm.barrier0
    %157 = llvm.icmp "ult" %48, %27 : i32
    %158 = llvm.add %53, %27  : i32
    %159 = llvm.icmp "ult" %158, %40 : i32
    %160 = llvm.and %157, %159  : i1
    llvm.cond_br %160, ^bb10, ^bb11
  ^bb10:  // pred: ^bb9
    %161 = llvm.add %46, %33  : i32
    %162 = llvm.mlir.undef : !llvm.struct<(ptr<f32, 3>, ptr<f32, 3>, i32, array<1 x i32>, array<1 x i32>)>
    %163 = llvm.extractvalue %26[0] : !llvm.struct<(ptr<f32, 3>, ptr<f32, 3>, i32, array<1 x i32>, array<1 x i32>)> 
    %164 = llvm.extractvalue %26[1] : !llvm.struct<(ptr<f32, 3>, ptr<f32, 3>, i32, array<1 x i32>, array<1 x i32>)> 
    %165 = llvm.insertvalue %163, %162[0] : !llvm.struct<(ptr<f32, 3>, ptr<f32, 3>, i32, array<1 x i32>, array<1 x i32>)> 
    %166 = llvm.insertvalue %164, %165[1] : !llvm.struct<(ptr<f32, 3>, ptr<f32, 3>, i32, array<1 x i32>, array<1 x i32>)> 
    %167 = llvm.mlir.constant(0 : index) : i32
    %168 = llvm.insertvalue %167, %166[2] : !llvm.struct<(ptr<f32, 3>, ptr<f32, 3>, i32, array<1 x i32>, array<1 x i32>)> 
    %169 = llvm.mlir.constant(256 : index) : i32
    %170 = llvm.insertvalue %169, %168[3, 0] : !llvm.struct<(ptr<f32, 3>, ptr<f32, 3>, i32, array<1 x i32>, array<1 x i32>)> 
    %171 = llvm.mlir.constant(1 : index) : i32
    %172 = llvm.insertvalue %171, %170[4, 0] : !llvm.struct<(ptr<f32, 3>, ptr<f32, 3>, i32, array<1 x i32>, array<1 x i32>)> 
    %173 = llvm.extractvalue %172[1] : !llvm.struct<(ptr<f32, 3>, ptr<f32, 3>, i32, array<1 x i32>, array<1 x i32>)> 
    %174 = llvm.getelementptr %173[%46] : (!llvm.ptr<f32, 3>, i32) -> !llvm.ptr<f32, 3>
    %175 = llvm.load %174 : !llvm.ptr<f32, 3>
    %176 = llvm.extractvalue %172[1] : !llvm.struct<(ptr<f32, 3>, ptr<f32, 3>, i32, array<1 x i32>, array<1 x i32>)> 
    %177 = llvm.getelementptr %176[%161] : (!llvm.ptr<f32, 3>, i32) -> !llvm.ptr<f32, 3>
    %178 = llvm.load %177 : !llvm.ptr<f32, 3>
    %179 = llvm.fcmp "ugt" %175, %178 : f32
    %180 = llvm.select %179, %175, %178 : i1, f32
    %181 = llvm.fcmp "uno" %178, %178 : f32
    %182 = llvm.select %181, %178, %180 : i1, f32
    %183 = llvm.extractvalue %172[1] : !llvm.struct<(ptr<f32, 3>, ptr<f32, 3>, i32, array<1 x i32>, array<1 x i32>)> 
    %184 = llvm.getelementptr %183[%46] : (!llvm.ptr<f32, 3>, i32) -> !llvm.ptr<f32, 3>
    llvm.store %182, %184 : !llvm.ptr<f32, 3>
    llvm.br ^bb11
  ^bb11:  // 2 preds: ^bb9, ^bb10
    nvvm.barrier0
    %185 = llvm.icmp "ult" %48, %32 : i32
    %186 = llvm.add %53, %32  : i32
    %187 = llvm.icmp "ult" %186, %40 : i32
    %188 = llvm.and %185, %187  : i1
    llvm.cond_br %188, ^bb12, ^bb13
  ^bb12:  // pred: ^bb11
    %189 = llvm.add %46, %30  : i32
    %190 = llvm.mlir.undef : !llvm.struct<(ptr<f32, 3>, ptr<f32, 3>, i32, array<1 x i32>, array<1 x i32>)>
    %191 = llvm.extractvalue %26[0] : !llvm.struct<(ptr<f32, 3>, ptr<f32, 3>, i32, array<1 x i32>, array<1 x i32>)> 
    %192 = llvm.extractvalue %26[1] : !llvm.struct<(ptr<f32, 3>, ptr<f32, 3>, i32, array<1 x i32>, array<1 x i32>)> 
    %193 = llvm.insertvalue %191, %190[0] : !llvm.struct<(ptr<f32, 3>, ptr<f32, 3>, i32, array<1 x i32>, array<1 x i32>)> 
    %194 = llvm.insertvalue %192, %193[1] : !llvm.struct<(ptr<f32, 3>, ptr<f32, 3>, i32, array<1 x i32>, array<1 x i32>)> 
    %195 = llvm.mlir.constant(0 : index) : i32
    %196 = llvm.insertvalue %195, %194[2] : !llvm.struct<(ptr<f32, 3>, ptr<f32, 3>, i32, array<1 x i32>, array<1 x i32>)> 
    %197 = llvm.mlir.constant(256 : index) : i32
    %198 = llvm.insertvalue %197, %196[3, 0] : !llvm.struct<(ptr<f32, 3>, ptr<f32, 3>, i32, array<1 x i32>, array<1 x i32>)> 
    %199 = llvm.mlir.constant(1 : index) : i32
    %200 = llvm.insertvalue %199, %198[4, 0] : !llvm.struct<(ptr<f32, 3>, ptr<f32, 3>, i32, array<1 x i32>, array<1 x i32>)> 
    %201 = llvm.extractvalue %200[1] : !llvm.struct<(ptr<f32, 3>, ptr<f32, 3>, i32, array<1 x i32>, array<1 x i32>)> 
    %202 = llvm.getelementptr %201[%46] : (!llvm.ptr<f32, 3>, i32) -> !llvm.ptr<f32, 3>
    %203 = llvm.load %202 : !llvm.ptr<f32, 3>
    %204 = llvm.extractvalue %200[1] : !llvm.struct<(ptr<f32, 3>, ptr<f32, 3>, i32, array<1 x i32>, array<1 x i32>)> 
    %205 = llvm.getelementptr %204[%189] : (!llvm.ptr<f32, 3>, i32) -> !llvm.ptr<f32, 3>
    %206 = llvm.load %205 : !llvm.ptr<f32, 3>
    %207 = llvm.fcmp "ugt" %203, %206 : f32
    %208 = llvm.select %207, %203, %206 : i1, f32
    %209 = llvm.fcmp "uno" %206, %206 : f32
    %210 = llvm.select %209, %206, %208 : i1, f32
    %211 = llvm.extractvalue %200[1] : !llvm.struct<(ptr<f32, 3>, ptr<f32, 3>, i32, array<1 x i32>, array<1 x i32>)> 
    %212 = llvm.getelementptr %211[%46] : (!llvm.ptr<f32, 3>, i32) -> !llvm.ptr<f32, 3>
    llvm.store %210, %212 : !llvm.ptr<f32, 3>
    llvm.br ^bb13
  ^bb13:  // 2 preds: ^bb11, ^bb12
    nvvm.barrier0
    %213 = llvm.icmp "eq" %48, %37 : i32
    %214 = llvm.and %213, %58  : i1
    llvm.cond_br %214, ^bb14, ^bb17
  ^bb14:  // pred: ^bb13
    %215 = llvm.add %46, %34  : i32
    %216 = llvm.mlir.undef : !llvm.struct<(ptr<f32, 3>, ptr<f32, 3>, i32, array<1 x i32>, array<1 x i32>)>
    %217 = llvm.extractvalue %26[0] : !llvm.struct<(ptr<f32, 3>, ptr<f32, 3>, i32, array<1 x i32>, array<1 x i32>)> 
    %218 = llvm.extractvalue %26[1] : !llvm.struct<(ptr<f32, 3>, ptr<f32, 3>, i32, array<1 x i32>, array<1 x i32>)> 
    %219 = llvm.insertvalue %217, %216[0] : !llvm.struct<(ptr<f32, 3>, ptr<f32, 3>, i32, array<1 x i32>, array<1 x i32>)> 
    %220 = llvm.insertvalue %218, %219[1] : !llvm.struct<(ptr<f32, 3>, ptr<f32, 3>, i32, array<1 x i32>, array<1 x i32>)> 
    %221 = llvm.mlir.constant(0 : index) : i32
    %222 = llvm.insertvalue %221, %220[2] : !llvm.struct<(ptr<f32, 3>, ptr<f32, 3>, i32, array<1 x i32>, array<1 x i32>)> 
    %223 = llvm.mlir.constant(256 : index) : i32
    %224 = llvm.insertvalue %223, %222[3, 0] : !llvm.struct<(ptr<f32, 3>, ptr<f32, 3>, i32, array<1 x i32>, array<1 x i32>)> 
    %225 = llvm.mlir.constant(1 : index) : i32
    %226 = llvm.insertvalue %225, %224[4, 0] : !llvm.struct<(ptr<f32, 3>, ptr<f32, 3>, i32, array<1 x i32>, array<1 x i32>)> 
    %227 = llvm.extractvalue %226[1] : !llvm.struct<(ptr<f32, 3>, ptr<f32, 3>, i32, array<1 x i32>, array<1 x i32>)> 
    %228 = llvm.getelementptr %227[%46] : (!llvm.ptr<f32, 3>, i32) -> !llvm.ptr<f32, 3>
    %229 = llvm.load %228 : !llvm.ptr<f32, 3>
    %230 = llvm.extractvalue %226[1] : !llvm.struct<(ptr<f32, 3>, ptr<f32, 3>, i32, array<1 x i32>, array<1 x i32>)> 
    %231 = llvm.getelementptr %230[%215] : (!llvm.ptr<f32, 3>, i32) -> !llvm.ptr<f32, 3>
    %232 = llvm.load %231 : !llvm.ptr<f32, 3>
    %233 = llvm.fcmp "ugt" %229, %232 : f32
    %234 = llvm.select %233, %229, %232 : i1, f32
    %235 = llvm.fcmp "uno" %232, %232 : f32
    %236 = llvm.select %235, %232, %234 : i1, f32
    %237 = llvm.extractvalue %15[1] : !llvm.struct<(ptr<f32>, ptr<f32>, i32, array<1 x i32>, array<1 x i32>)> 
    %238 = llvm.getelementptr %237[%55] : (!llvm.ptr<f32>, i32) -> !llvm.ptr<f32>
    %239 = llvm.load %238 : !llvm.ptr<f32>
    llvm.br ^bb15(%239 : f32)
  ^bb15(%240: f32):  // 2 preds: ^bb14, ^bb15
    %241 = llvm.fcmp "ogt" %240, %236 : f32
    %242 = llvm.select %241, %240, %236 : i1, f32
    %243 = llvm.bitcast %238 : !llvm.ptr<f32> to !llvm.ptr<i32>
    %244 = llvm.bitcast %240 : f32 to i32
    %245 = llvm.bitcast %242 : f32 to i32
    %246 = llvm.cmpxchg %243, %244, %245 acq_rel monotonic : !llvm.ptr<i32>, i32
    %247 = llvm.extractvalue %246[0] : !llvm.struct<(i32, i1)> 
    %248 = llvm.bitcast %247 : i32 to f32
    %249 = llvm.extractvalue %246[1] : !llvm.struct<(i32, i1)> 
    llvm.cond_br %249, ^bb16, ^bb15(%248 : f32)
  ^bb16:  // pred: ^bb15
    llvm.br ^bb17
  ^bb17:  // 2 preds: ^bb13, ^bb16
    llvm.br ^bb18
  ^bb18:  // 2 preds: ^bb1, ^bb17
    llvm.return
  }
}

// -----// IR Dump After LLVMInsertValueSimplifierPass (disc-llvm-insert-value-simplifier) //----- //
llvm.func @main_kColReduction_reduce__4_1_0___8w32h_1(%arg0: i32, %arg1: !llvm.ptr<f32>, %arg2: !llvm.ptr<f32>, %arg3: i32, %arg4: i32, %arg5: i32, %arg6: i32, %arg7: i32, %arg8: i32, %arg9: i32, %arg10: i32, %arg11: i32, %arg12: i32, %arg13: !llvm.ptr<f32>, %arg14: !llvm.ptr<f32>, %arg15: i32, %arg16: i32, %arg17: i32) attributes {gpu.kernel, nvvm.kernel} {
  %0 = llvm.mlir.constant(8 : index) : i32
  %1 = llvm.mlir.constant(32 : index) : i32
  %2 = llvm.mlir.constant(2 : index) : i32
  %3 = llvm.mlir.constant(0xFF800000 : f32) : f32
  %4 = llvm.mlir.constant(16 : index) : i32
  %5 = llvm.mlir.constant(128 : index) : i32
  %6 = llvm.mlir.constant(64 : index) : i32
  %7 = llvm.mlir.constant(4 : index) : i32
  %8 = llvm.mlir.constant(256 : index) : i32
  %9 = llvm.mlir.constant(0 : index) : i32
  %10 = llvm.mlir.addressof @__wg_main_kColReduction_reduce__4_1_0___8w32h_1_0 : !llvm.ptr<array<256 x f32>, 3>
  %11 = llvm.getelementptr %10[0, 0] : (!llvm.ptr<array<256 x f32>, 3>) -> !llvm.ptr<f32, 3>
  %12 = nvvm.read.ptx.sreg.ctaid.x : i32
  %13 = nvvm.read.ptx.sreg.tid.x : i32
  llvm.br ^bb1
^bb1:  // pred: ^bb0
  %14 = llvm.mul %12, %arg10  : i32
  %15 = llvm.add %13, %14  : i32
  %16 = llvm.icmp "ult" %15, %arg11 : i32
  llvm.cond_br %16, ^bb2, ^bb18
^bb2:  // pred: ^bb1
  %17 = llvm.srem %15, %8  : i32
  %18 = llvm.sdiv %15, %8  : i32
  %19 = llvm.udiv %17, %0  : i32
  %20 = llvm.urem %17, %0  : i32
  %21 = llvm.udiv %18, %arg12  : i32
  %22 = llvm.urem %18, %arg12  : i32
  %23 = llvm.mul %21, %1  : i32
  %24 = llvm.add %23, %19  : i32
  %25 = llvm.mul %22, %0  : i32
  %26 = llvm.add %25, %20  : i32
  %27 = llvm.icmp "ult" %24, %arg4 : i32
  %28 = llvm.icmp "ult" %26, %arg0 : i32
  %29 = llvm.and %27, %28  : i1
  llvm.cond_br %29, ^bb3, ^bb4
^bb3:  // pred: ^bb2
  %30 = llvm.mul %24, %arg0  : i32
  %31 = llvm.add %30, %26  : i32
  %32 = llvm.getelementptr %arg2[%31] : (!llvm.ptr<f32>, i32) -> !llvm.ptr<f32>
  %33 = llvm.load %32 : !llvm.ptr<f32>
  %34 = llvm.call @__nv_fabsf(%33) : (f32) -> f32
  %35 = llvm.getelementptr %11[%17] : (!llvm.ptr<f32, 3>, i32) -> !llvm.ptr<f32, 3>
  llvm.store %34, %35 : !llvm.ptr<f32, 3>
  llvm.br ^bb5
^bb4:  // pred: ^bb2
  %36 = llvm.getelementptr %11[%17] : (!llvm.ptr<f32, 3>, i32) -> !llvm.ptr<f32, 3>
  llvm.store %3, %36 : !llvm.ptr<f32, 3>
  llvm.br ^bb5
^bb5:  // 2 preds: ^bb3, ^bb4
  nvvm.barrier0
  %37 = llvm.icmp "ult" %19, %4 : i32
  %38 = llvm.add %24, %4  : i32
  %39 = llvm.icmp "ult" %38, %arg4 : i32
  %40 = llvm.and %37, %39  : i1
  llvm.cond_br %40, ^bb6, ^bb7
^bb6:  // pred: ^bb5
  %41 = llvm.add %17, %5  : i32
  %42 = llvm.getelementptr %11[%17] : (!llvm.ptr<f32, 3>, i32) -> !llvm.ptr<f32, 3>
  %43 = llvm.load %42 : !llvm.ptr<f32, 3>
  %44 = llvm.getelementptr %11[%41] : (!llvm.ptr<f32, 3>, i32) -> !llvm.ptr<f32, 3>
  %45 = llvm.load %44 : !llvm.ptr<f32, 3>
  %46 = llvm.fcmp "ugt" %43, %45 : f32
  %47 = llvm.select %46, %43, %45 : i1, f32
  %48 = llvm.fcmp "uno" %45, %45 : f32
  %49 = llvm.select %48, %45, %47 : i1, f32
  %50 = llvm.getelementptr %11[%17] : (!llvm.ptr<f32, 3>, i32) -> !llvm.ptr<f32, 3>
  llvm.store %49, %50 : !llvm.ptr<f32, 3>
  llvm.br ^bb7
^bb7:  // 2 preds: ^bb5, ^bb6
  nvvm.barrier0
  %51 = llvm.icmp "ult" %19, %0 : i32
  %52 = llvm.add %24, %0  : i32
  %53 = llvm.icmp "ult" %52, %arg4 : i32
  %54 = llvm.and %51, %53  : i1
  llvm.cond_br %54, ^bb8, ^bb9
^bb8:  // pred: ^bb7
  %55 = llvm.add %17, %6  : i32
  %56 = llvm.getelementptr %11[%17] : (!llvm.ptr<f32, 3>, i32) -> !llvm.ptr<f32, 3>
  %57 = llvm.load %56 : !llvm.ptr<f32, 3>
  %58 = llvm.getelementptr %11[%55] : (!llvm.ptr<f32, 3>, i32) -> !llvm.ptr<f32, 3>
  %59 = llvm.load %58 : !llvm.ptr<f32, 3>
  %60 = llvm.fcmp "ugt" %57, %59 : f32
  %61 = llvm.select %60, %57, %59 : i1, f32
  %62 = llvm.fcmp "uno" %59, %59 : f32
  %63 = llvm.select %62, %59, %61 : i1, f32
  %64 = llvm.getelementptr %11[%17] : (!llvm.ptr<f32, 3>, i32) -> !llvm.ptr<f32, 3>
  llvm.store %63, %64 : !llvm.ptr<f32, 3>
  llvm.br ^bb9
^bb9:  // 2 preds: ^bb7, ^bb8
  nvvm.barrier0
  %65 = llvm.icmp "ult" %19, %7 : i32
  %66 = llvm.add %24, %7  : i32
  %67 = llvm.icmp "ult" %66, %arg4 : i32
  %68 = llvm.and %65, %67  : i1
  llvm.cond_br %68, ^bb10, ^bb11
^bb10:  // pred: ^bb9
  %69 = llvm.add %17, %1  : i32
  %70 = llvm.getelementptr %11[%17] : (!llvm.ptr<f32, 3>, i32) -> !llvm.ptr<f32, 3>
  %71 = llvm.load %70 : !llvm.ptr<f32, 3>
  %72 = llvm.getelementptr %11[%69] : (!llvm.ptr<f32, 3>, i32) -> !llvm.ptr<f32, 3>
  %73 = llvm.load %72 : !llvm.ptr<f32, 3>
  %74 = llvm.fcmp "ugt" %71, %73 : f32
  %75 = llvm.select %74, %71, %73 : i1, f32
  %76 = llvm.fcmp "uno" %73, %73 : f32
  %77 = llvm.select %76, %73, %75 : i1, f32
  %78 = llvm.getelementptr %11[%17] : (!llvm.ptr<f32, 3>, i32) -> !llvm.ptr<f32, 3>
  llvm.store %77, %78 : !llvm.ptr<f32, 3>
  llvm.br ^bb11
^bb11:  // 2 preds: ^bb9, ^bb10
  nvvm.barrier0
  %79 = llvm.icmp "ult" %19, %2 : i32
  %80 = llvm.add %24, %2  : i32
  %81 = llvm.icmp "ult" %80, %arg4 : i32
  %82 = llvm.and %79, %81  : i1
  llvm.cond_br %82, ^bb12, ^bb13
^bb12:  // pred: ^bb11
  %83 = llvm.add %17, %4  : i32
  %84 = llvm.getelementptr %11[%17] : (!llvm.ptr<f32, 3>, i32) -> !llvm.ptr<f32, 3>
  %85 = llvm.load %84 : !llvm.ptr<f32, 3>
  %86 = llvm.getelementptr %11[%83] : (!llvm.ptr<f32, 3>, i32) -> !llvm.ptr<f32, 3>
  %87 = llvm.load %86 : !llvm.ptr<f32, 3>
  %88 = llvm.fcmp "ugt" %85, %87 : f32
  %89 = llvm.select %88, %85, %87 : i1, f32
  %90 = llvm.fcmp "uno" %87, %87 : f32
  %91 = llvm.select %90, %87, %89 : i1, f32
  %92 = llvm.getelementptr %11[%17] : (!llvm.ptr<f32, 3>, i32) -> !llvm.ptr<f32, 3>
  llvm.store %91, %92 : !llvm.ptr<f32, 3>
  llvm.br ^bb13
^bb13:  // 2 preds: ^bb11, ^bb12
  nvvm.barrier0
  %93 = llvm.icmp "eq" %19, %9 : i32
  %94 = llvm.and %93, %29  : i1
  llvm.cond_br %94, ^bb14, ^bb17
^bb14:  // pred: ^bb13
  %95 = llvm.add %17, %0  : i32
  %96 = llvm.getelementptr %11[%17] : (!llvm.ptr<f32, 3>, i32) -> !llvm.ptr<f32, 3>
  %97 = llvm.load %96 : !llvm.ptr<f32, 3>
  %98 = llvm.getelementptr %11[%95] : (!llvm.ptr<f32, 3>, i32) -> !llvm.ptr<f32, 3>
  %99 = llvm.load %98 : !llvm.ptr<f32, 3>
  %100 = llvm.fcmp "ugt" %97, %99 : f32
  %101 = llvm.select %100, %97, %99 : i1, f32
  %102 = llvm.fcmp "uno" %99, %99 : f32
  %103 = llvm.select %102, %99, %101 : i1, f32
  %104 = llvm.getelementptr %arg14[%26] : (!llvm.ptr<f32>, i32) -> !llvm.ptr<f32>
  %105 = llvm.load %104 : !llvm.ptr<f32>
  llvm.br ^bb15(%105 : f32)
^bb15(%106: f32):  // 2 preds: ^bb14, ^bb15
  %107 = llvm.fcmp "ogt" %106, %103 : f32
  %108 = llvm.select %107, %106, %103 : i1, f32
  %109 = llvm.bitcast %104 : !llvm.ptr<f32> to !llvm.ptr<i32>
  %110 = llvm.bitcast %106 : f32 to i32
  %111 = llvm.bitcast %108 : f32 to i32
  %112 = llvm.cmpxchg %109, %110, %111 acq_rel monotonic : !llvm.ptr<i32>, i32
  %113 = llvm.extractvalue %112[0] : !llvm.struct<(i32, i1)> 
  %114 = llvm.bitcast %113 : i32 to f32
  %115 = llvm.extractvalue %112[1] : !llvm.struct<(i32, i1)> 
  llvm.cond_br %115, ^bb16, ^bb15(%114 : f32)
^bb16:  // pred: ^bb15
  llvm.br ^bb17
^bb17:  // 2 preds: ^bb13, ^bb16
  llvm.br ^bb18
^bb18:  // 2 preds: ^bb1, ^bb17
  llvm.return
}

// -----// IR Dump After FunctionDeadArgumentEliminationPass (disc-function-dead-argument-elimination) //----- //
gpu.module @main_kernel_0 {
  llvm.mlir.global internal @__wg_main_kColReduction_reduce__4_1_0___8w32h_1_0() {addr_space = 3 : i32} : !llvm.array<256 x f32>
  llvm.func @__nv_fabsf(f32) -> f32
  llvm.func @main_kColReduction_reduce__4_1_0___8w32h_1(%arg0: i32, %arg1: !llvm.ptr<f32>, %arg2: i32, %arg3: i32, %arg4: i32, %arg5: i32, %arg6: !llvm.ptr<f32>) attributes {disc.elimargs = [1 : index, 3 : index, 5 : index, 6 : index, 7 : index, 8 : index, 9 : index, 13 : index, 15 : index, 16 : index, 17 : index], gpu.kernel, nvvm.kernel} {
    %0 = llvm.mlir.constant(8 : index) : i32
    %1 = llvm.mlir.constant(32 : index) : i32
    %2 = llvm.mlir.constant(2 : index) : i32
    %3 = llvm.mlir.constant(0xFF800000 : f32) : f32
    %4 = llvm.mlir.constant(16 : index) : i32
    %5 = llvm.mlir.constant(128 : index) : i32
    %6 = llvm.mlir.constant(64 : index) : i32
    %7 = llvm.mlir.constant(4 : index) : i32
    %8 = llvm.mlir.constant(256 : index) : i32
    %9 = llvm.mlir.constant(0 : index) : i32
    %10 = llvm.mlir.addressof @__wg_main_kColReduction_reduce__4_1_0___8w32h_1_0 : !llvm.ptr<array<256 x f32>, 3>
    %11 = llvm.getelementptr %10[0, 0] : (!llvm.ptr<array<256 x f32>, 3>) -> !llvm.ptr<f32, 3>
    %12 = nvvm.read.ptx.sreg.ctaid.x : i32
    %13 = nvvm.read.ptx.sreg.tid.x : i32
    llvm.br ^bb1
  ^bb1:  // pred: ^bb0
    %14 = llvm.mul %12, %arg3  : i32
    %15 = llvm.add %13, %14  : i32
    %16 = llvm.icmp "ult" %15, %arg4 : i32
    llvm.cond_br %16, ^bb2, ^bb18
  ^bb2:  // pred: ^bb1
    %17 = llvm.srem %15, %8  : i32
    %18 = llvm.sdiv %15, %8  : i32
    %19 = llvm.udiv %17, %0  : i32
    %20 = llvm.urem %17, %0  : i32
    %21 = llvm.udiv %18, %arg5  : i32
    %22 = llvm.urem %18, %arg5  : i32
    %23 = llvm.mul %21, %1  : i32
    %24 = llvm.add %23, %19  : i32
    %25 = llvm.mul %22, %0  : i32
    %26 = llvm.add %25, %20  : i32
    %27 = llvm.icmp "ult" %24, %arg2 : i32
    %28 = llvm.icmp "ult" %26, %arg0 : i32
    %29 = llvm.and %27, %28  : i1
    llvm.cond_br %29, ^bb3, ^bb4
  ^bb3:  // pred: ^bb2
    %30 = llvm.mul %24, %arg0  : i32
    %31 = llvm.add %30, %26  : i32
    %32 = llvm.getelementptr %arg1[%31] : (!llvm.ptr<f32>, i32) -> !llvm.ptr<f32>
    %33 = llvm.load %32 : !llvm.ptr<f32>
    %34 = llvm.call @__nv_fabsf(%33) : (f32) -> f32
    %35 = llvm.getelementptr %11[%17] : (!llvm.ptr<f32, 3>, i32) -> !llvm.ptr<f32, 3>
    llvm.store %34, %35 : !llvm.ptr<f32, 3>
    llvm.br ^bb5
  ^bb4:  // pred: ^bb2
    %36 = llvm.getelementptr %11[%17] : (!llvm.ptr<f32, 3>, i32) -> !llvm.ptr<f32, 3>
    llvm.store %3, %36 : !llvm.ptr<f32, 3>
    llvm.br ^bb5
  ^bb5:  // 2 preds: ^bb3, ^bb4
    nvvm.barrier0
    %37 = llvm.icmp "ult" %19, %4 : i32
    %38 = llvm.add %24, %4  : i32
    %39 = llvm.icmp "ult" %38, %arg2 : i32
    %40 = llvm.and %37, %39  : i1
    llvm.cond_br %40, ^bb6, ^bb7
  ^bb6:  // pred: ^bb5
    %41 = llvm.add %17, %5  : i32
    %42 = llvm.getelementptr %11[%17] : (!llvm.ptr<f32, 3>, i32) -> !llvm.ptr<f32, 3>
    %43 = llvm.load %42 : !llvm.ptr<f32, 3>
    %44 = llvm.getelementptr %11[%41] : (!llvm.ptr<f32, 3>, i32) -> !llvm.ptr<f32, 3>
    %45 = llvm.load %44 : !llvm.ptr<f32, 3>
    %46 = llvm.fcmp "ugt" %43, %45 : f32
    %47 = llvm.select %46, %43, %45 : i1, f32
    %48 = llvm.fcmp "uno" %45, %45 : f32
    %49 = llvm.select %48, %45, %47 : i1, f32
    %50 = llvm.getelementptr %11[%17] : (!llvm.ptr<f32, 3>, i32) -> !llvm.ptr<f32, 3>
    llvm.store %49, %50 : !llvm.ptr<f32, 3>
    llvm.br ^bb7
  ^bb7:  // 2 preds: ^bb5, ^bb6
    nvvm.barrier0
    %51 = llvm.icmp "ult" %19, %0 : i32
    %52 = llvm.add %24, %0  : i32
    %53 = llvm.icmp "ult" %52, %arg2 : i32
    %54 = llvm.and %51, %53  : i1
    llvm.cond_br %54, ^bb8, ^bb9
  ^bb8:  // pred: ^bb7
    %55 = llvm.add %17, %6  : i32
    %56 = llvm.getelementptr %11[%17] : (!llvm.ptr<f32, 3>, i32) -> !llvm.ptr<f32, 3>
    %57 = llvm.load %56 : !llvm.ptr<f32, 3>
    %58 = llvm.getelementptr %11[%55] : (!llvm.ptr<f32, 3>, i32) -> !llvm.ptr<f32, 3>
    %59 = llvm.load %58 : !llvm.ptr<f32, 3>
    %60 = llvm.fcmp "ugt" %57, %59 : f32
    %61 = llvm.select %60, %57, %59 : i1, f32
    %62 = llvm.fcmp "uno" %59, %59 : f32
    %63 = llvm.select %62, %59, %61 : i1, f32
    %64 = llvm.getelementptr %11[%17] : (!llvm.ptr<f32, 3>, i32) -> !llvm.ptr<f32, 3>
    llvm.store %63, %64 : !llvm.ptr<f32, 3>
    llvm.br ^bb9
  ^bb9:  // 2 preds: ^bb7, ^bb8
    nvvm.barrier0
    %65 = llvm.icmp "ult" %19, %7 : i32
    %66 = llvm.add %24, %7  : i32
    %67 = llvm.icmp "ult" %66, %arg2 : i32
    %68 = llvm.and %65, %67  : i1
    llvm.cond_br %68, ^bb10, ^bb11
  ^bb10:  // pred: ^bb9
    %69 = llvm.add %17, %1  : i32
    %70 = llvm.getelementptr %11[%17] : (!llvm.ptr<f32, 3>, i32) -> !llvm.ptr<f32, 3>
    %71 = llvm.load %70 : !llvm.ptr<f32, 3>
    %72 = llvm.getelementptr %11[%69] : (!llvm.ptr<f32, 3>, i32) -> !llvm.ptr<f32, 3>
    %73 = llvm.load %72 : !llvm.ptr<f32, 3>
    %74 = llvm.fcmp "ugt" %71, %73 : f32
    %75 = llvm.select %74, %71, %73 : i1, f32
    %76 = llvm.fcmp "uno" %73, %73 : f32
    %77 = llvm.select %76, %73, %75 : i1, f32
    %78 = llvm.getelementptr %11[%17] : (!llvm.ptr<f32, 3>, i32) -> !llvm.ptr<f32, 3>
    llvm.store %77, %78 : !llvm.ptr<f32, 3>
    llvm.br ^bb11
  ^bb11:  // 2 preds: ^bb9, ^bb10
    nvvm.barrier0
    %79 = llvm.icmp "ult" %19, %2 : i32
    %80 = llvm.add %24, %2  : i32
    %81 = llvm.icmp "ult" %80, %arg2 : i32
    %82 = llvm.and %79, %81  : i1
    llvm.cond_br %82, ^bb12, ^bb13
  ^bb12:  // pred: ^bb11
    %83 = llvm.add %17, %4  : i32
    %84 = llvm.getelementptr %11[%17] : (!llvm.ptr<f32, 3>, i32) -> !llvm.ptr<f32, 3>
    %85 = llvm.load %84 : !llvm.ptr<f32, 3>
    %86 = llvm.getelementptr %11[%83] : (!llvm.ptr<f32, 3>, i32) -> !llvm.ptr<f32, 3>
    %87 = llvm.load %86 : !llvm.ptr<f32, 3>
    %88 = llvm.fcmp "ugt" %85, %87 : f32
    %89 = llvm.select %88, %85, %87 : i1, f32
    %90 = llvm.fcmp "uno" %87, %87 : f32
    %91 = llvm.select %90, %87, %89 : i1, f32
    %92 = llvm.getelementptr %11[%17] : (!llvm.ptr<f32, 3>, i32) -> !llvm.ptr<f32, 3>
    llvm.store %91, %92 : !llvm.ptr<f32, 3>
    llvm.br ^bb13
  ^bb13:  // 2 preds: ^bb11, ^bb12
    nvvm.barrier0
    %93 = llvm.icmp "eq" %19, %9 : i32
    %94 = llvm.and %93, %29  : i1
    llvm.cond_br %94, ^bb14, ^bb17
  ^bb14:  // pred: ^bb13
    %95 = llvm.add %17, %0  : i32
    %96 = llvm.getelementptr %11[%17] : (!llvm.ptr<f32, 3>, i32) -> !llvm.ptr<f32, 3>
    %97 = llvm.load %96 : !llvm.ptr<f32, 3>
    %98 = llvm.getelementptr %11[%95] : (!llvm.ptr<f32, 3>, i32) -> !llvm.ptr<f32, 3>
    %99 = llvm.load %98 : !llvm.ptr<f32, 3>
    %100 = llvm.fcmp "ugt" %97, %99 : f32
    %101 = llvm.select %100, %97, %99 : i1, f32
    %102 = llvm.fcmp "uno" %99, %99 : f32
    %103 = llvm.select %102, %99, %101 : i1, f32
    %104 = llvm.getelementptr %arg6[%26] : (!llvm.ptr<f32>, i32) -> !llvm.ptr<f32>
    %105 = llvm.load %104 : !llvm.ptr<f32>
    llvm.br ^bb15(%105 : f32)
  ^bb15(%106: f32):  // 2 preds: ^bb14, ^bb15
    %107 = llvm.fcmp "ogt" %106, %103 : f32
    %108 = llvm.select %107, %106, %103 : i1, f32
    %109 = llvm.bitcast %104 : !llvm.ptr<f32> to !llvm.ptr<i32>
    %110 = llvm.bitcast %106 : f32 to i32
    %111 = llvm.bitcast %108 : f32 to i32
    %112 = llvm.cmpxchg %109, %110, %111 acq_rel monotonic : !llvm.ptr<i32>, i32
    %113 = llvm.extractvalue %112[0] : !llvm.struct<(i32, i1)> 
    %114 = llvm.bitcast %113 : i32 to f32
    %115 = llvm.extractvalue %112[1] : !llvm.struct<(i32, i1)> 
    llvm.cond_br %115, ^bb16, ^bb15(%114 : f32)
  ^bb16:  // pred: ^bb15
    llvm.br ^bb17
  ^bb17:  // 2 preds: ^bb13, ^bb16
    llvm.br ^bb18
  ^bb18:  // 2 preds: ^bb1, ^bb17
    llvm.return
  }
}

// -----// IR Dump After GpuKernelToBlobPass (disc-gpu-kernel-to-blob) //----- //
gpu.module @main_kernel_0 attributes {gpu.binary = "P\EDU\BA\01\00\10\00@\08\00\00\00\00\00\00\02\00\01\01@\00\00\00\00\08\00\00\00\00\00\00\F9\07\00\00\00\00\00\00\07\00\01\00P\00\00\00\00\00\00\00\00\00\00\00\11 \00\00\00\00\00\00\00\00\00\00\00\00\00\00(\14\00\00\00\00\00\00\A2\7FELF\02\01\013\07\00\01\00f\02\00\BE\00q\00\01\00#\80\13\08\00\11\10\07\00\F5\0E\00P\05P\00@\008\00\03\00@\00\0C\00\01\00\00.shstrtab\00.\08\00'ym\08\00\F0\1B_shndx\00.nv.info\00.text.main_kColReduction_r\0A\00\F5\03e__4_1_0___8w32h_1:\00\0F4\00\1Doshared6\00\1AOrela\A0\00\1F?rel\D5\00\22\9Fconstant09\00\1A\B2debug_frame{\00\09\11\00!nv\14\00\11aE\00\0F\9E\01 \0F\8A\00\17\0F\C9\01\F4\8F$____wg_3\00\17\00\0C\00/27\02\02'o_param\09\02\1C\0F\01\00\05\8C]\00\00\00\03\00\0A\00\01\00\11\C2\18\00,\0B\00\01\00 \9C\01\18\00,\09\00\01\00\11\DC\18\00,\04\00\01\00\11\FA\18\00,\07\00\01\00g2\00\00\00\12\10x\00\11\08\06\00v\00\FF\FF\FF\FF(\00\0C\00\00\01\00@\03\00\04|\08\00\D1\0F\0C\81\80\80(\00\08\FF\81\80(\08\0B\00\06(\00/0\00\01\00\03\13\F0{\00\10\04\9B\00R\04\14\00\00\00E\002\04\AC\01\18\00\80/\08\00\06\00\00\00\0E\0C\00\12#\0C\00\00\01\00'\04\12\0C\00\17\11\0C\00!7\04 \05\F1\08\015\00\00\04\0A\08\00\03\00\00\00`\01(\00\03\19(\00\04\17\0C$\00u\06\00 \00\00\F0!\10\00u\05\00\1C\00\00\F0\11\10\009\04\00\18\10\009\03\00\14\10\009\02\00\10\10\009\01\00\08P\00\01\01\00\F2\0A\F0\11\00\03\1B\FF\00\04\1C\0C\00P\00\00\00\10\06\00\00\10\07\00\00\04\1E\84\01#K\00\01\00v\02\02\08\10\0A/\22b\01\13\08\08\00\13\10\08\00\13\18\08\00\13 \08\00\13(\08\00\130\08\00\228\08\84\00\13\00\08\00\13\08\08\00\13\10\08\00\13\18\08\00\13 \08\00\13(\08\00\130\08\00\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\A2\00\00\00\14,\00\00\00H\00\01\00\11\02h\01\0F\01\00\FF\B0@$v\01\FF\7F\04\B2\FF\00\8E\07\00\C4\0F\00\19y\00\01\00\10%\8B\02a\0E\00\19y\03\00\01\00\10!-\00B\0E\00$z\B5\04\90\03\02\8E\07\00\CA\1F\00\0C\10\00\C5^\00\00p`\F0\03\00\DA\0F\00M\9B\04\F0\0C\80\03\00\EA\0F\00\06{\04\00\00_\00\00\00\90 \00\00\22\0E\00\19x\05\FF\1F\1F\00\D3\14\01\00\00\E2\0F\00$r\02\FF\FF\00\80\00 \E2\0FP\00\10\FF0\00@pP\F4\03\10\00\81\B9z\04\00\00F\00\00#\05p\E2\0F\00\11r\05\05?\00\B2\FF@\8F\07\00\C6\0F\00\08s\04\F6\04\00 \09\F1\07$\1E\00\10x\03\04\FE\FF\FF\0F\FF\E0\FF\07\00\CC\1F\00\05s\03$\04\00\C0\03\01\C0\00!r\07p\00\C0\03\0A\8E\07\00\C8\1F\00$z\07\07p\00\10\FF\D0\00\81\C8\0F\00'r\07\03\07\E0\02\02\90\00@\19x\02\FF\01\03\10\05\B0\00q\E4\0F\00\12x\05\05\F1\04!\C0\8E\80\00T'r\07\07\02\C0\00\10\C8\D0\00\11\09`\00\10\07`\00`\E4\0F\00$x\05\BC\03$\00\05\10\000z\03\09p\00\22\02\02@\00@\19x\00\FF\A0\00\22\05\16`\00f\0Cz\00\03\00_P\010\10\0A\03\10\00\11\80\D0\00p\E4\0F\00\10\08\07\07P\00\04\10\00\060\00\12\F20\00\1A\18 \001\12\AA\07P\01 \FF3\B0\01\00\A0\00\1B\03\A0\000\00\07  \01\15\02\A0\00\15\03\A0\00\10\E2\F0\00 \02\05\10\01#\FF\C0@\00Sx\09\03\08\00 \00\11\CA\80\003\09\00X\80\00\22\C8\0F\10\02p\\\00\00p\10\F2\04\90\00T$\14\03\FF\04 \01\00`\000\1A\02\000\00\13\09p\01c%\16\02\02\00ZP\02q\CC\0F\00\81\19\02\02\D0\01\C4\19\1E\0C\00\A2\00\00\10x\06\00\10\D0\00\000\02\92t\04\FF\00\00\80\FF\FF\00\A0\001\1Cx\00\01\001p\F0\F00\02E$x\07\05p\00q\E2\0F\04\0Cz\00\06\90\00#`\F6 \00\16\06 \00\80\C6\0F\00\0Cx\00\05\7F@\00\C5D\F6\01\00\E4\0F\04\10x\03\00\08p\00`\1F\00!\12\04\02q\00\01\F1\04!\E2O0\00\11\070\00 \F2\04\C0\00c\88s\00\07\04\00\DE\05f\E8\0F\00\1D{\00\01\00u\EC\0F\00\84\B9\0B\06~\05R\22\0E\00\10x\B0\02\01`\00Q\C4\0F\00\10x\1E\00\03p\00\000\004\02\06\00P\00qb\0E\00\0B\B2\00\0B\06\08\B1\80\F4\03\00\E4\1F\08\0B\B2\00\02\10\00\A3\C0\F8\03\00\E4/\00\1C\B8\00\00\01%p\01\10\02\01\F0\00B\F4\03\00\C8\E0\00\11?\B0\00\C3t\01\00\CE\0F\00\08\82\0B\0B\02\00'\07\1B\E4@\01\8F\C6\0F\00\88\B3\00\06\0B\D0\00\095\A9\03\06\8E\06[(\0E\00\84\A9\B0\001\A2\00\03p\03`\80\F6\03\00\C4\1F\10\00(\02\03\B0\00$\A8\00p\00\04\B0\00\15\04\A0\01\03\B0\00\14\1F\90\01\01\B0\00/\03\03\B0\00\0AO\A3\00\06\03\80\01\0AV\07\06\00\80\00\B0\00\0E`\01$\07\07`\01\00\B0\00O\B2\00\02\07`\01\0B\1C\00`\01\19\0F`\01\02\10\04\0F`\01\09\1F\07`\01\0D\17@\B0\00\13\A9g\07\0F`\01\07\1F\00`\01\06\12\DA@\01\14\00\90\00\1F\CA0\01\0F9M\19\00\C0\05G$t\02\FFP\03A\00\84y\05_\09\01@\00\96&\0E\00%v\02\09\00`0\04'\84y\B0\00fh\0E\00\81y\08\D0\03bb\05\00\0Br\00\C1\05\22\80\F0\C0\001r\00\00\10\00\92\C0\F2\03\00\D6/\00\08\82\1F\00p\00\00\80\04\00\C8O \00\11\08\11\00 @\F0\C0\03$\0EF\A0\06\00\A0\00b\E6\0F\00\08r\09 \00\00\01\00p\CC\0F\00\A9s\09\02{\00\C0\09\E1\1E\00\00\A4\0E\00\0Cr\00\09\10\00 pR@\00QO\00$r\08\00\05\10\09\D0\00\80\D8\0F\00G\09\00\00\90\D0\05!\FF\83\F0\00*My\00\01TGy\00\00\F0 \00f\C0\0F\00\18y\00\01\00\0F\10\00\B0\0F\01\00-#\01\00\80\02\0B\01\00\22@\00\01\00=\9E\01\000\00\08\01\00\1F\0B@\00\04\13\DE)\00?\09\02\00@\00\0A\22\13\00@\05\0C\01\00\13\E8U\00\03\0F\03\01$\00\13\05\97\02\00\01\00\22\18\00\01\00.q\01T\00\00\01\00\11\90\B5\02O\00\00p\00\80\00\0B\1F)'\00\03\03\D5\02$\00\00\18\0D\04\E4\00*\04\00\01\00\1Fc@\00\04*0\05\C0\00\13\03\03\09\0C@\00!\8F\01D\01\0D@\00\13\D8@\00*\D8\00\01\00\1B\08\08\00?~\01\00N\0E\002\00\00\B0\C6\03\01W\09\04\80\00\17\048\00\04\18\00\138@\01\0C\84\01\13\C0@\00\17\881\01\0F\C0\00\01\132T\01\02\E6\07\06\01\00\1B\80\A9\00\11\03$\00J\00\0E\80\00\01\00\13\97#\00*\03\00\01\00\040\13/\00\04\80\00\0B\13\06\AB\01\04h\13\0C\01\00\1B\A8\08\00\17\08\08\02\17\05\E8\00\0C\01\00*\C0\09\08\00\088\00\18\06\A0\00\0F\01\00\05\03\A9\00\80\08\00\00\00\00\00\00\00\00\00\00\00\00\00\00"} {
  llvm.mlir.global internal @__wg_main_kColReduction_reduce__4_1_0___8w32h_1_0() {addr_space = 3 : i32} : !llvm.array<256 x f32>
  llvm.func @__nv_fabsf(f32) -> f32
  llvm.func @main_kColReduction_reduce__4_1_0___8w32h_1(%arg0: i32, %arg1: !llvm.ptr<f32>, %arg2: i32, %arg3: i32, %arg4: i32, %arg5: i32, %arg6: !llvm.ptr<f32>) attributes {disc.elimargs = [1 : index, 3 : index, 5 : index, 6 : index, 7 : index, 8 : index, 9 : index, 13 : index, 15 : index, 16 : index, 17 : index], gpu.kernel, nvvm.kernel} {
    %0 = llvm.mlir.constant(8 : index) : i32
    %1 = llvm.mlir.constant(32 : index) : i32
    %2 = llvm.mlir.constant(2 : index) : i32
    %3 = llvm.mlir.constant(0xFF800000 : f32) : f32
    %4 = llvm.mlir.constant(16 : index) : i32
    %5 = llvm.mlir.constant(128 : index) : i32
    %6 = llvm.mlir.constant(64 : index) : i32
    %7 = llvm.mlir.constant(4 : index) : i32
    %8 = llvm.mlir.constant(256 : index) : i32
    %9 = llvm.mlir.constant(0 : index) : i32
    %10 = llvm.mlir.addressof @__wg_main_kColReduction_reduce__4_1_0___8w32h_1_0 : !llvm.ptr<array<256 x f32>, 3>
    %11 = llvm.getelementptr %10[0, 0] : (!llvm.ptr<array<256 x f32>, 3>) -> !llvm.ptr<f32, 3>
    %12 = nvvm.read.ptx.sreg.ctaid.x : i32
    %13 = nvvm.read.ptx.sreg.tid.x : i32
    llvm.br ^bb1
  ^bb1:  // pred: ^bb0
    %14 = llvm.mul %12, %arg3  : i32
    %15 = llvm.add %13, %14  : i32
    %16 = llvm.icmp "ult" %15, %arg4 : i32
    llvm.cond_br %16, ^bb2, ^bb18
  ^bb2:  // pred: ^bb1
    %17 = llvm.srem %15, %8  : i32
    %18 = llvm.sdiv %15, %8  : i32
    %19 = llvm.udiv %17, %0  : i32
    %20 = llvm.urem %17, %0  : i32
    %21 = llvm.udiv %18, %arg5  : i32
    %22 = llvm.urem %18, %arg5  : i32
    %23 = llvm.mul %21, %1  : i32
    %24 = llvm.add %23, %19  : i32
    %25 = llvm.mul %22, %0  : i32
    %26 = llvm.add %25, %20  : i32
    %27 = llvm.icmp "ult" %24, %arg2 : i32
    %28 = llvm.icmp "ult" %26, %arg0 : i32
    %29 = llvm.and %27, %28  : i1
    llvm.cond_br %29, ^bb3, ^bb4
  ^bb3:  // pred: ^bb2
    %30 = llvm.mul %24, %arg0  : i32
    %31 = llvm.add %30, %26  : i32
    %32 = llvm.getelementptr %arg1[%31] : (!llvm.ptr<f32>, i32) -> !llvm.ptr<f32>
    %33 = llvm.load %32 : !llvm.ptr<f32>
    %34 = llvm.call @__nv_fabsf(%33) : (f32) -> f32
    %35 = llvm.getelementptr %11[%17] : (!llvm.ptr<f32, 3>, i32) -> !llvm.ptr<f32, 3>
    llvm.store %34, %35 : !llvm.ptr<f32, 3>
    llvm.br ^bb5
  ^bb4:  // pred: ^bb2
    %36 = llvm.getelementptr %11[%17] : (!llvm.ptr<f32, 3>, i32) -> !llvm.ptr<f32, 3>
    llvm.store %3, %36 : !llvm.ptr<f32, 3>
    llvm.br ^bb5
  ^bb5:  // 2 preds: ^bb3, ^bb4
    nvvm.barrier0
    %37 = llvm.icmp "ult" %19, %4 : i32
    %38 = llvm.add %24, %4  : i32
    %39 = llvm.icmp "ult" %38, %arg2 : i32
    %40 = llvm.and %37, %39  : i1
    llvm.cond_br %40, ^bb6, ^bb7
  ^bb6:  // pred: ^bb5
    %41 = llvm.add %17, %5  : i32
    %42 = llvm.getelementptr %11[%17] : (!llvm.ptr<f32, 3>, i32) -> !llvm.ptr<f32, 3>
    %43 = llvm.load %42 : !llvm.ptr<f32, 3>
    %44 = llvm.getelementptr %11[%41] : (!llvm.ptr<f32, 3>, i32) -> !llvm.ptr<f32, 3>
    %45 = llvm.load %44 : !llvm.ptr<f32, 3>
    %46 = llvm.fcmp "ugt" %43, %45 : f32
    %47 = llvm.select %46, %43, %45 : i1, f32
    %48 = llvm.fcmp "uno" %45, %45 : f32
    %49 = llvm.select %48, %45, %47 : i1, f32
    %50 = llvm.getelementptr %11[%17] : (!llvm.ptr<f32, 3>, i32) -> !llvm.ptr<f32, 3>
    llvm.store %49, %50 : !llvm.ptr<f32, 3>
    llvm.br ^bb7
  ^bb7:  // 2 preds: ^bb5, ^bb6
    nvvm.barrier0
    %51 = llvm.icmp "ult" %19, %0 : i32
    %52 = llvm.add %24, %0  : i32
    %53 = llvm.icmp "ult" %52, %arg2 : i32
    %54 = llvm.and %51, %53  : i1
    llvm.cond_br %54, ^bb8, ^bb9
  ^bb8:  // pred: ^bb7
    %55 = llvm.add %17, %6  : i32
    %56 = llvm.getelementptr %11[%17] : (!llvm.ptr<f32, 3>, i32) -> !llvm.ptr<f32, 3>
    %57 = llvm.load %56 : !llvm.ptr<f32, 3>
    %58 = llvm.getelementptr %11[%55] : (!llvm.ptr<f32, 3>, i32) -> !llvm.ptr<f32, 3>
    %59 = llvm.load %58 : !llvm.ptr<f32, 3>
    %60 = llvm.fcmp "ugt" %57, %59 : f32
    %61 = llvm.select %60, %57, %59 : i1, f32
    %62 = llvm.fcmp "uno" %59, %59 : f32
    %63 = llvm.select %62, %59, %61 : i1, f32
    %64 = llvm.getelementptr %11[%17] : (!llvm.ptr<f32, 3>, i32) -> !llvm.ptr<f32, 3>
    llvm.store %63, %64 : !llvm.ptr<f32, 3>
    llvm.br ^bb9
  ^bb9:  // 2 preds: ^bb7, ^bb8
    nvvm.barrier0
    %65 = llvm.icmp "ult" %19, %7 : i32
    %66 = llvm.add %24, %7  : i32
    %67 = llvm.icmp "ult" %66, %arg2 : i32
    %68 = llvm.and %65, %67  : i1
    llvm.cond_br %68, ^bb10, ^bb11
  ^bb10:  // pred: ^bb9
    %69 = llvm.add %17, %1  : i32
    %70 = llvm.getelementptr %11[%17] : (!llvm.ptr<f32, 3>, i32) -> !llvm.ptr<f32, 3>
    %71 = llvm.load %70 : !llvm.ptr<f32, 3>
    %72 = llvm.getelementptr %11[%69] : (!llvm.ptr<f32, 3>, i32) -> !llvm.ptr<f32, 3>
    %73 = llvm.load %72 : !llvm.ptr<f32, 3>
    %74 = llvm.fcmp "ugt" %71, %73 : f32
    %75 = llvm.select %74, %71, %73 : i1, f32
    %76 = llvm.fcmp "uno" %73, %73 : f32
    %77 = llvm.select %76, %73, %75 : i1, f32
    %78 = llvm.getelementptr %11[%17] : (!llvm.ptr<f32, 3>, i32) -> !llvm.ptr<f32, 3>
    llvm.store %77, %78 : !llvm.ptr<f32, 3>
    llvm.br ^bb11
  ^bb11:  // 2 preds: ^bb9, ^bb10
    nvvm.barrier0
    %79 = llvm.icmp "ult" %19, %2 : i32
    %80 = llvm.add %24, %2  : i32
    %81 = llvm.icmp "ult" %80, %arg2 : i32
    %82 = llvm.and %79, %81  : i1
    llvm.cond_br %82, ^bb12, ^bb13
  ^bb12:  // pred: ^bb11
    %83 = llvm.add %17, %4  : i32
    %84 = llvm.getelementptr %11[%17] : (!llvm.ptr<f32, 3>, i32) -> !llvm.ptr<f32, 3>
    %85 = llvm.load %84 : !llvm.ptr<f32, 3>
    %86 = llvm.getelementptr %11[%83] : (!llvm.ptr<f32, 3>, i32) -> !llvm.ptr<f32, 3>
    %87 = llvm.load %86 : !llvm.ptr<f32, 3>
    %88 = llvm.fcmp "ugt" %85, %87 : f32
    %89 = llvm.select %88, %85, %87 : i1, f32
    %90 = llvm.fcmp "uno" %87, %87 : f32
    %91 = llvm.select %90, %87, %89 : i1, f32
    %92 = llvm.getelementptr %11[%17] : (!llvm.ptr<f32, 3>, i32) -> !llvm.ptr<f32, 3>
    llvm.store %91, %92 : !llvm.ptr<f32, 3>
    llvm.br ^bb13
  ^bb13:  // 2 preds: ^bb11, ^bb12
    nvvm.barrier0
    %93 = llvm.icmp "eq" %19, %9 : i32
    %94 = llvm.and %93, %29  : i1
    llvm.cond_br %94, ^bb14, ^bb17
  ^bb14:  // pred: ^bb13
    %95 = llvm.add %17, %0  : i32
    %96 = llvm.getelementptr %11[%17] : (!llvm.ptr<f32, 3>, i32) -> !llvm.ptr<f32, 3>
    %97 = llvm.load %96 : !llvm.ptr<f32, 3>
    %98 = llvm.getelementptr %11[%95] : (!llvm.ptr<f32, 3>, i32) -> !llvm.ptr<f32, 3>
    %99 = llvm.load %98 : !llvm.ptr<f32, 3>
    %100 = llvm.fcmp "ugt" %97, %99 : f32
    %101 = llvm.select %100, %97, %99 : i1, f32
    %102 = llvm.fcmp "uno" %99, %99 : f32
    %103 = llvm.select %102, %99, %101 : i1, f32
    %104 = llvm.getelementptr %arg6[%26] : (!llvm.ptr<f32>, i32) -> !llvm.ptr<f32>
    %105 = llvm.load %104 : !llvm.ptr<f32>
    llvm.br ^bb15(%105 : f32)
  ^bb15(%106: f32):  // 2 preds: ^bb14, ^bb15
    %107 = llvm.fcmp "ogt" %106, %103 : f32
    %108 = llvm.select %107, %106, %103 : i1, f32
    %109 = llvm.bitcast %104 : !llvm.ptr<f32> to !llvm.ptr<i32>
    %110 = llvm.bitcast %106 : f32 to i32
    %111 = llvm.bitcast %108 : f32 to i32
    %112 = llvm.cmpxchg %109, %110, %111 acq_rel monotonic : !llvm.ptr<i32>, i32
    %113 = llvm.extractvalue %112[0] : !llvm.struct<(i32, i1)> 
    %114 = llvm.bitcast %113 : i32 to f32
    %115 = llvm.extractvalue %112[1] : !llvm.struct<(i32, i1)> 
    llvm.cond_br %115, ^bb16, ^bb15(%114 : f32)
  ^bb16:  // pred: ^bb15
    llvm.br ^bb17
  ^bb17:  // 2 preds: ^bb13, ^bb16
    llvm.br ^bb18
  ^bb18:  // 2 preds: ^bb1, ^bb17
    llvm.return
  }
}

// -----// IR Dump After SideEffectLoopInvariantCodeMotionPass (disc-side-effect-loop-invariant-code-motion) //----- //
gpu.func @main_kColReduction_reduce__4_1_0___8w16h(%arg0: index, %arg1: index, %arg2: memref<?xf32, "gpu">) kernel {
  %cst = arith.constant 0xFF800000 : f32
  %c1 = arith.constant 1 : index
  %c0 = arith.constant 0 : index
  %0 = gpu.block_id  x
  %1 = gpu.thread_id  x
  cf.br ^bb1
^bb1:  // pred: ^bb0
  %2 = affine.apply affine_map<(d0)[s0, s1] -> (d0 * s0 + s1)>(%0)[%arg0, %c0]
  %3 = affine.apply affine_map<(d0)[s0, s1] -> (d0 * s0 + s1)>(%1)[%c1, %c0]
  %4 = arith.addi %3, %2 : index
  %5 = arith.addi %3, %2 : index
  %6 = arith.cmpi ult, %5, %arg1 : index
  scf.if %6 {
    %reinterpret_cast = memref.reinterpret_cast %arg2 to offset: [%c0], sizes: [%arg1], strides: [%c1] : memref<?xf32, "gpu"> to memref<?xf32, "gpu">
    memref.store %cst, %reinterpret_cast[%4] : memref<?xf32, "gpu">
  }
  gpu.return
}

// -----// IR Dump After CSE (cse) //----- //
gpu.module @main_kernel_1 {
  gpu.func @main_kColReduction_reduce__4_1_0___8w16h(%arg0: index, %arg1: index, %arg2: memref<?xf32, "gpu">) kernel {
    %cst = arith.constant 0xFF800000 : f32
    %c1 = arith.constant 1 : index
    %c0 = arith.constant 0 : index
    %0 = gpu.block_id  x
    %1 = gpu.thread_id  x
    cf.br ^bb1
  ^bb1:  // pred: ^bb0
    %2 = affine.apply affine_map<(d0)[s0, s1] -> (d0 * s0 + s1)>(%0)[%arg0, %c0]
    %3 = affine.apply affine_map<(d0)[s0, s1] -> (d0 * s0 + s1)>(%1)[%c1, %c0]
    %4 = arith.addi %3, %2 : index
    %5 = arith.cmpi ult, %4, %arg1 : index
    scf.if %5 {
      %reinterpret_cast = memref.reinterpret_cast %arg2 to offset: [%c0], sizes: [%arg1], strides: [%c1] : memref<?xf32, "gpu"> to memref<?xf32, "gpu">
      memref.store %cst, %reinterpret_cast[%4] : memref<?xf32, "gpu">
    }
    gpu.return
  }
}

// -----// IR Dump After SCFToControlFlow (convert-scf-to-cf) //----- //
gpu.module @main_kernel_1 {
  gpu.func @main_kColReduction_reduce__4_1_0___8w16h(%arg0: index, %arg1: index, %arg2: memref<?xf32, "gpu">) kernel {
    %cst = arith.constant 0xFF800000 : f32
    %c1 = arith.constant 1 : index
    %c0 = arith.constant 0 : index
    %0 = gpu.block_id  x
    %1 = gpu.thread_id  x
    cf.br ^bb1
  ^bb1:  // pred: ^bb0
    %2 = affine.apply affine_map<(d0)[s0, s1] -> (d0 * s0 + s1)>(%0)[%arg0, %c0]
    %3 = affine.apply affine_map<(d0)[s0, s1] -> (d0 * s0 + s1)>(%1)[%c1, %c0]
    %4 = arith.addi %3, %2 : index
    %5 = arith.cmpi ult, %4, %arg1 : index
    cf.cond_br %5, ^bb2, ^bb3
  ^bb2:  // pred: ^bb1
    %reinterpret_cast = memref.reinterpret_cast %arg2 to offset: [%c0], sizes: [%arg1], strides: [%c1] : memref<?xf32, "gpu"> to memref<?xf32, "gpu">
    memref.store %cst, %reinterpret_cast[%4] : memref<?xf32, "gpu">
    cf.br ^bb3
  ^bb3:  // 2 preds: ^bb1, ^bb2
    gpu.return
  }
}

// -----// IR Dump After ConvertAffineToStandard (lower-affine) //----- //
gpu.module @main_kernel_1 {
  gpu.func @main_kColReduction_reduce__4_1_0___8w16h(%arg0: index, %arg1: index, %arg2: memref<?xf32, "gpu">) kernel {
    %cst = arith.constant 0xFF800000 : f32
    %c1 = arith.constant 1 : index
    %c0 = arith.constant 0 : index
    %0 = gpu.block_id  x
    %1 = gpu.thread_id  x
    cf.br ^bb1
  ^bb1:  // pred: ^bb0
    %2 = arith.muli %0, %arg0 : index
    %3 = arith.addi %2, %c0 : index
    %4 = arith.muli %1, %c1 : index
    %5 = arith.addi %4, %c0 : index
    %6 = arith.addi %5, %3 : index
    %7 = arith.cmpi ult, %6, %arg1 : index
    cf.cond_br %7, ^bb2, ^bb3
  ^bb2:  // pred: ^bb1
    %reinterpret_cast = memref.reinterpret_cast %arg2 to offset: [%c0], sizes: [%arg1], strides: [%c1] : memref<?xf32, "gpu"> to memref<?xf32, "gpu">
    memref.store %cst, %reinterpret_cast[%6] : memref<?xf32, "gpu">
    cf.br ^bb3
  ^bb3:  // 2 preds: ^bb1, ^bb2
    gpu.return
  }
}

// -----// IR Dump After StripDebugInfo (strip-debuginfo) //----- //
gpu.module @main_kernel_1 {
  gpu.func @main_kColReduction_reduce__4_1_0___8w16h(%arg0: index, %arg1: index, %arg2: memref<?xf32, "gpu">) kernel {
    %cst = arith.constant 0xFF800000 : f32
    %c1 = arith.constant 1 : index
    %c0 = arith.constant 0 : index
    %0 = gpu.block_id  x
    %1 = gpu.thread_id  x
    cf.br ^bb1
  ^bb1:  // pred: ^bb0
    %2 = arith.muli %0, %arg0 : index
    %3 = arith.addi %2, %c0 : index
    %4 = arith.muli %1, %c1 : index
    %5 = arith.addi %4, %c0 : index
    %6 = arith.addi %5, %3 : index
    %7 = arith.cmpi ult, %6, %arg1 : index
    cf.cond_br %7, ^bb2, ^bb3
  ^bb2:  // pred: ^bb1
    %reinterpret_cast = memref.reinterpret_cast %arg2 to offset: [%c0], sizes: [%arg1], strides: [%c1] : memref<?xf32, "gpu"> to memref<?xf32, "gpu">
    memref.store %cst, %reinterpret_cast[%6] : memref<?xf32, "gpu">
    cf.br ^bb3
  ^bb3:  // 2 preds: ^bb1, ^bb2
    gpu.return
  }
}

// -----// IR Dump After DiscLowerGpuOpsToNVVMOpsPass (disc-convert-gpu-to-nvvm) //----- //
gpu.module @main_kernel_1 {
  llvm.func @main_kColReduction_reduce__4_1_0___8w16h(%arg0: i32, %arg1: i32, %arg2: !llvm.ptr<f32>, %arg3: !llvm.ptr<f32>, %arg4: i32, %arg5: i32, %arg6: i32) attributes {gpu.kernel, nvvm.kernel} {
    %0 = llvm.mlir.undef : !llvm.struct<(ptr<f32>, ptr<f32>, i32, array<1 x i32>, array<1 x i32>)>
    %1 = llvm.insertvalue %arg2, %0[0] : !llvm.struct<(ptr<f32>, ptr<f32>, i32, array<1 x i32>, array<1 x i32>)> 
    %2 = llvm.insertvalue %arg3, %1[1] : !llvm.struct<(ptr<f32>, ptr<f32>, i32, array<1 x i32>, array<1 x i32>)> 
    %3 = llvm.insertvalue %arg4, %2[2] : !llvm.struct<(ptr<f32>, ptr<f32>, i32, array<1 x i32>, array<1 x i32>)> 
    %4 = llvm.insertvalue %arg5, %3[3, 0] : !llvm.struct<(ptr<f32>, ptr<f32>, i32, array<1 x i32>, array<1 x i32>)> 
    %5 = llvm.insertvalue %arg6, %4[4, 0] : !llvm.struct<(ptr<f32>, ptr<f32>, i32, array<1 x i32>, array<1 x i32>)> 
    %6 = llvm.mlir.constant(0xFF800000 : f32) : f32
    %7 = llvm.mlir.constant(1 : index) : i32
    %8 = llvm.mlir.constant(0 : index) : i32
    %9 = nvvm.read.ptx.sreg.ctaid.x : i32
    %10 = nvvm.read.ptx.sreg.tid.x : i32
    llvm.br ^bb1
  ^bb1:  // pred: ^bb0
    %11 = llvm.mul %9, %arg0  : i32
    %12 = llvm.add %10, %11  : i32
    %13 = llvm.icmp "ult" %12, %arg1 : i32
    llvm.cond_br %13, ^bb2, ^bb3
  ^bb2:  // pred: ^bb1
    %14 = llvm.mlir.undef : !llvm.struct<(ptr<f32>, ptr<f32>, i32, array<1 x i32>, array<1 x i32>)>
    %15 = llvm.extractvalue %5[0] : !llvm.struct<(ptr<f32>, ptr<f32>, i32, array<1 x i32>, array<1 x i32>)> 
    %16 = llvm.extractvalue %5[1] : !llvm.struct<(ptr<f32>, ptr<f32>, i32, array<1 x i32>, array<1 x i32>)> 
    %17 = llvm.insertvalue %15, %14[0] : !llvm.struct<(ptr<f32>, ptr<f32>, i32, array<1 x i32>, array<1 x i32>)> 
    %18 = llvm.insertvalue %16, %17[1] : !llvm.struct<(ptr<f32>, ptr<f32>, i32, array<1 x i32>, array<1 x i32>)> 
    %19 = llvm.insertvalue %8, %18[2] : !llvm.struct<(ptr<f32>, ptr<f32>, i32, array<1 x i32>, array<1 x i32>)> 
    %20 = llvm.insertvalue %arg1, %19[3, 0] : !llvm.struct<(ptr<f32>, ptr<f32>, i32, array<1 x i32>, array<1 x i32>)> 
    %21 = llvm.insertvalue %7, %20[4, 0] : !llvm.struct<(ptr<f32>, ptr<f32>, i32, array<1 x i32>, array<1 x i32>)> 
    %22 = llvm.extractvalue %21[1] : !llvm.struct<(ptr<f32>, ptr<f32>, i32, array<1 x i32>, array<1 x i32>)> 
    %23 = llvm.getelementptr %22[%12] : (!llvm.ptr<f32>, i32) -> !llvm.ptr<f32>
    llvm.store %6, %23 : !llvm.ptr<f32>
    llvm.br ^bb3
  ^bb3:  // 2 preds: ^bb1, ^bb2
    llvm.return
  }
}

// -----// IR Dump After LLVMInsertValueSimplifierPass (disc-llvm-insert-value-simplifier) //----- //
llvm.func @main_kColReduction_reduce__4_1_0___8w16h(%arg0: i32, %arg1: i32, %arg2: !llvm.ptr<f32>, %arg3: !llvm.ptr<f32>, %arg4: i32, %arg5: i32, %arg6: i32) attributes {gpu.kernel, nvvm.kernel} {
  %0 = llvm.mlir.constant(0xFF800000 : f32) : f32
  %1 = nvvm.read.ptx.sreg.ctaid.x : i32
  %2 = nvvm.read.ptx.sreg.tid.x : i32
  llvm.br ^bb1
^bb1:  // pred: ^bb0
  %3 = llvm.mul %1, %arg0  : i32
  %4 = llvm.add %2, %3  : i32
  %5 = llvm.icmp "ult" %4, %arg1 : i32
  llvm.cond_br %5, ^bb2, ^bb3
^bb2:  // pred: ^bb1
  %6 = llvm.getelementptr %arg3[%4] : (!llvm.ptr<f32>, i32) -> !llvm.ptr<f32>
  llvm.store %0, %6 : !llvm.ptr<f32>
  llvm.br ^bb3
^bb3:  // 2 preds: ^bb1, ^bb2
  llvm.return
}

// -----// IR Dump After FunctionDeadArgumentEliminationPass (disc-function-dead-argument-elimination) //----- //
gpu.module @main_kernel_1 {
  llvm.func @main_kColReduction_reduce__4_1_0___8w16h(%arg0: i32, %arg1: i32, %arg2: !llvm.ptr<f32>) attributes {disc.elimargs = [2 : index, 4 : index, 5 : index, 6 : index], gpu.kernel, nvvm.kernel} {
    %0 = llvm.mlir.constant(0xFF800000 : f32) : f32
    %1 = nvvm.read.ptx.sreg.ctaid.x : i32
    %2 = nvvm.read.ptx.sreg.tid.x : i32
    llvm.br ^bb1
  ^bb1:  // pred: ^bb0
    %3 = llvm.mul %1, %arg0  : i32
    %4 = llvm.add %2, %3  : i32
    %5 = llvm.icmp "ult" %4, %arg1 : i32
    llvm.cond_br %5, ^bb2, ^bb3
  ^bb2:  // pred: ^bb1
    %6 = llvm.getelementptr %arg2[%4] : (!llvm.ptr<f32>, i32) -> !llvm.ptr<f32>
    llvm.store %0, %6 : !llvm.ptr<f32>
    llvm.br ^bb3
  ^bb3:  // 2 preds: ^bb1, ^bb2
    llvm.return
  }
}

// -----// IR Dump After GpuKernelToBlobPass (disc-gpu-kernel-to-blob) //----- //
gpu.module @main_kernel_1 attributes {gpu.binary = "P\EDU\BA\01\00\10\008\04\00\00\00\00\00\00\02\00\01\01@\00\00\00\F8\03\00\00\00\00\00\00\F8\03\00\00\00\00\00\00\07\00\01\00P\00\00\00\00\00\00\00\00\00\00\00\11 \00\00\00\00\00\00\00\00\00\00\00\00\00\00\E8\0B\00\00\00\00\00\00\A2\7FELF\02\01\013\07\00\01\00f\02\00\BE\00q\00\01\00!@\0B\07\001\00\80\08\07\00\F5\0E\00P\05P\00@\008\00\03\00@\00\0B\00\01\00\00.shstrtab\00.\08\00'ym\08\00\F0\1B_shndx\00.nv.info\00.text.main_kColReduction_r\0A\00\F5\01e__4_1_0___8w16h8\00\0F2\00\1Boshared4\00\1B\9Fconstant07\00\18\FA\01debug_frame\00.rel\11\00!nv\14\00\11aC\00\0F+\01 \0F\88\00\15\0FT\01\BAo_param[\01\1C\0F\01\00\06\8C[\00\00\00\03\00\0A\00\01\00\11\F0\18\00,\09\00\01\00 .\01\18\00,\04\00\01\00\11L\18\00,\07\00\01\00f2\00\00\00\12\10`\00!\80\01\07\00v\00\FF\FF\FF\FF(\00\0C\00\00\01\00@\03\00\04|\08\00\D1\0F\0C\81\80\80(\00\08\FF\81\80(\08\0B\00\06(\00/0\00\01\00\03\13pX\00\10\04\9B\00R\04\14\00\00\00E\00\01\0B\00\00\13\00p/\08\00\05\00\00\00\A7\03\22\04#\0C\00\00\01\00'\04\12\0C\00\17\11\0C\00!7\04\E8\03\F3\08\015\00\00\04\0A\08\00\02\00\00\00`\01\10\00\03\19\10\00\04\17\0C\0C\04U\08\00\00\F0!\10\00\10\01\18\01%\F0\11\10\00\01\01\00\F2\02\F0\11\00\03\1B\FF\00\04\1C\08\00P\00\00\00\B0\00\01\00#K\00\01\00s\02\02\08\10\0A/\22\9B\00\00\07\00\03\08\00\13\10\08\00\13\18\08\00\13 \08\00\13(\08\00\130\08\00\228\08|\00\13\00\08\00\13\08\08\00\13\10\08\00\13\18\08\00\13 \08\00\13(\08\00\130\08\00\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\A2\00\00\00\14,\00\00\00H\00\01\00\000\01/\05\00\01\00\FF\B0A\02z\01\00\1F\04\B1\0F\00\00\00\C4\0F\00\19y\02\00\01\00\10%\8B\02Q\0E\00\19y\03\0F\00\F5\1A\00!\00\00\00$\0E\00$z\02\02\00X\00\00\03\02\8E\07\00\CA\1F\00\0Cz\00\02\00Y\00\00p`\F0\03\00\DA\0F\00MS\04\A0\80\03\00\EA\0F\005t\03\FF\B3\03\10\FF\C0\03P\E2\0F\00\02x6\02B\80\FF\00\0F\10\00r\B9z\04\00\00F\00\84\00\94\D0\0F\00%v\02\02\00Z`\00`\0F\00\86y\00\022\00@\04\19\10\0C0\009My\00`\00PGy\00\00\F09\04\A6\FF\83\03\00\C0\0F\00\18y\00\01\00\0F\10\00\90\0F\01\00-\00W\01.\03\00\01\00\22@\00\01\00=+\01\000\00\08\01\00\1F\0B@\00\04\13k)\00\1F[@\00\0C\13\13\0C\04\0C\01\00\13\C8\15\00&\90\000\04#\04\00\85\04\00\F6\04\12\00\01\00\1F\FET\00\00\00\01\00\13X\95\00/p\00\80\00\0B\1F)'\00\03#\00\C8@\00\04P\06\04\E4\00*\04\00\01\00\1Fa@\00\04\13\F81\00&\\\00@\00\1F\0A@\00\00!\1C\01D\01\0D@\00\13X)\00*\D8\00\01\00\1B\08\08\00?\0B\01\00\86\07\00Q\00\000\05\00\01\00&\10\00\80\00\17\048\00\04\18\00\13\C7\14\01\0C\84\01*@\058\07\1F\00\C0\00\04\132@\00+\06\00\01\00\1A\07\D0\07\12\03\F0\05:\08\80\00\01\00\13\06\08\06\04(\0B\0C\01\00*\A8\00\08\00\04\F8\00\14\018\00/\05\00\01\00\029@\03\00\08\00\088\00/\06\00\01\00\17\80\08\00\00\00\00\00\00\00"} {
  llvm.func @main_kColReduction_reduce__4_1_0___8w16h(%arg0: i32, %arg1: i32, %arg2: !llvm.ptr<f32>) attributes {disc.elimargs = [2 : index, 4 : index, 5 : index, 6 : index], gpu.kernel, nvvm.kernel} {
    %0 = llvm.mlir.constant(0xFF800000 : f32) : f32
    %1 = nvvm.read.ptx.sreg.ctaid.x : i32
    %2 = nvvm.read.ptx.sreg.tid.x : i32
    llvm.br ^bb1
  ^bb1:  // pred: ^bb0
    %3 = llvm.mul %1, %arg0  : i32
    %4 = llvm.add %2, %3  : i32
    %5 = llvm.icmp "ult" %4, %arg1 : i32
    llvm.cond_br %5, ^bb2, ^bb3
  ^bb2:  // pred: ^bb1
    %6 = llvm.getelementptr %arg2[%4] : (!llvm.ptr<f32>, i32) -> !llvm.ptr<f32>
    llvm.store %0, %6 : !llvm.ptr<f32>
    llvm.br ^bb3
  ^bb3:  // 2 preds: ^bb1, ^bb2
    llvm.return
  }
}

// -----// IR Dump After SideEffectLoopInvariantCodeMotionPass (disc-side-effect-loop-invariant-code-motion) //----- //
gpu.func @main_kColReduction_reduce__4_1_0___8w16h_1(%arg0: index, %arg1: memref<?x?x?xf32, "gpu">, %arg2: index, %arg3: index, %arg4: index, %arg5: memref<?xf32, "gpu">) workgroup(%arg6 : memref<128xf32, #gpu.address_space<workgroup>>) kernel {
  %c32 = arith.constant 32 : index
  %c4 = arith.constant 4 : index
  %c64 = arith.constant 64 : index
  %cst = arith.constant 0xFF800000 : f32
  %c2 = arith.constant 2 : index
  %c16 = arith.constant 16 : index
  %c8 = arith.constant 8 : index
  %c128 = arith.constant 128 : index
  %c1 = arith.constant 1 : index
  %c0 = arith.constant 0 : index
  %0 = gpu.block_id  x
  %1 = gpu.thread_id  x
  cf.br ^bb1
^bb1:  // pred: ^bb0
  %dim = memref.dim %arg1, %c0 : memref<?x?x?xf32, "gpu">
  %dim_0 = memref.dim %arg1, %c1 : memref<?x?x?xf32, "gpu">
  %dim_1 = memref.dim %arg1, %c2 : memref<?x?x?xf32, "gpu">
  %2 = affine.apply affine_map<(d0)[s0, s1] -> (d0 * s0 + s1)>(%0)[%arg2, %c0]
  %3 = affine.apply affine_map<(d0)[s0, s1] -> (d0 * s0 + s1)>(%1)[%c1, %c0]
  %4 = arith.addi %3, %2 : index
  %5 = arith.addi %3, %2 : index
  %6 = arith.cmpi ult, %5, %arg3 : index
  scf.if %6 {
    %7 = arith.remsi %4, %c128 : index
    %8 = arith.divsi %4, %c128 : index
    %9 = arith.divui %7, %c8 : index
    %10 = arith.remui %7, %c8 : index
    %11 = arith.divui %8, %arg4 : index
    %12 = arith.remui %8, %arg4 : index
    %13 = arith.muli %11, %c16 : index
    %14 = arith.addi %13, %9 : index
    %15 = arith.muli %12, %c8 : index
    %16 = arith.addi %15, %10 : index
    %17 = arith.cmpi ult, %14, %dim : index
    %18 = arith.cmpi ult, %16, %arg0 : index
    %19 = arith.andi %17, %18 : i1
    scf.if %19 {
      %34 = arith.muli %14, %arg0 : index
      %35 = arith.addi %34, %16 : index
      %36 = arith.muli %dim, %dim_0 : index
      %37 = arith.muli %36, %dim_1 : index
      %reinterpret_cast = memref.reinterpret_cast %arg1 to offset: [%c0], sizes: [%37], strides: [%c1] : memref<?x?x?xf32, "gpu"> to memref<?xf32, "gpu">
      %38 = memref.load %reinterpret_cast[%35] : memref<?xf32, "gpu">
      %39 = math.absf %38 : f32
      %reinterpret_cast_2 = memref.reinterpret_cast %arg6 to offset: [0], sizes: [128], strides: [1] : memref<128xf32, #gpu.address_space<workgroup>> to memref<128xf32, #gpu.address_space<workgroup>>
      memref.store %39, %reinterpret_cast_2[%7] : memref<128xf32, #gpu.address_space<workgroup>>
    } else {
      %reinterpret_cast = memref.reinterpret_cast %arg6 to offset: [0], sizes: [128], strides: [1] : memref<128xf32, #gpu.address_space<workgroup>> to memref<128xf32, #gpu.address_space<workgroup>>
      memref.store %cst, %reinterpret_cast[%7] : memref<128xf32, #gpu.address_space<workgroup>>
    }
    gpu.barrier
    %20 = arith.cmpi ult, %9, %c8 : index
    %21 = arith.addi %14, %c8 : index
    %22 = arith.cmpi ult, %21, %dim : index
    %23 = arith.andi %20, %22 : i1
    scf.if %23 {
      %34 = arith.addi %7, %c64 : index
      %reinterpret_cast = memref.reinterpret_cast %arg6 to offset: [0], sizes: [128], strides: [1] : memref<128xf32, #gpu.address_space<workgroup>> to memref<128xf32, #gpu.address_space<workgroup>>
      %35 = memref.load %reinterpret_cast[%7] : memref<128xf32, #gpu.address_space<workgroup>>
      %36 = memref.load %reinterpret_cast[%34] : memref<128xf32, #gpu.address_space<workgroup>>
      %37 = arith.cmpf ugt, %35, %36 : f32
      %38 = arith.select %37, %35, %36 : f32
      %39 = arith.cmpf uno, %36, %36 : f32
      %40 = arith.select %39, %36, %38 : f32
      memref.store %40, %reinterpret_cast[%7] : memref<128xf32, #gpu.address_space<workgroup>>
    }
    gpu.barrier
    %24 = arith.cmpi ult, %9, %c4 : index
    %25 = arith.addi %14, %c4 : index
    %26 = arith.cmpi ult, %25, %dim : index
    %27 = arith.andi %24, %26 : i1
    scf.if %27 {
      %34 = arith.addi %7, %c32 : index
      %reinterpret_cast = memref.reinterpret_cast %arg6 to offset: [0], sizes: [128], strides: [1] : memref<128xf32, #gpu.address_space<workgroup>> to memref<128xf32, #gpu.address_space<workgroup>>
      %35 = memref.load %reinterpret_cast[%7] : memref<128xf32, #gpu.address_space<workgroup>>
      %36 = memref.load %reinterpret_cast[%34] : memref<128xf32, #gpu.address_space<workgroup>>
      %37 = arith.cmpf ugt, %35, %36 : f32
      %38 = arith.select %37, %35, %36 : f32
      %39 = arith.cmpf uno, %36, %36 : f32
      %40 = arith.select %39, %36, %38 : f32
      memref.store %40, %reinterpret_cast[%7] : memref<128xf32, #gpu.address_space<workgroup>>
    }
    gpu.barrier
    %28 = arith.cmpi ult, %9, %c2 : index
    %29 = arith.addi %14, %c2 : index
    %30 = arith.cmpi ult, %29, %dim : index
    %31 = arith.andi %28, %30 : i1
    scf.if %31 {
      %34 = arith.addi %7, %c16 : index
      %reinterpret_cast = memref.reinterpret_cast %arg6 to offset: [0], sizes: [128], strides: [1] : memref<128xf32, #gpu.address_space<workgroup>> to memref<128xf32, #gpu.address_space<workgroup>>
      %35 = memref.load %reinterpret_cast[%7] : memref<128xf32, #gpu.address_space<workgroup>>
      %36 = memref.load %reinterpret_cast[%34] : memref<128xf32, #gpu.address_space<workgroup>>
      %37 = arith.cmpf ugt, %35, %36 : f32
      %38 = arith.select %37, %35, %36 : f32
      %39 = arith.cmpf uno, %36, %36 : f32
      %40 = arith.select %39, %36, %38 : f32
      memref.store %40, %reinterpret_cast[%7] : memref<128xf32, #gpu.address_space<workgroup>>
    }
    gpu.barrier
    %32 = arith.cmpi eq, %9, %c0 : index
    %33 = arith.andi %32, %19 : i1
    scf.if %33 {
      %34 = arith.addi %7, %c8 : index
      %reinterpret_cast = memref.reinterpret_cast %arg6 to offset: [0], sizes: [128], strides: [1] : memref<128xf32, #gpu.address_space<workgroup>> to memref<128xf32, #gpu.address_space<workgroup>>
      %35 = memref.load %reinterpret_cast[%7] : memref<128xf32, #gpu.address_space<workgroup>>
      %36 = memref.load %reinterpret_cast[%34] : memref<128xf32, #gpu.address_space<workgroup>>
      %37 = arith.cmpf ugt, %35, %36 : f32
      %38 = arith.select %37, %35, %36 : f32
      %39 = arith.cmpf uno, %36, %36 : f32
      %40 = arith.select %39, %36, %38 : f32
      %41 = memref.generic_atomic_rmw %arg5[%16] : memref<?xf32, "gpu"> {
      ^bb0(%arg7: f32):
        %42 = arith.cmpf ogt, %arg7, %40 : f32
        %43 = arith.select %42, %arg7, %40 : f32
        memref.atomic_yield %43 : f32
      }
    }
  }
  gpu.return
}

// -----// IR Dump After CSE (cse) //----- //
gpu.module @main_kernel_2 {
  gpu.func @main_kColReduction_reduce__4_1_0___8w16h_1(%arg0: index, %arg1: memref<?x?x?xf32, "gpu">, %arg2: index, %arg3: index, %arg4: index, %arg5: memref<?xf32, "gpu">) workgroup(%arg6 : memref<128xf32, #gpu.address_space<workgroup>>) kernel {
    %c32 = arith.constant 32 : index
    %c4 = arith.constant 4 : index
    %c64 = arith.constant 64 : index
    %cst = arith.constant 0xFF800000 : f32
    %c2 = arith.constant 2 : index
    %c16 = arith.constant 16 : index
    %c8 = arith.constant 8 : index
    %c128 = arith.constant 128 : index
    %c1 = arith.constant 1 : index
    %c0 = arith.constant 0 : index
    %0 = gpu.block_id  x
    %1 = gpu.thread_id  x
    cf.br ^bb1
  ^bb1:  // pred: ^bb0
    %dim = memref.dim %arg1, %c0 : memref<?x?x?xf32, "gpu">
    %dim_0 = memref.dim %arg1, %c1 : memref<?x?x?xf32, "gpu">
    %dim_1 = memref.dim %arg1, %c2 : memref<?x?x?xf32, "gpu">
    %2 = affine.apply affine_map<(d0)[s0, s1] -> (d0 * s0 + s1)>(%0)[%arg2, %c0]
    %3 = affine.apply affine_map<(d0)[s0, s1] -> (d0 * s0 + s1)>(%1)[%c1, %c0]
    %4 = arith.addi %3, %2 : index
    %5 = arith.cmpi ult, %4, %arg3 : index
    scf.if %5 {
      %6 = arith.remsi %4, %c128 : index
      %7 = arith.divsi %4, %c128 : index
      %8 = arith.divui %6, %c8 : index
      %9 = arith.remui %6, %c8 : index
      %10 = arith.divui %7, %arg4 : index
      %11 = arith.remui %7, %arg4 : index
      %12 = arith.muli %10, %c16 : index
      %13 = arith.addi %12, %8 : index
      %14 = arith.muli %11, %c8 : index
      %15 = arith.addi %14, %9 : index
      %16 = arith.cmpi ult, %13, %dim : index
      %17 = arith.cmpi ult, %15, %arg0 : index
      %18 = arith.andi %16, %17 : i1
      scf.if %18 {
        %33 = arith.muli %13, %arg0 : index
        %34 = arith.addi %33, %15 : index
        %35 = arith.muli %dim, %dim_0 : index
        %36 = arith.muli %35, %dim_1 : index
        %reinterpret_cast = memref.reinterpret_cast %arg1 to offset: [%c0], sizes: [%36], strides: [%c1] : memref<?x?x?xf32, "gpu"> to memref<?xf32, "gpu">
        %37 = memref.load %reinterpret_cast[%34] : memref<?xf32, "gpu">
        %38 = math.absf %37 : f32
        %reinterpret_cast_2 = memref.reinterpret_cast %arg6 to offset: [0], sizes: [128], strides: [1] : memref<128xf32, #gpu.address_space<workgroup>> to memref<128xf32, #gpu.address_space<workgroup>>
        memref.store %38, %reinterpret_cast_2[%6] : memref<128xf32, #gpu.address_space<workgroup>>
      } else {
        %reinterpret_cast = memref.reinterpret_cast %arg6 to offset: [0], sizes: [128], strides: [1] : memref<128xf32, #gpu.address_space<workgroup>> to memref<128xf32, #gpu.address_space<workgroup>>
        memref.store %cst, %reinterpret_cast[%6] : memref<128xf32, #gpu.address_space<workgroup>>
      }
      gpu.barrier
      %19 = arith.cmpi ult, %8, %c8 : index
      %20 = arith.addi %13, %c8 : index
      %21 = arith.cmpi ult, %20, %dim : index
      %22 = arith.andi %19, %21 : i1
      scf.if %22 {
        %33 = arith.addi %6, %c64 : index
        %reinterpret_cast = memref.reinterpret_cast %arg6 to offset: [0], sizes: [128], strides: [1] : memref<128xf32, #gpu.address_space<workgroup>> to memref<128xf32, #gpu.address_space<workgroup>>
        %34 = memref.load %reinterpret_cast[%6] : memref<128xf32, #gpu.address_space<workgroup>>
        %35 = memref.load %reinterpret_cast[%33] : memref<128xf32, #gpu.address_space<workgroup>>
        %36 = arith.cmpf ugt, %34, %35 : f32
        %37 = arith.select %36, %34, %35 : f32
        %38 = arith.cmpf uno, %35, %35 : f32
        %39 = arith.select %38, %35, %37 : f32
        memref.store %39, %reinterpret_cast[%6] : memref<128xf32, #gpu.address_space<workgroup>>
      }
      gpu.barrier
      %23 = arith.cmpi ult, %8, %c4 : index
      %24 = arith.addi %13, %c4 : index
      %25 = arith.cmpi ult, %24, %dim : index
      %26 = arith.andi %23, %25 : i1
      scf.if %26 {
        %33 = arith.addi %6, %c32 : index
        %reinterpret_cast = memref.reinterpret_cast %arg6 to offset: [0], sizes: [128], strides: [1] : memref<128xf32, #gpu.address_space<workgroup>> to memref<128xf32, #gpu.address_space<workgroup>>
        %34 = memref.load %reinterpret_cast[%6] : memref<128xf32, #gpu.address_space<workgroup>>
        %35 = memref.load %reinterpret_cast[%33] : memref<128xf32, #gpu.address_space<workgroup>>
        %36 = arith.cmpf ugt, %34, %35 : f32
        %37 = arith.select %36, %34, %35 : f32
        %38 = arith.cmpf uno, %35, %35 : f32
        %39 = arith.select %38, %35, %37 : f32
        memref.store %39, %reinterpret_cast[%6] : memref<128xf32, #gpu.address_space<workgroup>>
      }
      gpu.barrier
      %27 = arith.cmpi ult, %8, %c2 : index
      %28 = arith.addi %13, %c2 : index
      %29 = arith.cmpi ult, %28, %dim : index
      %30 = arith.andi %27, %29 : i1
      scf.if %30 {
        %33 = arith.addi %6, %c16 : index
        %reinterpret_cast = memref.reinterpret_cast %arg6 to offset: [0], sizes: [128], strides: [1] : memref<128xf32, #gpu.address_space<workgroup>> to memref<128xf32, #gpu.address_space<workgroup>>
        %34 = memref.load %reinterpret_cast[%6] : memref<128xf32, #gpu.address_space<workgroup>>
        %35 = memref.load %reinterpret_cast[%33] : memref<128xf32, #gpu.address_space<workgroup>>
        %36 = arith.cmpf ugt, %34, %35 : f32
        %37 = arith.select %36, %34, %35 : f32
        %38 = arith.cmpf uno, %35, %35 : f32
        %39 = arith.select %38, %35, %37 : f32
        memref.store %39, %reinterpret_cast[%6] : memref<128xf32, #gpu.address_space<workgroup>>
      }
      gpu.barrier
      %31 = arith.cmpi eq, %8, %c0 : index
      %32 = arith.andi %31, %18 : i1
      scf.if %32 {
        %33 = arith.addi %6, %c8 : index
        %reinterpret_cast = memref.reinterpret_cast %arg6 to offset: [0], sizes: [128], strides: [1] : memref<128xf32, #gpu.address_space<workgroup>> to memref<128xf32, #gpu.address_space<workgroup>>
        %34 = memref.load %reinterpret_cast[%6] : memref<128xf32, #gpu.address_space<workgroup>>
        %35 = memref.load %reinterpret_cast[%33] : memref<128xf32, #gpu.address_space<workgroup>>
        %36 = arith.cmpf ugt, %34, %35 : f32
        %37 = arith.select %36, %34, %35 : f32
        %38 = arith.cmpf uno, %35, %35 : f32
        %39 = arith.select %38, %35, %37 : f32
        %40 = memref.generic_atomic_rmw %arg5[%15] : memref<?xf32, "gpu"> {
        ^bb0(%arg7: f32):
          %41 = arith.cmpf ogt, %arg7, %39 : f32
          %42 = arith.select %41, %arg7, %39 : f32
          memref.atomic_yield %42 : f32
        }
      }
    }
    gpu.return
  }
}

// -----// IR Dump After SCFToControlFlow (convert-scf-to-cf) //----- //
gpu.module @main_kernel_2 {
  gpu.func @main_kColReduction_reduce__4_1_0___8w16h_1(%arg0: index, %arg1: memref<?x?x?xf32, "gpu">, %arg2: index, %arg3: index, %arg4: index, %arg5: memref<?xf32, "gpu">) workgroup(%arg6 : memref<128xf32, #gpu.address_space<workgroup>>) kernel {
    %c32 = arith.constant 32 : index
    %c4 = arith.constant 4 : index
    %c64 = arith.constant 64 : index
    %cst = arith.constant 0xFF800000 : f32
    %c2 = arith.constant 2 : index
    %c16 = arith.constant 16 : index
    %c8 = arith.constant 8 : index
    %c128 = arith.constant 128 : index
    %c1 = arith.constant 1 : index
    %c0 = arith.constant 0 : index
    %0 = gpu.block_id  x
    %1 = gpu.thread_id  x
    cf.br ^bb1
  ^bb1:  // pred: ^bb0
    %dim = memref.dim %arg1, %c0 : memref<?x?x?xf32, "gpu">
    %dim_0 = memref.dim %arg1, %c1 : memref<?x?x?xf32, "gpu">
    %dim_1 = memref.dim %arg1, %c2 : memref<?x?x?xf32, "gpu">
    %2 = affine.apply affine_map<(d0)[s0, s1] -> (d0 * s0 + s1)>(%0)[%arg2, %c0]
    %3 = affine.apply affine_map<(d0)[s0, s1] -> (d0 * s0 + s1)>(%1)[%c1, %c0]
    %4 = arith.addi %3, %2 : index
    %5 = arith.cmpi ult, %4, %arg3 : index
    cf.cond_br %5, ^bb2, ^bb14
  ^bb2:  // pred: ^bb1
    %6 = arith.remsi %4, %c128 : index
    %7 = arith.divsi %4, %c128 : index
    %8 = arith.divui %6, %c8 : index
    %9 = arith.remui %6, %c8 : index
    %10 = arith.divui %7, %arg4 : index
    %11 = arith.remui %7, %arg4 : index
    %12 = arith.muli %10, %c16 : index
    %13 = arith.addi %12, %8 : index
    %14 = arith.muli %11, %c8 : index
    %15 = arith.addi %14, %9 : index
    %16 = arith.cmpi ult, %13, %dim : index
    %17 = arith.cmpi ult, %15, %arg0 : index
    %18 = arith.andi %16, %17 : i1
    cf.cond_br %18, ^bb3, ^bb4
  ^bb3:  // pred: ^bb2
    %19 = arith.muli %13, %arg0 : index
    %20 = arith.addi %19, %15 : index
    %21 = arith.muli %dim, %dim_0 : index
    %22 = arith.muli %21, %dim_1 : index
    %reinterpret_cast = memref.reinterpret_cast %arg1 to offset: [%c0], sizes: [%22], strides: [%c1] : memref<?x?x?xf32, "gpu"> to memref<?xf32, "gpu">
    %23 = memref.load %reinterpret_cast[%20] : memref<?xf32, "gpu">
    %24 = math.absf %23 : f32
    %reinterpret_cast_2 = memref.reinterpret_cast %arg6 to offset: [0], sizes: [128], strides: [1] : memref<128xf32, #gpu.address_space<workgroup>> to memref<128xf32, #gpu.address_space<workgroup>>
    memref.store %24, %reinterpret_cast_2[%6] : memref<128xf32, #gpu.address_space<workgroup>>
    cf.br ^bb5
  ^bb4:  // pred: ^bb2
    %reinterpret_cast_3 = memref.reinterpret_cast %arg6 to offset: [0], sizes: [128], strides: [1] : memref<128xf32, #gpu.address_space<workgroup>> to memref<128xf32, #gpu.address_space<workgroup>>
    memref.store %cst, %reinterpret_cast_3[%6] : memref<128xf32, #gpu.address_space<workgroup>>
    cf.br ^bb5
  ^bb5:  // 2 preds: ^bb3, ^bb4
    gpu.barrier
    %25 = arith.cmpi ult, %8, %c8 : index
    %26 = arith.addi %13, %c8 : index
    %27 = arith.cmpi ult, %26, %dim : index
    %28 = arith.andi %25, %27 : i1
    cf.cond_br %28, ^bb6, ^bb7
  ^bb6:  // pred: ^bb5
    %29 = arith.addi %6, %c64 : index
    %reinterpret_cast_4 = memref.reinterpret_cast %arg6 to offset: [0], sizes: [128], strides: [1] : memref<128xf32, #gpu.address_space<workgroup>> to memref<128xf32, #gpu.address_space<workgroup>>
    %30 = memref.load %reinterpret_cast_4[%6] : memref<128xf32, #gpu.address_space<workgroup>>
    %31 = memref.load %reinterpret_cast_4[%29] : memref<128xf32, #gpu.address_space<workgroup>>
    %32 = arith.cmpf ugt, %30, %31 : f32
    %33 = arith.select %32, %30, %31 : f32
    %34 = arith.cmpf uno, %31, %31 : f32
    %35 = arith.select %34, %31, %33 : f32
    memref.store %35, %reinterpret_cast_4[%6] : memref<128xf32, #gpu.address_space<workgroup>>
    cf.br ^bb7
  ^bb7:  // 2 preds: ^bb5, ^bb6
    gpu.barrier
    %36 = arith.cmpi ult, %8, %c4 : index
    %37 = arith.addi %13, %c4 : index
    %38 = arith.cmpi ult, %37, %dim : index
    %39 = arith.andi %36, %38 : i1
    cf.cond_br %39, ^bb8, ^bb9
  ^bb8:  // pred: ^bb7
    %40 = arith.addi %6, %c32 : index
    %reinterpret_cast_5 = memref.reinterpret_cast %arg6 to offset: [0], sizes: [128], strides: [1] : memref<128xf32, #gpu.address_space<workgroup>> to memref<128xf32, #gpu.address_space<workgroup>>
    %41 = memref.load %reinterpret_cast_5[%6] : memref<128xf32, #gpu.address_space<workgroup>>
    %42 = memref.load %reinterpret_cast_5[%40] : memref<128xf32, #gpu.address_space<workgroup>>
    %43 = arith.cmpf ugt, %41, %42 : f32
    %44 = arith.select %43, %41, %42 : f32
    %45 = arith.cmpf uno, %42, %42 : f32
    %46 = arith.select %45, %42, %44 : f32
    memref.store %46, %reinterpret_cast_5[%6] : memref<128xf32, #gpu.address_space<workgroup>>
    cf.br ^bb9
  ^bb9:  // 2 preds: ^bb7, ^bb8
    gpu.barrier
    %47 = arith.cmpi ult, %8, %c2 : index
    %48 = arith.addi %13, %c2 : index
    %49 = arith.cmpi ult, %48, %dim : index
    %50 = arith.andi %47, %49 : i1
    cf.cond_br %50, ^bb10, ^bb11
  ^bb10:  // pred: ^bb9
    %51 = arith.addi %6, %c16 : index
    %reinterpret_cast_6 = memref.reinterpret_cast %arg6 to offset: [0], sizes: [128], strides: [1] : memref<128xf32, #gpu.address_space<workgroup>> to memref<128xf32, #gpu.address_space<workgroup>>
    %52 = memref.load %reinterpret_cast_6[%6] : memref<128xf32, #gpu.address_space<workgroup>>
    %53 = memref.load %reinterpret_cast_6[%51] : memref<128xf32, #gpu.address_space<workgroup>>
    %54 = arith.cmpf ugt, %52, %53 : f32
    %55 = arith.select %54, %52, %53 : f32
    %56 = arith.cmpf uno, %53, %53 : f32
    %57 = arith.select %56, %53, %55 : f32
    memref.store %57, %reinterpret_cast_6[%6] : memref<128xf32, #gpu.address_space<workgroup>>
    cf.br ^bb11
  ^bb11:  // 2 preds: ^bb9, ^bb10
    gpu.barrier
    %58 = arith.cmpi eq, %8, %c0 : index
    %59 = arith.andi %58, %18 : i1
    cf.cond_br %59, ^bb12, ^bb13
  ^bb12:  // pred: ^bb11
    %60 = arith.addi %6, %c8 : index
    %reinterpret_cast_7 = memref.reinterpret_cast %arg6 to offset: [0], sizes: [128], strides: [1] : memref<128xf32, #gpu.address_space<workgroup>> to memref<128xf32, #gpu.address_space<workgroup>>
    %61 = memref.load %reinterpret_cast_7[%6] : memref<128xf32, #gpu.address_space<workgroup>>
    %62 = memref.load %reinterpret_cast_7[%60] : memref<128xf32, #gpu.address_space<workgroup>>
    %63 = arith.cmpf ugt, %61, %62 : f32
    %64 = arith.select %63, %61, %62 : f32
    %65 = arith.cmpf uno, %62, %62 : f32
    %66 = arith.select %65, %62, %64 : f32
    %67 = memref.generic_atomic_rmw %arg5[%15] : memref<?xf32, "gpu"> {
    ^bb0(%arg7: f32):
      %68 = arith.cmpf ogt, %arg7, %66 : f32
      %69 = arith.select %68, %arg7, %66 : f32
      memref.atomic_yield %69 : f32
    }
    cf.br ^bb13
  ^bb13:  // 2 preds: ^bb11, ^bb12
    cf.br ^bb14
  ^bb14:  // 2 preds: ^bb1, ^bb13
    gpu.return
  }
}

// -----// IR Dump After ConvertAffineToStandard (lower-affine) //----- //
gpu.module @main_kernel_2 {
  gpu.func @main_kColReduction_reduce__4_1_0___8w16h_1(%arg0: index, %arg1: memref<?x?x?xf32, "gpu">, %arg2: index, %arg3: index, %arg4: index, %arg5: memref<?xf32, "gpu">) workgroup(%arg6 : memref<128xf32, #gpu.address_space<workgroup>>) kernel {
    %c32 = arith.constant 32 : index
    %c4 = arith.constant 4 : index
    %c64 = arith.constant 64 : index
    %cst = arith.constant 0xFF800000 : f32
    %c2 = arith.constant 2 : index
    %c16 = arith.constant 16 : index
    %c8 = arith.constant 8 : index
    %c128 = arith.constant 128 : index
    %c1 = arith.constant 1 : index
    %c0 = arith.constant 0 : index
    %0 = gpu.block_id  x
    %1 = gpu.thread_id  x
    cf.br ^bb1
  ^bb1:  // pred: ^bb0
    %dim = memref.dim %arg1, %c0 : memref<?x?x?xf32, "gpu">
    %dim_0 = memref.dim %arg1, %c1 : memref<?x?x?xf32, "gpu">
    %dim_1 = memref.dim %arg1, %c2 : memref<?x?x?xf32, "gpu">
    %2 = arith.muli %0, %arg2 : index
    %3 = arith.addi %2, %c0 : index
    %4 = arith.muli %1, %c1 : index
    %5 = arith.addi %4, %c0 : index
    %6 = arith.addi %5, %3 : index
    %7 = arith.cmpi ult, %6, %arg3 : index
    cf.cond_br %7, ^bb2, ^bb14
  ^bb2:  // pred: ^bb1
    %8 = arith.remsi %6, %c128 : index
    %9 = arith.divsi %6, %c128 : index
    %10 = arith.divui %8, %c8 : index
    %11 = arith.remui %8, %c8 : index
    %12 = arith.divui %9, %arg4 : index
    %13 = arith.remui %9, %arg4 : index
    %14 = arith.muli %12, %c16 : index
    %15 = arith.addi %14, %10 : index
    %16 = arith.muli %13, %c8 : index
    %17 = arith.addi %16, %11 : index
    %18 = arith.cmpi ult, %15, %dim : index
    %19 = arith.cmpi ult, %17, %arg0 : index
    %20 = arith.andi %18, %19 : i1
    cf.cond_br %20, ^bb3, ^bb4
  ^bb3:  // pred: ^bb2
    %21 = arith.muli %15, %arg0 : index
    %22 = arith.addi %21, %17 : index
    %23 = arith.muli %dim, %dim_0 : index
    %24 = arith.muli %23, %dim_1 : index
    %reinterpret_cast = memref.reinterpret_cast %arg1 to offset: [%c0], sizes: [%24], strides: [%c1] : memref<?x?x?xf32, "gpu"> to memref<?xf32, "gpu">
    %25 = memref.load %reinterpret_cast[%22] : memref<?xf32, "gpu">
    %26 = math.absf %25 : f32
    %reinterpret_cast_2 = memref.reinterpret_cast %arg6 to offset: [0], sizes: [128], strides: [1] : memref<128xf32, #gpu.address_space<workgroup>> to memref<128xf32, #gpu.address_space<workgroup>>
    memref.store %26, %reinterpret_cast_2[%8] : memref<128xf32, #gpu.address_space<workgroup>>
    cf.br ^bb5
  ^bb4:  // pred: ^bb2
    %reinterpret_cast_3 = memref.reinterpret_cast %arg6 to offset: [0], sizes: [128], strides: [1] : memref<128xf32, #gpu.address_space<workgroup>> to memref<128xf32, #gpu.address_space<workgroup>>
    memref.store %cst, %reinterpret_cast_3[%8] : memref<128xf32, #gpu.address_space<workgroup>>
    cf.br ^bb5
  ^bb5:  // 2 preds: ^bb3, ^bb4
    gpu.barrier
    %27 = arith.cmpi ult, %10, %c8 : index
    %28 = arith.addi %15, %c8 : index
    %29 = arith.cmpi ult, %28, %dim : index
    %30 = arith.andi %27, %29 : i1
    cf.cond_br %30, ^bb6, ^bb7
  ^bb6:  // pred: ^bb5
    %31 = arith.addi %8, %c64 : index
    %reinterpret_cast_4 = memref.reinterpret_cast %arg6 to offset: [0], sizes: [128], strides: [1] : memref<128xf32, #gpu.address_space<workgroup>> to memref<128xf32, #gpu.address_space<workgroup>>
    %32 = memref.load %reinterpret_cast_4[%8] : memref<128xf32, #gpu.address_space<workgroup>>
    %33 = memref.load %reinterpret_cast_4[%31] : memref<128xf32, #gpu.address_space<workgroup>>
    %34 = arith.cmpf ugt, %32, %33 : f32
    %35 = arith.select %34, %32, %33 : f32
    %36 = arith.cmpf uno, %33, %33 : f32
    %37 = arith.select %36, %33, %35 : f32
    memref.store %37, %reinterpret_cast_4[%8] : memref<128xf32, #gpu.address_space<workgroup>>
    cf.br ^bb7
  ^bb7:  // 2 preds: ^bb5, ^bb6
    gpu.barrier
    %38 = arith.cmpi ult, %10, %c4 : index
    %39 = arith.addi %15, %c4 : index
    %40 = arith.cmpi ult, %39, %dim : index
    %41 = arith.andi %38, %40 : i1
    cf.cond_br %41, ^bb8, ^bb9
  ^bb8:  // pred: ^bb7
    %42 = arith.addi %8, %c32 : index
    %reinterpret_cast_5 = memref.reinterpret_cast %arg6 to offset: [0], sizes: [128], strides: [1] : memref<128xf32, #gpu.address_space<workgroup>> to memref<128xf32, #gpu.address_space<workgroup>>
    %43 = memref.load %reinterpret_cast_5[%8] : memref<128xf32, #gpu.address_space<workgroup>>
    %44 = memref.load %reinterpret_cast_5[%42] : memref<128xf32, #gpu.address_space<workgroup>>
    %45 = arith.cmpf ugt, %43, %44 : f32
    %46 = arith.select %45, %43, %44 : f32
    %47 = arith.cmpf uno, %44, %44 : f32
    %48 = arith.select %47, %44, %46 : f32
    memref.store %48, %reinterpret_cast_5[%8] : memref<128xf32, #gpu.address_space<workgroup>>
    cf.br ^bb9
  ^bb9:  // 2 preds: ^bb7, ^bb8
    gpu.barrier
    %49 = arith.cmpi ult, %10, %c2 : index
    %50 = arith.addi %15, %c2 : index
    %51 = arith.cmpi ult, %50, %dim : index
    %52 = arith.andi %49, %51 : i1
    cf.cond_br %52, ^bb10, ^bb11
  ^bb10:  // pred: ^bb9
    %53 = arith.addi %8, %c16 : index
    %reinterpret_cast_6 = memref.reinterpret_cast %arg6 to offset: [0], sizes: [128], strides: [1] : memref<128xf32, #gpu.address_space<workgroup>> to memref<128xf32, #gpu.address_space<workgroup>>
    %54 = memref.load %reinterpret_cast_6[%8] : memref<128xf32, #gpu.address_space<workgroup>>
    %55 = memref.load %reinterpret_cast_6[%53] : memref<128xf32, #gpu.address_space<workgroup>>
    %56 = arith.cmpf ugt, %54, %55 : f32
    %57 = arith.select %56, %54, %55 : f32
    %58 = arith.cmpf uno, %55, %55 : f32
    %59 = arith.select %58, %55, %57 : f32
    memref.store %59, %reinterpret_cast_6[%8] : memref<128xf32, #gpu.address_space<workgroup>>
    cf.br ^bb11
  ^bb11:  // 2 preds: ^bb9, ^bb10
    gpu.barrier
    %60 = arith.cmpi eq, %10, %c0 : index
    %61 = arith.andi %60, %20 : i1
    cf.cond_br %61, ^bb12, ^bb13
  ^bb12:  // pred: ^bb11
    %62 = arith.addi %8, %c8 : index
    %reinterpret_cast_7 = memref.reinterpret_cast %arg6 to offset: [0], sizes: [128], strides: [1] : memref<128xf32, #gpu.address_space<workgroup>> to memref<128xf32, #gpu.address_space<workgroup>>
    %63 = memref.load %reinterpret_cast_7[%8] : memref<128xf32, #gpu.address_space<workgroup>>
    %64 = memref.load %reinterpret_cast_7[%62] : memref<128xf32, #gpu.address_space<workgroup>>
    %65 = arith.cmpf ugt, %63, %64 : f32
    %66 = arith.select %65, %63, %64 : f32
    %67 = arith.cmpf uno, %64, %64 : f32
    %68 = arith.select %67, %64, %66 : f32
    %69 = memref.generic_atomic_rmw %arg5[%17] : memref<?xf32, "gpu"> {
    ^bb0(%arg7: f32):
      %70 = arith.cmpf ogt, %arg7, %68 : f32
      %71 = arith.select %70, %arg7, %68 : f32
      memref.atomic_yield %71 : f32
    }
    cf.br ^bb13
  ^bb13:  // 2 preds: ^bb11, ^bb12
    cf.br ^bb14
  ^bb14:  // 2 preds: ^bb1, ^bb13
    gpu.return
  }
}

// -----// IR Dump After StripDebugInfo (strip-debuginfo) //----- //
gpu.module @main_kernel_2 {
  gpu.func @main_kColReduction_reduce__4_1_0___8w16h_1(%arg0: index, %arg1: memref<?x?x?xf32, "gpu">, %arg2: index, %arg3: index, %arg4: index, %arg5: memref<?xf32, "gpu">) workgroup(%arg6 : memref<128xf32, #gpu.address_space<workgroup>>) kernel {
    %c32 = arith.constant 32 : index
    %c4 = arith.constant 4 : index
    %c64 = arith.constant 64 : index
    %cst = arith.constant 0xFF800000 : f32
    %c2 = arith.constant 2 : index
    %c16 = arith.constant 16 : index
    %c8 = arith.constant 8 : index
    %c128 = arith.constant 128 : index
    %c1 = arith.constant 1 : index
    %c0 = arith.constant 0 : index
    %0 = gpu.block_id  x
    %1 = gpu.thread_id  x
    cf.br ^bb1
  ^bb1:  // pred: ^bb0
    %dim = memref.dim %arg1, %c0 : memref<?x?x?xf32, "gpu">
    %dim_0 = memref.dim %arg1, %c1 : memref<?x?x?xf32, "gpu">
    %dim_1 = memref.dim %arg1, %c2 : memref<?x?x?xf32, "gpu">
    %2 = arith.muli %0, %arg2 : index
    %3 = arith.addi %2, %c0 : index
    %4 = arith.muli %1, %c1 : index
    %5 = arith.addi %4, %c0 : index
    %6 = arith.addi %5, %3 : index
    %7 = arith.cmpi ult, %6, %arg3 : index
    cf.cond_br %7, ^bb2, ^bb14
  ^bb2:  // pred: ^bb1
    %8 = arith.remsi %6, %c128 : index
    %9 = arith.divsi %6, %c128 : index
    %10 = arith.divui %8, %c8 : index
    %11 = arith.remui %8, %c8 : index
    %12 = arith.divui %9, %arg4 : index
    %13 = arith.remui %9, %arg4 : index
    %14 = arith.muli %12, %c16 : index
    %15 = arith.addi %14, %10 : index
    %16 = arith.muli %13, %c8 : index
    %17 = arith.addi %16, %11 : index
    %18 = arith.cmpi ult, %15, %dim : index
    %19 = arith.cmpi ult, %17, %arg0 : index
    %20 = arith.andi %18, %19 : i1
    cf.cond_br %20, ^bb3, ^bb4
  ^bb3:  // pred: ^bb2
    %21 = arith.muli %15, %arg0 : index
    %22 = arith.addi %21, %17 : index
    %23 = arith.muli %dim, %dim_0 : index
    %24 = arith.muli %23, %dim_1 : index
    %reinterpret_cast = memref.reinterpret_cast %arg1 to offset: [%c0], sizes: [%24], strides: [%c1] : memref<?x?x?xf32, "gpu"> to memref<?xf32, "gpu">
    %25 = memref.load %reinterpret_cast[%22] : memref<?xf32, "gpu">
    %26 = math.absf %25 : f32
    %reinterpret_cast_2 = memref.reinterpret_cast %arg6 to offset: [0], sizes: [128], strides: [1] : memref<128xf32, #gpu.address_space<workgroup>> to memref<128xf32, #gpu.address_space<workgroup>>
    memref.store %26, %reinterpret_cast_2[%8] : memref<128xf32, #gpu.address_space<workgroup>>
    cf.br ^bb5
  ^bb4:  // pred: ^bb2
    %reinterpret_cast_3 = memref.reinterpret_cast %arg6 to offset: [0], sizes: [128], strides: [1] : memref<128xf32, #gpu.address_space<workgroup>> to memref<128xf32, #gpu.address_space<workgroup>>
    memref.store %cst, %reinterpret_cast_3[%8] : memref<128xf32, #gpu.address_space<workgroup>>
    cf.br ^bb5
  ^bb5:  // 2 preds: ^bb3, ^bb4
    gpu.barrier
    %27 = arith.cmpi ult, %10, %c8 : index
    %28 = arith.addi %15, %c8 : index
    %29 = arith.cmpi ult, %28, %dim : index
    %30 = arith.andi %27, %29 : i1
    cf.cond_br %30, ^bb6, ^bb7
  ^bb6:  // pred: ^bb5
    %31 = arith.addi %8, %c64 : index
    %reinterpret_cast_4 = memref.reinterpret_cast %arg6 to offset: [0], sizes: [128], strides: [1] : memref<128xf32, #gpu.address_space<workgroup>> to memref<128xf32, #gpu.address_space<workgroup>>
    %32 = memref.load %reinterpret_cast_4[%8] : memref<128xf32, #gpu.address_space<workgroup>>
    %33 = memref.load %reinterpret_cast_4[%31] : memref<128xf32, #gpu.address_space<workgroup>>
    %34 = arith.cmpf ugt, %32, %33 : f32
    %35 = arith.select %34, %32, %33 : f32
    %36 = arith.cmpf uno, %33, %33 : f32
    %37 = arith.select %36, %33, %35 : f32
    memref.store %37, %reinterpret_cast_4[%8] : memref<128xf32, #gpu.address_space<workgroup>>
    cf.br ^bb7
  ^bb7:  // 2 preds: ^bb5, ^bb6
    gpu.barrier
    %38 = arith.cmpi ult, %10, %c4 : index
    %39 = arith.addi %15, %c4 : index
    %40 = arith.cmpi ult, %39, %dim : index
    %41 = arith.andi %38, %40 : i1
    cf.cond_br %41, ^bb8, ^bb9
  ^bb8:  // pred: ^bb7
    %42 = arith.addi %8, %c32 : index
    %reinterpret_cast_5 = memref.reinterpret_cast %arg6 to offset: [0], sizes: [128], strides: [1] : memref<128xf32, #gpu.address_space<workgroup>> to memref<128xf32, #gpu.address_space<workgroup>>
    %43 = memref.load %reinterpret_cast_5[%8] : memref<128xf32, #gpu.address_space<workgroup>>
    %44 = memref.load %reinterpret_cast_5[%42] : memref<128xf32, #gpu.address_space<workgroup>>
    %45 = arith.cmpf ugt, %43, %44 : f32
    %46 = arith.select %45, %43, %44 : f32
    %47 = arith.cmpf uno, %44, %44 : f32
    %48 = arith.select %47, %44, %46 : f32
    memref.store %48, %reinterpret_cast_5[%8] : memref<128xf32, #gpu.address_space<workgroup>>
    cf.br ^bb9
  ^bb9:  // 2 preds: ^bb7, ^bb8
    gpu.barrier
    %49 = arith.cmpi ult, %10, %c2 : index
    %50 = arith.addi %15, %c2 : index
    %51 = arith.cmpi ult, %50, %dim : index
    %52 = arith.andi %49, %51 : i1
    cf.cond_br %52, ^bb10, ^bb11
  ^bb10:  // pred: ^bb9
    %53 = arith.addi %8, %c16 : index
    %reinterpret_cast_6 = memref.reinterpret_cast %arg6 to offset: [0], sizes: [128], strides: [1] : memref<128xf32, #gpu.address_space<workgroup>> to memref<128xf32, #gpu.address_space<workgroup>>
    %54 = memref.load %reinterpret_cast_6[%8] : memref<128xf32, #gpu.address_space<workgroup>>
    %55 = memref.load %reinterpret_cast_6[%53] : memref<128xf32, #gpu.address_space<workgroup>>
    %56 = arith.cmpf ugt, %54, %55 : f32
    %57 = arith.select %56, %54, %55 : f32
    %58 = arith.cmpf uno, %55, %55 : f32
    %59 = arith.select %58, %55, %57 : f32
    memref.store %59, %reinterpret_cast_6[%8] : memref<128xf32, #gpu.address_space<workgroup>>
    cf.br ^bb11
  ^bb11:  // 2 preds: ^bb9, ^bb10
    gpu.barrier
    %60 = arith.cmpi eq, %10, %c0 : index
    %61 = arith.andi %60, %20 : i1
    cf.cond_br %61, ^bb12, ^bb13
  ^bb12:  // pred: ^bb11
    %62 = arith.addi %8, %c8 : index
    %reinterpret_cast_7 = memref.reinterpret_cast %arg6 to offset: [0], sizes: [128], strides: [1] : memref<128xf32, #gpu.address_space<workgroup>> to memref<128xf32, #gpu.address_space<workgroup>>
    %63 = memref.load %reinterpret_cast_7[%8] : memref<128xf32, #gpu.address_space<workgroup>>
    %64 = memref.load %reinterpret_cast_7[%62] : memref<128xf32, #gpu.address_space<workgroup>>
    %65 = arith.cmpf ugt, %63, %64 : f32
    %66 = arith.select %65, %63, %64 : f32
    %67 = arith.cmpf uno, %64, %64 : f32
    %68 = arith.select %67, %64, %66 : f32
    %69 = memref.generic_atomic_rmw %arg5[%17] : memref<?xf32, "gpu"> {
    ^bb0(%arg7: f32):
      %70 = arith.cmpf ogt, %arg7, %68 : f32
      %71 = arith.select %70, %arg7, %68 : f32
      memref.atomic_yield %71 : f32
    }
    cf.br ^bb13
  ^bb13:  // 2 preds: ^bb11, ^bb12
    cf.br ^bb14
  ^bb14:  // 2 preds: ^bb1, ^bb13
    gpu.return
  }
}

// -----// IR Dump After DiscLowerGpuOpsToNVVMOpsPass (disc-convert-gpu-to-nvvm) //----- //
gpu.module @main_kernel_2 {
  llvm.mlir.global internal @__wg_main_kColReduction_reduce__4_1_0___8w16h_1_0() {addr_space = 3 : i32} : !llvm.array<128 x f32>
  llvm.func @__nv_fabsf(f32) -> f32
  llvm.func @main_kColReduction_reduce__4_1_0___8w16h_1(%arg0: i32, %arg1: !llvm.ptr<f32>, %arg2: !llvm.ptr<f32>, %arg3: i32, %arg4: i32, %arg5: i32, %arg6: i32, %arg7: i32, %arg8: i32, %arg9: i32, %arg10: i32, %arg11: i32, %arg12: i32, %arg13: !llvm.ptr<f32>, %arg14: !llvm.ptr<f32>, %arg15: i32, %arg16: i32, %arg17: i32) attributes {gpu.kernel, nvvm.kernel} {
    %0 = llvm.mlir.undef : !llvm.struct<(ptr<f32>, ptr<f32>, i32, array<3 x i32>, array<3 x i32>)>
    %1 = llvm.insertvalue %arg1, %0[0] : !llvm.struct<(ptr<f32>, ptr<f32>, i32, array<3 x i32>, array<3 x i32>)> 
    %2 = llvm.insertvalue %arg2, %1[1] : !llvm.struct<(ptr<f32>, ptr<f32>, i32, array<3 x i32>, array<3 x i32>)> 
    %3 = llvm.insertvalue %arg3, %2[2] : !llvm.struct<(ptr<f32>, ptr<f32>, i32, array<3 x i32>, array<3 x i32>)> 
    %4 = llvm.insertvalue %arg4, %3[3, 0] : !llvm.struct<(ptr<f32>, ptr<f32>, i32, array<3 x i32>, array<3 x i32>)> 
    %5 = llvm.insertvalue %arg7, %4[4, 0] : !llvm.struct<(ptr<f32>, ptr<f32>, i32, array<3 x i32>, array<3 x i32>)> 
    %6 = llvm.insertvalue %arg5, %5[3, 1] : !llvm.struct<(ptr<f32>, ptr<f32>, i32, array<3 x i32>, array<3 x i32>)> 
    %7 = llvm.insertvalue %arg8, %6[4, 1] : !llvm.struct<(ptr<f32>, ptr<f32>, i32, array<3 x i32>, array<3 x i32>)> 
    %8 = llvm.insertvalue %arg6, %7[3, 2] : !llvm.struct<(ptr<f32>, ptr<f32>, i32, array<3 x i32>, array<3 x i32>)> 
    %9 = llvm.insertvalue %arg9, %8[4, 2] : !llvm.struct<(ptr<f32>, ptr<f32>, i32, array<3 x i32>, array<3 x i32>)> 
    %10 = llvm.mlir.undef : !llvm.struct<(ptr<f32>, ptr<f32>, i32, array<1 x i32>, array<1 x i32>)>
    %11 = llvm.insertvalue %arg13, %10[0] : !llvm.struct<(ptr<f32>, ptr<f32>, i32, array<1 x i32>, array<1 x i32>)> 
    %12 = llvm.insertvalue %arg14, %11[1] : !llvm.struct<(ptr<f32>, ptr<f32>, i32, array<1 x i32>, array<1 x i32>)> 
    %13 = llvm.insertvalue %arg15, %12[2] : !llvm.struct<(ptr<f32>, ptr<f32>, i32, array<1 x i32>, array<1 x i32>)> 
    %14 = llvm.insertvalue %arg16, %13[3, 0] : !llvm.struct<(ptr<f32>, ptr<f32>, i32, array<1 x i32>, array<1 x i32>)> 
    %15 = llvm.insertvalue %arg17, %14[4, 0] : !llvm.struct<(ptr<f32>, ptr<f32>, i32, array<1 x i32>, array<1 x i32>)> 
    %16 = llvm.mlir.addressof @__wg_main_kColReduction_reduce__4_1_0___8w16h_1_0 : !llvm.ptr<array<128 x f32>, 3>
    %17 = llvm.getelementptr %16[0, 0] : (!llvm.ptr<array<128 x f32>, 3>) -> !llvm.ptr<f32, 3>
    %18 = llvm.mlir.undef : !llvm.struct<(ptr<f32, 3>, ptr<f32, 3>, i32, array<1 x i32>, array<1 x i32>)>
    %19 = llvm.insertvalue %17, %18[0] : !llvm.struct<(ptr<f32, 3>, ptr<f32, 3>, i32, array<1 x i32>, array<1 x i32>)> 
    %20 = llvm.insertvalue %17, %19[1] : !llvm.struct<(ptr<f32, 3>, ptr<f32, 3>, i32, array<1 x i32>, array<1 x i32>)> 
    %21 = llvm.mlir.constant(0 : index) : i32
    %22 = llvm.insertvalue %21, %20[2] : !llvm.struct<(ptr<f32, 3>, ptr<f32, 3>, i32, array<1 x i32>, array<1 x i32>)> 
    %23 = llvm.mlir.constant(128 : index) : i32
    %24 = llvm.insertvalue %23, %22[3, 0] : !llvm.struct<(ptr<f32, 3>, ptr<f32, 3>, i32, array<1 x i32>, array<1 x i32>)> 
    %25 = llvm.mlir.constant(1 : index) : i32
    %26 = llvm.insertvalue %25, %24[4, 0] : !llvm.struct<(ptr<f32, 3>, ptr<f32, 3>, i32, array<1 x i32>, array<1 x i32>)> 
    %27 = llvm.mlir.constant(32 : index) : i32
    %28 = llvm.mlir.constant(4 : index) : i32
    %29 = llvm.mlir.constant(64 : index) : i32
    %30 = llvm.mlir.constant(0xFF800000 : f32) : f32
    %31 = llvm.mlir.constant(2 : index) : i32
    %32 = llvm.mlir.constant(16 : index) : i32
    %33 = llvm.mlir.constant(8 : index) : i32
    %34 = llvm.mlir.constant(128 : index) : i32
    %35 = llvm.mlir.constant(1 : index) : i32
    %36 = llvm.mlir.constant(0 : index) : i32
    %37 = nvvm.read.ptx.sreg.ctaid.x : i32
    %38 = nvvm.read.ptx.sreg.tid.x : i32
    llvm.br ^bb1
  ^bb1:  // pred: ^bb0
    %39 = llvm.extractvalue %9[3, 0] : !llvm.struct<(ptr<f32>, ptr<f32>, i32, array<3 x i32>, array<3 x i32>)> 
    %40 = llvm.extractvalue %9[3, 1] : !llvm.struct<(ptr<f32>, ptr<f32>, i32, array<3 x i32>, array<3 x i32>)> 
    %41 = llvm.extractvalue %9[3, 2] : !llvm.struct<(ptr<f32>, ptr<f32>, i32, array<3 x i32>, array<3 x i32>)> 
    %42 = llvm.mul %37, %arg10  : i32
    %43 = llvm.add %38, %42  : i32
    %44 = llvm.icmp "ult" %43, %arg11 : i32
    llvm.cond_br %44, ^bb2, ^bb16
  ^bb2:  // pred: ^bb1
    %45 = llvm.srem %43, %34  : i32
    %46 = llvm.sdiv %43, %34  : i32
    %47 = llvm.udiv %45, %33  : i32
    %48 = llvm.urem %45, %33  : i32
    %49 = llvm.udiv %46, %arg12  : i32
    %50 = llvm.urem %46, %arg12  : i32
    %51 = llvm.mul %49, %32  : i32
    %52 = llvm.add %51, %47  : i32
    %53 = llvm.mul %50, %33  : i32
    %54 = llvm.add %53, %48  : i32
    %55 = llvm.icmp "ult" %52, %39 : i32
    %56 = llvm.icmp "ult" %54, %arg0 : i32
    %57 = llvm.and %55, %56  : i1
    llvm.cond_br %57, ^bb3, ^bb4
  ^bb3:  // pred: ^bb2
    %58 = llvm.mul %52, %arg0  : i32
    %59 = llvm.add %58, %54  : i32
    %60 = llvm.mul %39, %40  : i32
    %61 = llvm.mul %60, %41  : i32
    %62 = llvm.mlir.undef : !llvm.struct<(ptr<f32>, ptr<f32>, i32, array<1 x i32>, array<1 x i32>)>
    %63 = llvm.extractvalue %9[0] : !llvm.struct<(ptr<f32>, ptr<f32>, i32, array<3 x i32>, array<3 x i32>)> 
    %64 = llvm.extractvalue %9[1] : !llvm.struct<(ptr<f32>, ptr<f32>, i32, array<3 x i32>, array<3 x i32>)> 
    %65 = llvm.insertvalue %63, %62[0] : !llvm.struct<(ptr<f32>, ptr<f32>, i32, array<1 x i32>, array<1 x i32>)> 
    %66 = llvm.insertvalue %64, %65[1] : !llvm.struct<(ptr<f32>, ptr<f32>, i32, array<1 x i32>, array<1 x i32>)> 
    %67 = llvm.insertvalue %36, %66[2] : !llvm.struct<(ptr<f32>, ptr<f32>, i32, array<1 x i32>, array<1 x i32>)> 
    %68 = llvm.insertvalue %61, %67[3, 0] : !llvm.struct<(ptr<f32>, ptr<f32>, i32, array<1 x i32>, array<1 x i32>)> 
    %69 = llvm.insertvalue %35, %68[4, 0] : !llvm.struct<(ptr<f32>, ptr<f32>, i32, array<1 x i32>, array<1 x i32>)> 
    %70 = llvm.extractvalue %69[1] : !llvm.struct<(ptr<f32>, ptr<f32>, i32, array<1 x i32>, array<1 x i32>)> 
    %71 = llvm.getelementptr %70[%59] : (!llvm.ptr<f32>, i32) -> !llvm.ptr<f32>
    %72 = llvm.load %71 : !llvm.ptr<f32>
    %73 = llvm.call @__nv_fabsf(%72) : (f32) -> f32
    %74 = llvm.mlir.undef : !llvm.struct<(ptr<f32, 3>, ptr<f32, 3>, i32, array<1 x i32>, array<1 x i32>)>
    %75 = llvm.extractvalue %26[0] : !llvm.struct<(ptr<f32, 3>, ptr<f32, 3>, i32, array<1 x i32>, array<1 x i32>)> 
    %76 = llvm.extractvalue %26[1] : !llvm.struct<(ptr<f32, 3>, ptr<f32, 3>, i32, array<1 x i32>, array<1 x i32>)> 
    %77 = llvm.insertvalue %75, %74[0] : !llvm.struct<(ptr<f32, 3>, ptr<f32, 3>, i32, array<1 x i32>, array<1 x i32>)> 
    %78 = llvm.insertvalue %76, %77[1] : !llvm.struct<(ptr<f32, 3>, ptr<f32, 3>, i32, array<1 x i32>, array<1 x i32>)> 
    %79 = llvm.mlir.constant(0 : index) : i32
    %80 = llvm.insertvalue %79, %78[2] : !llvm.struct<(ptr<f32, 3>, ptr<f32, 3>, i32, array<1 x i32>, array<1 x i32>)> 
    %81 = llvm.mlir.constant(128 : index) : i32
    %82 = llvm.insertvalue %81, %80[3, 0] : !llvm.struct<(ptr<f32, 3>, ptr<f32, 3>, i32, array<1 x i32>, array<1 x i32>)> 
    %83 = llvm.mlir.constant(1 : index) : i32
    %84 = llvm.insertvalue %83, %82[4, 0] : !llvm.struct<(ptr<f32, 3>, ptr<f32, 3>, i32, array<1 x i32>, array<1 x i32>)> 
    %85 = llvm.extractvalue %84[1] : !llvm.struct<(ptr<f32, 3>, ptr<f32, 3>, i32, array<1 x i32>, array<1 x i32>)> 
    %86 = llvm.getelementptr %85[%45] : (!llvm.ptr<f32, 3>, i32) -> !llvm.ptr<f32, 3>
    llvm.store %73, %86 : !llvm.ptr<f32, 3>
    llvm.br ^bb5
  ^bb4:  // pred: ^bb2
    %87 = llvm.mlir.undef : !llvm.struct<(ptr<f32, 3>, ptr<f32, 3>, i32, array<1 x i32>, array<1 x i32>)>
    %88 = llvm.extractvalue %26[0] : !llvm.struct<(ptr<f32, 3>, ptr<f32, 3>, i32, array<1 x i32>, array<1 x i32>)> 
    %89 = llvm.extractvalue %26[1] : !llvm.struct<(ptr<f32, 3>, ptr<f32, 3>, i32, array<1 x i32>, array<1 x i32>)> 
    %90 = llvm.insertvalue %88, %87[0] : !llvm.struct<(ptr<f32, 3>, ptr<f32, 3>, i32, array<1 x i32>, array<1 x i32>)> 
    %91 = llvm.insertvalue %89, %90[1] : !llvm.struct<(ptr<f32, 3>, ptr<f32, 3>, i32, array<1 x i32>, array<1 x i32>)> 
    %92 = llvm.mlir.constant(0 : index) : i32
    %93 = llvm.insertvalue %92, %91[2] : !llvm.struct<(ptr<f32, 3>, ptr<f32, 3>, i32, array<1 x i32>, array<1 x i32>)> 
    %94 = llvm.mlir.constant(128 : index) : i32
    %95 = llvm.insertvalue %94, %93[3, 0] : !llvm.struct<(ptr<f32, 3>, ptr<f32, 3>, i32, array<1 x i32>, array<1 x i32>)> 
    %96 = llvm.mlir.constant(1 : index) : i32
    %97 = llvm.insertvalue %96, %95[4, 0] : !llvm.struct<(ptr<f32, 3>, ptr<f32, 3>, i32, array<1 x i32>, array<1 x i32>)> 
    %98 = llvm.extractvalue %97[1] : !llvm.struct<(ptr<f32, 3>, ptr<f32, 3>, i32, array<1 x i32>, array<1 x i32>)> 
    %99 = llvm.getelementptr %98[%45] : (!llvm.ptr<f32, 3>, i32) -> !llvm.ptr<f32, 3>
    llvm.store %30, %99 : !llvm.ptr<f32, 3>
    llvm.br ^bb5
  ^bb5:  // 2 preds: ^bb3, ^bb4
    nvvm.barrier0
    %100 = llvm.icmp "ult" %47, %33 : i32
    %101 = llvm.add %52, %33  : i32
    %102 = llvm.icmp "ult" %101, %39 : i32
    %103 = llvm.and %100, %102  : i1
    llvm.cond_br %103, ^bb6, ^bb7
  ^bb6:  // pred: ^bb5
    %104 = llvm.add %45, %29  : i32
    %105 = llvm.mlir.undef : !llvm.struct<(ptr<f32, 3>, ptr<f32, 3>, i32, array<1 x i32>, array<1 x i32>)>
    %106 = llvm.extractvalue %26[0] : !llvm.struct<(ptr<f32, 3>, ptr<f32, 3>, i32, array<1 x i32>, array<1 x i32>)> 
    %107 = llvm.extractvalue %26[1] : !llvm.struct<(ptr<f32, 3>, ptr<f32, 3>, i32, array<1 x i32>, array<1 x i32>)> 
    %108 = llvm.insertvalue %106, %105[0] : !llvm.struct<(ptr<f32, 3>, ptr<f32, 3>, i32, array<1 x i32>, array<1 x i32>)> 
    %109 = llvm.insertvalue %107, %108[1] : !llvm.struct<(ptr<f32, 3>, ptr<f32, 3>, i32, array<1 x i32>, array<1 x i32>)> 
    %110 = llvm.mlir.constant(0 : index) : i32
    %111 = llvm.insertvalue %110, %109[2] : !llvm.struct<(ptr<f32, 3>, ptr<f32, 3>, i32, array<1 x i32>, array<1 x i32>)> 
    %112 = llvm.mlir.constant(128 : index) : i32
    %113 = llvm.insertvalue %112, %111[3, 0] : !llvm.struct<(ptr<f32, 3>, ptr<f32, 3>, i32, array<1 x i32>, array<1 x i32>)> 
    %114 = llvm.mlir.constant(1 : index) : i32
    %115 = llvm.insertvalue %114, %113[4, 0] : !llvm.struct<(ptr<f32, 3>, ptr<f32, 3>, i32, array<1 x i32>, array<1 x i32>)> 
    %116 = llvm.extractvalue %115[1] : !llvm.struct<(ptr<f32, 3>, ptr<f32, 3>, i32, array<1 x i32>, array<1 x i32>)> 
    %117 = llvm.getelementptr %116[%45] : (!llvm.ptr<f32, 3>, i32) -> !llvm.ptr<f32, 3>
    %118 = llvm.load %117 : !llvm.ptr<f32, 3>
    %119 = llvm.extractvalue %115[1] : !llvm.struct<(ptr<f32, 3>, ptr<f32, 3>, i32, array<1 x i32>, array<1 x i32>)> 
    %120 = llvm.getelementptr %119[%104] : (!llvm.ptr<f32, 3>, i32) -> !llvm.ptr<f32, 3>
    %121 = llvm.load %120 : !llvm.ptr<f32, 3>
    %122 = llvm.fcmp "ugt" %118, %121 : f32
    %123 = llvm.select %122, %118, %121 : i1, f32
    %124 = llvm.fcmp "uno" %121, %121 : f32
    %125 = llvm.select %124, %121, %123 : i1, f32
    %126 = llvm.extractvalue %115[1] : !llvm.struct<(ptr<f32, 3>, ptr<f32, 3>, i32, array<1 x i32>, array<1 x i32>)> 
    %127 = llvm.getelementptr %126[%45] : (!llvm.ptr<f32, 3>, i32) -> !llvm.ptr<f32, 3>
    llvm.store %125, %127 : !llvm.ptr<f32, 3>
    llvm.br ^bb7
  ^bb7:  // 2 preds: ^bb5, ^bb6
    nvvm.barrier0
    %128 = llvm.icmp "ult" %47, %28 : i32
    %129 = llvm.add %52, %28  : i32
    %130 = llvm.icmp "ult" %129, %39 : i32
    %131 = llvm.and %128, %130  : i1
    llvm.cond_br %131, ^bb8, ^bb9
  ^bb8:  // pred: ^bb7
    %132 = llvm.add %45, %27  : i32
    %133 = llvm.mlir.undef : !llvm.struct<(ptr<f32, 3>, ptr<f32, 3>, i32, array<1 x i32>, array<1 x i32>)>
    %134 = llvm.extractvalue %26[0] : !llvm.struct<(ptr<f32, 3>, ptr<f32, 3>, i32, array<1 x i32>, array<1 x i32>)> 
    %135 = llvm.extractvalue %26[1] : !llvm.struct<(ptr<f32, 3>, ptr<f32, 3>, i32, array<1 x i32>, array<1 x i32>)> 
    %136 = llvm.insertvalue %134, %133[0] : !llvm.struct<(ptr<f32, 3>, ptr<f32, 3>, i32, array<1 x i32>, array<1 x i32>)> 
    %137 = llvm.insertvalue %135, %136[1] : !llvm.struct<(ptr<f32, 3>, ptr<f32, 3>, i32, array<1 x i32>, array<1 x i32>)> 
    %138 = llvm.mlir.constant(0 : index) : i32
    %139 = llvm.insertvalue %138, %137[2] : !llvm.struct<(ptr<f32, 3>, ptr<f32, 3>, i32, array<1 x i32>, array<1 x i32>)> 
    %140 = llvm.mlir.constant(128 : index) : i32
    %141 = llvm.insertvalue %140, %139[3, 0] : !llvm.struct<(ptr<f32, 3>, ptr<f32, 3>, i32, array<1 x i32>, array<1 x i32>)> 
    %142 = llvm.mlir.constant(1 : index) : i32
    %143 = llvm.insertvalue %142, %141[4, 0] : !llvm.struct<(ptr<f32, 3>, ptr<f32, 3>, i32, array<1 x i32>, array<1 x i32>)> 
    %144 = llvm.extractvalue %143[1] : !llvm.struct<(ptr<f32, 3>, ptr<f32, 3>, i32, array<1 x i32>, array<1 x i32>)> 
    %145 = llvm.getelementptr %144[%45] : (!llvm.ptr<f32, 3>, i32) -> !llvm.ptr<f32, 3>
    %146 = llvm.load %145 : !llvm.ptr<f32, 3>
    %147 = llvm.extractvalue %143[1] : !llvm.struct<(ptr<f32, 3>, ptr<f32, 3>, i32, array<1 x i32>, array<1 x i32>)> 
    %148 = llvm.getelementptr %147[%132] : (!llvm.ptr<f32, 3>, i32) -> !llvm.ptr<f32, 3>
    %149 = llvm.load %148 : !llvm.ptr<f32, 3>
    %150 = llvm.fcmp "ugt" %146, %149 : f32
    %151 = llvm.select %150, %146, %149 : i1, f32
    %152 = llvm.fcmp "uno" %149, %149 : f32
    %153 = llvm.select %152, %149, %151 : i1, f32
    %154 = llvm.extractvalue %143[1] : !llvm.struct<(ptr<f32, 3>, ptr<f32, 3>, i32, array<1 x i32>, array<1 x i32>)> 
    %155 = llvm.getelementptr %154[%45] : (!llvm.ptr<f32, 3>, i32) -> !llvm.ptr<f32, 3>
    llvm.store %153, %155 : !llvm.ptr<f32, 3>
    llvm.br ^bb9
  ^bb9:  // 2 preds: ^bb7, ^bb8
    nvvm.barrier0
    %156 = llvm.icmp "ult" %47, %31 : i32
    %157 = llvm.add %52, %31  : i32
    %158 = llvm.icmp "ult" %157, %39 : i32
    %159 = llvm.and %156, %158  : i1
    llvm.cond_br %159, ^bb10, ^bb11
  ^bb10:  // pred: ^bb9
    %160 = llvm.add %45, %32  : i32
    %161 = llvm.mlir.undef : !llvm.struct<(ptr<f32, 3>, ptr<f32, 3>, i32, array<1 x i32>, array<1 x i32>)>
    %162 = llvm.extractvalue %26[0] : !llvm.struct<(ptr<f32, 3>, ptr<f32, 3>, i32, array<1 x i32>, array<1 x i32>)> 
    %163 = llvm.extractvalue %26[1] : !llvm.struct<(ptr<f32, 3>, ptr<f32, 3>, i32, array<1 x i32>, array<1 x i32>)> 
    %164 = llvm.insertvalue %162, %161[0] : !llvm.struct<(ptr<f32, 3>, ptr<f32, 3>, i32, array<1 x i32>, array<1 x i32>)> 
    %165 = llvm.insertvalue %163, %164[1] : !llvm.struct<(ptr<f32, 3>, ptr<f32, 3>, i32, array<1 x i32>, array<1 x i32>)> 
    %166 = llvm.mlir.constant(0 : index) : i32
    %167 = llvm.insertvalue %166, %165[2] : !llvm.struct<(ptr<f32, 3>, ptr<f32, 3>, i32, array<1 x i32>, array<1 x i32>)> 
    %168 = llvm.mlir.constant(128 : index) : i32
    %169 = llvm.insertvalue %168, %167[3, 0] : !llvm.struct<(ptr<f32, 3>, ptr<f32, 3>, i32, array<1 x i32>, array<1 x i32>)> 
    %170 = llvm.mlir.constant(1 : index) : i32
    %171 = llvm.insertvalue %170, %169[4, 0] : !llvm.struct<(ptr<f32, 3>, ptr<f32, 3>, i32, array<1 x i32>, array<1 x i32>)> 
    %172 = llvm.extractvalue %171[1] : !llvm.struct<(ptr<f32, 3>, ptr<f32, 3>, i32, array<1 x i32>, array<1 x i32>)> 
    %173 = llvm.getelementptr %172[%45] : (!llvm.ptr<f32, 3>, i32) -> !llvm.ptr<f32, 3>
    %174 = llvm.load %173 : !llvm.ptr<f32, 3>
    %175 = llvm.extractvalue %171[1] : !llvm.struct<(ptr<f32, 3>, ptr<f32, 3>, i32, array<1 x i32>, array<1 x i32>)> 
    %176 = llvm.getelementptr %175[%160] : (!llvm.ptr<f32, 3>, i32) -> !llvm.ptr<f32, 3>
    %177 = llvm.load %176 : !llvm.ptr<f32, 3>
    %178 = llvm.fcmp "ugt" %174, %177 : f32
    %179 = llvm.select %178, %174, %177 : i1, f32
    %180 = llvm.fcmp "uno" %177, %177 : f32
    %181 = llvm.select %180, %177, %179 : i1, f32
    %182 = llvm.extractvalue %171[1] : !llvm.struct<(ptr<f32, 3>, ptr<f32, 3>, i32, array<1 x i32>, array<1 x i32>)> 
    %183 = llvm.getelementptr %182[%45] : (!llvm.ptr<f32, 3>, i32) -> !llvm.ptr<f32, 3>
    llvm.store %181, %183 : !llvm.ptr<f32, 3>
    llvm.br ^bb11
  ^bb11:  // 2 preds: ^bb9, ^bb10
    nvvm.barrier0
    %184 = llvm.icmp "eq" %47, %36 : i32
    %185 = llvm.and %184, %57  : i1
    llvm.cond_br %185, ^bb12, ^bb15
  ^bb12:  // pred: ^bb11
    %186 = llvm.add %45, %33  : i32
    %187 = llvm.mlir.undef : !llvm.struct<(ptr<f32, 3>, ptr<f32, 3>, i32, array<1 x i32>, array<1 x i32>)>
    %188 = llvm.extractvalue %26[0] : !llvm.struct<(ptr<f32, 3>, ptr<f32, 3>, i32, array<1 x i32>, array<1 x i32>)> 
    %189 = llvm.extractvalue %26[1] : !llvm.struct<(ptr<f32, 3>, ptr<f32, 3>, i32, array<1 x i32>, array<1 x i32>)> 
    %190 = llvm.insertvalue %188, %187[0] : !llvm.struct<(ptr<f32, 3>, ptr<f32, 3>, i32, array<1 x i32>, array<1 x i32>)> 
    %191 = llvm.insertvalue %189, %190[1] : !llvm.struct<(ptr<f32, 3>, ptr<f32, 3>, i32, array<1 x i32>, array<1 x i32>)> 
    %192 = llvm.mlir.constant(0 : index) : i32
    %193 = llvm.insertvalue %192, %191[2] : !llvm.struct<(ptr<f32, 3>, ptr<f32, 3>, i32, array<1 x i32>, array<1 x i32>)> 
    %194 = llvm.mlir.constant(128 : index) : i32
    %195 = llvm.insertvalue %194, %193[3, 0] : !llvm.struct<(ptr<f32, 3>, ptr<f32, 3>, i32, array<1 x i32>, array<1 x i32>)> 
    %196 = llvm.mlir.constant(1 : index) : i32
    %197 = llvm.insertvalue %196, %195[4, 0] : !llvm.struct<(ptr<f32, 3>, ptr<f32, 3>, i32, array<1 x i32>, array<1 x i32>)> 
    %198 = llvm.extractvalue %197[1] : !llvm.struct<(ptr<f32, 3>, ptr<f32, 3>, i32, array<1 x i32>, array<1 x i32>)> 
    %199 = llvm.getelementptr %198[%45] : (!llvm.ptr<f32, 3>, i32) -> !llvm.ptr<f32, 3>
    %200 = llvm.load %199 : !llvm.ptr<f32, 3>
    %201 = llvm.extractvalue %197[1] : !llvm.struct<(ptr<f32, 3>, ptr<f32, 3>, i32, array<1 x i32>, array<1 x i32>)> 
    %202 = llvm.getelementptr %201[%186] : (!llvm.ptr<f32, 3>, i32) -> !llvm.ptr<f32, 3>
    %203 = llvm.load %202 : !llvm.ptr<f32, 3>
    %204 = llvm.fcmp "ugt" %200, %203 : f32
    %205 = llvm.select %204, %200, %203 : i1, f32
    %206 = llvm.fcmp "uno" %203, %203 : f32
    %207 = llvm.select %206, %203, %205 : i1, f32
    %208 = llvm.extractvalue %15[1] : !llvm.struct<(ptr<f32>, ptr<f32>, i32, array<1 x i32>, array<1 x i32>)> 
    %209 = llvm.getelementptr %208[%54] : (!llvm.ptr<f32>, i32) -> !llvm.ptr<f32>
    %210 = llvm.load %209 : !llvm.ptr<f32>
    llvm.br ^bb13(%210 : f32)
  ^bb13(%211: f32):  // 2 preds: ^bb12, ^bb13
    %212 = llvm.fcmp "ogt" %211, %207 : f32
    %213 = llvm.select %212, %211, %207 : i1, f32
    %214 = llvm.bitcast %209 : !llvm.ptr<f32> to !llvm.ptr<i32>
    %215 = llvm.bitcast %211 : f32 to i32
    %216 = llvm.bitcast %213 : f32 to i32
    %217 = llvm.cmpxchg %214, %215, %216 acq_rel monotonic : !llvm.ptr<i32>, i32
    %218 = llvm.extractvalue %217[0] : !llvm.struct<(i32, i1)> 
    %219 = llvm.bitcast %218 : i32 to f32
    %220 = llvm.extractvalue %217[1] : !llvm.struct<(i32, i1)> 
    llvm.cond_br %220, ^bb14, ^bb13(%219 : f32)
  ^bb14:  // pred: ^bb13
    llvm.br ^bb15
  ^bb15:  // 2 preds: ^bb11, ^bb14
    llvm.br ^bb16
  ^bb16:  // 2 preds: ^bb1, ^bb15
    llvm.return
  }
}

// -----// IR Dump After LLVMInsertValueSimplifierPass (disc-llvm-insert-value-simplifier) //----- //
llvm.func @main_kColReduction_reduce__4_1_0___8w16h_1(%arg0: i32, %arg1: !llvm.ptr<f32>, %arg2: !llvm.ptr<f32>, %arg3: i32, %arg4: i32, %arg5: i32, %arg6: i32, %arg7: i32, %arg8: i32, %arg9: i32, %arg10: i32, %arg11: i32, %arg12: i32, %arg13: !llvm.ptr<f32>, %arg14: !llvm.ptr<f32>, %arg15: i32, %arg16: i32, %arg17: i32) attributes {gpu.kernel, nvvm.kernel} {
  %0 = llvm.mlir.constant(8 : index) : i32
  %1 = llvm.mlir.constant(16 : index) : i32
  %2 = llvm.mlir.constant(2 : index) : i32
  %3 = llvm.mlir.constant(0xFF800000 : f32) : f32
  %4 = llvm.mlir.constant(64 : index) : i32
  %5 = llvm.mlir.constant(4 : index) : i32
  %6 = llvm.mlir.constant(32 : index) : i32
  %7 = llvm.mlir.constant(128 : index) : i32
  %8 = llvm.mlir.constant(0 : index) : i32
  %9 = llvm.mlir.addressof @__wg_main_kColReduction_reduce__4_1_0___8w16h_1_0 : !llvm.ptr<array<128 x f32>, 3>
  %10 = llvm.getelementptr %9[0, 0] : (!llvm.ptr<array<128 x f32>, 3>) -> !llvm.ptr<f32, 3>
  %11 = nvvm.read.ptx.sreg.ctaid.x : i32
  %12 = nvvm.read.ptx.sreg.tid.x : i32
  llvm.br ^bb1
^bb1:  // pred: ^bb0
  %13 = llvm.mul %11, %arg10  : i32
  %14 = llvm.add %12, %13  : i32
  %15 = llvm.icmp "ult" %14, %arg11 : i32
  llvm.cond_br %15, ^bb2, ^bb16
^bb2:  // pred: ^bb1
  %16 = llvm.srem %14, %7  : i32
  %17 = llvm.sdiv %14, %7  : i32
  %18 = llvm.udiv %16, %0  : i32
  %19 = llvm.urem %16, %0  : i32
  %20 = llvm.udiv %17, %arg12  : i32
  %21 = llvm.urem %17, %arg12  : i32
  %22 = llvm.mul %20, %1  : i32
  %23 = llvm.add %22, %18  : i32
  %24 = llvm.mul %21, %0  : i32
  %25 = llvm.add %24, %19  : i32
  %26 = llvm.icmp "ult" %23, %arg4 : i32
  %27 = llvm.icmp "ult" %25, %arg0 : i32
  %28 = llvm.and %26, %27  : i1
  llvm.cond_br %28, ^bb3, ^bb4
^bb3:  // pred: ^bb2
  %29 = llvm.mul %23, %arg0  : i32
  %30 = llvm.add %29, %25  : i32
  %31 = llvm.getelementptr %arg2[%30] : (!llvm.ptr<f32>, i32) -> !llvm.ptr<f32>
  %32 = llvm.load %31 : !llvm.ptr<f32>
  %33 = llvm.call @__nv_fabsf(%32) : (f32) -> f32
  %34 = llvm.getelementptr %10[%16] : (!llvm.ptr<f32, 3>, i32) -> !llvm.ptr<f32, 3>
  llvm.store %33, %34 : !llvm.ptr<f32, 3>
  llvm.br ^bb5
^bb4:  // pred: ^bb2
  %35 = llvm.getelementptr %10[%16] : (!llvm.ptr<f32, 3>, i32) -> !llvm.ptr<f32, 3>
  llvm.store %3, %35 : !llvm.ptr<f32, 3>
  llvm.br ^bb5
^bb5:  // 2 preds: ^bb3, ^bb4
  nvvm.barrier0
  %36 = llvm.icmp "ult" %18, %0 : i32
  %37 = llvm.add %23, %0  : i32
  %38 = llvm.icmp "ult" %37, %arg4 : i32
  %39 = llvm.and %36, %38  : i1
  llvm.cond_br %39, ^bb6, ^bb7
^bb6:  // pred: ^bb5
  %40 = llvm.add %16, %4  : i32
  %41 = llvm.getelementptr %10[%16] : (!llvm.ptr<f32, 3>, i32) -> !llvm.ptr<f32, 3>
  %42 = llvm.load %41 : !llvm.ptr<f32, 3>
  %43 = llvm.getelementptr %10[%40] : (!llvm.ptr<f32, 3>, i32) -> !llvm.ptr<f32, 3>
  %44 = llvm.load %43 : !llvm.ptr<f32, 3>
  %45 = llvm.fcmp "ugt" %42, %44 : f32
  %46 = llvm.select %45, %42, %44 : i1, f32
  %47 = llvm.fcmp "uno" %44, %44 : f32
  %48 = llvm.select %47, %44, %46 : i1, f32
  %49 = llvm.getelementptr %10[%16] : (!llvm.ptr<f32, 3>, i32) -> !llvm.ptr<f32, 3>
  llvm.store %48, %49 : !llvm.ptr<f32, 3>
  llvm.br ^bb7
^bb7:  // 2 preds: ^bb5, ^bb6
  nvvm.barrier0
  %50 = llvm.icmp "ult" %18, %5 : i32
  %51 = llvm.add %23, %5  : i32
  %52 = llvm.icmp "ult" %51, %arg4 : i32
  %53 = llvm.and %50, %52  : i1
  llvm.cond_br %53, ^bb8, ^bb9
^bb8:  // pred: ^bb7
  %54 = llvm.add %16, %6  : i32
  %55 = llvm.getelementptr %10[%16] : (!llvm.ptr<f32, 3>, i32) -> !llvm.ptr<f32, 3>
  %56 = llvm.load %55 : !llvm.ptr<f32, 3>
  %57 = llvm.getelementptr %10[%54] : (!llvm.ptr<f32, 3>, i32) -> !llvm.ptr<f32, 3>
  %58 = llvm.load %57 : !llvm.ptr<f32, 3>
  %59 = llvm.fcmp "ugt" %56, %58 : f32
  %60 = llvm.select %59, %56, %58 : i1, f32
  %61 = llvm.fcmp "uno" %58, %58 : f32
  %62 = llvm.select %61, %58, %60 : i1, f32
  %63 = llvm.getelementptr %10[%16] : (!llvm.ptr<f32, 3>, i32) -> !llvm.ptr<f32, 3>
  llvm.store %62, %63 : !llvm.ptr<f32, 3>
  llvm.br ^bb9
^bb9:  // 2 preds: ^bb7, ^bb8
  nvvm.barrier0
  %64 = llvm.icmp "ult" %18, %2 : i32
  %65 = llvm.add %23, %2  : i32
  %66 = llvm.icmp "ult" %65, %arg4 : i32
  %67 = llvm.and %64, %66  : i1
  llvm.cond_br %67, ^bb10, ^bb11
^bb10:  // pred: ^bb9
  %68 = llvm.add %16, %1  : i32
  %69 = llvm.getelementptr %10[%16] : (!llvm.ptr<f32, 3>, i32) -> !llvm.ptr<f32, 3>
  %70 = llvm.load %69 : !llvm.ptr<f32, 3>
  %71 = llvm.getelementptr %10[%68] : (!llvm.ptr<f32, 3>, i32) -> !llvm.ptr<f32, 3>
  %72 = llvm.load %71 : !llvm.ptr<f32, 3>
  %73 = llvm.fcmp "ugt" %70, %72 : f32
  %74 = llvm.select %73, %70, %72 : i1, f32
  %75 = llvm.fcmp "uno" %72, %72 : f32
  %76 = llvm.select %75, %72, %74 : i1, f32
  %77 = llvm.getelementptr %10[%16] : (!llvm.ptr<f32, 3>, i32) -> !llvm.ptr<f32, 3>
  llvm.store %76, %77 : !llvm.ptr<f32, 3>
  llvm.br ^bb11
^bb11:  // 2 preds: ^bb9, ^bb10
  nvvm.barrier0
  %78 = llvm.icmp "eq" %18, %8 : i32
  %79 = llvm.and %78, %28  : i1
  llvm.cond_br %79, ^bb12, ^bb15
^bb12:  // pred: ^bb11
  %80 = llvm.add %16, %0  : i32
  %81 = llvm.getelementptr %10[%16] : (!llvm.ptr<f32, 3>, i32) -> !llvm.ptr<f32, 3>
  %82 = llvm.load %81 : !llvm.ptr<f32, 3>
  %83 = llvm.getelementptr %10[%80] : (!llvm.ptr<f32, 3>, i32) -> !llvm.ptr<f32, 3>
  %84 = llvm.load %83 : !llvm.ptr<f32, 3>
  %85 = llvm.fcmp "ugt" %82, %84 : f32
  %86 = llvm.select %85, %82, %84 : i1, f32
  %87 = llvm.fcmp "uno" %84, %84 : f32
  %88 = llvm.select %87, %84, %86 : i1, f32
  %89 = llvm.getelementptr %arg14[%25] : (!llvm.ptr<f32>, i32) -> !llvm.ptr<f32>
  %90 = llvm.load %89 : !llvm.ptr<f32>
  llvm.br ^bb13(%90 : f32)
^bb13(%91: f32):  // 2 preds: ^bb12, ^bb13
  %92 = llvm.fcmp "ogt" %91, %88 : f32
  %93 = llvm.select %92, %91, %88 : i1, f32
  %94 = llvm.bitcast %89 : !llvm.ptr<f32> to !llvm.ptr<i32>
  %95 = llvm.bitcast %91 : f32 to i32
  %96 = llvm.bitcast %93 : f32 to i32
  %97 = llvm.cmpxchg %94, %95, %96 acq_rel monotonic : !llvm.ptr<i32>, i32
  %98 = llvm.extractvalue %97[0] : !llvm.struct<(i32, i1)> 
  %99 = llvm.bitcast %98 : i32 to f32
  %100 = llvm.extractvalue %97[1] : !llvm.struct<(i32, i1)> 
  llvm.cond_br %100, ^bb14, ^bb13(%99 : f32)
^bb14:  // pred: ^bb13
  llvm.br ^bb15
^bb15:  // 2 preds: ^bb11, ^bb14
  llvm.br ^bb16
^bb16:  // 2 preds: ^bb1, ^bb15
  llvm.return
}

// -----// IR Dump After FunctionDeadArgumentEliminationPass (disc-function-dead-argument-elimination) //----- //
gpu.module @main_kernel_2 {
  llvm.mlir.global internal @__wg_main_kColReduction_reduce__4_1_0___8w16h_1_0() {addr_space = 3 : i32} : !llvm.array<128 x f32>
  llvm.func @__nv_fabsf(f32) -> f32
  llvm.func @main_kColReduction_reduce__4_1_0___8w16h_1(%arg0: i32, %arg1: !llvm.ptr<f32>, %arg2: i32, %arg3: i32, %arg4: i32, %arg5: i32, %arg6: !llvm.ptr<f32>) attributes {disc.elimargs = [1 : index, 3 : index, 5 : index, 6 : index, 7 : index, 8 : index, 9 : index, 13 : index, 15 : index, 16 : index, 17 : index], gpu.kernel, nvvm.kernel} {
    %0 = llvm.mlir.constant(8 : index) : i32
    %1 = llvm.mlir.constant(16 : index) : i32
    %2 = llvm.mlir.constant(2 : index) : i32
    %3 = llvm.mlir.constant(0xFF800000 : f32) : f32
    %4 = llvm.mlir.constant(64 : index) : i32
    %5 = llvm.mlir.constant(4 : index) : i32
    %6 = llvm.mlir.constant(32 : index) : i32
    %7 = llvm.mlir.constant(128 : index) : i32
    %8 = llvm.mlir.constant(0 : index) : i32
    %9 = llvm.mlir.addressof @__wg_main_kColReduction_reduce__4_1_0___8w16h_1_0 : !llvm.ptr<array<128 x f32>, 3>
    %10 = llvm.getelementptr %9[0, 0] : (!llvm.ptr<array<128 x f32>, 3>) -> !llvm.ptr<f32, 3>
    %11 = nvvm.read.ptx.sreg.ctaid.x : i32
    %12 = nvvm.read.ptx.sreg.tid.x : i32
    llvm.br ^bb1
  ^bb1:  // pred: ^bb0
    %13 = llvm.mul %11, %arg3  : i32
    %14 = llvm.add %12, %13  : i32
    %15 = llvm.icmp "ult" %14, %arg4 : i32
    llvm.cond_br %15, ^bb2, ^bb16
  ^bb2:  // pred: ^bb1
    %16 = llvm.srem %14, %7  : i32
    %17 = llvm.sdiv %14, %7  : i32
    %18 = llvm.udiv %16, %0  : i32
    %19 = llvm.urem %16, %0  : i32
    %20 = llvm.udiv %17, %arg5  : i32
    %21 = llvm.urem %17, %arg5  : i32
    %22 = llvm.mul %20, %1  : i32
    %23 = llvm.add %22, %18  : i32
    %24 = llvm.mul %21, %0  : i32
    %25 = llvm.add %24, %19  : i32
    %26 = llvm.icmp "ult" %23, %arg2 : i32
    %27 = llvm.icmp "ult" %25, %arg0 : i32
    %28 = llvm.and %26, %27  : i1
    llvm.cond_br %28, ^bb3, ^bb4
  ^bb3:  // pred: ^bb2
    %29 = llvm.mul %23, %arg0  : i32
    %30 = llvm.add %29, %25  : i32
    %31 = llvm.getelementptr %arg1[%30] : (!llvm.ptr<f32>, i32) -> !llvm.ptr<f32>
    %32 = llvm.load %31 : !llvm.ptr<f32>
    %33 = llvm.call @__nv_fabsf(%32) : (f32) -> f32
    %34 = llvm.getelementptr %10[%16] : (!llvm.ptr<f32, 3>, i32) -> !llvm.ptr<f32, 3>
    llvm.store %33, %34 : !llvm.ptr<f32, 3>
    llvm.br ^bb5
  ^bb4:  // pred: ^bb2
    %35 = llvm.getelementptr %10[%16] : (!llvm.ptr<f32, 3>, i32) -> !llvm.ptr<f32, 3>
    llvm.store %3, %35 : !llvm.ptr<f32, 3>
    llvm.br ^bb5
  ^bb5:  // 2 preds: ^bb3, ^bb4
    nvvm.barrier0
    %36 = llvm.icmp "ult" %18, %0 : i32
    %37 = llvm.add %23, %0  : i32
    %38 = llvm.icmp "ult" %37, %arg2 : i32
    %39 = llvm.and %36, %38  : i1
    llvm.cond_br %39, ^bb6, ^bb7
  ^bb6:  // pred: ^bb5
    %40 = llvm.add %16, %4  : i32
    %41 = llvm.getelementptr %10[%16] : (!llvm.ptr<f32, 3>, i32) -> !llvm.ptr<f32, 3>
    %42 = llvm.load %41 : !llvm.ptr<f32, 3>
    %43 = llvm.getelementptr %10[%40] : (!llvm.ptr<f32, 3>, i32) -> !llvm.ptr<f32, 3>
    %44 = llvm.load %43 : !llvm.ptr<f32, 3>
    %45 = llvm.fcmp "ugt" %42, %44 : f32
    %46 = llvm.select %45, %42, %44 : i1, f32
    %47 = llvm.fcmp "uno" %44, %44 : f32
    %48 = llvm.select %47, %44, %46 : i1, f32
    %49 = llvm.getelementptr %10[%16] : (!llvm.ptr<f32, 3>, i32) -> !llvm.ptr<f32, 3>
    llvm.store %48, %49 : !llvm.ptr<f32, 3>
    llvm.br ^bb7
  ^bb7:  // 2 preds: ^bb5, ^bb6
    nvvm.barrier0
    %50 = llvm.icmp "ult" %18, %5 : i32
    %51 = llvm.add %23, %5  : i32
    %52 = llvm.icmp "ult" %51, %arg2 : i32
    %53 = llvm.and %50, %52  : i1
    llvm.cond_br %53, ^bb8, ^bb9
  ^bb8:  // pred: ^bb7
    %54 = llvm.add %16, %6  : i32
    %55 = llvm.getelementptr %10[%16] : (!llvm.ptr<f32, 3>, i32) -> !llvm.ptr<f32, 3>
    %56 = llvm.load %55 : !llvm.ptr<f32, 3>
    %57 = llvm.getelementptr %10[%54] : (!llvm.ptr<f32, 3>, i32) -> !llvm.ptr<f32, 3>
    %58 = llvm.load %57 : !llvm.ptr<f32, 3>
    %59 = llvm.fcmp "ugt" %56, %58 : f32
    %60 = llvm.select %59, %56, %58 : i1, f32
    %61 = llvm.fcmp "uno" %58, %58 : f32
    %62 = llvm.select %61, %58, %60 : i1, f32
    %63 = llvm.getelementptr %10[%16] : (!llvm.ptr<f32, 3>, i32) -> !llvm.ptr<f32, 3>
    llvm.store %62, %63 : !llvm.ptr<f32, 3>
    llvm.br ^bb9
  ^bb9:  // 2 preds: ^bb7, ^bb8
    nvvm.barrier0
    %64 = llvm.icmp "ult" %18, %2 : i32
    %65 = llvm.add %23, %2  : i32
    %66 = llvm.icmp "ult" %65, %arg2 : i32
    %67 = llvm.and %64, %66  : i1
    llvm.cond_br %67, ^bb10, ^bb11
  ^bb10:  // pred: ^bb9
    %68 = llvm.add %16, %1  : i32
    %69 = llvm.getelementptr %10[%16] : (!llvm.ptr<f32, 3>, i32) -> !llvm.ptr<f32, 3>
    %70 = llvm.load %69 : !llvm.ptr<f32, 3>
    %71 = llvm.getelementptr %10[%68] : (!llvm.ptr<f32, 3>, i32) -> !llvm.ptr<f32, 3>
    %72 = llvm.load %71 : !llvm.ptr<f32, 3>
    %73 = llvm.fcmp "ugt" %70, %72 : f32
    %74 = llvm.select %73, %70, %72 : i1, f32
    %75 = llvm.fcmp "uno" %72, %72 : f32
    %76 = llvm.select %75, %72, %74 : i1, f32
    %77 = llvm.getelementptr %10[%16] : (!llvm.ptr<f32, 3>, i32) -> !llvm.ptr<f32, 3>
    llvm.store %76, %77 : !llvm.ptr<f32, 3>
    llvm.br ^bb11
  ^bb11:  // 2 preds: ^bb9, ^bb10
    nvvm.barrier0
    %78 = llvm.icmp "eq" %18, %8 : i32
    %79 = llvm.and %78, %28  : i1
    llvm.cond_br %79, ^bb12, ^bb15
  ^bb12:  // pred: ^bb11
    %80 = llvm.add %16, %0  : i32
    %81 = llvm.getelementptr %10[%16] : (!llvm.ptr<f32, 3>, i32) -> !llvm.ptr<f32, 3>
    %82 = llvm.load %81 : !llvm.ptr<f32, 3>
    %83 = llvm.getelementptr %10[%80] : (!llvm.ptr<f32, 3>, i32) -> !llvm.ptr<f32, 3>
    %84 = llvm.load %83 : !llvm.ptr<f32, 3>
    %85 = llvm.fcmp "ugt" %82, %84 : f32
    %86 = llvm.select %85, %82, %84 : i1, f32
    %87 = llvm.fcmp "uno" %84, %84 : f32
    %88 = llvm.select %87, %84, %86 : i1, f32
    %89 = llvm.getelementptr %arg6[%25] : (!llvm.ptr<f32>, i32) -> !llvm.ptr<f32>
    %90 = llvm.load %89 : !llvm.ptr<f32>
    llvm.br ^bb13(%90 : f32)
  ^bb13(%91: f32):  // 2 preds: ^bb12, ^bb13
    %92 = llvm.fcmp "ogt" %91, %88 : f32
    %93 = llvm.select %92, %91, %88 : i1, f32
    %94 = llvm.bitcast %89 : !llvm.ptr<f32> to !llvm.ptr<i32>
    %95 = llvm.bitcast %91 : f32 to i32
    %96 = llvm.bitcast %93 : f32 to i32
    %97 = llvm.cmpxchg %94, %95, %96 acq_rel monotonic : !llvm.ptr<i32>, i32
    %98 = llvm.extractvalue %97[0] : !llvm.struct<(i32, i1)> 
    %99 = llvm.bitcast %98 : i32 to f32
    %100 = llvm.extractvalue %97[1] : !llvm.struct<(i32, i1)> 
    llvm.cond_br %100, ^bb14, ^bb13(%99 : f32)
  ^bb14:  // pred: ^bb13
    llvm.br ^bb15
  ^bb15:  // 2 preds: ^bb11, ^bb14
    llvm.br ^bb16
  ^bb16:  // 2 preds: ^bb1, ^bb15
    llvm.return
  }
}

// -----// IR Dump After GpuKernelToBlobPass (disc-gpu-kernel-to-blob) //----- //
gpu.module @main_kernel_2 attributes {gpu.binary = "P\EDU\BA\01\00\10\00\10\08\00\00\00\00\00\00\02\00\01\01@\00\00\00\D0\07\00\00\00\00\00\00\CB\07\00\00\00\00\00\00\07\00\01\00P\00\00\00\00\00\00\00\00\00\00\00\11 \00\00\00\00\00\00\00\00\00\00\00\00\00\00(\13\00\00\00\00\00\00\A2\7FELF\02\01\013\07\00\01\00f\02\00\BE\00q\00\01\00#\80\12\08\00\11\0F\07\00\F5\0E\00P\05P\00@\008\00\03\00@\00\0C\00\01\00\00.shstrtab\00.\08\00'ym\08\00\F0\1B_shndx\00.nv.info\00.text.main_kColReduction_r\0A\00\F5\03e__4_1_0___8w16h_1:\00\0F4\00\1Doshared6\00\1AOrela\A0\00\1F?rel\D5\00\22\9Fconstant09\00\1A\B2debug_frame{\00\09\11\00!nv\14\00\11aE\00\0F\9E\01 \0F\8A\00\17\0F\C9\01\F4\8F$____wg_3\00\17\00\0C\00/27\02\02'o_param\09\02\1C\0F\01\00\05\8C]\00\00\00\03\00\0A\00\01\00\11\C2\18\00,\0B\00\01\00 \9C\01\18\00,\09\00\01\00\11\DC\18\00,\04\00\01\00\11\FA\18\00,\07\00\01\00g2\00\00\00\12\10x\00\03#\00f\FF\FF\FF\FF(\00\0C\00\00\01\00@\03\00\04|\08\00\D1\0F\0C\81\80\80(\00\08\FF\81\80(\08\0B\00\06(\00/0\00\01\00\03!\F0\06\07\00 \00\04\9B\00R\04\14\00\00\00E\002\04|\01\18\000/\08\00#\00\10\0E\0C\00\12#\0C\00\00\01\00'\04\12\0C\00\17\11\0C\00!7\04 \05\F1\08\015\00\00\04\0A\08\00\03\00\00\00`\01(\00\03\19(\00\04\17\0C$\00u\06\00 \00\00\F0!\10\00u\05\00\1C\00\00\F0\11\10\009\04\00\18\10\009\03\00\14\10\009\02\00\10\10\009\01\00\08P\00\01\01\00\C1\F0\11\00\03\1B\FF\00\04\1C\0C\00P\98\05\82\00\00P\06\00\00\04\1E\84\01#K\00\01\00\B1\02\02\08\10\0A/\22\00\00\00\08\10\00#\00\08\08\00\13\10\08\00\13\18\08\00\13 \08\00\13(\08\00\130\08\00\228\08\84\00\13\00\08\00\13\08\08\00\13\10\08\00\13\18\08\00\13 \08\00\13(\08\00\130\08\00\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\A2\00\00\00\14,\00\00\00H\00\01\00\11\02h\01\0F\01\00\FF\B0@$v\01\FF\7F\04\B2\FF\00\8E\07\00\C4\0F\00\19y\00\01\00\10%\8B\02a\0E\00\19y\03\00\01\00\10!-\00B\0E\00$z\B5\04\90\03\02\8E\07\00\CA\1F\00\0C\10\00\C5^\00\00p`\F0\03\00\DA\0F\00M\9B\04\F0\0C\80\03\00\EA\0F\00\06{\04\00\00_\00\00\00\90 \00\00\22\0E\00\19x\05\FF\1F\1F\00\D3\14\01\00\00\E2\0F\00$r\02\FF\FF\00\80\00 \E2\0FP\00\10\FF0\00@pP\F4\03\10\00\81\B9z\04\00\00F\00\00#\05p\E2\0F\00\11r\05\05?\00\B2\FF8\8F\07\00\C6\0F\00\08s\04\F6\04\10\10\A0\00\F1\06\1E\00\10x\03\04\FE\FF\FF\0F\FF\E0\FF\07\00\CC\1F\00\05s\03$\04\00\C0\03\01\C0\00!r\07p\00\C0\03\0A\8E\07\00\C8\1F\00$z\07\07p\00\10\FF\D0\00\81\C8\0F\00'r\07\03\07\E0\02\02\90\00@\19x\02\FF\10\00\10\05\B0\00\80\E4\0F\00\12x\05\05\80\F1\04!\C0\8E\80\00T'r\07\07\02\C0\00\10\C8\D0\00\11\09`\00\10\07`\00`\E4\0F\00$x\05\BC\03$\00\05\10\000z\03\09p\00\22\02\02@\00@\19x\00\FF\A0\00\22\05\16`\00f\0Cz\00\03\00_P\010\10\0A\03\10\00\11\80\D0\00p\E4\0F\00\10\08\07\07P\00\04\10\00\060\00\12\F20\00\1A\18 \001\12\AA\07P\01 \FF3\B0\01\00\A0\00\1B\03\A0\001\00\07\10\D1\03\05\A0\00\15\03\A0\00\10\E2\F0\00 \02\05\00\01#\FF\C0@\00Sx\09\03\08\00 \00\11\CA\80\003\09\00X\80\00\22\C8\0F\10\02p\\\00\00p\10\F2\04\90\00T$\14\03\FF\04 \01\00`\000\1A\02\000\00\13\09p\01c%\16\02\02\00ZP\02q\CC\0F\00\81\19\02\02\D0\01\C4\19\1E\0C\00\A2\00\00\10x\06\00\08\D0\00\000\02\92t\04\FF\00\00\80\FF\FF\00\A0\001\1Cx\00\01\001p\F0\F00\02E$x\07\05p\00q\E2\0F\04\0Cz\00\06\90\00#`\F6 \00\16\06 \00\80\C6\0F\00\0Cx\00\05?@\00\92D\F6\01\00\E4\0F\04\10xF\07\02@\01B\1F\04\10x\EC\04\04\80\00@!\12\04\02\81\00\01\15\00!\E2O@\00\11\07@\00 \F2\04\D0\00c\88s\00\07\04\00\EE\05f\E8\0F\00\1D{\00\01\00u\EC\0F\00\84\B9\0B\06\CE\05\84(\0E\00\84\B9\02\06\000\00qb\0E\00\0B\B2\00\0B\F6\07`\80\F4\03\00\C4\1F\10\00\11\02\10\00\A3\C0\F8\03\00\E4/\00\1C\B8\00\F0\00%p\01\00\02\01\E0\00B\F4\03\00\C8\D0\00\11\1F\90\00\C3t\01\00\CE\0F\00\08\82\0B\0B\02\00\17\07\1B\E40\01\8F\C6\0F\00\88\B3\00\06\0B\B0\00\09f\A9\03\06\00\80\00\B0\00\1B\A9\B0\001\A2\00\03`\03\22\80\F6\B0\00H\A2\00\02\03\B0\00$\A8\00p\00\04\B0\00\15\00\90\01\03\B0\00\14\0F\80\01\01\B0\00/\03\03\B0\00\0AO\A3\00\06\03`\01\0AG\07\06\00@\B0\00\13\B9\A7\06\06`\01*\07\07`\01/\00\07`\01\05\10\DA\90\00\02/\00\01\90\00\12\CA0\01\1F\07\80\00\089M\19\00\00\05G$t\02\FF\90\02A\00\84y\05\9F\08\01@\00\96&\0E\00%v\02\09\00`p\03'\84y\B0\00fh\0E\00\81y\08\10\03bb\05\00\0Br\00\01\05\22\80\F0\C0\001r\00\00\10\00\92\C0\F2\03\00\D6/\00\08\82\1F\00p\00\00\80\04\00\C8O \00\11\08\11\00 @\F0\00\03$\0EF\E0\05\00\A0\00b\E6\0F\00\08r\09 \00\00\01\00p\CC\0F\00\A9s\09\02{\00\C0\09\E1\1E\00\00\A4\0E\00\0Cr\00\09\10\00 pR@\00QO\00$r\08@\04\10\09\D0\00\80\D8\0F\00G\09\00\00\90\10\05!\FF\83\F0\00*My\00\01TGy\00\00\F0 \00f\C0\0F\00\18y\00\01\00\0F\10\00p\0F\01\00-\11\01@\0A\0E\01\00\22@\00\01\00=\9E\01\000\00\08\01\00\1F\0B@\00\04\13\DE)\00?\09\02\00@\00\0A\22\13\00\A0\04\0C\01\00\13\E8U\00\03\7F\03\01$\00\13\05W\02\00\01\00\22\18\00\01\00.q\01T\00\00\01\00\11\90u\02O\00\00p\00\80\00\0B\1F)'\00\03\03\95\02$\00\00\18\0C\04\E4\00*\04\00\01\00\1Fc@\00\04*0\05\C0\00\13\03\03\08\0C@\00!\8F\01D\01\0D@\00\13\D8@\00*\D8\00\01\00\1B\08\08\00?~\01\00N\0D\002\00\00\B0\86\03\01W\08\04\80\00\17\048\00\04\18\00\138@\01\0C\84\01\13\C0@\00\17\881\01\0F\C0\00\01\132T\01\15\06R\00\03>\03\1A\08\98\0D\11\03$\00J\00\0E\80\00\01\00\13\97\94\00*\03\00\01\00\040\12/\00\02\80\00\0B\13\06\AB\01\04h\12\0C\01\00\1B\A8\08\00\04\97\00\13\018\00\04\E8\00\0C\01\00*\C0\08\08\00\088\00\18\06\A0\00\0F\01\00\05\03\B8\00\80\08\00\00\00\00\00\00\00\00\00\00\00\00"} {
  llvm.mlir.global internal @__wg_main_kColReduction_reduce__4_1_0___8w16h_1_0() {addr_space = 3 : i32} : !llvm.array<128 x f32>
  llvm.func @__nv_fabsf(f32) -> f32
  llvm.func @main_kColReduction_reduce__4_1_0___8w16h_1(%arg0: i32, %arg1: !llvm.ptr<f32>, %arg2: i32, %arg3: i32, %arg4: i32, %arg5: i32, %arg6: !llvm.ptr<f32>) attributes {disc.elimargs = [1 : index, 3 : index, 5 : index, 6 : index, 7 : index, 8 : index, 9 : index, 13 : index, 15 : index, 16 : index, 17 : index], gpu.kernel, nvvm.kernel} {
    %0 = llvm.mlir.constant(8 : index) : i32
    %1 = llvm.mlir.constant(16 : index) : i32
    %2 = llvm.mlir.constant(2 : index) : i32
    %3 = llvm.mlir.constant(0xFF800000 : f32) : f32
    %4 = llvm.mlir.constant(64 : index) : i32
    %5 = llvm.mlir.constant(4 : index) : i32
    %6 = llvm.mlir.constant(32 : index) : i32
    %7 = llvm.mlir.constant(128 : index) : i32
    %8 = llvm.mlir.constant(0 : index) : i32
    %9 = llvm.mlir.addressof @__wg_main_kColReduction_reduce__4_1_0___8w16h_1_0 : !llvm.ptr<array<128 x f32>, 3>
    %10 = llvm.getelementptr %9[0, 0] : (!llvm.ptr<array<128 x f32>, 3>) -> !llvm.ptr<f32, 3>
    %11 = nvvm.read.ptx.sreg.ctaid.x : i32
    %12 = nvvm.read.ptx.sreg.tid.x : i32
    llvm.br ^bb1
  ^bb1:  // pred: ^bb0
    %13 = llvm.mul %11, %arg3  : i32
    %14 = llvm.add %12, %13  : i32
    %15 = llvm.icmp "ult" %14, %arg4 : i32
    llvm.cond_br %15, ^bb2, ^bb16
  ^bb2:  // pred: ^bb1
    %16 = llvm.srem %14, %7  : i32
    %17 = llvm.sdiv %14, %7  : i32
    %18 = llvm.udiv %16, %0  : i32
    %19 = llvm.urem %16, %0  : i32
    %20 = llvm.udiv %17, %arg5  : i32
    %21 = llvm.urem %17, %arg5  : i32
    %22 = llvm.mul %20, %1  : i32
    %23 = llvm.add %22, %18  : i32
    %24 = llvm.mul %21, %0  : i32
    %25 = llvm.add %24, %19  : i32
    %26 = llvm.icmp "ult" %23, %arg2 : i32
    %27 = llvm.icmp "ult" %25, %arg0 : i32
    %28 = llvm.and %26, %27  : i1
    llvm.cond_br %28, ^bb3, ^bb4
  ^bb3:  // pred: ^bb2
    %29 = llvm.mul %23, %arg0  : i32
    %30 = llvm.add %29, %25  : i32
    %31 = llvm.getelementptr %arg1[%30] : (!llvm.ptr<f32>, i32) -> !llvm.ptr<f32>
    %32 = llvm.load %31 : !llvm.ptr<f32>
    %33 = llvm.call @__nv_fabsf(%32) : (f32) -> f32
    %34 = llvm.getelementptr %10[%16] : (!llvm.ptr<f32, 3>, i32) -> !llvm.ptr<f32, 3>
    llvm.store %33, %34 : !llvm.ptr<f32, 3>
    llvm.br ^bb5
  ^bb4:  // pred: ^bb2
    %35 = llvm.getelementptr %10[%16] : (!llvm.ptr<f32, 3>, i32) -> !llvm.ptr<f32, 3>
    llvm.store %3, %35 : !llvm.ptr<f32, 3>
    llvm.br ^bb5
  ^bb5:  // 2 preds: ^bb3, ^bb4
    nvvm.barrier0
    %36 = llvm.icmp "ult" %18, %0 : i32
    %37 = llvm.add %23, %0  : i32
    %38 = llvm.icmp "ult" %37, %arg2 : i32
    %39 = llvm.and %36, %38  : i1
    llvm.cond_br %39, ^bb6, ^bb7
  ^bb6:  // pred: ^bb5
    %40 = llvm.add %16, %4  : i32
    %41 = llvm.getelementptr %10[%16] : (!llvm.ptr<f32, 3>, i32) -> !llvm.ptr<f32, 3>
    %42 = llvm.load %41 : !llvm.ptr<f32, 3>
    %43 = llvm.getelementptr %10[%40] : (!llvm.ptr<f32, 3>, i32) -> !llvm.ptr<f32, 3>
    %44 = llvm.load %43 : !llvm.ptr<f32, 3>
    %45 = llvm.fcmp "ugt" %42, %44 : f32
    %46 = llvm.select %45, %42, %44 : i1, f32
    %47 = llvm.fcmp "uno" %44, %44 : f32
    %48 = llvm.select %47, %44, %46 : i1, f32
    %49 = llvm.getelementptr %10[%16] : (!llvm.ptr<f32, 3>, i32) -> !llvm.ptr<f32, 3>
    llvm.store %48, %49 : !llvm.ptr<f32, 3>
    llvm.br ^bb7
  ^bb7:  // 2 preds: ^bb5, ^bb6
    nvvm.barrier0
    %50 = llvm.icmp "ult" %18, %5 : i32
    %51 = llvm.add %23, %5  : i32
    %52 = llvm.icmp "ult" %51, %arg2 : i32
    %53 = llvm.and %50, %52  : i1
    llvm.cond_br %53, ^bb8, ^bb9
  ^bb8:  // pred: ^bb7
    %54 = llvm.add %16, %6  : i32
    %55 = llvm.getelementptr %10[%16] : (!llvm.ptr<f32, 3>, i32) -> !llvm.ptr<f32, 3>
    %56 = llvm.load %55 : !llvm.ptr<f32, 3>
    %57 = llvm.getelementptr %10[%54] : (!llvm.ptr<f32, 3>, i32) -> !llvm.ptr<f32, 3>
    %58 = llvm.load %57 : !llvm.ptr<f32, 3>
    %59 = llvm.fcmp "ugt" %56, %58 : f32
    %60 = llvm.select %59, %56, %58 : i1, f32
    %61 = llvm.fcmp "uno" %58, %58 : f32
    %62 = llvm.select %61, %58, %60 : i1, f32
    %63 = llvm.getelementptr %10[%16] : (!llvm.ptr<f32, 3>, i32) -> !llvm.ptr<f32, 3>
    llvm.store %62, %63 : !llvm.ptr<f32, 3>
    llvm.br ^bb9
  ^bb9:  // 2 preds: ^bb7, ^bb8
    nvvm.barrier0
    %64 = llvm.icmp "ult" %18, %2 : i32
    %65 = llvm.add %23, %2  : i32
    %66 = llvm.icmp "ult" %65, %arg2 : i32
    %67 = llvm.and %64, %66  : i1
    llvm.cond_br %67, ^bb10, ^bb11
  ^bb10:  // pred: ^bb9
    %68 = llvm.add %16, %1  : i32
    %69 = llvm.getelementptr %10[%16] : (!llvm.ptr<f32, 3>, i32) -> !llvm.ptr<f32, 3>
    %70 = llvm.load %69 : !llvm.ptr<f32, 3>
    %71 = llvm.getelementptr %10[%68] : (!llvm.ptr<f32, 3>, i32) -> !llvm.ptr<f32, 3>
    %72 = llvm.load %71 : !llvm.ptr<f32, 3>
    %73 = llvm.fcmp "ugt" %70, %72 : f32
    %74 = llvm.select %73, %70, %72 : i1, f32
    %75 = llvm.fcmp "uno" %72, %72 : f32
    %76 = llvm.select %75, %72, %74 : i1, f32
    %77 = llvm.getelementptr %10[%16] : (!llvm.ptr<f32, 3>, i32) -> !llvm.ptr<f32, 3>
    llvm.store %76, %77 : !llvm.ptr<f32, 3>
    llvm.br ^bb11
  ^bb11:  // 2 preds: ^bb9, ^bb10
    nvvm.barrier0
    %78 = llvm.icmp "eq" %18, %8 : i32
    %79 = llvm.and %78, %28  : i1
    llvm.cond_br %79, ^bb12, ^bb15
  ^bb12:  // pred: ^bb11
    %80 = llvm.add %16, %0  : i32
    %81 = llvm.getelementptr %10[%16] : (!llvm.ptr<f32, 3>, i32) -> !llvm.ptr<f32, 3>
    %82 = llvm.load %81 : !llvm.ptr<f32, 3>
    %83 = llvm.getelementptr %10[%80] : (!llvm.ptr<f32, 3>, i32) -> !llvm.ptr<f32, 3>
    %84 = llvm.load %83 : !llvm.ptr<f32, 3>
    %85 = llvm.fcmp "ugt" %82, %84 : f32
    %86 = llvm.select %85, %82, %84 : i1, f32
    %87 = llvm.fcmp "uno" %84, %84 : f32
    %88 = llvm.select %87, %84, %86 : i1, f32
    %89 = llvm.getelementptr %arg6[%25] : (!llvm.ptr<f32>, i32) -> !llvm.ptr<f32>
    %90 = llvm.load %89 : !llvm.ptr<f32>
    llvm.br ^bb13(%90 : f32)
  ^bb13(%91: f32):  // 2 preds: ^bb12, ^bb13
    %92 = llvm.fcmp "ogt" %91, %88 : f32
    %93 = llvm.select %92, %91, %88 : i1, f32
    %94 = llvm.bitcast %89 : !llvm.ptr<f32> to !llvm.ptr<i32>
    %95 = llvm.bitcast %91 : f32 to i32
    %96 = llvm.bitcast %93 : f32 to i32
    %97 = llvm.cmpxchg %94, %95, %96 acq_rel monotonic : !llvm.ptr<i32>, i32
    %98 = llvm.extractvalue %97[0] : !llvm.struct<(i32, i1)> 
    %99 = llvm.bitcast %98 : i32 to f32
    %100 = llvm.extractvalue %97[1] : !llvm.struct<(i32, i1)> 
    llvm.cond_br %100, ^bb14, ^bb13(%99 : f32)
  ^bb14:  // pred: ^bb13
    llvm.br ^bb15
  ^bb15:  // 2 preds: ^bb11, ^bb14
    llvm.br ^bb16
  ^bb16:  // 2 preds: ^bb1, ^bb15
    llvm.return
  }
}

// -----// IR Dump After Canonicalizer (canonicalize) //----- //
func.func @main(%arg0: !disc_ral.context) attributes {tf.entry_function = {input_placements = "gpu", inputs = "input0", output_placements = "gpu", outputs = "output0"}} {
  %c-1 = arith.constant -1 : index
  %c128 = arith.constant 128 : index
  %c16 = arith.constant 16 : index
  %c32 = arith.constant 32 : index
  %c8 = arith.constant 8 : index
  %0 = llvm.mlir.constant(0 : i32) : i32
  %c0 = arith.constant 0 : index
  %c1 = arith.constant 1 : index
  %c2 = arith.constant 2 : index
  %c256 = arith.constant 256 : index
  %c108 = arith.constant 108 : index
  %1 = "disc_ral.dispatch"(%arg0, %c0) {backend_config = "", call_target_name = "ral_recv_input", device = "cpu", has_side_effect = false} : (!disc_ral.context, index) -> memref<?x?x?xf32, "gpu">
  %dim = memref.dim %1, %c2 : memref<?x?x?xf32, "gpu">
  %dim_0 = memref.dim %1, %c1 : memref<?x?x?xf32, "gpu">
  %dim_1 = memref.dim %1, %c0 : memref<?x?x?xf32, "gpu">
  %2 = arith.index_cast %dim_0 : index to i32
  %3 = arith.index_cast %dim : index to i32
  %4 = arith.muli %2, %3 : i32
  %5 = arith.index_cast %4 : i32 to index
  %alloc = memref.alloc(%5) {kDiscSymbolicDimAttr = [@S3]} : memref<?xf32, "gpu">
  %6 = arith.muli %dim_1, %5 : index
  %7 = arith.addi %6, %c-1 : index
  %8 = arith.divsi %7, %c256 : index
  %9 = arith.addi %8, %c1 : index
  %10 = arith.subi %c0, %6 : index
  %11 = arith.divsi %10, %c256 : index
  %12 = arith.subi %c0, %11 : index
  %13 = arith.cmpi sgt, %6, %c0 : index
  %14 = arith.select %13, %9, %12 : index
  %15 = arith.cmpi sgt, %14, %c108 : index
  scf.if %15 {
    %18 = affine.apply affine_map<()[s0] -> (s0 ceildiv 256)>()[%5]
    gpu.launch_func  @main_kernel::@main_kColReduction_reduce__4_1_0___8w32h blocks in (%18, %c1, %c1) threads in (%c256, %c1, %c1) args(%c256 : index, %5 : index, %alloc : memref<?xf32, "gpu">)
    %19 = arith.addi %5, %c-1 : index
    %20 = arith.divsi %19, %c8 : index
    %21 = arith.addi %20, %c1 : index
    %22 = arith.subi %c0, %5 : index
    %23 = arith.divsi %22, %c8 : index
    %24 = arith.subi %c0, %23 : index
    %25 = arith.cmpi sgt, %5, %c0 : index
    %26 = arith.select %25, %21, %24 : index
    %27 = arith.addi %dim_1, %c-1 : index
    %28 = arith.divsi %27, %c32 : index
    %29 = arith.addi %28, %c1 : index
    %30 = arith.subi %c0, %dim_1 : index
    %31 = arith.divsi %30, %c32 : index
    %32 = arith.subi %c0, %31 : index
    %33 = arith.cmpi sgt, %dim_1, %c0 : index
    %34 = arith.select %33, %29, %32 : index
    %35 = arith.muli %26, %34 : index
    %36 = arith.muli %35, %c256 : index
    %37 = affine.apply affine_map<(d0) -> (d0 ceildiv 256)>(%36)
    gpu.launch_func  @main_kernel_0::@main_kColReduction_reduce__4_1_0___8w32h_1 blocks in (%37, %c1, %c1) threads in (%c256, %c1, %c1) args(%5 : index, %1 : memref<?x?x?xf32, "gpu">, %c256 : index, %36 : index, %26 : index, %alloc : memref<?xf32, "gpu">)
  } else {
    %18 = affine.apply affine_map<()[s0] -> (s0 ceildiv 128)>()[%5]
    gpu.launch_func  @main_kernel_1::@main_kColReduction_reduce__4_1_0___8w16h blocks in (%18, %c1, %c1) threads in (%c128, %c1, %c1) args(%c128 : index, %5 : index, %alloc : memref<?xf32, "gpu">)
    %19 = arith.addi %5, %c-1 : index
    %20 = arith.divsi %19, %c8 : index
    %21 = arith.addi %20, %c1 : index
    %22 = arith.subi %c0, %5 : index
    %23 = arith.divsi %22, %c8 : index
    %24 = arith.subi %c0, %23 : index
    %25 = arith.cmpi sgt, %5, %c0 : index
    %26 = arith.select %25, %21, %24 : index
    %27 = arith.addi %dim_1, %c-1 : index
    %28 = arith.divsi %27, %c16 : index
    %29 = arith.addi %28, %c1 : index
    %30 = arith.subi %c0, %dim_1 : index
    %31 = arith.divsi %30, %c16 : index
    %32 = arith.subi %c0, %31 : index
    %33 = arith.cmpi sgt, %dim_1, %c0 : index
    %34 = arith.select %33, %29, %32 : index
    %35 = arith.muli %26, %34 : index
    %36 = arith.muli %35, %c128 : index
    %37 = affine.apply affine_map<(d0) -> (d0 ceildiv 128)>(%36)
    gpu.launch_func  @main_kernel_2::@main_kColReduction_reduce__4_1_0___8w16h_1 blocks in (%37, %c1, %c1) threads in (%c128, %c1, %c1) args(%5 : index, %1 : memref<?x?x?xf32, "gpu">, %c128 : index, %36 : index, %26 : index, %alloc : memref<?xf32, "gpu">)
  }
  %16 = llvm.inttoptr %0 : i32 to !llvm.ptr<i8>
  %alloca = memref.alloca() : memref<2xindex, "cpu">
  memref.store %dim_0, %alloca[%c0] : memref<2xindex, "cpu">
  memref.store %dim, %alloca[%c1] : memref<2xindex, "cpu">
  %17 = "disc_ral.dispatch"(%arg0, %16, %alloc, %alloca) {backend_config = "", call_target_name = "inc_ref", device = "gpu", has_side_effect = false} : (!disc_ral.context, !llvm.ptr<i8>, memref<?xf32, "gpu">, memref<2xindex, "cpu">) -> memref<?x?xf32, "gpu">
  %reinterpret_cast = memref.reinterpret_cast %17 to offset: [0], sizes: [%dim_0, %dim], strides: [%dim, 1] {kDiscSymbolicDimAttr = [@S1, @S2]} : memref<?x?xf32, "gpu"> to memref<?x?xf32, "gpu">
  memref.dealloc %alloc : memref<?xf32, "gpu">
  "disc_ral.dispatch"(%arg0, %c0, %reinterpret_cast) {backend_config = "", call_target_name = "ral_send_output", device = "cpu", has_side_effect = false} : (!disc_ral.context, index, memref<?x?xf32, "gpu">) -> ()
  return
}

// -----// IR Dump After SCFToControlFlow (convert-scf-to-cf) //----- //
#map = affine_map<()[s0] -> (s0 ceildiv 256)>
#map1 = affine_map<(d0) -> (d0 ceildiv 256)>
#map2 = affine_map<()[s0] -> (s0 ceildiv 128)>
#map3 = affine_map<(d0) -> (d0 ceildiv 128)>
module attributes {gpu.container_module} {
  func.func @main(%arg0: !disc_ral.context) attributes {tf.entry_function = {input_placements = "gpu", inputs = "input0", output_placements = "gpu", outputs = "output0"}} {
    %c-1 = arith.constant -1 : index
    %c128 = arith.constant 128 : index
    %c16 = arith.constant 16 : index
    %c32 = arith.constant 32 : index
    %c8 = arith.constant 8 : index
    %0 = llvm.mlir.constant(0 : i32) : i32
    %c0 = arith.constant 0 : index
    %c1 = arith.constant 1 : index
    %c2 = arith.constant 2 : index
    %c256 = arith.constant 256 : index
    %c108 = arith.constant 108 : index
    %1 = "disc_ral.dispatch"(%arg0, %c0) {backend_config = "", call_target_name = "ral_recv_input", device = "cpu", has_side_effect = false} : (!disc_ral.context, index) -> memref<?x?x?xf32, "gpu">
    %dim = memref.dim %1, %c2 : memref<?x?x?xf32, "gpu">
    %dim_0 = memref.dim %1, %c1 : memref<?x?x?xf32, "gpu">
    %dim_1 = memref.dim %1, %c0 : memref<?x?x?xf32, "gpu">
    %2 = arith.index_cast %dim_0 : index to i32
    %3 = arith.index_cast %dim : index to i32
    %4 = arith.muli %2, %3 : i32
    %5 = arith.index_cast %4 : i32 to index
    %alloc = memref.alloc(%5) {kDiscSymbolicDimAttr = [@S3]} : memref<?xf32, "gpu">
    %6 = arith.muli %dim_1, %5 : index
    %7 = arith.addi %6, %c-1 : index
    %8 = arith.divsi %7, %c256 : index
    %9 = arith.addi %8, %c1 : index
    %10 = arith.subi %c0, %6 : index
    %11 = arith.divsi %10, %c256 : index
    %12 = arith.subi %c0, %11 : index
    %13 = arith.cmpi sgt, %6, %c0 : index
    %14 = arith.select %13, %9, %12 : index
    %15 = arith.cmpi sgt, %14, %c108 : index
    cf.cond_br %15, ^bb1, ^bb2
  ^bb1:  // pred: ^bb0
    %16 = affine.apply #map()[%5]
    gpu.launch_func  @main_kernel::@main_kColReduction_reduce__4_1_0___8w32h blocks in (%16, %c1, %c1) threads in (%c256, %c1, %c1) args(%c256 : index, %5 : index, %alloc : memref<?xf32, "gpu">)
    %17 = arith.addi %5, %c-1 : index
    %18 = arith.divsi %17, %c8 : index
    %19 = arith.addi %18, %c1 : index
    %20 = arith.subi %c0, %5 : index
    %21 = arith.divsi %20, %c8 : index
    %22 = arith.subi %c0, %21 : index
    %23 = arith.cmpi sgt, %5, %c0 : index
    %24 = arith.select %23, %19, %22 : index
    %25 = arith.addi %dim_1, %c-1 : index
    %26 = arith.divsi %25, %c32 : index
    %27 = arith.addi %26, %c1 : index
    %28 = arith.subi %c0, %dim_1 : index
    %29 = arith.divsi %28, %c32 : index
    %30 = arith.subi %c0, %29 : index
    %31 = arith.cmpi sgt, %dim_1, %c0 : index
    %32 = arith.select %31, %27, %30 : index
    %33 = arith.muli %24, %32 : index
    %34 = arith.muli %33, %c256 : index
    %35 = affine.apply #map1(%34)
    gpu.launch_func  @main_kernel_0::@main_kColReduction_reduce__4_1_0___8w32h_1 blocks in (%35, %c1, %c1) threads in (%c256, %c1, %c1) args(%5 : index, %1 : memref<?x?x?xf32, "gpu">, %c256 : index, %34 : index, %24 : index, %alloc : memref<?xf32, "gpu">)
    cf.br ^bb3
  ^bb2:  // pred: ^bb0
    %36 = affine.apply #map2()[%5]
    gpu.launch_func  @main_kernel_1::@main_kColReduction_reduce__4_1_0___8w16h blocks in (%36, %c1, %c1) threads in (%c128, %c1, %c1) args(%c128 : index, %5 : index, %alloc : memref<?xf32, "gpu">)
    %37 = arith.addi %5, %c-1 : index
    %38 = arith.divsi %37, %c8 : index
    %39 = arith.addi %38, %c1 : index
    %40 = arith.subi %c0, %5 : index
    %41 = arith.divsi %40, %c8 : index
    %42 = arith.subi %c0, %41 : index
    %43 = arith.cmpi sgt, %5, %c0 : index
    %44 = arith.select %43, %39, %42 : index
    %45 = arith.addi %dim_1, %c-1 : index
    %46 = arith.divsi %45, %c16 : index
    %47 = arith.addi %46, %c1 : index
    %48 = arith.subi %c0, %dim_1 : index
    %49 = arith.divsi %48, %c16 : index
    %50 = arith.subi %c0, %49 : index
    %51 = arith.cmpi sgt, %dim_1, %c0 : index
    %52 = arith.select %51, %47, %50 : index
    %53 = arith.muli %44, %52 : index
    %54 = arith.muli %53, %c128 : index
    %55 = affine.apply #map3(%54)
    gpu.launch_func  @main_kernel_2::@main_kColReduction_reduce__4_1_0___8w16h_1 blocks in (%55, %c1, %c1) threads in (%c128, %c1, %c1) args(%5 : index, %1 : memref<?x?x?xf32, "gpu">, %c128 : index, %54 : index, %44 : index, %alloc : memref<?xf32, "gpu">)
    cf.br ^bb3
  ^bb3:  // 2 preds: ^bb1, ^bb2
    %56 = llvm.inttoptr %0 : i32 to !llvm.ptr<i8>
    %alloca = memref.alloca() : memref<2xindex, "cpu">
    memref.store %dim_0, %alloca[%c0] : memref<2xindex, "cpu">
    memref.store %dim, %alloca[%c1] : memref<2xindex, "cpu">
    %57 = "disc_ral.dispatch"(%arg0, %56, %alloc, %alloca) {backend_config = "", call_target_name = "inc_ref", device = "gpu", has_side_effect = false} : (!disc_ral.context, !llvm.ptr<i8>, memref<?xf32, "gpu">, memref<2xindex, "cpu">) -> memref<?x?xf32, "gpu">
    %reinterpret_cast = memref.reinterpret_cast %57 to offset: [0], sizes: [%dim_0, %dim], strides: [%dim, 1] {kDiscSymbolicDimAttr = [@S1, @S2]} : memref<?x?xf32, "gpu"> to memref<?x?xf32, "gpu">
    memref.dealloc %alloc : memref<?xf32, "gpu">
    "disc_ral.dispatch"(%arg0, %c0, %reinterpret_cast) {backend_config = "", call_target_name = "ral_send_output", device = "cpu", has_side_effect = false} : (!disc_ral.context, index, memref<?x?xf32, "gpu">) -> ()
    return
  }
  gpu.module @main_kernel attributes {gpu.binary = "P\EDU\BA\01\00\10\008\04\00\00\00\00\00\00\02\00\01\01@\00\00\00\F8\03\00\00\00\00\00\00\F8\03\00\00\00\00\00\00\07\00\01\00P\00\00\00\00\00\00\00\00\00\00\00\11 \00\00\00\00\00\00\00\00\00\00\00\00\00\00\E8\0B\00\00\00\00\00\00\A2\7FELF\02\01\013\07\00\01\00f\02\00\BE\00q\00\01\00!@\0B\07\001\00\80\08\07\00\F5\0E\00P\05P\00@\008\00\03\00@\00\0B\00\01\00\00.shstrtab\00.\08\00'ym\08\00\F0\1B_shndx\00.nv.info\00.text.main_kColReduction_r\0A\00\F5\01e__4_1_0___8w32h8\00\0F2\00\1Boshared4\00\1B\9Fconstant07\00\18\FA\01debug_frame\00.rel\11\00!nv\14\00\11aC\00\0F+\01 \0F\88\00\15\0FT\01\BAo_param[\01\1C\0F\01\00\06\8C[\00\00\00\03\00\0A\00\01\00\11\F0\18\00,\09\00\01\00 .\01\18\00,\04\00\01\00\11L\18\00,\07\00\01\00f2\00\00\00\12\10`\00!\80\01\07\00v\00\FF\FF\FF\FF(\00\0C\00\00\01\00@\03\00\04|\08\00\D1\0F\0C\81\80\80(\00\08\FF\81\80(\08\0B\00\06(\00/0\00\01\00\03\13pX\00\10\04\9B\00R\04\14\00\00\00E\00\01\0B\00\00\13\00p/\08\00\05\00\00\00\A7\03\22\04#\0C\00\00\01\00'\04\12\0C\00\17\11\0C\00!7\04\E8\03\F3\08\015\00\00\04\0A\08\00\02\00\00\00`\01\10\00\03\19\10\00\04\17\0C\0C\04U\08\00\00\F0!\10\00\10\01\18\01%\F0\11\10\00\01\01\00\F2\02\F0\11\00\03\1B\FF\00\04\1C\08\00P\00\00\00\B0\00\01\00#K\00\01\00s\02\02\08\10\0A/\22\9B\00\00\07\00\03\08\00\13\10\08\00\13\18\08\00\13 \08\00\13(\08\00\130\08\00\228\08|\00\13\00\08\00\13\08\08\00\13\10\08\00\13\18\08\00\13 \08\00\13(\08\00\130\08\00\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\A2\00\00\00\14,\00\00\00H\00\01\00\000\01/\05\00\01\00\FF\B0A\02z\01\00\1F\04\B1\0F\00\00\00\C4\0F\00\19y\02\00\01\00\10%\8B\02Q\0E\00\19y\03\0F\00\F5\1A\00!\00\00\00$\0E\00$z\02\02\00X\00\00\03\02\8E\07\00\CA\1F\00\0Cz\00\02\00Y\00\00p`\F0\03\00\DA\0F\00MS\04\A0\80\03\00\EA\0F\005t\03\FF\B3\03\10\FF\C0\03P\E2\0F\00\02x6\02B\80\FF\00\0F\10\00r\B9z\04\00\00F\00\84\00\94\D0\0F\00%v\02\02\00Z`\00`\0F\00\86y\00\022\00@\04\19\10\0C0\009My\00`\00PGy\00\00\F09\04\A6\FF\83\03\00\C0\0F\00\18y\00\01\00\0F\10\00\90\0F\01\00-\00W\01.\03\00\01\00\22@\00\01\00=+\01\000\00\08\01\00\1F\0B@\00\04\13k)\00\1F[@\00\0C\13\13\0C\04\0C\01\00\13\C8\15\00&\90\000\04#\04\00\85\04\00\F6\04\12\00\01\00\1F\FET\00\00\00\01\00\13X\95\00/p\00\80\00\0B\1F)'\00\03#\00\C8@\00\04P\06\04\E4\00*\04\00\01\00\1Fa@\00\04\13\F81\00&\\\00@\00\1F\0A@\00\00!\1C\01D\01\0D@\00\13X)\00*\D8\00\01\00\1B\08\08\00?\0B\01\00\86\07\00Q\00\000\05\00\01\00&\10\00\80\00\17\048\00\04\18\00\13\C7\14\01\0C\84\01*@\058\07\1F\00\C0\00\04\132@\00+\06\00\01\00\1A\07\D0\07\12\03\F0\05:\08\80\00\01\00\13\06\08\06\04(\0B\0C\01\00*\A8\00\08\00\04\F8\00\14\018\00/\05\00\01\00\029@\03\00\08\00\088\00/\06\00\01\00\17\80\08\00\00\00\00\00\00\00"} {
    llvm.func @main_kColReduction_reduce__4_1_0___8w32h(%arg0: i32, %arg1: i32, %arg2: !llvm.ptr<f32>) attributes {disc.elimargs = [2 : index, 4 : index, 5 : index, 6 : index], gpu.kernel, nvvm.kernel} {
      %0 = llvm.mlir.constant(0xFF800000 : f32) : f32
      %1 = nvvm.read.ptx.sreg.ctaid.x : i32
      %2 = nvvm.read.ptx.sreg.tid.x : i32
      llvm.br ^bb1
    ^bb1:  // pred: ^bb0
      %3 = llvm.mul %1, %arg0  : i32
      %4 = llvm.add %2, %3  : i32
      %5 = llvm.icmp "ult" %4, %arg1 : i32
      llvm.cond_br %5, ^bb2, ^bb3
    ^bb2:  // pred: ^bb1
      %6 = llvm.getelementptr %arg2[%4] : (!llvm.ptr<f32>, i32) -> !llvm.ptr<f32>
      llvm.store %0, %6 : !llvm.ptr<f32>
      llvm.br ^bb3
    ^bb3:  // 2 preds: ^bb1, ^bb2
      llvm.return
    }
  }
  gpu.module @main_kernel_0 attributes {gpu.binary = "P\EDU\BA\01\00\10\00@\08\00\00\00\00\00\00\02\00\01\01@\00\00\00\00\08\00\00\00\00\00\00\F9\07\00\00\00\00\00\00\07\00\01\00P\00\00\00\00\00\00\00\00\00\00\00\11 \00\00\00\00\00\00\00\00\00\00\00\00\00\00(\14\00\00\00\00\00\00\A2\7FELF\02\01\013\07\00\01\00f\02\00\BE\00q\00\01\00#\80\13\08\00\11\10\07\00\F5\0E\00P\05P\00@\008\00\03\00@\00\0C\00\01\00\00.shstrtab\00.\08\00'ym\08\00\F0\1B_shndx\00.nv.info\00.text.main_kColReduction_r\0A\00\F5\03e__4_1_0___8w32h_1:\00\0F4\00\1Doshared6\00\1AOrela\A0\00\1F?rel\D5\00\22\9Fconstant09\00\1A\B2debug_frame{\00\09\11\00!nv\14\00\11aE\00\0F\9E\01 \0F\8A\00\17\0F\C9\01\F4\8F$____wg_3\00\17\00\0C\00/27\02\02'o_param\09\02\1C\0F\01\00\05\8C]\00\00\00\03\00\0A\00\01\00\11\C2\18\00,\0B\00\01\00 \9C\01\18\00,\09\00\01\00\11\DC\18\00,\04\00\01\00\11\FA\18\00,\07\00\01\00g2\00\00\00\12\10x\00\11\08\06\00v\00\FF\FF\FF\FF(\00\0C\00\00\01\00@\03\00\04|\08\00\D1\0F\0C\81\80\80(\00\08\FF\81\80(\08\0B\00\06(\00/0\00\01\00\03\13\F0{\00\10\04\9B\00R\04\14\00\00\00E\002\04\AC\01\18\00\80/\08\00\06\00\00\00\0E\0C\00\12#\0C\00\00\01\00'\04\12\0C\00\17\11\0C\00!7\04 \05\F1\08\015\00\00\04\0A\08\00\03\00\00\00`\01(\00\03\19(\00\04\17\0C$\00u\06\00 \00\00\F0!\10\00u\05\00\1C\00\00\F0\11\10\009\04\00\18\10\009\03\00\14\10\009\02\00\10\10\009\01\00\08P\00\01\01\00\F2\0A\F0\11\00\03\1B\FF\00\04\1C\0C\00P\00\00\00\10\06\00\00\10\07\00\00\04\1E\84\01#K\00\01\00v\02\02\08\10\0A/\22b\01\13\08\08\00\13\10\08\00\13\18\08\00\13 \08\00\13(\08\00\130\08\00\228\08\84\00\13\00\08\00\13\08\08\00\13\10\08\00\13\18\08\00\13 \08\00\13(\08\00\130\08\00\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\A2\00\00\00\14,\00\00\00H\00\01\00\11\02h\01\0F\01\00\FF\B0@$v\01\FF\7F\04\B2\FF\00\8E\07\00\C4\0F\00\19y\00\01\00\10%\8B\02a\0E\00\19y\03\00\01\00\10!-\00B\0E\00$z\B5\04\90\03\02\8E\07\00\CA\1F\00\0C\10\00\C5^\00\00p`\F0\03\00\DA\0F\00M\9B\04\F0\0C\80\03\00\EA\0F\00\06{\04\00\00_\00\00\00\90 \00\00\22\0E\00\19x\05\FF\1F\1F\00\D3\14\01\00\00\E2\0F\00$r\02\FF\FF\00\80\00 \E2\0FP\00\10\FF0\00@pP\F4\03\10\00\81\B9z\04\00\00F\00\00#\05p\E2\0F\00\11r\05\05?\00\B2\FF@\8F\07\00\C6\0F\00\08s\04\F6\04\00 \09\F1\07$\1E\00\10x\03\04\FE\FF\FF\0F\FF\E0\FF\07\00\CC\1F\00\05s\03$\04\00\C0\03\01\C0\00!r\07p\00\C0\03\0A\8E\07\00\C8\1F\00$z\07\07p\00\10\FF\D0\00\81\C8\0F\00'r\07\03\07\E0\02\02\90\00@\19x\02\FF\01\03\10\05\B0\00q\E4\0F\00\12x\05\05\F1\04!\C0\8E\80\00T'r\07\07\02\C0\00\10\C8\D0\00\11\09`\00\10\07`\00`\E4\0F\00$x\05\BC\03$\00\05\10\000z\03\09p\00\22\02\02@\00@\19x\00\FF\A0\00\22\05\16`\00f\0Cz\00\03\00_P\010\10\0A\03\10\00\11\80\D0\00p\E4\0F\00\10\08\07\07P\00\04\10\00\060\00\12\F20\00\1A\18 \001\12\AA\07P\01 \FF3\B0\01\00\A0\00\1B\03\A0\000\00\07  \01\15\02\A0\00\15\03\A0\00\10\E2\F0\00 \02\05\10\01#\FF\C0@\00Sx\09\03\08\00 \00\11\CA\80\003\09\00X\80\00\22\C8\0F\10\02p\\\00\00p\10\F2\04\90\00T$\14\03\FF\04 \01\00`\000\1A\02\000\00\13\09p\01c%\16\02\02\00ZP\02q\CC\0F\00\81\19\02\02\D0\01\C4\19\1E\0C\00\A2\00\00\10x\06\00\10\D0\00\000\02\92t\04\FF\00\00\80\FF\FF\00\A0\001\1Cx\00\01\001p\F0\F00\02E$x\07\05p\00q\E2\0F\04\0Cz\00\06\90\00#`\F6 \00\16\06 \00\80\C6\0F\00\0Cx\00\05\7F@\00\C5D\F6\01\00\E4\0F\04\10x\03\00\08p\00`\1F\00!\12\04\02q\00\01\F1\04!\E2O0\00\11\070\00 \F2\04\C0\00c\88s\00\07\04\00\DE\05f\E8\0F\00\1D{\00\01\00u\EC\0F\00\84\B9\0B\06~\05R\22\0E\00\10x\B0\02\01`\00Q\C4\0F\00\10x\1E\00\03p\00\000\004\02\06\00P\00qb\0E\00\0B\B2\00\0B\06\08\B1\80\F4\03\00\E4\1F\08\0B\B2\00\02\10\00\A3\C0\F8\03\00\E4/\00\1C\B8\00\00\01%p\01\10\02\01\F0\00B\F4\03\00\C8\E0\00\11?\B0\00\C3t\01\00\CE\0F\00\08\82\0B\0B\02\00'\07\1B\E4@\01\8F\C6\0F\00\88\B3\00\06\0B\D0\00\095\A9\03\06\8E\06[(\0E\00\84\A9\B0\001\A2\00\03p\03`\80\F6\03\00\C4\1F\10\00(\02\03\B0\00$\A8\00p\00\04\B0\00\15\04\A0\01\03\B0\00\14\1F\90\01\01\B0\00/\03\03\B0\00\0AO\A3\00\06\03\80\01\0AV\07\06\00\80\00\B0\00\0E`\01$\07\07`\01\00\B0\00O\B2\00\02\07`\01\0B\1C\00`\01\19\0F`\01\02\10\04\0F`\01\09\1F\07`\01\0D\17@\B0\00\13\A9g\07\0F`\01\07\1F\00`\01\06\12\DA@\01\14\00\90\00\1F\CA0\01\0F9M\19\00\C0\05G$t\02\FFP\03A\00\84y\05_\09\01@\00\96&\0E\00%v\02\09\00`0\04'\84y\B0\00fh\0E\00\81y\08\D0\03bb\05\00\0Br\00\C1\05\22\80\F0\C0\001r\00\00\10\00\92\C0\F2\03\00\D6/\00\08\82\1F\00p\00\00\80\04\00\C8O \00\11\08\11\00 @\F0\C0\03$\0EF\A0\06\00\A0\00b\E6\0F\00\08r\09 \00\00\01\00p\CC\0F\00\A9s\09\02{\00\C0\09\E1\1E\00\00\A4\0E\00\0Cr\00\09\10\00 pR@\00QO\00$r\08\00\05\10\09\D0\00\80\D8\0F\00G\09\00\00\90\D0\05!\FF\83\F0\00*My\00\01TGy\00\00\F0 \00f\C0\0F\00\18y\00\01\00\0F\10\00\B0\0F\01\00-#\01\00\80\02\0B\01\00\22@\00\01\00=\9E\01\000\00\08\01\00\1F\0B@\00\04\13\DE)\00?\09\02\00@\00\0A\22\13\00@\05\0C\01\00\13\E8U\00\03\0F\03\01$\00\13\05\97\02\00\01\00\22\18\00\01\00.q\01T\00\00\01\00\11\90\B5\02O\00\00p\00\80\00\0B\1F)'\00\03\03\D5\02$\00\00\18\0D\04\E4\00*\04\00\01\00\1Fc@\00\04*0\05\C0\00\13\03\03\09\0C@\00!\8F\01D\01\0D@\00\13\D8@\00*\D8\00\01\00\1B\08\08\00?~\01\00N\0E\002\00\00\B0\C6\03\01W\09\04\80\00\17\048\00\04\18\00\138@\01\0C\84\01\13\C0@\00\17\881\01\0F\C0\00\01\132T\01\02\E6\07\06\01\00\1B\80\A9\00\11\03$\00J\00\0E\80\00\01\00\13\97#\00*\03\00\01\00\040\13/\00\04\80\00\0B\13\06\AB\01\04h\13\0C\01\00\1B\A8\08\00\17\08\08\02\17\05\E8\00\0C\01\00*\C0\09\08\00\088\00\18\06\A0\00\0F\01\00\05\03\A9\00\80\08\00\00\00\00\00\00\00\00\00\00\00\00\00\00"} {
    llvm.mlir.global internal @__wg_main_kColReduction_reduce__4_1_0___8w32h_1_0() {addr_space = 3 : i32} : !llvm.array<256 x f32>
    llvm.func @__nv_fabsf(f32) -> f32
    llvm.func @main_kColReduction_reduce__4_1_0___8w32h_1(%arg0: i32, %arg1: !llvm.ptr<f32>, %arg2: i32, %arg3: i32, %arg4: i32, %arg5: i32, %arg6: !llvm.ptr<f32>) attributes {disc.elimargs = [1 : index, 3 : index, 5 : index, 6 : index, 7 : index, 8 : index, 9 : index, 13 : index, 15 : index, 16 : index, 17 : index], gpu.kernel, nvvm.kernel} {
      %0 = llvm.mlir.constant(8 : index) : i32
      %1 = llvm.mlir.constant(32 : index) : i32
      %2 = llvm.mlir.constant(2 : index) : i32
      %3 = llvm.mlir.constant(0xFF800000 : f32) : f32
      %4 = llvm.mlir.constant(16 : index) : i32
      %5 = llvm.mlir.constant(128 : index) : i32
      %6 = llvm.mlir.constant(64 : index) : i32
      %7 = llvm.mlir.constant(4 : index) : i32
      %8 = llvm.mlir.constant(256 : index) : i32
      %9 = llvm.mlir.constant(0 : index) : i32
      %10 = llvm.mlir.addressof @__wg_main_kColReduction_reduce__4_1_0___8w32h_1_0 : !llvm.ptr<array<256 x f32>, 3>
      %11 = llvm.getelementptr %10[0, 0] : (!llvm.ptr<array<256 x f32>, 3>) -> !llvm.ptr<f32, 3>
      %12 = nvvm.read.ptx.sreg.ctaid.x : i32
      %13 = nvvm.read.ptx.sreg.tid.x : i32
      llvm.br ^bb1
    ^bb1:  // pred: ^bb0
      %14 = llvm.mul %12, %arg3  : i32
      %15 = llvm.add %13, %14  : i32
      %16 = llvm.icmp "ult" %15, %arg4 : i32
      llvm.cond_br %16, ^bb2, ^bb18
    ^bb2:  // pred: ^bb1
      %17 = llvm.srem %15, %8  : i32
      %18 = llvm.sdiv %15, %8  : i32
      %19 = llvm.udiv %17, %0  : i32
      %20 = llvm.urem %17, %0  : i32
      %21 = llvm.udiv %18, %arg5  : i32
      %22 = llvm.urem %18, %arg5  : i32
      %23 = llvm.mul %21, %1  : i32
      %24 = llvm.add %23, %19  : i32
      %25 = llvm.mul %22, %0  : i32
      %26 = llvm.add %25, %20  : i32
      %27 = llvm.icmp "ult" %24, %arg2 : i32
      %28 = llvm.icmp "ult" %26, %arg0 : i32
      %29 = llvm.and %27, %28  : i1
      llvm.cond_br %29, ^bb3, ^bb4
    ^bb3:  // pred: ^bb2
      %30 = llvm.mul %24, %arg0  : i32
      %31 = llvm.add %30, %26  : i32
      %32 = llvm.getelementptr %arg1[%31] : (!llvm.ptr<f32>, i32) -> !llvm.ptr<f32>
      %33 = llvm.load %32 : !llvm.ptr<f32>
      %34 = llvm.call @__nv_fabsf(%33) : (f32) -> f32
      %35 = llvm.getelementptr %11[%17] : (!llvm.ptr<f32, 3>, i32) -> !llvm.ptr<f32, 3>
      llvm.store %34, %35 : !llvm.ptr<f32, 3>
      llvm.br ^bb5
    ^bb4:  // pred: ^bb2
      %36 = llvm.getelementptr %11[%17] : (!llvm.ptr<f32, 3>, i32) -> !llvm.ptr<f32, 3>
      llvm.store %3, %36 : !llvm.ptr<f32, 3>
      llvm.br ^bb5
    ^bb5:  // 2 preds: ^bb3, ^bb4
      nvvm.barrier0
      %37 = llvm.icmp "ult" %19, %4 : i32
      %38 = llvm.add %24, %4  : i32
      %39 = llvm.icmp "ult" %38, %arg2 : i32
      %40 = llvm.and %37, %39  : i1
      llvm.cond_br %40, ^bb6, ^bb7
    ^bb6:  // pred: ^bb5
      %41 = llvm.add %17, %5  : i32
      %42 = llvm.getelementptr %11[%17] : (!llvm.ptr<f32, 3>, i32) -> !llvm.ptr<f32, 3>
      %43 = llvm.load %42 : !llvm.ptr<f32, 3>
      %44 = llvm.getelementptr %11[%41] : (!llvm.ptr<f32, 3>, i32) -> !llvm.ptr<f32, 3>
      %45 = llvm.load %44 : !llvm.ptr<f32, 3>
      %46 = llvm.fcmp "ugt" %43, %45 : f32
      %47 = llvm.select %46, %43, %45 : i1, f32
      %48 = llvm.fcmp "uno" %45, %45 : f32
      %49 = llvm.select %48, %45, %47 : i1, f32
      %50 = llvm.getelementptr %11[%17] : (!llvm.ptr<f32, 3>, i32) -> !llvm.ptr<f32, 3>
      llvm.store %49, %50 : !llvm.ptr<f32, 3>
      llvm.br ^bb7
    ^bb7:  // 2 preds: ^bb5, ^bb6
      nvvm.barrier0
      %51 = llvm.icmp "ult" %19, %0 : i32
      %52 = llvm.add %24, %0  : i32
      %53 = llvm.icmp "ult" %52, %arg2 : i32
      %54 = llvm.and %51, %53  : i1
      llvm.cond_br %54, ^bb8, ^bb9
    ^bb8:  // pred: ^bb7
      %55 = llvm.add %17, %6  : i32
      %56 = llvm.getelementptr %11[%17] : (!llvm.ptr<f32, 3>, i32) -> !llvm.ptr<f32, 3>
      %57 = llvm.load %56 : !llvm.ptr<f32, 3>
      %58 = llvm.getelementptr %11[%55] : (!llvm.ptr<f32, 3>, i32) -> !llvm.ptr<f32, 3>
      %59 = llvm.load %58 : !llvm.ptr<f32, 3>
      %60 = llvm.fcmp "ugt" %57, %59 : f32
      %61 = llvm.select %60, %57, %59 : i1, f32
      %62 = llvm.fcmp "uno" %59, %59 : f32
      %63 = llvm.select %62, %59, %61 : i1, f32
      %64 = llvm.getelementptr %11[%17] : (!llvm.ptr<f32, 3>, i32) -> !llvm.ptr<f32, 3>
      llvm.store %63, %64 : !llvm.ptr<f32, 3>
      llvm.br ^bb9
    ^bb9:  // 2 preds: ^bb7, ^bb8
      nvvm.barrier0
      %65 = llvm.icmp "ult" %19, %7 : i32
      %66 = llvm.add %24, %7  : i32
      %67 = llvm.icmp "ult" %66, %arg2 : i32
      %68 = llvm.and %65, %67  : i1
      llvm.cond_br %68, ^bb10, ^bb11
    ^bb10:  // pred: ^bb9
      %69 = llvm.add %17, %1  : i32
      %70 = llvm.getelementptr %11[%17] : (!llvm.ptr<f32, 3>, i32) -> !llvm.ptr<f32, 3>
      %71 = llvm.load %70 : !llvm.ptr<f32, 3>
      %72 = llvm.getelementptr %11[%69] : (!llvm.ptr<f32, 3>, i32) -> !llvm.ptr<f32, 3>
      %73 = llvm.load %72 : !llvm.ptr<f32, 3>
      %74 = llvm.fcmp "ugt" %71, %73 : f32
      %75 = llvm.select %74, %71, %73 : i1, f32
      %76 = llvm.fcmp "uno" %73, %73 : f32
      %77 = llvm.select %76, %73, %75 : i1, f32
      %78 = llvm.getelementptr %11[%17] : (!llvm.ptr<f32, 3>, i32) -> !llvm.ptr<f32, 3>
      llvm.store %77, %78 : !llvm.ptr<f32, 3>
      llvm.br ^bb11
    ^bb11:  // 2 preds: ^bb9, ^bb10
      nvvm.barrier0
      %79 = llvm.icmp "ult" %19, %2 : i32
      %80 = llvm.add %24, %2  : i32
      %81 = llvm.icmp "ult" %80, %arg2 : i32
      %82 = llvm.and %79, %81  : i1
      llvm.cond_br %82, ^bb12, ^bb13
    ^bb12:  // pred: ^bb11
      %83 = llvm.add %17, %4  : i32
      %84 = llvm.getelementptr %11[%17] : (!llvm.ptr<f32, 3>, i32) -> !llvm.ptr<f32, 3>
      %85 = llvm.load %84 : !llvm.ptr<f32, 3>
      %86 = llvm.getelementptr %11[%83] : (!llvm.ptr<f32, 3>, i32) -> !llvm.ptr<f32, 3>
      %87 = llvm.load %86 : !llvm.ptr<f32, 3>
      %88 = llvm.fcmp "ugt" %85, %87 : f32
      %89 = llvm.select %88, %85, %87 : i1, f32
      %90 = llvm.fcmp "uno" %87, %87 : f32
      %91 = llvm.select %90, %87, %89 : i1, f32
      %92 = llvm.getelementptr %11[%17] : (!llvm.ptr<f32, 3>, i32) -> !llvm.ptr<f32, 3>
      llvm.store %91, %92 : !llvm.ptr<f32, 3>
      llvm.br ^bb13
    ^bb13:  // 2 preds: ^bb11, ^bb12
      nvvm.barrier0
      %93 = llvm.icmp "eq" %19, %9 : i32
      %94 = llvm.and %93, %29  : i1
      llvm.cond_br %94, ^bb14, ^bb17
    ^bb14:  // pred: ^bb13
      %95 = llvm.add %17, %0  : i32
      %96 = llvm.getelementptr %11[%17] : (!llvm.ptr<f32, 3>, i32) -> !llvm.ptr<f32, 3>
      %97 = llvm.load %96 : !llvm.ptr<f32, 3>
      %98 = llvm.getelementptr %11[%95] : (!llvm.ptr<f32, 3>, i32) -> !llvm.ptr<f32, 3>
      %99 = llvm.load %98 : !llvm.ptr<f32, 3>
      %100 = llvm.fcmp "ugt" %97, %99 : f32
      %101 = llvm.select %100, %97, %99 : i1, f32
      %102 = llvm.fcmp "uno" %99, %99 : f32
      %103 = llvm.select %102, %99, %101 : i1, f32
      %104 = llvm.getelementptr %arg6[%26] : (!llvm.ptr<f32>, i32) -> !llvm.ptr<f32>
      %105 = llvm.load %104 : !llvm.ptr<f32>
      llvm.br ^bb15(%105 : f32)
    ^bb15(%106: f32):  // 2 preds: ^bb14, ^bb15
      %107 = llvm.fcmp "ogt" %106, %103 : f32
      %108 = llvm.select %107, %106, %103 : i1, f32
      %109 = llvm.bitcast %104 : !llvm.ptr<f32> to !llvm.ptr<i32>
      %110 = llvm.bitcast %106 : f32 to i32
      %111 = llvm.bitcast %108 : f32 to i32
      %112 = llvm.cmpxchg %109, %110, %111 acq_rel monotonic : !llvm.ptr<i32>, i32
      %113 = llvm.extractvalue %112[0] : !llvm.struct<(i32, i1)> 
      %114 = llvm.bitcast %113 : i32 to f32
      %115 = llvm.extractvalue %112[1] : !llvm.struct<(i32, i1)> 
      llvm.cond_br %115, ^bb16, ^bb15(%114 : f32)
    ^bb16:  // pred: ^bb15
      llvm.br ^bb17
    ^bb17:  // 2 preds: ^bb13, ^bb16
      llvm.br ^bb18
    ^bb18:  // 2 preds: ^bb1, ^bb17
      llvm.return
    }
  }
  gpu.module @main_kernel_1 attributes {gpu.binary = "P\EDU\BA\01\00\10\008\04\00\00\00\00\00\00\02\00\01\01@\00\00\00\F8\03\00\00\00\00\00\00\F8\03\00\00\00\00\00\00\07\00\01\00P\00\00\00\00\00\00\00\00\00\00\00\11 \00\00\00\00\00\00\00\00\00\00\00\00\00\00\E8\0B\00\00\00\00\00\00\A2\7FELF\02\01\013\07\00\01\00f\02\00\BE\00q\00\01\00!@\0B\07\001\00\80\08\07\00\F5\0E\00P\05P\00@\008\00\03\00@\00\0B\00\01\00\00.shstrtab\00.\08\00'ym\08\00\F0\1B_shndx\00.nv.info\00.text.main_kColReduction_r\0A\00\F5\01e__4_1_0___8w16h8\00\0F2\00\1Boshared4\00\1B\9Fconstant07\00\18\FA\01debug_frame\00.rel\11\00!nv\14\00\11aC\00\0F+\01 \0F\88\00\15\0FT\01\BAo_param[\01\1C\0F\01\00\06\8C[\00\00\00\03\00\0A\00\01\00\11\F0\18\00,\09\00\01\00 .\01\18\00,\04\00\01\00\11L\18\00,\07\00\01\00f2\00\00\00\12\10`\00!\80\01\07\00v\00\FF\FF\FF\FF(\00\0C\00\00\01\00@\03\00\04|\08\00\D1\0F\0C\81\80\80(\00\08\FF\81\80(\08\0B\00\06(\00/0\00\01\00\03\13pX\00\10\04\9B\00R\04\14\00\00\00E\00\01\0B\00\00\13\00p/\08\00\05\00\00\00\A7\03\22\04#\0C\00\00\01\00'\04\12\0C\00\17\11\0C\00!7\04\E8\03\F3\08\015\00\00\04\0A\08\00\02\00\00\00`\01\10\00\03\19\10\00\04\17\0C\0C\04U\08\00\00\F0!\10\00\10\01\18\01%\F0\11\10\00\01\01\00\F2\02\F0\11\00\03\1B\FF\00\04\1C\08\00P\00\00\00\B0\00\01\00#K\00\01\00s\02\02\08\10\0A/\22\9B\00\00\07\00\03\08\00\13\10\08\00\13\18\08\00\13 \08\00\13(\08\00\130\08\00\228\08|\00\13\00\08\00\13\08\08\00\13\10\08\00\13\18\08\00\13 \08\00\13(\08\00\130\08\00\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\A2\00\00\00\14,\00\00\00H\00\01\00\000\01/\05\00\01\00\FF\B0A\02z\01\00\1F\04\B1\0F\00\00\00\C4\0F\00\19y\02\00\01\00\10%\8B\02Q\0E\00\19y\03\0F\00\F5\1A\00!\00\00\00$\0E\00$z\02\02\00X\00\00\03\02\8E\07\00\CA\1F\00\0Cz\00\02\00Y\00\00p`\F0\03\00\DA\0F\00MS\04\A0\80\03\00\EA\0F\005t\03\FF\B3\03\10\FF\C0\03P\E2\0F\00\02x6\02B\80\FF\00\0F\10\00r\B9z\04\00\00F\00\84\00\94\D0\0F\00%v\02\02\00Z`\00`\0F\00\86y\00\022\00@\04\19\10\0C0\009My\00`\00PGy\00\00\F09\04\A6\FF\83\03\00\C0\0F\00\18y\00\01\00\0F\10\00\90\0F\01\00-\00W\01.\03\00\01\00\22@\00\01\00=+\01\000\00\08\01\00\1F\0B@\00\04\13k)\00\1F[@\00\0C\13\13\0C\04\0C\01\00\13\C8\15\00&\90\000\04#\04\00\85\04\00\F6\04\12\00\01\00\1F\FET\00\00\00\01\00\13X\95\00/p\00\80\00\0B\1F)'\00\03#\00\C8@\00\04P\06\04\E4\00*\04\00\01\00\1Fa@\00\04\13\F81\00&\\\00@\00\1F\0A@\00\00!\1C\01D\01\0D@\00\13X)\00*\D8\00\01\00\1B\08\08\00?\0B\01\00\86\07\00Q\00\000\05\00\01\00&\10\00\80\00\17\048\00\04\18\00\13\C7\14\01\0C\84\01*@\058\07\1F\00\C0\00\04\132@\00+\06\00\01\00\1A\07\D0\07\12\03\F0\05:\08\80\00\01\00\13\06\08\06\04(\0B\0C\01\00*\A8\00\08\00\04\F8\00\14\018\00/\05\00\01\00\029@\03\00\08\00\088\00/\06\00\01\00\17\80\08\00\00\00\00\00\00\00"} {
    llvm.func @main_kColReduction_reduce__4_1_0___8w16h(%arg0: i32, %arg1: i32, %arg2: !llvm.ptr<f32>) attributes {disc.elimargs = [2 : index, 4 : index, 5 : index, 6 : index], gpu.kernel, nvvm.kernel} {
      %0 = llvm.mlir.constant(0xFF800000 : f32) : f32
      %1 = nvvm.read.ptx.sreg.ctaid.x : i32
      %2 = nvvm.read.ptx.sreg.tid.x : i32
      llvm.br ^bb1
    ^bb1:  // pred: ^bb0
      %3 = llvm.mul %1, %arg0  : i32
      %4 = llvm.add %2, %3  : i32
      %5 = llvm.icmp "ult" %4, %arg1 : i32
      llvm.cond_br %5, ^bb2, ^bb3
    ^bb2:  // pred: ^bb1
      %6 = llvm.getelementptr %arg2[%4] : (!llvm.ptr<f32>, i32) -> !llvm.ptr<f32>
      llvm.store %0, %6 : !llvm.ptr<f32>
      llvm.br ^bb3
    ^bb3:  // 2 preds: ^bb1, ^bb2
      llvm.return
    }
  }
  gpu.module @main_kernel_2 attributes {gpu.binary = "P\EDU\BA\01\00\10\00\10\08\00\00\00\00\00\00\02\00\01\01@\00\00\00\D0\07\00\00\00\00\00\00\CB\07\00\00\00\00\00\00\07\00\01\00P\00\00\00\00\00\00\00\00\00\00\00\11 \00\00\00\00\00\00\00\00\00\00\00\00\00\00(\13\00\00\00\00\00\00\A2\7FELF\02\01\013\07\00\01\00f\02\00\BE\00q\00\01\00#\80\12\08\00\11\0F\07\00\F5\0E\00P\05P\00@\008\00\03\00@\00\0C\00\01\00\00.shstrtab\00.\08\00'ym\08\00\F0\1B_shndx\00.nv.info\00.text.main_kColReduction_r\0A\00\F5\03e__4_1_0___8w16h_1:\00\0F4\00\1Doshared6\00\1AOrela\A0\00\1F?rel\D5\00\22\9Fconstant09\00\1A\B2debug_frame{\00\09\11\00!nv\14\00\11aE\00\0F\9E\01 \0F\8A\00\17\0F\C9\01\F4\8F$____wg_3\00\17\00\0C\00/27\02\02'o_param\09\02\1C\0F\01\00\05\8C]\00\00\00\03\00\0A\00\01\00\11\C2\18\00,\0B\00\01\00 \9C\01\18\00,\09\00\01\00\11\DC\18\00,\04\00\01\00\11\FA\18\00,\07\00\01\00g2\00\00\00\12\10x\00\03#\00f\FF\FF\FF\FF(\00\0C\00\00\01\00@\03\00\04|\08\00\D1\0F\0C\81\80\80(\00\08\FF\81\80(\08\0B\00\06(\00/0\00\01\00\03!\F0\06\07\00 \00\04\9B\00R\04\14\00\00\00E\002\04|\01\18\000/\08\00#\00\10\0E\0C\00\12#\0C\00\00\01\00'\04\12\0C\00\17\11\0C\00!7\04 \05\F1\08\015\00\00\04\0A\08\00\03\00\00\00`\01(\00\03\19(\00\04\17\0C$\00u\06\00 \00\00\F0!\10\00u\05\00\1C\00\00\F0\11\10\009\04\00\18\10\009\03\00\14\10\009\02\00\10\10\009\01\00\08P\00\01\01\00\C1\F0\11\00\03\1B\FF\00\04\1C\0C\00P\98\05\82\00\00P\06\00\00\04\1E\84\01#K\00\01\00\B1\02\02\08\10\0A/\22\00\00\00\08\10\00#\00\08\08\00\13\10\08\00\13\18\08\00\13 \08\00\13(\08\00\130\08\00\228\08\84\00\13\00\08\00\13\08\08\00\13\10\08\00\13\18\08\00\13 \08\00\13(\08\00\130\08\00\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\A2\00\00\00\14,\00\00\00H\00\01\00\11\02h\01\0F\01\00\FF\B0@$v\01\FF\7F\04\B2\FF\00\8E\07\00\C4\0F\00\19y\00\01\00\10%\8B\02a\0E\00\19y\03\00\01\00\10!-\00B\0E\00$z\B5\04\90\03\02\8E\07\00\CA\1F\00\0C\10\00\C5^\00\00p`\F0\03\00\DA\0F\00M\9B\04\F0\0C\80\03\00\EA\0F\00\06{\04\00\00_\00\00\00\90 \00\00\22\0E\00\19x\05\FF\1F\1F\00\D3\14\01\00\00\E2\0F\00$r\02\FF\FF\00\80\00 \E2\0FP\00\10\FF0\00@pP\F4\03\10\00\81\B9z\04\00\00F\00\00#\05p\E2\0F\00\11r\05\05?\00\B2\FF8\8F\07\00\C6\0F\00\08s\04\F6\04\10\10\A0\00\F1\06\1E\00\10x\03\04\FE\FF\FF\0F\FF\E0\FF\07\00\CC\1F\00\05s\03$\04\00\C0\03\01\C0\00!r\07p\00\C0\03\0A\8E\07\00\C8\1F\00$z\07\07p\00\10\FF\D0\00\81\C8\0F\00'r\07\03\07\E0\02\02\90\00@\19x\02\FF\10\00\10\05\B0\00\80\E4\0F\00\12x\05\05\80\F1\04!\C0\8E\80\00T'r\07\07\02\C0\00\10\C8\D0\00\11\09`\00\10\07`\00`\E4\0F\00$x\05\BC\03$\00\05\10\000z\03\09p\00\22\02\02@\00@\19x\00\FF\A0\00\22\05\16`\00f\0Cz\00\03\00_P\010\10\0A\03\10\00\11\80\D0\00p\E4\0F\00\10\08\07\07P\00\04\10\00\060\00\12\F20\00\1A\18 \001\12\AA\07P\01 \FF3\B0\01\00\A0\00\1B\03\A0\001\00\07\10\D1\03\05\A0\00\15\03\A0\00\10\E2\F0\00 \02\05\00\01#\FF\C0@\00Sx\09\03\08\00 \00\11\CA\80\003\09\00X\80\00\22\C8\0F\10\02p\\\00\00p\10\F2\04\90\00T$\14\03\FF\04 \01\00`\000\1A\02\000\00\13\09p\01c%\16\02\02\00ZP\02q\CC\0F\00\81\19\02\02\D0\01\C4\19\1E\0C\00\A2\00\00\10x\06\00\08\D0\00\000\02\92t\04\FF\00\00\80\FF\FF\00\A0\001\1Cx\00\01\001p\F0\F00\02E$x\07\05p\00q\E2\0F\04\0Cz\00\06\90\00#`\F6 \00\16\06 \00\80\C6\0F\00\0Cx\00\05?@\00\92D\F6\01\00\E4\0F\04\10xF\07\02@\01B\1F\04\10x\EC\04\04\80\00@!\12\04\02\81\00\01\15\00!\E2O@\00\11\07@\00 \F2\04\D0\00c\88s\00\07\04\00\EE\05f\E8\0F\00\1D{\00\01\00u\EC\0F\00\84\B9\0B\06\CE\05\84(\0E\00\84\B9\02\06\000\00qb\0E\00\0B\B2\00\0B\F6\07`\80\F4\03\00\C4\1F\10\00\11\02\10\00\A3\C0\F8\03\00\E4/\00\1C\B8\00\F0\00%p\01\00\02\01\E0\00B\F4\03\00\C8\D0\00\11\1F\90\00\C3t\01\00\CE\0F\00\08\82\0B\0B\02\00\17\07\1B\E40\01\8F\C6\0F\00\88\B3\00\06\0B\B0\00\09f\A9\03\06\00\80\00\B0\00\1B\A9\B0\001\A2\00\03`\03\22\80\F6\B0\00H\A2\00\02\03\B0\00$\A8\00p\00\04\B0\00\15\00\90\01\03\B0\00\14\0F\80\01\01\B0\00/\03\03\B0\00\0AO\A3\00\06\03`\01\0AG\07\06\00@\B0\00\13\B9\A7\06\06`\01*\07\07`\01/\00\07`\01\05\10\DA\90\00\02/\00\01\90\00\12\CA0\01\1F\07\80\00\089M\19\00\00\05G$t\02\FF\90\02A\00\84y\05\9F\08\01@\00\96&\0E\00%v\02\09\00`p\03'\84y\B0\00fh\0E\00\81y\08\10\03bb\05\00\0Br\00\01\05\22\80\F0\C0\001r\00\00\10\00\92\C0\F2\03\00\D6/\00\08\82\1F\00p\00\00\80\04\00\C8O \00\11\08\11\00 @\F0\00\03$\0EF\E0\05\00\A0\00b\E6\0F\00\08r\09 \00\00\01\00p\CC\0F\00\A9s\09\02{\00\C0\09\E1\1E\00\00\A4\0E\00\0Cr\00\09\10\00 pR@\00QO\00$r\08@\04\10\09\D0\00\80\D8\0F\00G\09\00\00\90\10\05!\FF\83\F0\00*My\00\01TGy\00\00\F0 \00f\C0\0F\00\18y\00\01\00\0F\10\00p\0F\01\00-\11\01@\0A\0E\01\00\22@\00\01\00=\9E\01\000\00\08\01\00\1F\0B@\00\04\13\DE)\00?\09\02\00@\00\0A\22\13\00\A0\04\0C\01\00\13\E8U\00\03\7F\03\01$\00\13\05W\02\00\01\00\22\18\00\01\00.q\01T\00\00\01\00\11\90u\02O\00\00p\00\80\00\0B\1F)'\00\03\03\95\02$\00\00\18\0C\04\E4\00*\04\00\01\00\1Fc@\00\04*0\05\C0\00\13\03\03\08\0C@\00!\8F\01D\01\0D@\00\13\D8@\00*\D8\00\01\00\1B\08\08\00?~\01\00N\0D\002\00\00\B0\86\03\01W\08\04\80\00\17\048\00\04\18\00\138@\01\0C\84\01\13\C0@\00\17\881\01\0F\C0\00\01\132T\01\15\06R\00\03>\03\1A\08\98\0D\11\03$\00J\00\0E\80\00\01\00\13\97\94\00*\03\00\01\00\040\12/\00\02\80\00\0B\13\06\AB\01\04h\12\0C\01\00\1B\A8\08\00\04\97\00\13\018\00\04\E8\00\0C\01\00*\C0\08\08\00\088\00\18\06\A0\00\0F\01\00\05\03\B8\00\80\08\00\00\00\00\00\00\00\00\00\00\00\00"} {
    llvm.mlir.global internal @__wg_main_kColReduction_reduce__4_1_0___8w16h_1_0() {addr_space = 3 : i32} : !llvm.array<128 x f32>
    llvm.func @__nv_fabsf(f32) -> f32
    llvm.func @main_kColReduction_reduce__4_1_0___8w16h_1(%arg0: i32, %arg1: !llvm.ptr<f32>, %arg2: i32, %arg3: i32, %arg4: i32, %arg5: i32, %arg6: !llvm.ptr<f32>) attributes {disc.elimargs = [1 : index, 3 : index, 5 : index, 6 : index, 7 : index, 8 : index, 9 : index, 13 : index, 15 : index, 16 : index, 17 : index], gpu.kernel, nvvm.kernel} {
      %0 = llvm.mlir.constant(8 : index) : i32
      %1 = llvm.mlir.constant(16 : index) : i32
      %2 = llvm.mlir.constant(2 : index) : i32
      %3 = llvm.mlir.constant(0xFF800000 : f32) : f32
      %4 = llvm.mlir.constant(64 : index) : i32
      %5 = llvm.mlir.constant(4 : index) : i32
      %6 = llvm.mlir.constant(32 : index) : i32
      %7 = llvm.mlir.constant(128 : index) : i32
      %8 = llvm.mlir.constant(0 : index) : i32
      %9 = llvm.mlir.addressof @__wg_main_kColReduction_reduce__4_1_0___8w16h_1_0 : !llvm.ptr<array<128 x f32>, 3>
      %10 = llvm.getelementptr %9[0, 0] : (!llvm.ptr<array<128 x f32>, 3>) -> !llvm.ptr<f32, 3>
      %11 = nvvm.read.ptx.sreg.ctaid.x : i32
      %12 = nvvm.read.ptx.sreg.tid.x : i32
      llvm.br ^bb1
    ^bb1:  // pred: ^bb0
      %13 = llvm.mul %11, %arg3  : i32
      %14 = llvm.add %12, %13  : i32
      %15 = llvm.icmp "ult" %14, %arg4 : i32
      llvm.cond_br %15, ^bb2, ^bb16
    ^bb2:  // pred: ^bb1
      %16 = llvm.srem %14, %7  : i32
      %17 = llvm.sdiv %14, %7  : i32
      %18 = llvm.udiv %16, %0  : i32
      %19 = llvm.urem %16, %0  : i32
      %20 = llvm.udiv %17, %arg5  : i32
      %21 = llvm.urem %17, %arg5  : i32
      %22 = llvm.mul %20, %1  : i32
      %23 = llvm.add %22, %18  : i32
      %24 = llvm.mul %21, %0  : i32
      %25 = llvm.add %24, %19  : i32
      %26 = llvm.icmp "ult" %23, %arg2 : i32
      %27 = llvm.icmp "ult" %25, %arg0 : i32
      %28 = llvm.and %26, %27  : i1
      llvm.cond_br %28, ^bb3, ^bb4
    ^bb3:  // pred: ^bb2
      %29 = llvm.mul %23, %arg0  : i32
      %30 = llvm.add %29, %25  : i32
      %31 = llvm.getelementptr %arg1[%30] : (!llvm.ptr<f32>, i32) -> !llvm.ptr<f32>
      %32 = llvm.load %31 : !llvm.ptr<f32>
      %33 = llvm.call @__nv_fabsf(%32) : (f32) -> f32
      %34 = llvm.getelementptr %10[%16] : (!llvm.ptr<f32, 3>, i32) -> !llvm.ptr<f32, 3>
      llvm.store %33, %34 : !llvm.ptr<f32, 3>
      llvm.br ^bb5
    ^bb4:  // pred: ^bb2
      %35 = llvm.getelementptr %10[%16] : (!llvm.ptr<f32, 3>, i32) -> !llvm.ptr<f32, 3>
      llvm.store %3, %35 : !llvm.ptr<f32, 3>
      llvm.br ^bb5
    ^bb5:  // 2 preds: ^bb3, ^bb4
      nvvm.barrier0
      %36 = llvm.icmp "ult" %18, %0 : i32
      %37 = llvm.add %23, %0  : i32
      %38 = llvm.icmp "ult" %37, %arg2 : i32
      %39 = llvm.and %36, %38  : i1
      llvm.cond_br %39, ^bb6, ^bb7
    ^bb6:  // pred: ^bb5
      %40 = llvm.add %16, %4  : i32
      %41 = llvm.getelementptr %10[%16] : (!llvm.ptr<f32, 3>, i32) -> !llvm.ptr<f32, 3>
      %42 = llvm.load %41 : !llvm.ptr<f32, 3>
      %43 = llvm.getelementptr %10[%40] : (!llvm.ptr<f32, 3>, i32) -> !llvm.ptr<f32, 3>
      %44 = llvm.load %43 : !llvm.ptr<f32, 3>
      %45 = llvm.fcmp "ugt" %42, %44 : f32
      %46 = llvm.select %45, %42, %44 : i1, f32
      %47 = llvm.fcmp "uno" %44, %44 : f32
      %48 = llvm.select %47, %44, %46 : i1, f32
      %49 = llvm.getelementptr %10[%16] : (!llvm.ptr<f32, 3>, i32) -> !llvm.ptr<f32, 3>
      llvm.store %48, %49 : !llvm.ptr<f32, 3>
      llvm.br ^bb7
    ^bb7:  // 2 preds: ^bb5, ^bb6
      nvvm.barrier0
      %50 = llvm.icmp "ult" %18, %5 : i32
      %51 = llvm.add %23, %5  : i32
      %52 = llvm.icmp "ult" %51, %arg2 : i32
      %53 = llvm.and %50, %52  : i1
      llvm.cond_br %53, ^bb8, ^bb9
    ^bb8:  // pred: ^bb7
      %54 = llvm.add %16, %6  : i32
      %55 = llvm.getelementptr %10[%16] : (!llvm.ptr<f32, 3>, i32) -> !llvm.ptr<f32, 3>
      %56 = llvm.load %55 : !llvm.ptr<f32, 3>
      %57 = llvm.getelementptr %10[%54] : (!llvm.ptr<f32, 3>, i32) -> !llvm.ptr<f32, 3>
      %58 = llvm.load %57 : !llvm.ptr<f32, 3>
      %59 = llvm.fcmp "ugt" %56, %58 : f32
      %60 = llvm.select %59, %56, %58 : i1, f32
      %61 = llvm.fcmp "uno" %58, %58 : f32
      %62 = llvm.select %61, %58, %60 : i1, f32
      %63 = llvm.getelementptr %10[%16] : (!llvm.ptr<f32, 3>, i32) -> !llvm.ptr<f32, 3>
      llvm.store %62, %63 : !llvm.ptr<f32, 3>
      llvm.br ^bb9
    ^bb9:  // 2 preds: ^bb7, ^bb8
      nvvm.barrier0
      %64 = llvm.icmp "ult" %18, %2 : i32
      %65 = llvm.add %23, %2  : i32
      %66 = llvm.icmp "ult" %65, %arg2 : i32
      %67 = llvm.and %64, %66  : i1
      llvm.cond_br %67, ^bb10, ^bb11
    ^bb10:  // pred: ^bb9
      %68 = llvm.add %16, %1  : i32
      %69 = llvm.getelementptr %10[%16] : (!llvm.ptr<f32, 3>, i32) -> !llvm.ptr<f32, 3>
      %70 = llvm.load %69 : !llvm.ptr<f32, 3>
      %71 = llvm.getelementptr %10[%68] : (!llvm.ptr<f32, 3>, i32) -> !llvm.ptr<f32, 3>
      %72 = llvm.load %71 : !llvm.ptr<f32, 3>
      %73 = llvm.fcmp "ugt" %70, %72 : f32
      %74 = llvm.select %73, %70, %72 : i1, f32
      %75 = llvm.fcmp "uno" %72, %72 : f32
      %76 = llvm.select %75, %72, %74 : i1, f32
      %77 = llvm.getelementptr %10[%16] : (!llvm.ptr<f32, 3>, i32) -> !llvm.ptr<f32, 3>
      llvm.store %76, %77 : !llvm.ptr<f32, 3>
      llvm.br ^bb11
    ^bb11:  // 2 preds: ^bb9, ^bb10
      nvvm.barrier0
      %78 = llvm.icmp "eq" %18, %8 : i32
      %79 = llvm.and %78, %28  : i1
      llvm.cond_br %79, ^bb12, ^bb15
    ^bb12:  // pred: ^bb11
      %80 = llvm.add %16, %0  : i32
      %81 = llvm.getelementptr %10[%16] : (!llvm.ptr<f32, 3>, i32) -> !llvm.ptr<f32, 3>
      %82 = llvm.load %81 : !llvm.ptr<f32, 3>
      %83 = llvm.getelementptr %10[%80] : (!llvm.ptr<f32, 3>, i32) -> !llvm.ptr<f32, 3>
      %84 = llvm.load %83 : !llvm.ptr<f32, 3>
      %85 = llvm.fcmp "ugt" %82, %84 : f32
      %86 = llvm.select %85, %82, %84 : i1, f32
      %87 = llvm.fcmp "uno" %84, %84 : f32
      %88 = llvm.select %87, %84, %86 : i1, f32
      %89 = llvm.getelementptr %arg6[%25] : (!llvm.ptr<f32>, i32) -> !llvm.ptr<f32>
      %90 = llvm.load %89 : !llvm.ptr<f32>
      llvm.br ^bb13(%90 : f32)
    ^bb13(%91: f32):  // 2 preds: ^bb12, ^bb13
      %92 = llvm.fcmp "ogt" %91, %88 : f32
      %93 = llvm.select %92, %91, %88 : i1, f32
      %94 = llvm.bitcast %89 : !llvm.ptr<f32> to !llvm.ptr<i32>
      %95 = llvm.bitcast %91 : f32 to i32
      %96 = llvm.bitcast %93 : f32 to i32
      %97 = llvm.cmpxchg %94, %95, %96 acq_rel monotonic : !llvm.ptr<i32>, i32
      %98 = llvm.extractvalue %97[0] : !llvm.struct<(i32, i1)> 
      %99 = llvm.bitcast %98 : i32 to f32
      %100 = llvm.extractvalue %97[1] : !llvm.struct<(i32, i1)> 
      llvm.cond_br %100, ^bb14, ^bb13(%99 : f32)
    ^bb14:  // pred: ^bb13
      llvm.br ^bb15
    ^bb15:  // 2 preds: ^bb11, ^bb14
      llvm.br ^bb16
    ^bb16:  // 2 preds: ^bb1, ^bb15
      llvm.return
    }
  }
  "disc_shape.SymbolicDim"() {knownNegativeOne = false, knownNonNegative = true, knownNonSizeOne = false, knownNonSizeZero = false, sym_name = "S0", value = -9223372036854775808 : i64} : () -> ()
  "disc_shape.SymbolicDim"() {knownNegativeOne = false, knownNonNegative = true, knownNonSizeOne = false, knownNonSizeZero = false, sym_name = "S1", value = -9223372036854775808 : i64} : () -> ()
  "disc_shape.SymbolicDim"() {knownNegativeOne = false, knownNonNegative = true, knownNonSizeOne = false, knownNonSizeZero = false, sym_name = "S2", value = -9223372036854775808 : i64} : () -> ()
  "disc_shape.SymbolicDim"() {knownNegativeOne = false, knownNonNegative = true, knownNonSizeOne = false, knownNonSizeZero = false, sym_name = "S3", value = -9223372036854775808 : i64} : () -> ()
  func.func @shape_constraint_graph() {
    %0 = "disc_shape.dim"() {name = @S3} : () -> index
    %1 = "disc_shape.dim"() {name = @S1} : () -> index
    %2 = "disc_shape.dim"() {name = @S2} : () -> index
    "disc_shape.tie_product_equal"(%0, %1, %2) {operand_segment_sizes = array<i32: 1, 2>} : (index, index, index) -> ()
    return
  }
}


// -----// IR Dump After Canonicalizer (canonicalize) //----- //
func.func @main(%arg0: !disc_ral.context) attributes {tf.entry_function = {input_placements = "gpu", inputs = "input0", output_placements = "gpu", outputs = "output0"}} {
  %c-1 = arith.constant -1 : index
  %c128 = arith.constant 128 : index
  %c16 = arith.constant 16 : index
  %c32 = arith.constant 32 : index
  %c8 = arith.constant 8 : index
  %0 = llvm.mlir.constant(0 : i32) : i32
  %c0 = arith.constant 0 : index
  %c1 = arith.constant 1 : index
  %c2 = arith.constant 2 : index
  %c256 = arith.constant 256 : index
  %c108 = arith.constant 108 : index
  %1 = "disc_ral.dispatch"(%arg0, %c0) {backend_config = "", call_target_name = "ral_recv_input", device = "cpu", has_side_effect = false} : (!disc_ral.context, index) -> memref<?x?x?xf32, "gpu">
  %dim = memref.dim %1, %c2 : memref<?x?x?xf32, "gpu">
  %dim_0 = memref.dim %1, %c1 : memref<?x?x?xf32, "gpu">
  %dim_1 = memref.dim %1, %c0 : memref<?x?x?xf32, "gpu">
  %2 = arith.index_cast %dim_0 : index to i32
  %3 = arith.index_cast %dim : index to i32
  %4 = arith.muli %2, %3 : i32
  %5 = arith.index_cast %4 : i32 to index
  %alloc = memref.alloc(%5) {kDiscSymbolicDimAttr = [@S3]} : memref<?xf32, "gpu">
  %6 = arith.muli %dim_1, %5 : index
  %7 = arith.addi %6, %c-1 : index
  %8 = arith.divsi %7, %c256 : index
  %9 = arith.addi %8, %c1 : index
  %10 = arith.subi %c0, %6 : index
  %11 = arith.divsi %10, %c256 : index
  %12 = arith.subi %c0, %11 : index
  %13 = arith.cmpi sgt, %6, %c0 : index
  %14 = arith.select %13, %9, %12 : index
  %15 = arith.cmpi sgt, %14, %c108 : index
  cf.cond_br %15, ^bb1, ^bb2
^bb1:  // pred: ^bb0
  %16 = affine.apply affine_map<()[s0] -> (s0 ceildiv 256)>()[%5]
  gpu.launch_func  @main_kernel::@main_kColReduction_reduce__4_1_0___8w32h blocks in (%16, %c1, %c1) threads in (%c256, %c1, %c1) args(%c256 : index, %5 : index, %alloc : memref<?xf32, "gpu">)
  %17 = arith.addi %5, %c-1 : index
  %18 = arith.divsi %17, %c8 : index
  %19 = arith.addi %18, %c1 : index
  %20 = arith.subi %c0, %5 : index
  %21 = arith.divsi %20, %c8 : index
  %22 = arith.subi %c0, %21 : index
  %23 = arith.cmpi sgt, %5, %c0 : index
  %24 = arith.select %23, %19, %22 : index
  %25 = arith.addi %dim_1, %c-1 : index
  %26 = arith.divsi %25, %c32 : index
  %27 = arith.addi %26, %c1 : index
  %28 = arith.subi %c0, %dim_1 : index
  %29 = arith.divsi %28, %c32 : index
  %30 = arith.subi %c0, %29 : index
  %31 = arith.cmpi sgt, %dim_1, %c0 : index
  %32 = arith.select %31, %27, %30 : index
  %33 = arith.muli %24, %32 : index
  %34 = arith.muli %33, %c256 : index
  %35 = affine.apply affine_map<()[s0] -> (s0 ceildiv 256)>()[%34]
  gpu.launch_func  @main_kernel_0::@main_kColReduction_reduce__4_1_0___8w32h_1 blocks in (%35, %c1, %c1) threads in (%c256, %c1, %c1) args(%5 : index, %1 : memref<?x?x?xf32, "gpu">, %c256 : index, %34 : index, %24 : index, %alloc : memref<?xf32, "gpu">)
  cf.br ^bb3
^bb2:  // pred: ^bb0
  %36 = affine.apply affine_map<()[s0] -> (s0 ceildiv 128)>()[%5]
  gpu.launch_func  @main_kernel_1::@main_kColReduction_reduce__4_1_0___8w16h blocks in (%36, %c1, %c1) threads in (%c128, %c1, %c1) args(%c128 : index, %5 : index, %alloc : memref<?xf32, "gpu">)
  %37 = arith.addi %5, %c-1 : index
  %38 = arith.divsi %37, %c8 : index
  %39 = arith.addi %38, %c1 : index
  %40 = arith.subi %c0, %5 : index
  %41 = arith.divsi %40, %c8 : index
  %42 = arith.subi %c0, %41 : index
  %43 = arith.cmpi sgt, %5, %c0 : index
  %44 = arith.select %43, %39, %42 : index
  %45 = arith.addi %dim_1, %c-1 : index
  %46 = arith.divsi %45, %c16 : index
  %47 = arith.addi %46, %c1 : index
  %48 = arith.subi %c0, %dim_1 : index
  %49 = arith.divsi %48, %c16 : index
  %50 = arith.subi %c0, %49 : index
  %51 = arith.cmpi sgt, %dim_1, %c0 : index
  %52 = arith.select %51, %47, %50 : index
  %53 = arith.muli %44, %52 : index
  %54 = arith.muli %53, %c128 : index
  %55 = affine.apply affine_map<()[s0] -> (s0 ceildiv 128)>()[%54]
  gpu.launch_func  @main_kernel_2::@main_kColReduction_reduce__4_1_0___8w16h_1 blocks in (%55, %c1, %c1) threads in (%c128, %c1, %c1) args(%5 : index, %1 : memref<?x?x?xf32, "gpu">, %c128 : index, %54 : index, %44 : index, %alloc : memref<?xf32, "gpu">)
  cf.br ^bb3
^bb3:  // 2 preds: ^bb1, ^bb2
  %56 = llvm.inttoptr %0 : i32 to !llvm.ptr<i8>
  %alloca = memref.alloca() : memref<2xindex, "cpu">
  memref.store %dim_0, %alloca[%c0] : memref<2xindex, "cpu">
  memref.store %dim, %alloca[%c1] : memref<2xindex, "cpu">
  %57 = "disc_ral.dispatch"(%arg0, %56, %alloc, %alloca) {backend_config = "", call_target_name = "inc_ref", device = "gpu", has_side_effect = false} : (!disc_ral.context, !llvm.ptr<i8>, memref<?xf32, "gpu">, memref<2xindex, "cpu">) -> memref<?x?xf32, "gpu">
  %reinterpret_cast = memref.reinterpret_cast %57 to offset: [0], sizes: [%dim_0, %dim], strides: [%dim, 1] {kDiscSymbolicDimAttr = [@S1, @S2]} : memref<?x?xf32, "gpu"> to memref<?x?xf32, "gpu">
  memref.dealloc %alloc : memref<?xf32, "gpu">
  "disc_ral.dispatch"(%arg0, %c0, %reinterpret_cast) {backend_config = "", call_target_name = "ral_send_output", device = "cpu", has_side_effect = false} : (!disc_ral.context, index, memref<?x?xf32, "gpu">) -> ()
  return
}

// -----// IR Dump After ConvertAffineToStandard (lower-affine) //----- //
module attributes {gpu.container_module} {
  func.func @main(%arg0: !disc_ral.context) attributes {tf.entry_function = {input_placements = "gpu", inputs = "input0", output_placements = "gpu", outputs = "output0"}} {
    %c-1 = arith.constant -1 : index
    %c128 = arith.constant 128 : index
    %c16 = arith.constant 16 : index
    %c32 = arith.constant 32 : index
    %c8 = arith.constant 8 : index
    %0 = llvm.mlir.constant(0 : i32) : i32
    %c0 = arith.constant 0 : index
    %c1 = arith.constant 1 : index
    %c2 = arith.constant 2 : index
    %c256 = arith.constant 256 : index
    %c108 = arith.constant 108 : index
    %1 = "disc_ral.dispatch"(%arg0, %c0) {backend_config = "", call_target_name = "ral_recv_input", device = "cpu", has_side_effect = false} : (!disc_ral.context, index) -> memref<?x?x?xf32, "gpu">
    %dim = memref.dim %1, %c2 : memref<?x?x?xf32, "gpu">
    %dim_0 = memref.dim %1, %c1 : memref<?x?x?xf32, "gpu">
    %dim_1 = memref.dim %1, %c0 : memref<?x?x?xf32, "gpu">
    %2 = arith.index_cast %dim_0 : index to i32
    %3 = arith.index_cast %dim : index to i32
    %4 = arith.muli %2, %3 : i32
    %5 = arith.index_cast %4 : i32 to index
    %alloc = memref.alloc(%5) {kDiscSymbolicDimAttr = [@S3]} : memref<?xf32, "gpu">
    %6 = arith.muli %dim_1, %5 : index
    %7 = arith.addi %6, %c-1 : index
    %8 = arith.divsi %7, %c256 : index
    %9 = arith.addi %8, %c1 : index
    %10 = arith.subi %c0, %6 : index
    %11 = arith.divsi %10, %c256 : index
    %12 = arith.subi %c0, %11 : index
    %13 = arith.cmpi sgt, %6, %c0 : index
    %14 = arith.select %13, %9, %12 : index
    %15 = arith.cmpi sgt, %14, %c108 : index
    cf.cond_br %15, ^bb1, ^bb2
  ^bb1:  // pred: ^bb0
    %c256_2 = arith.constant 256 : index
    %c0_3 = arith.constant 0 : index
    %c1_4 = arith.constant 1 : index
    %16 = arith.cmpi sle, %5, %c0_3 : index
    %17 = arith.subi %c0_3, %5 : index
    %18 = arith.subi %5, %c1_4 : index
    %19 = arith.select %16, %17, %18 : index
    %20 = arith.divsi %19, %c256_2 : index
    %21 = arith.subi %c0_3, %20 : index
    %22 = arith.addi %20, %c1_4 : index
    %23 = arith.select %16, %21, %22 : index
    gpu.launch_func  @main_kernel::@main_kColReduction_reduce__4_1_0___8w32h blocks in (%23, %c1, %c1) threads in (%c256, %c1, %c1) args(%c256 : index, %5 : index, %alloc : memref<?xf32, "gpu">)
    %24 = arith.addi %5, %c-1 : index
    %25 = arith.divsi %24, %c8 : index
    %26 = arith.addi %25, %c1 : index
    %27 = arith.subi %c0, %5 : index
    %28 = arith.divsi %27, %c8 : index
    %29 = arith.subi %c0, %28 : index
    %30 = arith.cmpi sgt, %5, %c0 : index
    %31 = arith.select %30, %26, %29 : index
    %32 = arith.addi %dim_1, %c-1 : index
    %33 = arith.divsi %32, %c32 : index
    %34 = arith.addi %33, %c1 : index
    %35 = arith.subi %c0, %dim_1 : index
    %36 = arith.divsi %35, %c32 : index
    %37 = arith.subi %c0, %36 : index
    %38 = arith.cmpi sgt, %dim_1, %c0 : index
    %39 = arith.select %38, %34, %37 : index
    %40 = arith.muli %31, %39 : index
    %41 = arith.muli %40, %c256 : index
    %c256_5 = arith.constant 256 : index
    %c0_6 = arith.constant 0 : index
    %c1_7 = arith.constant 1 : index
    %42 = arith.cmpi sle, %41, %c0_6 : index
    %43 = arith.subi %c0_6, %41 : index
    %44 = arith.subi %41, %c1_7 : index
    %45 = arith.select %42, %43, %44 : index
    %46 = arith.divsi %45, %c256_5 : index
    %47 = arith.subi %c0_6, %46 : index
    %48 = arith.addi %46, %c1_7 : index
    %49 = arith.select %42, %47, %48 : index
    gpu.launch_func  @main_kernel_0::@main_kColReduction_reduce__4_1_0___8w32h_1 blocks in (%49, %c1, %c1) threads in (%c256, %c1, %c1) args(%5 : index, %1 : memref<?x?x?xf32, "gpu">, %c256 : index, %41 : index, %31 : index, %alloc : memref<?xf32, "gpu">)
    cf.br ^bb3
  ^bb2:  // pred: ^bb0
    %c128_8 = arith.constant 128 : index
    %c0_9 = arith.constant 0 : index
    %c1_10 = arith.constant 1 : index
    %50 = arith.cmpi sle, %5, %c0_9 : index
    %51 = arith.subi %c0_9, %5 : index
    %52 = arith.subi %5, %c1_10 : index
    %53 = arith.select %50, %51, %52 : index
    %54 = arith.divsi %53, %c128_8 : index
    %55 = arith.subi %c0_9, %54 : index
    %56 = arith.addi %54, %c1_10 : index
    %57 = arith.select %50, %55, %56 : index
    gpu.launch_func  @main_kernel_1::@main_kColReduction_reduce__4_1_0___8w16h blocks in (%57, %c1, %c1) threads in (%c128, %c1, %c1) args(%c128 : index, %5 : index, %alloc : memref<?xf32, "gpu">)
    %58 = arith.addi %5, %c-1 : index
    %59 = arith.divsi %58, %c8 : index
    %60 = arith.addi %59, %c1 : index
    %61 = arith.subi %c0, %5 : index
    %62 = arith.divsi %61, %c8 : index
    %63 = arith.subi %c0, %62 : index
    %64 = arith.cmpi sgt, %5, %c0 : index
    %65 = arith.select %64, %60, %63 : index
    %66 = arith.addi %dim_1, %c-1 : index
    %67 = arith.divsi %66, %c16 : index
    %68 = arith.addi %67, %c1 : index
    %69 = arith.subi %c0, %dim_1 : index
    %70 = arith.divsi %69, %c16 : index
    %71 = arith.subi %c0, %70 : index
    %72 = arith.cmpi sgt, %dim_1, %c0 : index
    %73 = arith.select %72, %68, %71 : index
    %74 = arith.muli %65, %73 : index
    %75 = arith.muli %74, %c128 : index
    %c128_11 = arith.constant 128 : index
    %c0_12 = arith.constant 0 : index
    %c1_13 = arith.constant 1 : index
    %76 = arith.cmpi sle, %75, %c0_12 : index
    %77 = arith.subi %c0_12, %75 : index
    %78 = arith.subi %75, %c1_13 : index
    %79 = arith.select %76, %77, %78 : index
    %80 = arith.divsi %79, %c128_11 : index
    %81 = arith.subi %c0_12, %80 : index
    %82 = arith.addi %80, %c1_13 : index
    %83 = arith.select %76, %81, %82 : index
    gpu.launch_func  @main_kernel_2::@main_kColReduction_reduce__4_1_0___8w16h_1 blocks in (%83, %c1, %c1) threads in (%c128, %c1, %c1) args(%5 : index, %1 : memref<?x?x?xf32, "gpu">, %c128 : index, %75 : index, %65 : index, %alloc : memref<?xf32, "gpu">)
    cf.br ^bb3
  ^bb3:  // 2 preds: ^bb1, ^bb2
    %84 = llvm.inttoptr %0 : i32 to !llvm.ptr<i8>
    %alloca = memref.alloca() : memref<2xindex, "cpu">
    memref.store %dim_0, %alloca[%c0] : memref<2xindex, "cpu">
    memref.store %dim, %alloca[%c1] : memref<2xindex, "cpu">
    %85 = "disc_ral.dispatch"(%arg0, %84, %alloc, %alloca) {backend_config = "", call_target_name = "inc_ref", device = "gpu", has_side_effect = false} : (!disc_ral.context, !llvm.ptr<i8>, memref<?xf32, "gpu">, memref<2xindex, "cpu">) -> memref<?x?xf32, "gpu">
    %reinterpret_cast = memref.reinterpret_cast %85 to offset: [0], sizes: [%dim_0, %dim], strides: [%dim, 1] {kDiscSymbolicDimAttr = [@S1, @S2]} : memref<?x?xf32, "gpu"> to memref<?x?xf32, "gpu">
    memref.dealloc %alloc : memref<?xf32, "gpu">
    "disc_ral.dispatch"(%arg0, %c0, %reinterpret_cast) {backend_config = "", call_target_name = "ral_send_output", device = "cpu", has_side_effect = false} : (!disc_ral.context, index, memref<?x?xf32, "gpu">) -> ()
    return
  }
  gpu.module @main_kernel attributes {gpu.binary = "P\EDU\BA\01\00\10\008\04\00\00\00\00\00\00\02\00\01\01@\00\00\00\F8\03\00\00\00\00\00\00\F8\03\00\00\00\00\00\00\07\00\01\00P\00\00\00\00\00\00\00\00\00\00\00\11 \00\00\00\00\00\00\00\00\00\00\00\00\00\00\E8\0B\00\00\00\00\00\00\A2\7FELF\02\01\013\07\00\01\00f\02\00\BE\00q\00\01\00!@\0B\07\001\00\80\08\07\00\F5\0E\00P\05P\00@\008\00\03\00@\00\0B\00\01\00\00.shstrtab\00.\08\00'ym\08\00\F0\1B_shndx\00.nv.info\00.text.main_kColReduction_r\0A\00\F5\01e__4_1_0___8w32h8\00\0F2\00\1Boshared4\00\1B\9Fconstant07\00\18\FA\01debug_frame\00.rel\11\00!nv\14\00\11aC\00\0F+\01 \0F\88\00\15\0FT\01\BAo_param[\01\1C\0F\01\00\06\8C[\00\00\00\03\00\0A\00\01\00\11\F0\18\00,\09\00\01\00 .\01\18\00,\04\00\01\00\11L\18\00,\07\00\01\00f2\00\00\00\12\10`\00!\80\01\07\00v\00\FF\FF\FF\FF(\00\0C\00\00\01\00@\03\00\04|\08\00\D1\0F\0C\81\80\80(\00\08\FF\81\80(\08\0B\00\06(\00/0\00\01\00\03\13pX\00\10\04\9B\00R\04\14\00\00\00E\00\01\0B\00\00\13\00p/\08\00\05\00\00\00\A7\03\22\04#\0C\00\00\01\00'\04\12\0C\00\17\11\0C\00!7\04\E8\03\F3\08\015\00\00\04\0A\08\00\02\00\00\00`\01\10\00\03\19\10\00\04\17\0C\0C\04U\08\00\00\F0!\10\00\10\01\18\01%\F0\11\10\00\01\01\00\F2\02\F0\11\00\03\1B\FF\00\04\1C\08\00P\00\00\00\B0\00\01\00#K\00\01\00s\02\02\08\10\0A/\22\9B\00\00\07\00\03\08\00\13\10\08\00\13\18\08\00\13 \08\00\13(\08\00\130\08\00\228\08|\00\13\00\08\00\13\08\08\00\13\10\08\00\13\18\08\00\13 \08\00\13(\08\00\130\08\00\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\A2\00\00\00\14,\00\00\00H\00\01\00\000\01/\05\00\01\00\FF\B0A\02z\01\00\1F\04\B1\0F\00\00\00\C4\0F\00\19y\02\00\01\00\10%\8B\02Q\0E\00\19y\03\0F\00\F5\1A\00!\00\00\00$\0E\00$z\02\02\00X\00\00\03\02\8E\07\00\CA\1F\00\0Cz\00\02\00Y\00\00p`\F0\03\00\DA\0F\00MS\04\A0\80\03\00\EA\0F\005t\03\FF\B3\03\10\FF\C0\03P\E2\0F\00\02x6\02B\80\FF\00\0F\10\00r\B9z\04\00\00F\00\84\00\94\D0\0F\00%v\02\02\00Z`\00`\0F\00\86y\00\022\00@\04\19\10\0C0\009My\00`\00PGy\00\00\F09\04\A6\FF\83\03\00\C0\0F\00\18y\00\01\00\0F\10\00\90\0F\01\00-\00W\01.\03\00\01\00\22@\00\01\00=+\01\000\00\08\01\00\1F\0B@\00\04\13k)\00\1F[@\00\0C\13\13\0C\04\0C\01\00\13\C8\15\00&\90\000\04#\04\00\85\04\00\F6\04\12\00\01\00\1F\FET\00\00\00\01\00\13X\95\00/p\00\80\00\0B\1F)'\00\03#\00\C8@\00\04P\06\04\E4\00*\04\00\01\00\1Fa@\00\04\13\F81\00&\\\00@\00\1F\0A@\00\00!\1C\01D\01\0D@\00\13X)\00*\D8\00\01\00\1B\08\08\00?\0B\01\00\86\07\00Q\00\000\05\00\01\00&\10\00\80\00\17\048\00\04\18\00\13\C7\14\01\0C\84\01*@\058\07\1F\00\C0\00\04\132@\00+\06\00\01\00\1A\07\D0\07\12\03\F0\05:\08\80\00\01\00\13\06\08\06\04(\0B\0C\01\00*\A8\00\08\00\04\F8\00\14\018\00/\05\00\01\00\029@\03\00\08\00\088\00/\06\00\01\00\17\80\08\00\00\00\00\00\00\00"} {
    llvm.func @main_kColReduction_reduce__4_1_0___8w32h(%arg0: i32, %arg1: i32, %arg2: !llvm.ptr<f32>) attributes {disc.elimargs = [2 : index, 4 : index, 5 : index, 6 : index], gpu.kernel, nvvm.kernel} {
      %0 = llvm.mlir.constant(0xFF800000 : f32) : f32
      %1 = nvvm.read.ptx.sreg.ctaid.x : i32
      %2 = nvvm.read.ptx.sreg.tid.x : i32
      llvm.br ^bb1
    ^bb1:  // pred: ^bb0
      %3 = llvm.mul %1, %arg0  : i32
      %4 = llvm.add %2, %3  : i32
      %5 = llvm.icmp "ult" %4, %arg1 : i32
      llvm.cond_br %5, ^bb2, ^bb3
    ^bb2:  // pred: ^bb1
      %6 = llvm.getelementptr %arg2[%4] : (!llvm.ptr<f32>, i32) -> !llvm.ptr<f32>
      llvm.store %0, %6 : !llvm.ptr<f32>
      llvm.br ^bb3
    ^bb3:  // 2 preds: ^bb1, ^bb2
      llvm.return
    }
  }
  gpu.module @main_kernel_0 attributes {gpu.binary = "P\EDU\BA\01\00\10\00@\08\00\00\00\00\00\00\02\00\01\01@\00\00\00\00\08\00\00\00\00\00\00\F9\07\00\00\00\00\00\00\07\00\01\00P\00\00\00\00\00\00\00\00\00\00\00\11 \00\00\00\00\00\00\00\00\00\00\00\00\00\00(\14\00\00\00\00\00\00\A2\7FELF\02\01\013\07\00\01\00f\02\00\BE\00q\00\01\00#\80\13\08\00\11\10\07\00\F5\0E\00P\05P\00@\008\00\03\00@\00\0C\00\01\00\00.shstrtab\00.\08\00'ym\08\00\F0\1B_shndx\00.nv.info\00.text.main_kColReduction_r\0A\00\F5\03e__4_1_0___8w32h_1:\00\0F4\00\1Doshared6\00\1AOrela\A0\00\1F?rel\D5\00\22\9Fconstant09\00\1A\B2debug_frame{\00\09\11\00!nv\14\00\11aE\00\0F\9E\01 \0F\8A\00\17\0F\C9\01\F4\8F$____wg_3\00\17\00\0C\00/27\02\02'o_param\09\02\1C\0F\01\00\05\8C]\00\00\00\03\00\0A\00\01\00\11\C2\18\00,\0B\00\01\00 \9C\01\18\00,\09\00\01\00\11\DC\18\00,\04\00\01\00\11\FA\18\00,\07\00\01\00g2\00\00\00\12\10x\00\11\08\06\00v\00\FF\FF\FF\FF(\00\0C\00\00\01\00@\03\00\04|\08\00\D1\0F\0C\81\80\80(\00\08\FF\81\80(\08\0B\00\06(\00/0\00\01\00\03\13\F0{\00\10\04\9B\00R\04\14\00\00\00E\002\04\AC\01\18\00\80/\08\00\06\00\00\00\0E\0C\00\12#\0C\00\00\01\00'\04\12\0C\00\17\11\0C\00!7\04 \05\F1\08\015\00\00\04\0A\08\00\03\00\00\00`\01(\00\03\19(\00\04\17\0C$\00u\06\00 \00\00\F0!\10\00u\05\00\1C\00\00\F0\11\10\009\04\00\18\10\009\03\00\14\10\009\02\00\10\10\009\01\00\08P\00\01\01\00\F2\0A\F0\11\00\03\1B\FF\00\04\1C\0C\00P\00\00\00\10\06\00\00\10\07\00\00\04\1E\84\01#K\00\01\00v\02\02\08\10\0A/\22b\01\13\08\08\00\13\10\08\00\13\18\08\00\13 \08\00\13(\08\00\130\08\00\228\08\84\00\13\00\08\00\13\08\08\00\13\10\08\00\13\18\08\00\13 \08\00\13(\08\00\130\08\00\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\A2\00\00\00\14,\00\00\00H\00\01\00\11\02h\01\0F\01\00\FF\B0@$v\01\FF\7F\04\B2\FF\00\8E\07\00\C4\0F\00\19y\00\01\00\10%\8B\02a\0E\00\19y\03\00\01\00\10!-\00B\0E\00$z\B5\04\90\03\02\8E\07\00\CA\1F\00\0C\10\00\C5^\00\00p`\F0\03\00\DA\0F\00M\9B\04\F0\0C\80\03\00\EA\0F\00\06{\04\00\00_\00\00\00\90 \00\00\22\0E\00\19x\05\FF\1F\1F\00\D3\14\01\00\00\E2\0F\00$r\02\FF\FF\00\80\00 \E2\0FP\00\10\FF0\00@pP\F4\03\10\00\81\B9z\04\00\00F\00\00#\05p\E2\0F\00\11r\05\05?\00\B2\FF@\8F\07\00\C6\0F\00\08s\04\F6\04\00 \09\F1\07$\1E\00\10x\03\04\FE\FF\FF\0F\FF\E0\FF\07\00\CC\1F\00\05s\03$\04\00\C0\03\01\C0\00!r\07p\00\C0\03\0A\8E\07\00\C8\1F\00$z\07\07p\00\10\FF\D0\00\81\C8\0F\00'r\07\03\07\E0\02\02\90\00@\19x\02\FF\01\03\10\05\B0\00q\E4\0F\00\12x\05\05\F1\04!\C0\8E\80\00T'r\07\07\02\C0\00\10\C8\D0\00\11\09`\00\10\07`\00`\E4\0F\00$x\05\BC\03$\00\05\10\000z\03\09p\00\22\02\02@\00@\19x\00\FF\A0\00\22\05\16`\00f\0Cz\00\03\00_P\010\10\0A\03\10\00\11\80\D0\00p\E4\0F\00\10\08\07\07P\00\04\10\00\060\00\12\F20\00\1A\18 \001\12\AA\07P\01 \FF3\B0\01\00\A0\00\1B\03\A0\000\00\07  \01\15\02\A0\00\15\03\A0\00\10\E2\F0\00 \02\05\10\01#\FF\C0@\00Sx\09\03\08\00 \00\11\CA\80\003\09\00X\80\00\22\C8\0F\10\02p\\\00\00p\10\F2\04\90\00T$\14\03\FF\04 \01\00`\000\1A\02\000\00\13\09p\01c%\16\02\02\00ZP\02q\CC\0F\00\81\19\02\02\D0\01\C4\19\1E\0C\00\A2\00\00\10x\06\00\10\D0\00\000\02\92t\04\FF\00\00\80\FF\FF\00\A0\001\1Cx\00\01\001p\F0\F00\02E$x\07\05p\00q\E2\0F\04\0Cz\00\06\90\00#`\F6 \00\16\06 \00\80\C6\0F\00\0Cx\00\05\7F@\00\C5D\F6\01\00\E4\0F\04\10x\03\00\08p\00`\1F\00!\12\04\02q\00\01\F1\04!\E2O0\00\11\070\00 \F2\04\C0\00c\88s\00\07\04\00\DE\05f\E8\0F\00\1D{\00\01\00u\EC\0F\00\84\B9\0B\06~\05R\22\0E\00\10x\B0\02\01`\00Q\C4\0F\00\10x\1E\00\03p\00\000\004\02\06\00P\00qb\0E\00\0B\B2\00\0B\06\08\B1\80\F4\03\00\E4\1F\08\0B\B2\00\02\10\00\A3\C0\F8\03\00\E4/\00\1C\B8\00\00\01%p\01\10\02\01\F0\00B\F4\03\00\C8\E0\00\11?\B0\00\C3t\01\00\CE\0F\00\08\82\0B\0B\02\00'\07\1B\E4@\01\8F\C6\0F\00\88\B3\00\06\0B\D0\00\095\A9\03\06\8E\06[(\0E\00\84\A9\B0\001\A2\00\03p\03`\80\F6\03\00\C4\1F\10\00(\02\03\B0\00$\A8\00p\00\04\B0\00\15\04\A0\01\03\B0\00\14\1F\90\01\01\B0\00/\03\03\B0\00\0AO\A3\00\06\03\80\01\0AV\07\06\00\80\00\B0\00\0E`\01$\07\07`\01\00\B0\00O\B2\00\02\07`\01\0B\1C\00`\01\19\0F`\01\02\10\04\0F`\01\09\1F\07`\01\0D\17@\B0\00\13\A9g\07\0F`\01\07\1F\00`\01\06\12\DA@\01\14\00\90\00\1F\CA0\01\0F9M\19\00\C0\05G$t\02\FFP\03A\00\84y\05_\09\01@\00\96&\0E\00%v\02\09\00`0\04'\84y\B0\00fh\0E\00\81y\08\D0\03bb\05\00\0Br\00\C1\05\22\80\F0\C0\001r\00\00\10\00\92\C0\F2\03\00\D6/\00\08\82\1F\00p\00\00\80\04\00\C8O \00\11\08\11\00 @\F0\C0\03$\0EF\A0\06\00\A0\00b\E6\0F\00\08r\09 \00\00\01\00p\CC\0F\00\A9s\09\02{\00\C0\09\E1\1E\00\00\A4\0E\00\0Cr\00\09\10\00 pR@\00QO\00$r\08\00\05\10\09\D0\00\80\D8\0F\00G\09\00\00\90\D0\05!\FF\83\F0\00*My\00\01TGy\00\00\F0 \00f\C0\0F\00\18y\00\01\00\0F\10\00\B0\0F\01\00-#\01\00\80\02\0B\01\00\22@\00\01\00=\9E\01\000\00\08\01\00\1F\0B@\00\04\13\DE)\00?\09\02\00@\00\0A\22\13\00@\05\0C\01\00\13\E8U\00\03\0F\03\01$\00\13\05\97\02\00\01\00\22\18\00\01\00.q\01T\00\00\01\00\11\90\B5\02O\00\00p\00\80\00\0B\1F)'\00\03\03\D5\02$\00\00\18\0D\04\E4\00*\04\00\01\00\1Fc@\00\04*0\05\C0\00\13\03\03\09\0C@\00!\8F\01D\01\0D@\00\13\D8@\00*\D8\00\01\00\1B\08\08\00?~\01\00N\0E\002\00\00\B0\C6\03\01W\09\04\80\00\17\048\00\04\18\00\138@\01\0C\84\01\13\C0@\00\17\881\01\0F\C0\00\01\132T\01\02\E6\07\06\01\00\1B\80\A9\00\11\03$\00J\00\0E\80\00\01\00\13\97#\00*\03\00\01\00\040\13/\00\04\80\00\0B\13\06\AB\01\04h\13\0C\01\00\1B\A8\08\00\17\08\08\02\17\05\E8\00\0C\01\00*\C0\09\08\00\088\00\18\06\A0\00\0F\01\00\05\03\A9\00\80\08\00\00\00\00\00\00\00\00\00\00\00\00\00\00"} {
    llvm.mlir.global internal @__wg_main_kColReduction_reduce__4_1_0___8w32h_1_0() {addr_space = 3 : i32} : !llvm.array<256 x f32>
    llvm.func @__nv_fabsf(f32) -> f32
    llvm.func @main_kColReduction_reduce__4_1_0___8w32h_1(%arg0: i32, %arg1: !llvm.ptr<f32>, %arg2: i32, %arg3: i32, %arg4: i32, %arg5: i32, %arg6: !llvm.ptr<f32>) attributes {disc.elimargs = [1 : index, 3 : index, 5 : index, 6 : index, 7 : index, 8 : index, 9 : index, 13 : index, 15 : index, 16 : index, 17 : index], gpu.kernel, nvvm.kernel} {
      %0 = llvm.mlir.constant(8 : index) : i32
      %1 = llvm.mlir.constant(32 : index) : i32
      %2 = llvm.mlir.constant(2 : index) : i32
      %3 = llvm.mlir.constant(0xFF800000 : f32) : f32
      %4 = llvm.mlir.constant(16 : index) : i32
      %5 = llvm.mlir.constant(128 : index) : i32
      %6 = llvm.mlir.constant(64 : index) : i32
      %7 = llvm.mlir.constant(4 : index) : i32
      %8 = llvm.mlir.constant(256 : index) : i32
      %9 = llvm.mlir.constant(0 : index) : i32
      %10 = llvm.mlir.addressof @__wg_main_kColReduction_reduce__4_1_0___8w32h_1_0 : !llvm.ptr<array<256 x f32>, 3>
      %11 = llvm.getelementptr %10[0, 0] : (!llvm.ptr<array<256 x f32>, 3>) -> !llvm.ptr<f32, 3>
      %12 = nvvm.read.ptx.sreg.ctaid.x : i32
      %13 = nvvm.read.ptx.sreg.tid.x : i32
      llvm.br ^bb1
    ^bb1:  // pred: ^bb0
      %14 = llvm.mul %12, %arg3  : i32
      %15 = llvm.add %13, %14  : i32
      %16 = llvm.icmp "ult" %15, %arg4 : i32
      llvm.cond_br %16, ^bb2, ^bb18
    ^bb2:  // pred: ^bb1
      %17 = llvm.srem %15, %8  : i32
      %18 = llvm.sdiv %15, %8  : i32
      %19 = llvm.udiv %17, %0  : i32
      %20 = llvm.urem %17, %0  : i32
      %21 = llvm.udiv %18, %arg5  : i32
      %22 = llvm.urem %18, %arg5  : i32
      %23 = llvm.mul %21, %1  : i32
      %24 = llvm.add %23, %19  : i32
      %25 = llvm.mul %22, %0  : i32
      %26 = llvm.add %25, %20  : i32
      %27 = llvm.icmp "ult" %24, %arg2 : i32
      %28 = llvm.icmp "ult" %26, %arg0 : i32
      %29 = llvm.and %27, %28  : i1
      llvm.cond_br %29, ^bb3, ^bb4
    ^bb3:  // pred: ^bb2
      %30 = llvm.mul %24, %arg0  : i32
      %31 = llvm.add %30, %26  : i32
      %32 = llvm.getelementptr %arg1[%31] : (!llvm.ptr<f32>, i32) -> !llvm.ptr<f32>
      %33 = llvm.load %32 : !llvm.ptr<f32>
      %34 = llvm.call @__nv_fabsf(%33) : (f32) -> f32
      %35 = llvm.getelementptr %11[%17] : (!llvm.ptr<f32, 3>, i32) -> !llvm.ptr<f32, 3>
      llvm.store %34, %35 : !llvm.ptr<f32, 3>
      llvm.br ^bb5
    ^bb4:  // pred: ^bb2
      %36 = llvm.getelementptr %11[%17] : (!llvm.ptr<f32, 3>, i32) -> !llvm.ptr<f32, 3>
      llvm.store %3, %36 : !llvm.ptr<f32, 3>
      llvm.br ^bb5
    ^bb5:  // 2 preds: ^bb3, ^bb4
      nvvm.barrier0
      %37 = llvm.icmp "ult" %19, %4 : i32
      %38 = llvm.add %24, %4  : i32
      %39 = llvm.icmp "ult" %38, %arg2 : i32
      %40 = llvm.and %37, %39  : i1
      llvm.cond_br %40, ^bb6, ^bb7
    ^bb6:  // pred: ^bb5
      %41 = llvm.add %17, %5  : i32
      %42 = llvm.getelementptr %11[%17] : (!llvm.ptr<f32, 3>, i32) -> !llvm.ptr<f32, 3>
      %43 = llvm.load %42 : !llvm.ptr<f32, 3>
      %44 = llvm.getelementptr %11[%41] : (!llvm.ptr<f32, 3>, i32) -> !llvm.ptr<f32, 3>
      %45 = llvm.load %44 : !llvm.ptr<f32, 3>
      %46 = llvm.fcmp "ugt" %43, %45 : f32
      %47 = llvm.select %46, %43, %45 : i1, f32
      %48 = llvm.fcmp "uno" %45, %45 : f32
      %49 = llvm.select %48, %45, %47 : i1, f32
      %50 = llvm.getelementptr %11[%17] : (!llvm.ptr<f32, 3>, i32) -> !llvm.ptr<f32, 3>
      llvm.store %49, %50 : !llvm.ptr<f32, 3>
      llvm.br ^bb7
    ^bb7:  // 2 preds: ^bb5, ^bb6
      nvvm.barrier0
      %51 = llvm.icmp "ult" %19, %0 : i32
      %52 = llvm.add %24, %0  : i32
      %53 = llvm.icmp "ult" %52, %arg2 : i32
      %54 = llvm.and %51, %53  : i1
      llvm.cond_br %54, ^bb8, ^bb9
    ^bb8:  // pred: ^bb7
      %55 = llvm.add %17, %6  : i32
      %56 = llvm.getelementptr %11[%17] : (!llvm.ptr<f32, 3>, i32) -> !llvm.ptr<f32, 3>
      %57 = llvm.load %56 : !llvm.ptr<f32, 3>
      %58 = llvm.getelementptr %11[%55] : (!llvm.ptr<f32, 3>, i32) -> !llvm.ptr<f32, 3>
      %59 = llvm.load %58 : !llvm.ptr<f32, 3>
      %60 = llvm.fcmp "ugt" %57, %59 : f32
      %61 = llvm.select %60, %57, %59 : i1, f32
      %62 = llvm.fcmp "uno" %59, %59 : f32
      %63 = llvm.select %62, %59, %61 : i1, f32
      %64 = llvm.getelementptr %11[%17] : (!llvm.ptr<f32, 3>, i32) -> !llvm.ptr<f32, 3>
      llvm.store %63, %64 : !llvm.ptr<f32, 3>
      llvm.br ^bb9
    ^bb9:  // 2 preds: ^bb7, ^bb8
      nvvm.barrier0
      %65 = llvm.icmp "ult" %19, %7 : i32
      %66 = llvm.add %24, %7  : i32
      %67 = llvm.icmp "ult" %66, %arg2 : i32
      %68 = llvm.and %65, %67  : i1
      llvm.cond_br %68, ^bb10, ^bb11
    ^bb10:  // pred: ^bb9
      %69 = llvm.add %17, %1  : i32
      %70 = llvm.getelementptr %11[%17] : (!llvm.ptr<f32, 3>, i32) -> !llvm.ptr<f32, 3>
      %71 = llvm.load %70 : !llvm.ptr<f32, 3>
      %72 = llvm.getelementptr %11[%69] : (!llvm.ptr<f32, 3>, i32) -> !llvm.ptr<f32, 3>
      %73 = llvm.load %72 : !llvm.ptr<f32, 3>
      %74 = llvm.fcmp "ugt" %71, %73 : f32
      %75 = llvm.select %74, %71, %73 : i1, f32
      %76 = llvm.fcmp "uno" %73, %73 : f32
      %77 = llvm.select %76, %73, %75 : i1, f32
      %78 = llvm.getelementptr %11[%17] : (!llvm.ptr<f32, 3>, i32) -> !llvm.ptr<f32, 3>
      llvm.store %77, %78 : !llvm.ptr<f32, 3>
      llvm.br ^bb11
    ^bb11:  // 2 preds: ^bb9, ^bb10
      nvvm.barrier0
      %79 = llvm.icmp "ult" %19, %2 : i32
      %80 = llvm.add %24, %2  : i32
      %81 = llvm.icmp "ult" %80, %arg2 : i32
      %82 = llvm.and %79, %81  : i1
      llvm.cond_br %82, ^bb12, ^bb13
    ^bb12:  // pred: ^bb11
      %83 = llvm.add %17, %4  : i32
      %84 = llvm.getelementptr %11[%17] : (!llvm.ptr<f32, 3>, i32) -> !llvm.ptr<f32, 3>
      %85 = llvm.load %84 : !llvm.ptr<f32, 3>
      %86 = llvm.getelementptr %11[%83] : (!llvm.ptr<f32, 3>, i32) -> !llvm.ptr<f32, 3>
      %87 = llvm.load %86 : !llvm.ptr<f32, 3>
      %88 = llvm.fcmp "ugt" %85, %87 : f32
      %89 = llvm.select %88, %85, %87 : i1, f32
      %90 = llvm.fcmp "uno" %87, %87 : f32
      %91 = llvm.select %90, %87, %89 : i1, f32
      %92 = llvm.getelementptr %11[%17] : (!llvm.ptr<f32, 3>, i32) -> !llvm.ptr<f32, 3>
      llvm.store %91, %92 : !llvm.ptr<f32, 3>
      llvm.br ^bb13
    ^bb13:  // 2 preds: ^bb11, ^bb12
      nvvm.barrier0
      %93 = llvm.icmp "eq" %19, %9 : i32
      %94 = llvm.and %93, %29  : i1
      llvm.cond_br %94, ^bb14, ^bb17
    ^bb14:  // pred: ^bb13
      %95 = llvm.add %17, %0  : i32
      %96 = llvm.getelementptr %11[%17] : (!llvm.ptr<f32, 3>, i32) -> !llvm.ptr<f32, 3>
      %97 = llvm.load %96 : !llvm.ptr<f32, 3>
      %98 = llvm.getelementptr %11[%95] : (!llvm.ptr<f32, 3>, i32) -> !llvm.ptr<f32, 3>
      %99 = llvm.load %98 : !llvm.ptr<f32, 3>
      %100 = llvm.fcmp "ugt" %97, %99 : f32
      %101 = llvm.select %100, %97, %99 : i1, f32
      %102 = llvm.fcmp "uno" %99, %99 : f32
      %103 = llvm.select %102, %99, %101 : i1, f32
      %104 = llvm.getelementptr %arg6[%26] : (!llvm.ptr<f32>, i32) -> !llvm.ptr<f32>
      %105 = llvm.load %104 : !llvm.ptr<f32>
      llvm.br ^bb15(%105 : f32)
    ^bb15(%106: f32):  // 2 preds: ^bb14, ^bb15
      %107 = llvm.fcmp "ogt" %106, %103 : f32
      %108 = llvm.select %107, %106, %103 : i1, f32
      %109 = llvm.bitcast %104 : !llvm.ptr<f32> to !llvm.ptr<i32>
      %110 = llvm.bitcast %106 : f32 to i32
      %111 = llvm.bitcast %108 : f32 to i32
      %112 = llvm.cmpxchg %109, %110, %111 acq_rel monotonic : !llvm.ptr<i32>, i32
      %113 = llvm.extractvalue %112[0] : !llvm.struct<(i32, i1)> 
      %114 = llvm.bitcast %113 : i32 to f32
      %115 = llvm.extractvalue %112[1] : !llvm.struct<(i32, i1)> 
      llvm.cond_br %115, ^bb16, ^bb15(%114 : f32)
    ^bb16:  // pred: ^bb15
      llvm.br ^bb17
    ^bb17:  // 2 preds: ^bb13, ^bb16
      llvm.br ^bb18
    ^bb18:  // 2 preds: ^bb1, ^bb17
      llvm.return
    }
  }
  gpu.module @main_kernel_1 attributes {gpu.binary = "P\EDU\BA\01\00\10\008\04\00\00\00\00\00\00\02\00\01\01@\00\00\00\F8\03\00\00\00\00\00\00\F8\03\00\00\00\00\00\00\07\00\01\00P\00\00\00\00\00\00\00\00\00\00\00\11 \00\00\00\00\00\00\00\00\00\00\00\00\00\00\E8\0B\00\00\00\00\00\00\A2\7FELF\02\01\013\07\00\01\00f\02\00\BE\00q\00\01\00!@\0B\07\001\00\80\08\07\00\F5\0E\00P\05P\00@\008\00\03\00@\00\0B\00\01\00\00.shstrtab\00.\08\00'ym\08\00\F0\1B_shndx\00.nv.info\00.text.main_kColReduction_r\0A\00\F5\01e__4_1_0___8w16h8\00\0F2\00\1Boshared4\00\1B\9Fconstant07\00\18\FA\01debug_frame\00.rel\11\00!nv\14\00\11aC\00\0F+\01 \0F\88\00\15\0FT\01\BAo_param[\01\1C\0F\01\00\06\8C[\00\00\00\03\00\0A\00\01\00\11\F0\18\00,\09\00\01\00 .\01\18\00,\04\00\01\00\11L\18\00,\07\00\01\00f2\00\00\00\12\10`\00!\80\01\07\00v\00\FF\FF\FF\FF(\00\0C\00\00\01\00@\03\00\04|\08\00\D1\0F\0C\81\80\80(\00\08\FF\81\80(\08\0B\00\06(\00/0\00\01\00\03\13pX\00\10\04\9B\00R\04\14\00\00\00E\00\01\0B\00\00\13\00p/\08\00\05\00\00\00\A7\03\22\04#\0C\00\00\01\00'\04\12\0C\00\17\11\0C\00!7\04\E8\03\F3\08\015\00\00\04\0A\08\00\02\00\00\00`\01\10\00\03\19\10\00\04\17\0C\0C\04U\08\00\00\F0!\10\00\10\01\18\01%\F0\11\10\00\01\01\00\F2\02\F0\11\00\03\1B\FF\00\04\1C\08\00P\00\00\00\B0\00\01\00#K\00\01\00s\02\02\08\10\0A/\22\9B\00\00\07\00\03\08\00\13\10\08\00\13\18\08\00\13 \08\00\13(\08\00\130\08\00\228\08|\00\13\00\08\00\13\08\08\00\13\10\08\00\13\18\08\00\13 \08\00\13(\08\00\130\08\00\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\A2\00\00\00\14,\00\00\00H\00\01\00\000\01/\05\00\01\00\FF\B0A\02z\01\00\1F\04\B1\0F\00\00\00\C4\0F\00\19y\02\00\01\00\10%\8B\02Q\0E\00\19y\03\0F\00\F5\1A\00!\00\00\00$\0E\00$z\02\02\00X\00\00\03\02\8E\07\00\CA\1F\00\0Cz\00\02\00Y\00\00p`\F0\03\00\DA\0F\00MS\04\A0\80\03\00\EA\0F\005t\03\FF\B3\03\10\FF\C0\03P\E2\0F\00\02x6\02B\80\FF\00\0F\10\00r\B9z\04\00\00F\00\84\00\94\D0\0F\00%v\02\02\00Z`\00`\0F\00\86y\00\022\00@\04\19\10\0C0\009My\00`\00PGy\00\00\F09\04\A6\FF\83\03\00\C0\0F\00\18y\00\01\00\0F\10\00\90\0F\01\00-\00W\01.\03\00\01\00\22@\00\01\00=+\01\000\00\08\01\00\1F\0B@\00\04\13k)\00\1F[@\00\0C\13\13\0C\04\0C\01\00\13\C8\15\00&\90\000\04#\04\00\85\04\00\F6\04\12\00\01\00\1F\FET\00\00\00\01\00\13X\95\00/p\00\80\00\0B\1F)'\00\03#\00\C8@\00\04P\06\04\E4\00*\04\00\01\00\1Fa@\00\04\13\F81\00&\\\00@\00\1F\0A@\00\00!\1C\01D\01\0D@\00\13X)\00*\D8\00\01\00\1B\08\08\00?\0B\01\00\86\07\00Q\00\000\05\00\01\00&\10\00\80\00\17\048\00\04\18\00\13\C7\14\01\0C\84\01*@\058\07\1F\00\C0\00\04\132@\00+\06\00\01\00\1A\07\D0\07\12\03\F0\05:\08\80\00\01\00\13\06\08\06\04(\0B\0C\01\00*\A8\00\08\00\04\F8\00\14\018\00/\05\00\01\00\029@\03\00\08\00\088\00/\06\00\01\00\17\80\08\00\00\00\00\00\00\00"} {
    llvm.func @main_kColReduction_reduce__4_1_0___8w16h(%arg0: i32, %arg1: i32, %arg2: !llvm.ptr<f32>) attributes {disc.elimargs = [2 : index, 4 : index, 5 : index, 6 : index], gpu.kernel, nvvm.kernel} {
      %0 = llvm.mlir.constant(0xFF800000 : f32) : f32
      %1 = nvvm.read.ptx.sreg.ctaid.x : i32
      %2 = nvvm.read.ptx.sreg.tid.x : i32
      llvm.br ^bb1
    ^bb1:  // pred: ^bb0
      %3 = llvm.mul %1, %arg0  : i32
      %4 = llvm.add %2, %3  : i32
      %5 = llvm.icmp "ult" %4, %arg1 : i32
      llvm.cond_br %5, ^bb2, ^bb3
    ^bb2:  // pred: ^bb1
      %6 = llvm.getelementptr %arg2[%4] : (!llvm.ptr<f32>, i32) -> !llvm.ptr<f32>
      llvm.store %0, %6 : !llvm.ptr<f32>
      llvm.br ^bb3
    ^bb3:  // 2 preds: ^bb1, ^bb2
      llvm.return
    }
  }
  gpu.module @main_kernel_2 attributes {gpu.binary = "P\EDU\BA\01\00\10\00\10\08\00\00\00\00\00\00\02\00\01\01@\00\00\00\D0\07\00\00\00\00\00\00\CB\07\00\00\00\00\00\00\07\00\01\00P\00\00\00\00\00\00\00\00\00\00\00\11 \00\00\00\00\00\00\00\00\00\00\00\00\00\00(\13\00\00\00\00\00\00\A2\7FELF\02\01\013\07\00\01\00f\02\00\BE\00q\00\01\00#\80\12\08\00\11\0F\07\00\F5\0E\00P\05P\00@\008\00\03\00@\00\0C\00\01\00\00.shstrtab\00.\08\00'ym\08\00\F0\1B_shndx\00.nv.info\00.text.main_kColReduction_r\0A\00\F5\03e__4_1_0___8w16h_1:\00\0F4\00\1Doshared6\00\1AOrela\A0\00\1F?rel\D5\00\22\9Fconstant09\00\1A\B2debug_frame{\00\09\11\00!nv\14\00\11aE\00\0F\9E\01 \0F\8A\00\17\0F\C9\01\F4\8F$____wg_3\00\17\00\0C\00/27\02\02'o_param\09\02\1C\0F\01\00\05\8C]\00\00\00\03\00\0A\00\01\00\11\C2\18\00,\0B\00\01\00 \9C\01\18\00,\09\00\01\00\11\DC\18\00,\04\00\01\00\11\FA\18\00,\07\00\01\00g2\00\00\00\12\10x\00\03#\00f\FF\FF\FF\FF(\00\0C\00\00\01\00@\03\00\04|\08\00\D1\0F\0C\81\80\80(\00\08\FF\81\80(\08\0B\00\06(\00/0\00\01\00\03!\F0\06\07\00 \00\04\9B\00R\04\14\00\00\00E\002\04|\01\18\000/\08\00#\00\10\0E\0C\00\12#\0C\00\00\01\00'\04\12\0C\00\17\11\0C\00!7\04 \05\F1\08\015\00\00\04\0A\08\00\03\00\00\00`\01(\00\03\19(\00\04\17\0C$\00u\06\00 \00\00\F0!\10\00u\05\00\1C\00\00\F0\11\10\009\04\00\18\10\009\03\00\14\10\009\02\00\10\10\009\01\00\08P\00\01\01\00\C1\F0\11\00\03\1B\FF\00\04\1C\0C\00P\98\05\82\00\00P\06\00\00\04\1E\84\01#K\00\01\00\B1\02\02\08\10\0A/\22\00\00\00\08\10\00#\00\08\08\00\13\10\08\00\13\18\08\00\13 \08\00\13(\08\00\130\08\00\228\08\84\00\13\00\08\00\13\08\08\00\13\10\08\00\13\18\08\00\13 \08\00\13(\08\00\130\08\00\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\A2\00\00\00\14,\00\00\00H\00\01\00\11\02h\01\0F\01\00\FF\B0@$v\01\FF\7F\04\B2\FF\00\8E\07\00\C4\0F\00\19y\00\01\00\10%\8B\02a\0E\00\19y\03\00\01\00\10!-\00B\0E\00$z\B5\04\90\03\02\8E\07\00\CA\1F\00\0C\10\00\C5^\00\00p`\F0\03\00\DA\0F\00M\9B\04\F0\0C\80\03\00\EA\0F\00\06{\04\00\00_\00\00\00\90 \00\00\22\0E\00\19x\05\FF\1F\1F\00\D3\14\01\00\00\E2\0F\00$r\02\FF\FF\00\80\00 \E2\0FP\00\10\FF0\00@pP\F4\03\10\00\81\B9z\04\00\00F\00\00#\05p\E2\0F\00\11r\05\05?\00\B2\FF8\8F\07\00\C6\0F\00\08s\04\F6\04\10\10\A0\00\F1\06\1E\00\10x\03\04\FE\FF\FF\0F\FF\E0\FF\07\00\CC\1F\00\05s\03$\04\00\C0\03\01\C0\00!r\07p\00\C0\03\0A\8E\07\00\C8\1F\00$z\07\07p\00\10\FF\D0\00\81\C8\0F\00'r\07\03\07\E0\02\02\90\00@\19x\02\FF\10\00\10\05\B0\00\80\E4\0F\00\12x\05\05\80\F1\04!\C0\8E\80\00T'r\07\07\02\C0\00\10\C8\D0\00\11\09`\00\10\07`\00`\E4\0F\00$x\05\BC\03$\00\05\10\000z\03\09p\00\22\02\02@\00@\19x\00\FF\A0\00\22\05\16`\00f\0Cz\00\03\00_P\010\10\0A\03\10\00\11\80\D0\00p\E4\0F\00\10\08\07\07P\00\04\10\00\060\00\12\F20\00\1A\18 \001\12\AA\07P\01 \FF3\B0\01\00\A0\00\1B\03\A0\001\00\07\10\D1\03\05\A0\00\15\03\A0\00\10\E2\F0\00 \02\05\00\01#\FF\C0@\00Sx\09\03\08\00 \00\11\CA\80\003\09\00X\80\00\22\C8\0F\10\02p\\\00\00p\10\F2\04\90\00T$\14\03\FF\04 \01\00`\000\1A\02\000\00\13\09p\01c%\16\02\02\00ZP\02q\CC\0F\00\81\19\02\02\D0\01\C4\19\1E\0C\00\A2\00\00\10x\06\00\08\D0\00\000\02\92t\04\FF\00\00\80\FF\FF\00\A0\001\1Cx\00\01\001p\F0\F00\02E$x\07\05p\00q\E2\0F\04\0Cz\00\06\90\00#`\F6 \00\16\06 \00\80\C6\0F\00\0Cx\00\05?@\00\92D\F6\01\00\E4\0F\04\10xF\07\02@\01B\1F\04\10x\EC\04\04\80\00@!\12\04\02\81\00\01\15\00!\E2O@\00\11\07@\00 \F2\04\D0\00c\88s\00\07\04\00\EE\05f\E8\0F\00\1D{\00\01\00u\EC\0F\00\84\B9\0B\06\CE\05\84(\0E\00\84\B9\02\06\000\00qb\0E\00\0B\B2\00\0B\F6\07`\80\F4\03\00\C4\1F\10\00\11\02\10\00\A3\C0\F8\03\00\E4/\00\1C\B8\00\F0\00%p\01\00\02\01\E0\00B\F4\03\00\C8\D0\00\11\1F\90\00\C3t\01\00\CE\0F\00\08\82\0B\0B\02\00\17\07\1B\E40\01\8F\C6\0F\00\88\B3\00\06\0B\B0\00\09f\A9\03\06\00\80\00\B0\00\1B\A9\B0\001\A2\00\03`\03\22\80\F6\B0\00H\A2\00\02\03\B0\00$\A8\00p\00\04\B0\00\15\00\90\01\03\B0\00\14\0F\80\01\01\B0\00/\03\03\B0\00\0AO\A3\00\06\03`\01\0AG\07\06\00@\B0\00\13\B9\A7\06\06`\01*\07\07`\01/\00\07`\01\05\10\DA\90\00\02/\00\01\90\00\12\CA0\01\1F\07\80\00\089M\19\00\00\05G$t\02\FF\90\02A\00\84y\05\9F\08\01@\00\96&\0E\00%v\02\09\00`p\03'\84y\B0\00fh\0E\00\81y\08\10\03bb\05\00\0Br\00\01\05\22\80\F0\C0\001r\00\00\10\00\92\C0\F2\03\00\D6/\00\08\82\1F\00p\00\00\80\04\00\C8O \00\11\08\11\00 @\F0\00\03$\0EF\E0\05\00\A0\00b\E6\0F\00\08r\09 \00\00\01\00p\CC\0F\00\A9s\09\02{\00\C0\09\E1\1E\00\00\A4\0E\00\0Cr\00\09\10\00 pR@\00QO\00$r\08@\04\10\09\D0\00\80\D8\0F\00G\09\00\00\90\10\05!\FF\83\F0\00*My\00\01TGy\00\00\F0 \00f\C0\0F\00\18y\00\01\00\0F\10\00p\0F\01\00-\11\01@\0A\0E\01\00\22@\00\01\00=\9E\01\000\00\08\01\00\1F\0B@\00\04\13\DE)\00?\09\02\00@\00\0A\22\13\00\A0\04\0C\01\00\13\E8U\00\03\7F\03\01$\00\13\05W\02\00\01\00\22\18\00\01\00.q\01T\00\00\01\00\11\90u\02O\00\00p\00\80\00\0B\1F)'\00\03\03\95\02$\00\00\18\0C\04\E4\00*\04\00\01\00\1Fc@\00\04*0\05\C0\00\13\03\03\08\0C@\00!\8F\01D\01\0D@\00\13\D8@\00*\D8\00\01\00\1B\08\08\00?~\01\00N\0D\002\00\00\B0\86\03\01W\08\04\80\00\17\048\00\04\18\00\138@\01\0C\84\01\13\C0@\00\17\881\01\0F\C0\00\01\132T\01\15\06R\00\03>\03\1A\08\98\0D\11\03$\00J\00\0E\80\00\01\00\13\97\94\00*\03\00\01\00\040\12/\00\02\80\00\0B\13\06\AB\01\04h\12\0C\01\00\1B\A8\08\00\04\97\00\13\018\00\04\E8\00\0C\01\00*\C0\08\08\00\088\00\18\06\A0\00\0F\01\00\05\03\B8\00\80\08\00\00\00\00\00\00\00\00\00\00\00\00"} {
    llvm.mlir.global internal @__wg_main_kColReduction_reduce__4_1_0___8w16h_1_0() {addr_space = 3 : i32} : !llvm.array<128 x f32>
    llvm.func @__nv_fabsf(f32) -> f32
    llvm.func @main_kColReduction_reduce__4_1_0___8w16h_1(%arg0: i32, %arg1: !llvm.ptr<f32>, %arg2: i32, %arg3: i32, %arg4: i32, %arg5: i32, %arg6: !llvm.ptr<f32>) attributes {disc.elimargs = [1 : index, 3 : index, 5 : index, 6 : index, 7 : index, 8 : index, 9 : index, 13 : index, 15 : index, 16 : index, 17 : index], gpu.kernel, nvvm.kernel} {
      %0 = llvm.mlir.constant(8 : index) : i32
      %1 = llvm.mlir.constant(16 : index) : i32
      %2 = llvm.mlir.constant(2 : index) : i32
      %3 = llvm.mlir.constant(0xFF800000 : f32) : f32
      %4 = llvm.mlir.constant(64 : index) : i32
      %5 = llvm.mlir.constant(4 : index) : i32
      %6 = llvm.mlir.constant(32 : index) : i32
      %7 = llvm.mlir.constant(128 : index) : i32
      %8 = llvm.mlir.constant(0 : index) : i32
      %9 = llvm.mlir.addressof @__wg_main_kColReduction_reduce__4_1_0___8w16h_1_0 : !llvm.ptr<array<128 x f32>, 3>
      %10 = llvm.getelementptr %9[0, 0] : (!llvm.ptr<array<128 x f32>, 3>) -> !llvm.ptr<f32, 3>
      %11 = nvvm.read.ptx.sreg.ctaid.x : i32
      %12 = nvvm.read.ptx.sreg.tid.x : i32
      llvm.br ^bb1
    ^bb1:  // pred: ^bb0
      %13 = llvm.mul %11, %arg3  : i32
      %14 = llvm.add %12, %13  : i32
      %15 = llvm.icmp "ult" %14, %arg4 : i32
      llvm.cond_br %15, ^bb2, ^bb16
    ^bb2:  // pred: ^bb1
      %16 = llvm.srem %14, %7  : i32
      %17 = llvm.sdiv %14, %7  : i32
      %18 = llvm.udiv %16, %0  : i32
      %19 = llvm.urem %16, %0  : i32
      %20 = llvm.udiv %17, %arg5  : i32
      %21 = llvm.urem %17, %arg5  : i32
      %22 = llvm.mul %20, %1  : i32
      %23 = llvm.add %22, %18  : i32
      %24 = llvm.mul %21, %0  : i32
      %25 = llvm.add %24, %19  : i32
      %26 = llvm.icmp "ult" %23, %arg2 : i32
      %27 = llvm.icmp "ult" %25, %arg0 : i32
      %28 = llvm.and %26, %27  : i1
      llvm.cond_br %28, ^bb3, ^bb4
    ^bb3:  // pred: ^bb2
      %29 = llvm.mul %23, %arg0  : i32
      %30 = llvm.add %29, %25  : i32
      %31 = llvm.getelementptr %arg1[%30] : (!llvm.ptr<f32>, i32) -> !llvm.ptr<f32>
      %32 = llvm.load %31 : !llvm.ptr<f32>
      %33 = llvm.call @__nv_fabsf(%32) : (f32) -> f32
      %34 = llvm.getelementptr %10[%16] : (!llvm.ptr<f32, 3>, i32) -> !llvm.ptr<f32, 3>
      llvm.store %33, %34 : !llvm.ptr<f32, 3>
      llvm.br ^bb5
    ^bb4:  // pred: ^bb2
      %35 = llvm.getelementptr %10[%16] : (!llvm.ptr<f32, 3>, i32) -> !llvm.ptr<f32, 3>
      llvm.store %3, %35 : !llvm.ptr<f32, 3>
      llvm.br ^bb5
    ^bb5:  // 2 preds: ^bb3, ^bb4
      nvvm.barrier0
      %36 = llvm.icmp "ult" %18, %0 : i32
      %37 = llvm.add %23, %0  : i32
      %38 = llvm.icmp "ult" %37, %arg2 : i32
      %39 = llvm.and %36, %38  : i1
      llvm.cond_br %39, ^bb6, ^bb7
    ^bb6:  // pred: ^bb5
      %40 = llvm.add %16, %4  : i32
      %41 = llvm.getelementptr %10[%16] : (!llvm.ptr<f32, 3>, i32) -> !llvm.ptr<f32, 3>
      %42 = llvm.load %41 : !llvm.ptr<f32, 3>
      %43 = llvm.getelementptr %10[%40] : (!llvm.ptr<f32, 3>, i32) -> !llvm.ptr<f32, 3>
      %44 = llvm.load %43 : !llvm.ptr<f32, 3>
      %45 = llvm.fcmp "ugt" %42, %44 : f32
      %46 = llvm.select %45, %42, %44 : i1, f32
      %47 = llvm.fcmp "uno" %44, %44 : f32
      %48 = llvm.select %47, %44, %46 : i1, f32
      %49 = llvm.getelementptr %10[%16] : (!llvm.ptr<f32, 3>, i32) -> !llvm.ptr<f32, 3>
      llvm.store %48, %49 : !llvm.ptr<f32, 3>
      llvm.br ^bb7
    ^bb7:  // 2 preds: ^bb5, ^bb6
      nvvm.barrier0
      %50 = llvm.icmp "ult" %18, %5 : i32
      %51 = llvm.add %23, %5  : i32
      %52 = llvm.icmp "ult" %51, %arg2 : i32
      %53 = llvm.and %50, %52  : i1
      llvm.cond_br %53, ^bb8, ^bb9
    ^bb8:  // pred: ^bb7
      %54 = llvm.add %16, %6  : i32
      %55 = llvm.getelementptr %10[%16] : (!llvm.ptr<f32, 3>, i32) -> !llvm.ptr<f32, 3>
      %56 = llvm.load %55 : !llvm.ptr<f32, 3>
      %57 = llvm.getelementptr %10[%54] : (!llvm.ptr<f32, 3>, i32) -> !llvm.ptr<f32, 3>
      %58 = llvm.load %57 : !llvm.ptr<f32, 3>
      %59 = llvm.fcmp "ugt" %56, %58 : f32
      %60 = llvm.select %59, %56, %58 : i1, f32
      %61 = llvm.fcmp "uno" %58, %58 : f32
      %62 = llvm.select %61, %58, %60 : i1, f32
      %63 = llvm.getelementptr %10[%16] : (!llvm.ptr<f32, 3>, i32) -> !llvm.ptr<f32, 3>
      llvm.store %62, %63 : !llvm.ptr<f32, 3>
      llvm.br ^bb9
    ^bb9:  // 2 preds: ^bb7, ^bb8
      nvvm.barrier0
      %64 = llvm.icmp "ult" %18, %2 : i32
      %65 = llvm.add %23, %2  : i32
      %66 = llvm.icmp "ult" %65, %arg2 : i32
      %67 = llvm.and %64, %66  : i1
      llvm.cond_br %67, ^bb10, ^bb11
    ^bb10:  // pred: ^bb9
      %68 = llvm.add %16, %1  : i32
      %69 = llvm.getelementptr %10[%16] : (!llvm.ptr<f32, 3>, i32) -> !llvm.ptr<f32, 3>
      %70 = llvm.load %69 : !llvm.ptr<f32, 3>
      %71 = llvm.getelementptr %10[%68] : (!llvm.ptr<f32, 3>, i32) -> !llvm.ptr<f32, 3>
      %72 = llvm.load %71 : !llvm.ptr<f32, 3>
      %73 = llvm.fcmp "ugt" %70, %72 : f32
      %74 = llvm.select %73, %70, %72 : i1, f32
      %75 = llvm.fcmp "uno" %72, %72 : f32
      %76 = llvm.select %75, %72, %74 : i1, f32
      %77 = llvm.getelementptr %10[%16] : (!llvm.ptr<f32, 3>, i32) -> !llvm.ptr<f32, 3>
      llvm.store %76, %77 : !llvm.ptr<f32, 3>
      llvm.br ^bb11
    ^bb11:  // 2 preds: ^bb9, ^bb10
      nvvm.barrier0
      %78 = llvm.icmp "eq" %18, %8 : i32
      %79 = llvm.and %78, %28  : i1
      llvm.cond_br %79, ^bb12, ^bb15
    ^bb12:  // pred: ^bb11
      %80 = llvm.add %16, %0  : i32
      %81 = llvm.getelementptr %10[%16] : (!llvm.ptr<f32, 3>, i32) -> !llvm.ptr<f32, 3>
      %82 = llvm.load %81 : !llvm.ptr<f32, 3>
      %83 = llvm.getelementptr %10[%80] : (!llvm.ptr<f32, 3>, i32) -> !llvm.ptr<f32, 3>
      %84 = llvm.load %83 : !llvm.ptr<f32, 3>
      %85 = llvm.fcmp "ugt" %82, %84 : f32
      %86 = llvm.select %85, %82, %84 : i1, f32
      %87 = llvm.fcmp "uno" %84, %84 : f32
      %88 = llvm.select %87, %84, %86 : i1, f32
      %89 = llvm.getelementptr %arg6[%25] : (!llvm.ptr<f32>, i32) -> !llvm.ptr<f32>
      %90 = llvm.load %89 : !llvm.ptr<f32>
      llvm.br ^bb13(%90 : f32)
    ^bb13(%91: f32):  // 2 preds: ^bb12, ^bb13
      %92 = llvm.fcmp "ogt" %91, %88 : f32
      %93 = llvm.select %92, %91, %88 : i1, f32
      %94 = llvm.bitcast %89 : !llvm.ptr<f32> to !llvm.ptr<i32>
      %95 = llvm.bitcast %91 : f32 to i32
      %96 = llvm.bitcast %93 : f32 to i32
      %97 = llvm.cmpxchg %94, %95, %96 acq_rel monotonic : !llvm.ptr<i32>, i32
      %98 = llvm.extractvalue %97[0] : !llvm.struct<(i32, i1)> 
      %99 = llvm.bitcast %98 : i32 to f32
      %100 = llvm.extractvalue %97[1] : !llvm.struct<(i32, i1)> 
      llvm.cond_br %100, ^bb14, ^bb13(%99 : f32)
    ^bb14:  // pred: ^bb13
      llvm.br ^bb15
    ^bb15:  // 2 preds: ^bb11, ^bb14
      llvm.br ^bb16
    ^bb16:  // 2 preds: ^bb1, ^bb15
      llvm.return
    }
  }
  "disc_shape.SymbolicDim"() {knownNegativeOne = false, knownNonNegative = true, knownNonSizeOne = false, knownNonSizeZero = false, sym_name = "S0", value = -9223372036854775808 : i64} : () -> ()
  "disc_shape.SymbolicDim"() {knownNegativeOne = false, knownNonNegative = true, knownNonSizeOne = false, knownNonSizeZero = false, sym_name = "S1", value = -9223372036854775808 : i64} : () -> ()
  "disc_shape.SymbolicDim"() {knownNegativeOne = false, knownNonNegative = true, knownNonSizeOne = false, knownNonSizeZero = false, sym_name = "S2", value = -9223372036854775808 : i64} : () -> ()
  "disc_shape.SymbolicDim"() {knownNegativeOne = false, knownNonNegative = true, knownNonSizeOne = false, knownNonSizeZero = false, sym_name = "S3", value = -9223372036854775808 : i64} : () -> ()
  func.func @shape_constraint_graph() {
    %0 = "disc_shape.dim"() {name = @S3} : () -> index
    %1 = "disc_shape.dim"() {name = @S1} : () -> index
    %2 = "disc_shape.dim"() {name = @S2} : () -> index
    "disc_shape.tie_product_equal"(%0, %1, %2) {operand_segment_sizes = array<i32: 1, 2>} : (index, index, index) -> ()
    return
  }
}


// -----// IR Dump After StripDebugInfo (strip-debuginfo) //----- //
module attributes {gpu.container_module} {
  func.func @main(%arg0: !disc_ral.context) attributes {tf.entry_function = {input_placements = "gpu", inputs = "input0", output_placements = "gpu", outputs = "output0"}} {
    %c-1 = arith.constant -1 : index
    %c128 = arith.constant 128 : index
    %c16 = arith.constant 16 : index
    %c32 = arith.constant 32 : index
    %c8 = arith.constant 8 : index
    %0 = llvm.mlir.constant(0 : i32) : i32
    %c0 = arith.constant 0 : index
    %c1 = arith.constant 1 : index
    %c2 = arith.constant 2 : index
    %c256 = arith.constant 256 : index
    %c108 = arith.constant 108 : index
    %1 = "disc_ral.dispatch"(%arg0, %c0) {backend_config = "", call_target_name = "ral_recv_input", device = "cpu", has_side_effect = false} : (!disc_ral.context, index) -> memref<?x?x?xf32, "gpu">
    %dim = memref.dim %1, %c2 : memref<?x?x?xf32, "gpu">
    %dim_0 = memref.dim %1, %c1 : memref<?x?x?xf32, "gpu">
    %dim_1 = memref.dim %1, %c0 : memref<?x?x?xf32, "gpu">
    %2 = arith.index_cast %dim_0 : index to i32
    %3 = arith.index_cast %dim : index to i32
    %4 = arith.muli %2, %3 : i32
    %5 = arith.index_cast %4 : i32 to index
    %alloc = memref.alloc(%5) {kDiscSymbolicDimAttr = [@S3]} : memref<?xf32, "gpu">
    %6 = arith.muli %dim_1, %5 : index
    %7 = arith.addi %6, %c-1 : index
    %8 = arith.divsi %7, %c256 : index
    %9 = arith.addi %8, %c1 : index
    %10 = arith.subi %c0, %6 : index
    %11 = arith.divsi %10, %c256 : index
    %12 = arith.subi %c0, %11 : index
    %13 = arith.cmpi sgt, %6, %c0 : index
    %14 = arith.select %13, %9, %12 : index
    %15 = arith.cmpi sgt, %14, %c108 : index
    cf.cond_br %15, ^bb1, ^bb2
  ^bb1:  // pred: ^bb0
    %c256_2 = arith.constant 256 : index
    %c0_3 = arith.constant 0 : index
    %c1_4 = arith.constant 1 : index
    %16 = arith.cmpi sle, %5, %c0_3 : index
    %17 = arith.subi %c0_3, %5 : index
    %18 = arith.subi %5, %c1_4 : index
    %19 = arith.select %16, %17, %18 : index
    %20 = arith.divsi %19, %c256_2 : index
    %21 = arith.subi %c0_3, %20 : index
    %22 = arith.addi %20, %c1_4 : index
    %23 = arith.select %16, %21, %22 : index
    gpu.launch_func  @main_kernel::@main_kColReduction_reduce__4_1_0___8w32h blocks in (%23, %c1, %c1) threads in (%c256, %c1, %c1) args(%c256 : index, %5 : index, %alloc : memref<?xf32, "gpu">)
    %24 = arith.addi %5, %c-1 : index
    %25 = arith.divsi %24, %c8 : index
    %26 = arith.addi %25, %c1 : index
    %27 = arith.subi %c0, %5 : index
    %28 = arith.divsi %27, %c8 : index
    %29 = arith.subi %c0, %28 : index
    %30 = arith.cmpi sgt, %5, %c0 : index
    %31 = arith.select %30, %26, %29 : index
    %32 = arith.addi %dim_1, %c-1 : index
    %33 = arith.divsi %32, %c32 : index
    %34 = arith.addi %33, %c1 : index
    %35 = arith.subi %c0, %dim_1 : index
    %36 = arith.divsi %35, %c32 : index
    %37 = arith.subi %c0, %36 : index
    %38 = arith.cmpi sgt, %dim_1, %c0 : index
    %39 = arith.select %38, %34, %37 : index
    %40 = arith.muli %31, %39 : index
    %41 = arith.muli %40, %c256 : index
    %c256_5 = arith.constant 256 : index
    %c0_6 = arith.constant 0 : index
    %c1_7 = arith.constant 1 : index
    %42 = arith.cmpi sle, %41, %c0_6 : index
    %43 = arith.subi %c0_6, %41 : index
    %44 = arith.subi %41, %c1_7 : index
    %45 = arith.select %42, %43, %44 : index
    %46 = arith.divsi %45, %c256_5 : index
    %47 = arith.subi %c0_6, %46 : index
    %48 = arith.addi %46, %c1_7 : index
    %49 = arith.select %42, %47, %48 : index
    gpu.launch_func  @main_kernel_0::@main_kColReduction_reduce__4_1_0___8w32h_1 blocks in (%49, %c1, %c1) threads in (%c256, %c1, %c1) args(%5 : index, %1 : memref<?x?x?xf32, "gpu">, %c256 : index, %41 : index, %31 : index, %alloc : memref<?xf32, "gpu">)
    cf.br ^bb3
  ^bb2:  // pred: ^bb0
    %c128_8 = arith.constant 128 : index
    %c0_9 = arith.constant 0 : index
    %c1_10 = arith.constant 1 : index
    %50 = arith.cmpi sle, %5, %c0_9 : index
    %51 = arith.subi %c0_9, %5 : index
    %52 = arith.subi %5, %c1_10 : index
    %53 = arith.select %50, %51, %52 : index
    %54 = arith.divsi %53, %c128_8 : index
    %55 = arith.subi %c0_9, %54 : index
    %56 = arith.addi %54, %c1_10 : index
    %57 = arith.select %50, %55, %56 : index
    gpu.launch_func  @main_kernel_1::@main_kColReduction_reduce__4_1_0___8w16h blocks in (%57, %c1, %c1) threads in (%c128, %c1, %c1) args(%c128 : index, %5 : index, %alloc : memref<?xf32, "gpu">)
    %58 = arith.addi %5, %c-1 : index
    %59 = arith.divsi %58, %c8 : index
    %60 = arith.addi %59, %c1 : index
    %61 = arith.subi %c0, %5 : index
    %62 = arith.divsi %61, %c8 : index
    %63 = arith.subi %c0, %62 : index
    %64 = arith.cmpi sgt, %5, %c0 : index
    %65 = arith.select %64, %60, %63 : index
    %66 = arith.addi %dim_1, %c-1 : index
    %67 = arith.divsi %66, %c16 : index
    %68 = arith.addi %67, %c1 : index
    %69 = arith.subi %c0, %dim_1 : index
    %70 = arith.divsi %69, %c16 : index
    %71 = arith.subi %c0, %70 : index
    %72 = arith.cmpi sgt, %dim_1, %c0 : index
    %73 = arith.select %72, %68, %71 : index
    %74 = arith.muli %65, %73 : index
    %75 = arith.muli %74, %c128 : index
    %c128_11 = arith.constant 128 : index
    %c0_12 = arith.constant 0 : index
    %c1_13 = arith.constant 1 : index
    %76 = arith.cmpi sle, %75, %c0_12 : index
    %77 = arith.subi %c0_12, %75 : index
    %78 = arith.subi %75, %c1_13 : index
    %79 = arith.select %76, %77, %78 : index
    %80 = arith.divsi %79, %c128_11 : index
    %81 = arith.subi %c0_12, %80 : index
    %82 = arith.addi %80, %c1_13 : index
    %83 = arith.select %76, %81, %82 : index
    gpu.launch_func  @main_kernel_2::@main_kColReduction_reduce__4_1_0___8w16h_1 blocks in (%83, %c1, %c1) threads in (%c128, %c1, %c1) args(%5 : index, %1 : memref<?x?x?xf32, "gpu">, %c128 : index, %75 : index, %65 : index, %alloc : memref<?xf32, "gpu">)
    cf.br ^bb3
  ^bb3:  // 2 preds: ^bb1, ^bb2
    %84 = llvm.inttoptr %0 : i32 to !llvm.ptr<i8>
    %alloca = memref.alloca() : memref<2xindex, "cpu">
    memref.store %dim_0, %alloca[%c0] : memref<2xindex, "cpu">
    memref.store %dim, %alloca[%c1] : memref<2xindex, "cpu">
    %85 = "disc_ral.dispatch"(%arg0, %84, %alloc, %alloca) {backend_config = "", call_target_name = "inc_ref", device = "gpu", has_side_effect = false} : (!disc_ral.context, !llvm.ptr<i8>, memref<?xf32, "gpu">, memref<2xindex, "cpu">) -> memref<?x?xf32, "gpu">
    %reinterpret_cast = memref.reinterpret_cast %85 to offset: [0], sizes: [%dim_0, %dim], strides: [%dim, 1] {kDiscSymbolicDimAttr = [@S1, @S2]} : memref<?x?xf32, "gpu"> to memref<?x?xf32, "gpu">
    memref.dealloc %alloc : memref<?xf32, "gpu">
    "disc_ral.dispatch"(%arg0, %c0, %reinterpret_cast) {backend_config = "", call_target_name = "ral_send_output", device = "cpu", has_side_effect = false} : (!disc_ral.context, index, memref<?x?xf32, "gpu">) -> ()
    return
  }
  gpu.module @main_kernel attributes {gpu.binary = "P\EDU\BA\01\00\10\008\04\00\00\00\00\00\00\02\00\01\01@\00\00\00\F8\03\00\00\00\00\00\00\F8\03\00\00\00\00\00\00\07\00\01\00P\00\00\00\00\00\00\00\00\00\00\00\11 \00\00\00\00\00\00\00\00\00\00\00\00\00\00\E8\0B\00\00\00\00\00\00\A2\7FELF\02\01\013\07\00\01\00f\02\00\BE\00q\00\01\00!@\0B\07\001\00\80\08\07\00\F5\0E\00P\05P\00@\008\00\03\00@\00\0B\00\01\00\00.shstrtab\00.\08\00'ym\08\00\F0\1B_shndx\00.nv.info\00.text.main_kColReduction_r\0A\00\F5\01e__4_1_0___8w32h8\00\0F2\00\1Boshared4\00\1B\9Fconstant07\00\18\FA\01debug_frame\00.rel\11\00!nv\14\00\11aC\00\0F+\01 \0F\88\00\15\0FT\01\BAo_param[\01\1C\0F\01\00\06\8C[\00\00\00\03\00\0A\00\01\00\11\F0\18\00,\09\00\01\00 .\01\18\00,\04\00\01\00\11L\18\00,\07\00\01\00f2\00\00\00\12\10`\00!\80\01\07\00v\00\FF\FF\FF\FF(\00\0C\00\00\01\00@\03\00\04|\08\00\D1\0F\0C\81\80\80(\00\08\FF\81\80(\08\0B\00\06(\00/0\00\01\00\03\13pX\00\10\04\9B\00R\04\14\00\00\00E\00\01\0B\00\00\13\00p/\08\00\05\00\00\00\A7\03\22\04#\0C\00\00\01\00'\04\12\0C\00\17\11\0C\00!7\04\E8\03\F3\08\015\00\00\04\0A\08\00\02\00\00\00`\01\10\00\03\19\10\00\04\17\0C\0C\04U\08\00\00\F0!\10\00\10\01\18\01%\F0\11\10\00\01\01\00\F2\02\F0\11\00\03\1B\FF\00\04\1C\08\00P\00\00\00\B0\00\01\00#K\00\01\00s\02\02\08\10\0A/\22\9B\00\00\07\00\03\08\00\13\10\08\00\13\18\08\00\13 \08\00\13(\08\00\130\08\00\228\08|\00\13\00\08\00\13\08\08\00\13\10\08\00\13\18\08\00\13 \08\00\13(\08\00\130\08\00\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\A2\00\00\00\14,\00\00\00H\00\01\00\000\01/\05\00\01\00\FF\B0A\02z\01\00\1F\04\B1\0F\00\00\00\C4\0F\00\19y\02\00\01\00\10%\8B\02Q\0E\00\19y\03\0F\00\F5\1A\00!\00\00\00$\0E\00$z\02\02\00X\00\00\03\02\8E\07\00\CA\1F\00\0Cz\00\02\00Y\00\00p`\F0\03\00\DA\0F\00MS\04\A0\80\03\00\EA\0F\005t\03\FF\B3\03\10\FF\C0\03P\E2\0F\00\02x6\02B\80\FF\00\0F\10\00r\B9z\04\00\00F\00\84\00\94\D0\0F\00%v\02\02\00Z`\00`\0F\00\86y\00\022\00@\04\19\10\0C0\009My\00`\00PGy\00\00\F09\04\A6\FF\83\03\00\C0\0F\00\18y\00\01\00\0F\10\00\90\0F\01\00-\00W\01.\03\00\01\00\22@\00\01\00=+\01\000\00\08\01\00\1F\0B@\00\04\13k)\00\1F[@\00\0C\13\13\0C\04\0C\01\00\13\C8\15\00&\90\000\04#\04\00\85\04\00\F6\04\12\00\01\00\1F\FET\00\00\00\01\00\13X\95\00/p\00\80\00\0B\1F)'\00\03#\00\C8@\00\04P\06\04\E4\00*\04\00\01\00\1Fa@\00\04\13\F81\00&\\\00@\00\1F\0A@\00\00!\1C\01D\01\0D@\00\13X)\00*\D8\00\01\00\1B\08\08\00?\0B\01\00\86\07\00Q\00\000\05\00\01\00&\10\00\80\00\17\048\00\04\18\00\13\C7\14\01\0C\84\01*@\058\07\1F\00\C0\00\04\132@\00+\06\00\01\00\1A\07\D0\07\12\03\F0\05:\08\80\00\01\00\13\06\08\06\04(\0B\0C\01\00*\A8\00\08\00\04\F8\00\14\018\00/\05\00\01\00\029@\03\00\08\00\088\00/\06\00\01\00\17\80\08\00\00\00\00\00\00\00"} {
    llvm.func @main_kColReduction_reduce__4_1_0___8w32h(%arg0: i32, %arg1: i32, %arg2: !llvm.ptr<f32>) attributes {disc.elimargs = [2 : index, 4 : index, 5 : index, 6 : index], gpu.kernel, nvvm.kernel} {
      %0 = llvm.mlir.constant(0xFF800000 : f32) : f32
      %1 = nvvm.read.ptx.sreg.ctaid.x : i32
      %2 = nvvm.read.ptx.sreg.tid.x : i32
      llvm.br ^bb1
    ^bb1:  // pred: ^bb0
      %3 = llvm.mul %1, %arg0  : i32
      %4 = llvm.add %2, %3  : i32
      %5 = llvm.icmp "ult" %4, %arg1 : i32
      llvm.cond_br %5, ^bb2, ^bb3
    ^bb2:  // pred: ^bb1
      %6 = llvm.getelementptr %arg2[%4] : (!llvm.ptr<f32>, i32) -> !llvm.ptr<f32>
      llvm.store %0, %6 : !llvm.ptr<f32>
      llvm.br ^bb3
    ^bb3:  // 2 preds: ^bb1, ^bb2
      llvm.return
    }
  }
  gpu.module @main_kernel_0 attributes {gpu.binary = "P\EDU\BA\01\00\10\00@\08\00\00\00\00\00\00\02\00\01\01@\00\00\00\00\08\00\00\00\00\00\00\F9\07\00\00\00\00\00\00\07\00\01\00P\00\00\00\00\00\00\00\00\00\00\00\11 \00\00\00\00\00\00\00\00\00\00\00\00\00\00(\14\00\00\00\00\00\00\A2\7FELF\02\01\013\07\00\01\00f\02\00\BE\00q\00\01\00#\80\13\08\00\11\10\07\00\F5\0E\00P\05P\00@\008\00\03\00@\00\0C\00\01\00\00.shstrtab\00.\08\00'ym\08\00\F0\1B_shndx\00.nv.info\00.text.main_kColReduction_r\0A\00\F5\03e__4_1_0___8w32h_1:\00\0F4\00\1Doshared6\00\1AOrela\A0\00\1F?rel\D5\00\22\9Fconstant09\00\1A\B2debug_frame{\00\09\11\00!nv\14\00\11aE\00\0F\9E\01 \0F\8A\00\17\0F\C9\01\F4\8F$____wg_3\00\17\00\0C\00/27\02\02'o_param\09\02\1C\0F\01\00\05\8C]\00\00\00\03\00\0A\00\01\00\11\C2\18\00,\0B\00\01\00 \9C\01\18\00,\09\00\01\00\11\DC\18\00,\04\00\01\00\11\FA\18\00,\07\00\01\00g2\00\00\00\12\10x\00\11\08\06\00v\00\FF\FF\FF\FF(\00\0C\00\00\01\00@\03\00\04|\08\00\D1\0F\0C\81\80\80(\00\08\FF\81\80(\08\0B\00\06(\00/0\00\01\00\03\13\F0{\00\10\04\9B\00R\04\14\00\00\00E\002\04\AC\01\18\00\80/\08\00\06\00\00\00\0E\0C\00\12#\0C\00\00\01\00'\04\12\0C\00\17\11\0C\00!7\04 \05\F1\08\015\00\00\04\0A\08\00\03\00\00\00`\01(\00\03\19(\00\04\17\0C$\00u\06\00 \00\00\F0!\10\00u\05\00\1C\00\00\F0\11\10\009\04\00\18\10\009\03\00\14\10\009\02\00\10\10\009\01\00\08P\00\01\01\00\F2\0A\F0\11\00\03\1B\FF\00\04\1C\0C\00P\00\00\00\10\06\00\00\10\07\00\00\04\1E\84\01#K\00\01\00v\02\02\08\10\0A/\22b\01\13\08\08\00\13\10\08\00\13\18\08\00\13 \08\00\13(\08\00\130\08\00\228\08\84\00\13\00\08\00\13\08\08\00\13\10\08\00\13\18\08\00\13 \08\00\13(\08\00\130\08\00\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\A2\00\00\00\14,\00\00\00H\00\01\00\11\02h\01\0F\01\00\FF\B0@$v\01\FF\7F\04\B2\FF\00\8E\07\00\C4\0F\00\19y\00\01\00\10%\8B\02a\0E\00\19y\03\00\01\00\10!-\00B\0E\00$z\B5\04\90\03\02\8E\07\00\CA\1F\00\0C\10\00\C5^\00\00p`\F0\03\00\DA\0F\00M\9B\04\F0\0C\80\03\00\EA\0F\00\06{\04\00\00_\00\00\00\90 \00\00\22\0E\00\19x\05\FF\1F\1F\00\D3\14\01\00\00\E2\0F\00$r\02\FF\FF\00\80\00 \E2\0FP\00\10\FF0\00@pP\F4\03\10\00\81\B9z\04\00\00F\00\00#\05p\E2\0F\00\11r\05\05?\00\B2\FF@\8F\07\00\C6\0F\00\08s\04\F6\04\00 \09\F1\07$\1E\00\10x\03\04\FE\FF\FF\0F\FF\E0\FF\07\00\CC\1F\00\05s\03$\04\00\C0\03\01\C0\00!r\07p\00\C0\03\0A\8E\07\00\C8\1F\00$z\07\07p\00\10\FF\D0\00\81\C8\0F\00'r\07\03\07\E0\02\02\90\00@\19x\02\FF\01\03\10\05\B0\00q\E4\0F\00\12x\05\05\F1\04!\C0\8E\80\00T'r\07\07\02\C0\00\10\C8\D0\00\11\09`\00\10\07`\00`\E4\0F\00$x\05\BC\03$\00\05\10\000z\03\09p\00\22\02\02@\00@\19x\00\FF\A0\00\22\05\16`\00f\0Cz\00\03\00_P\010\10\0A\03\10\00\11\80\D0\00p\E4\0F\00\10\08\07\07P\00\04\10\00\060\00\12\F20\00\1A\18 \001\12\AA\07P\01 \FF3\B0\01\00\A0\00\1B\03\A0\000\00\07  \01\15\02\A0\00\15\03\A0\00\10\E2\F0\00 \02\05\10\01#\FF\C0@\00Sx\09\03\08\00 \00\11\CA\80\003\09\00X\80\00\22\C8\0F\10\02p\\\00\00p\10\F2\04\90\00T$\14\03\FF\04 \01\00`\000\1A\02\000\00\13\09p\01c%\16\02\02\00ZP\02q\CC\0F\00\81\19\02\02\D0\01\C4\19\1E\0C\00\A2\00\00\10x\06\00\10\D0\00\000\02\92t\04\FF\00\00\80\FF\FF\00\A0\001\1Cx\00\01\001p\F0\F00\02E$x\07\05p\00q\E2\0F\04\0Cz\00\06\90\00#`\F6 \00\16\06 \00\80\C6\0F\00\0Cx\00\05\7F@\00\C5D\F6\01\00\E4\0F\04\10x\03\00\08p\00`\1F\00!\12\04\02q\00\01\F1\04!\E2O0\00\11\070\00 \F2\04\C0\00c\88s\00\07\04\00\DE\05f\E8\0F\00\1D{\00\01\00u\EC\0F\00\84\B9\0B\06~\05R\22\0E\00\10x\B0\02\01`\00Q\C4\0F\00\10x\1E\00\03p\00\000\004\02\06\00P\00qb\0E\00\0B\B2\00\0B\06\08\B1\80\F4\03\00\E4\1F\08\0B\B2\00\02\10\00\A3\C0\F8\03\00\E4/\00\1C\B8\00\00\01%p\01\10\02\01\F0\00B\F4\03\00\C8\E0\00\11?\B0\00\C3t\01\00\CE\0F\00\08\82\0B\0B\02\00'\07\1B\E4@\01\8F\C6\0F\00\88\B3\00\06\0B\D0\00\095\A9\03\06\8E\06[(\0E\00\84\A9\B0\001\A2\00\03p\03`\80\F6\03\00\C4\1F\10\00(\02\03\B0\00$\A8\00p\00\04\B0\00\15\04\A0\01\03\B0\00\14\1F\90\01\01\B0\00/\03\03\B0\00\0AO\A3\00\06\03\80\01\0AV\07\06\00\80\00\B0\00\0E`\01$\07\07`\01\00\B0\00O\B2\00\02\07`\01\0B\1C\00`\01\19\0F`\01\02\10\04\0F`\01\09\1F\07`\01\0D\17@\B0\00\13\A9g\07\0F`\01\07\1F\00`\01\06\12\DA@\01\14\00\90\00\1F\CA0\01\0F9M\19\00\C0\05G$t\02\FFP\03A\00\84y\05_\09\01@\00\96&\0E\00%v\02\09\00`0\04'\84y\B0\00fh\0E\00\81y\08\D0\03bb\05\00\0Br\00\C1\05\22\80\F0\C0\001r\00\00\10\00\92\C0\F2\03\00\D6/\00\08\82\1F\00p\00\00\80\04\00\C8O \00\11\08\11\00 @\F0\C0\03$\0EF\A0\06\00\A0\00b\E6\0F\00\08r\09 \00\00\01\00p\CC\0F\00\A9s\09\02{\00\C0\09\E1\1E\00\00\A4\0E\00\0Cr\00\09\10\00 pR@\00QO\00$r\08\00\05\10\09\D0\00\80\D8\0F\00G\09\00\00\90\D0\05!\FF\83\F0\00*My\00\01TGy\00\00\F0 \00f\C0\0F\00\18y\00\01\00\0F\10\00\B0\0F\01\00-#\01\00\80\02\0B\01\00\22@\00\01\00=\9E\01\000\00\08\01\00\1F\0B@\00\04\13\DE)\00?\09\02\00@\00\0A\22\13\00@\05\0C\01\00\13\E8U\00\03\0F\03\01$\00\13\05\97\02\00\01\00\22\18\00\01\00.q\01T\00\00\01\00\11\90\B5\02O\00\00p\00\80\00\0B\1F)'\00\03\03\D5\02$\00\00\18\0D\04\E4\00*\04\00\01\00\1Fc@\00\04*0\05\C0\00\13\03\03\09\0C@\00!\8F\01D\01\0D@\00\13\D8@\00*\D8\00\01\00\1B\08\08\00?~\01\00N\0E\002\00\00\B0\C6\03\01W\09\04\80\00\17\048\00\04\18\00\138@\01\0C\84\01\13\C0@\00\17\881\01\0F\C0\00\01\132T\01\02\E6\07\06\01\00\1B\80\A9\00\11\03$\00J\00\0E\80\00\01\00\13\97#\00*\03\00\01\00\040\13/\00\04\80\00\0B\13\06\AB\01\04h\13\0C\01\00\1B\A8\08\00\17\08\08\02\17\05\E8\00\0C\01\00*\C0\09\08\00\088\00\18\06\A0\00\0F\01\00\05\03\A9\00\80\08\00\00\00\00\00\00\00\00\00\00\00\00\00\00"} {
    llvm.mlir.global internal @__wg_main_kColReduction_reduce__4_1_0___8w32h_1_0() {addr_space = 3 : i32} : !llvm.array<256 x f32>
    llvm.func @__nv_fabsf(f32) -> f32
    llvm.func @main_kColReduction_reduce__4_1_0___8w32h_1(%arg0: i32, %arg1: !llvm.ptr<f32>, %arg2: i32, %arg3: i32, %arg4: i32, %arg5: i32, %arg6: !llvm.ptr<f32>) attributes {disc.elimargs = [1 : index, 3 : index, 5 : index, 6 : index, 7 : index, 8 : index, 9 : index, 13 : index, 15 : index, 16 : index, 17 : index], gpu.kernel, nvvm.kernel} {
      %0 = llvm.mlir.constant(8 : index) : i32
      %1 = llvm.mlir.constant(32 : index) : i32
      %2 = llvm.mlir.constant(2 : index) : i32
      %3 = llvm.mlir.constant(0xFF800000 : f32) : f32
      %4 = llvm.mlir.constant(16 : index) : i32
      %5 = llvm.mlir.constant(128 : index) : i32
      %6 = llvm.mlir.constant(64 : index) : i32
      %7 = llvm.mlir.constant(4 : index) : i32
      %8 = llvm.mlir.constant(256 : index) : i32
      %9 = llvm.mlir.constant(0 : index) : i32
      %10 = llvm.mlir.addressof @__wg_main_kColReduction_reduce__4_1_0___8w32h_1_0 : !llvm.ptr<array<256 x f32>, 3>
      %11 = llvm.getelementptr %10[0, 0] : (!llvm.ptr<array<256 x f32>, 3>) -> !llvm.ptr<f32, 3>
      %12 = nvvm.read.ptx.sreg.ctaid.x : i32
      %13 = nvvm.read.ptx.sreg.tid.x : i32
      llvm.br ^bb1
    ^bb1:  // pred: ^bb0
      %14 = llvm.mul %12, %arg3  : i32
      %15 = llvm.add %13, %14  : i32
      %16 = llvm.icmp "ult" %15, %arg4 : i32
      llvm.cond_br %16, ^bb2, ^bb18
    ^bb2:  // pred: ^bb1
      %17 = llvm.srem %15, %8  : i32
      %18 = llvm.sdiv %15, %8  : i32
      %19 = llvm.udiv %17, %0  : i32
      %20 = llvm.urem %17, %0  : i32
      %21 = llvm.udiv %18, %arg5  : i32
      %22 = llvm.urem %18, %arg5  : i32
      %23 = llvm.mul %21, %1  : i32
      %24 = llvm.add %23, %19  : i32
      %25 = llvm.mul %22, %0  : i32
      %26 = llvm.add %25, %20  : i32
      %27 = llvm.icmp "ult" %24, %arg2 : i32
      %28 = llvm.icmp "ult" %26, %arg0 : i32
      %29 = llvm.and %27, %28  : i1
      llvm.cond_br %29, ^bb3, ^bb4
    ^bb3:  // pred: ^bb2
      %30 = llvm.mul %24, %arg0  : i32
      %31 = llvm.add %30, %26  : i32
      %32 = llvm.getelementptr %arg1[%31] : (!llvm.ptr<f32>, i32) -> !llvm.ptr<f32>
      %33 = llvm.load %32 : !llvm.ptr<f32>
      %34 = llvm.call @__nv_fabsf(%33) : (f32) -> f32
      %35 = llvm.getelementptr %11[%17] : (!llvm.ptr<f32, 3>, i32) -> !llvm.ptr<f32, 3>
      llvm.store %34, %35 : !llvm.ptr<f32, 3>
      llvm.br ^bb5
    ^bb4:  // pred: ^bb2
      %36 = llvm.getelementptr %11[%17] : (!llvm.ptr<f32, 3>, i32) -> !llvm.ptr<f32, 3>
      llvm.store %3, %36 : !llvm.ptr<f32, 3>
      llvm.br ^bb5
    ^bb5:  // 2 preds: ^bb3, ^bb4
      nvvm.barrier0
      %37 = llvm.icmp "ult" %19, %4 : i32
      %38 = llvm.add %24, %4  : i32
      %39 = llvm.icmp "ult" %38, %arg2 : i32
      %40 = llvm.and %37, %39  : i1
      llvm.cond_br %40, ^bb6, ^bb7
    ^bb6:  // pred: ^bb5
      %41 = llvm.add %17, %5  : i32
      %42 = llvm.getelementptr %11[%17] : (!llvm.ptr<f32, 3>, i32) -> !llvm.ptr<f32, 3>
      %43 = llvm.load %42 : !llvm.ptr<f32, 3>
      %44 = llvm.getelementptr %11[%41] : (!llvm.ptr<f32, 3>, i32) -> !llvm.ptr<f32, 3>
      %45 = llvm.load %44 : !llvm.ptr<f32, 3>
      %46 = llvm.fcmp "ugt" %43, %45 : f32
      %47 = llvm.select %46, %43, %45 : i1, f32
      %48 = llvm.fcmp "uno" %45, %45 : f32
      %49 = llvm.select %48, %45, %47 : i1, f32
      %50 = llvm.getelementptr %11[%17] : (!llvm.ptr<f32, 3>, i32) -> !llvm.ptr<f32, 3>
      llvm.store %49, %50 : !llvm.ptr<f32, 3>
      llvm.br ^bb7
    ^bb7:  // 2 preds: ^bb5, ^bb6
      nvvm.barrier0
      %51 = llvm.icmp "ult" %19, %0 : i32
      %52 = llvm.add %24, %0  : i32
      %53 = llvm.icmp "ult" %52, %arg2 : i32
      %54 = llvm.and %51, %53  : i1
      llvm.cond_br %54, ^bb8, ^bb9
    ^bb8:  // pred: ^bb7
      %55 = llvm.add %17, %6  : i32
      %56 = llvm.getelementptr %11[%17] : (!llvm.ptr<f32, 3>, i32) -> !llvm.ptr<f32, 3>
      %57 = llvm.load %56 : !llvm.ptr<f32, 3>
      %58 = llvm.getelementptr %11[%55] : (!llvm.ptr<f32, 3>, i32) -> !llvm.ptr<f32, 3>
      %59 = llvm.load %58 : !llvm.ptr<f32, 3>
      %60 = llvm.fcmp "ugt" %57, %59 : f32
      %61 = llvm.select %60, %57, %59 : i1, f32
      %62 = llvm.fcmp "uno" %59, %59 : f32
      %63 = llvm.select %62, %59, %61 : i1, f32
      %64 = llvm.getelementptr %11[%17] : (!llvm.ptr<f32, 3>, i32) -> !llvm.ptr<f32, 3>
      llvm.store %63, %64 : !llvm.ptr<f32, 3>
      llvm.br ^bb9
    ^bb9:  // 2 preds: ^bb7, ^bb8
      nvvm.barrier0
      %65 = llvm.icmp "ult" %19, %7 : i32
      %66 = llvm.add %24, %7  : i32
      %67 = llvm.icmp "ult" %66, %arg2 : i32
      %68 = llvm.and %65, %67  : i1
      llvm.cond_br %68, ^bb10, ^bb11
    ^bb10:  // pred: ^bb9
      %69 = llvm.add %17, %1  : i32
      %70 = llvm.getelementptr %11[%17] : (!llvm.ptr<f32, 3>, i32) -> !llvm.ptr<f32, 3>
      %71 = llvm.load %70 : !llvm.ptr<f32, 3>
      %72 = llvm.getelementptr %11[%69] : (!llvm.ptr<f32, 3>, i32) -> !llvm.ptr<f32, 3>
      %73 = llvm.load %72 : !llvm.ptr<f32, 3>
      %74 = llvm.fcmp "ugt" %71, %73 : f32
      %75 = llvm.select %74, %71, %73 : i1, f32
      %76 = llvm.fcmp "uno" %73, %73 : f32
      %77 = llvm.select %76, %73, %75 : i1, f32
      %78 = llvm.getelementptr %11[%17] : (!llvm.ptr<f32, 3>, i32) -> !llvm.ptr<f32, 3>
      llvm.store %77, %78 : !llvm.ptr<f32, 3>
      llvm.br ^bb11
    ^bb11:  // 2 preds: ^bb9, ^bb10
      nvvm.barrier0
      %79 = llvm.icmp "ult" %19, %2 : i32
      %80 = llvm.add %24, %2  : i32
      %81 = llvm.icmp "ult" %80, %arg2 : i32
      %82 = llvm.and %79, %81  : i1
      llvm.cond_br %82, ^bb12, ^bb13
    ^bb12:  // pred: ^bb11
      %83 = llvm.add %17, %4  : i32
      %84 = llvm.getelementptr %11[%17] : (!llvm.ptr<f32, 3>, i32) -> !llvm.ptr<f32, 3>
      %85 = llvm.load %84 : !llvm.ptr<f32, 3>
      %86 = llvm.getelementptr %11[%83] : (!llvm.ptr<f32, 3>, i32) -> !llvm.ptr<f32, 3>
      %87 = llvm.load %86 : !llvm.ptr<f32, 3>
      %88 = llvm.fcmp "ugt" %85, %87 : f32
      %89 = llvm.select %88, %85, %87 : i1, f32
      %90 = llvm.fcmp "uno" %87, %87 : f32
      %91 = llvm.select %90, %87, %89 : i1, f32
      %92 = llvm.getelementptr %11[%17] : (!llvm.ptr<f32, 3>, i32) -> !llvm.ptr<f32, 3>
      llvm.store %91, %92 : !llvm.ptr<f32, 3>
      llvm.br ^bb13
    ^bb13:  // 2 preds: ^bb11, ^bb12
      nvvm.barrier0
      %93 = llvm.icmp "eq" %19, %9 : i32
      %94 = llvm.and %93, %29  : i1
      llvm.cond_br %94, ^bb14, ^bb17
    ^bb14:  // pred: ^bb13
      %95 = llvm.add %17, %0  : i32
      %96 = llvm.getelementptr %11[%17] : (!llvm.ptr<f32, 3>, i32) -> !llvm.ptr<f32, 3>
      %97 = llvm.load %96 : !llvm.ptr<f32, 3>
      %98 = llvm.getelementptr %11[%95] : (!llvm.ptr<f32, 3>, i32) -> !llvm.ptr<f32, 3>
      %99 = llvm.load %98 : !llvm.ptr<f32, 3>
      %100 = llvm.fcmp "ugt" %97, %99 : f32
      %101 = llvm.select %100, %97, %99 : i1, f32
      %102 = llvm.fcmp "uno" %99, %99 : f32
      %103 = llvm.select %102, %99, %101 : i1, f32
      %104 = llvm.getelementptr %arg6[%26] : (!llvm.ptr<f32>, i32) -> !llvm.ptr<f32>
      %105 = llvm.load %104 : !llvm.ptr<f32>
      llvm.br ^bb15(%105 : f32)
    ^bb15(%106: f32):  // 2 preds: ^bb14, ^bb15
      %107 = llvm.fcmp "ogt" %106, %103 : f32
      %108 = llvm.select %107, %106, %103 : i1, f32
      %109 = llvm.bitcast %104 : !llvm.ptr<f32> to !llvm.ptr<i32>
      %110 = llvm.bitcast %106 : f32 to i32
      %111 = llvm.bitcast %108 : f32 to i32
      %112 = llvm.cmpxchg %109, %110, %111 acq_rel monotonic : !llvm.ptr<i32>, i32
      %113 = llvm.extractvalue %112[0] : !llvm.struct<(i32, i1)> 
      %114 = llvm.bitcast %113 : i32 to f32
      %115 = llvm.extractvalue %112[1] : !llvm.struct<(i32, i1)> 
      llvm.cond_br %115, ^bb16, ^bb15(%114 : f32)
    ^bb16:  // pred: ^bb15
      llvm.br ^bb17
    ^bb17:  // 2 preds: ^bb13, ^bb16
      llvm.br ^bb18
    ^bb18:  // 2 preds: ^bb1, ^bb17
      llvm.return
    }
  }
  gpu.module @main_kernel_1 attributes {gpu.binary = "P\EDU\BA\01\00\10\008\04\00\00\00\00\00\00\02\00\01\01@\00\00\00\F8\03\00\00\00\00\00\00\F8\03\00\00\00\00\00\00\07\00\01\00P\00\00\00\00\00\00\00\00\00\00\00\11 \00\00\00\00\00\00\00\00\00\00\00\00\00\00\E8\0B\00\00\00\00\00\00\A2\7FELF\02\01\013\07\00\01\00f\02\00\BE\00q\00\01\00!@\0B\07\001\00\80\08\07\00\F5\0E\00P\05P\00@\008\00\03\00@\00\0B\00\01\00\00.shstrtab\00.\08\00'ym\08\00\F0\1B_shndx\00.nv.info\00.text.main_kColReduction_r\0A\00\F5\01e__4_1_0___8w16h8\00\0F2\00\1Boshared4\00\1B\9Fconstant07\00\18\FA\01debug_frame\00.rel\11\00!nv\14\00\11aC\00\0F+\01 \0F\88\00\15\0FT\01\BAo_param[\01\1C\0F\01\00\06\8C[\00\00\00\03\00\0A\00\01\00\11\F0\18\00,\09\00\01\00 .\01\18\00,\04\00\01\00\11L\18\00,\07\00\01\00f2\00\00\00\12\10`\00!\80\01\07\00v\00\FF\FF\FF\FF(\00\0C\00\00\01\00@\03\00\04|\08\00\D1\0F\0C\81\80\80(\00\08\FF\81\80(\08\0B\00\06(\00/0\00\01\00\03\13pX\00\10\04\9B\00R\04\14\00\00\00E\00\01\0B\00\00\13\00p/\08\00\05\00\00\00\A7\03\22\04#\0C\00\00\01\00'\04\12\0C\00\17\11\0C\00!7\04\E8\03\F3\08\015\00\00\04\0A\08\00\02\00\00\00`\01\10\00\03\19\10\00\04\17\0C\0C\04U\08\00\00\F0!\10\00\10\01\18\01%\F0\11\10\00\01\01\00\F2\02\F0\11\00\03\1B\FF\00\04\1C\08\00P\00\00\00\B0\00\01\00#K\00\01\00s\02\02\08\10\0A/\22\9B\00\00\07\00\03\08\00\13\10\08\00\13\18\08\00\13 \08\00\13(\08\00\130\08\00\228\08|\00\13\00\08\00\13\08\08\00\13\10\08\00\13\18\08\00\13 \08\00\13(\08\00\130\08\00\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\A2\00\00\00\14,\00\00\00H\00\01\00\000\01/\05\00\01\00\FF\B0A\02z\01\00\1F\04\B1\0F\00\00\00\C4\0F\00\19y\02\00\01\00\10%\8B\02Q\0E\00\19y\03\0F\00\F5\1A\00!\00\00\00$\0E\00$z\02\02\00X\00\00\03\02\8E\07\00\CA\1F\00\0Cz\00\02\00Y\00\00p`\F0\03\00\DA\0F\00MS\04\A0\80\03\00\EA\0F\005t\03\FF\B3\03\10\FF\C0\03P\E2\0F\00\02x6\02B\80\FF\00\0F\10\00r\B9z\04\00\00F\00\84\00\94\D0\0F\00%v\02\02\00Z`\00`\0F\00\86y\00\022\00@\04\19\10\0C0\009My\00`\00PGy\00\00\F09\04\A6\FF\83\03\00\C0\0F\00\18y\00\01\00\0F\10\00\90\0F\01\00-\00W\01.\03\00\01\00\22@\00\01\00=+\01\000\00\08\01\00\1F\0B@\00\04\13k)\00\1F[@\00\0C\13\13\0C\04\0C\01\00\13\C8\15\00&\90\000\04#\04\00\85\04\00\F6\04\12\00\01\00\1F\FET\00\00\00\01\00\13X\95\00/p\00\80\00\0B\1F)'\00\03#\00\C8@\00\04P\06\04\E4\00*\04\00\01\00\1Fa@\00\04\13\F81\00&\\\00@\00\1F\0A@\00\00!\1C\01D\01\0D@\00\13X)\00*\D8\00\01\00\1B\08\08\00?\0B\01\00\86\07\00Q\00\000\05\00\01\00&\10\00\80\00\17\048\00\04\18\00\13\C7\14\01\0C\84\01*@\058\07\1F\00\C0\00\04\132@\00+\06\00\01\00\1A\07\D0\07\12\03\F0\05:\08\80\00\01\00\13\06\08\06\04(\0B\0C\01\00*\A8\00\08\00\04\F8\00\14\018\00/\05\00\01\00\029@\03\00\08\00\088\00/\06\00\01\00\17\80\08\00\00\00\00\00\00\00"} {
    llvm.func @main_kColReduction_reduce__4_1_0___8w16h(%arg0: i32, %arg1: i32, %arg2: !llvm.ptr<f32>) attributes {disc.elimargs = [2 : index, 4 : index, 5 : index, 6 : index], gpu.kernel, nvvm.kernel} {
      %0 = llvm.mlir.constant(0xFF800000 : f32) : f32
      %1 = nvvm.read.ptx.sreg.ctaid.x : i32
      %2 = nvvm.read.ptx.sreg.tid.x : i32
      llvm.br ^bb1
    ^bb1:  // pred: ^bb0
      %3 = llvm.mul %1, %arg0  : i32
      %4 = llvm.add %2, %3  : i32
      %5 = llvm.icmp "ult" %4, %arg1 : i32
      llvm.cond_br %5, ^bb2, ^bb3
    ^bb2:  // pred: ^bb1
      %6 = llvm.getelementptr %arg2[%4] : (!llvm.ptr<f32>, i32) -> !llvm.ptr<f32>
      llvm.store %0, %6 : !llvm.ptr<f32>
      llvm.br ^bb3
    ^bb3:  // 2 preds: ^bb1, ^bb2
      llvm.return
    }
  }
  gpu.module @main_kernel_2 attributes {gpu.binary = "P\EDU\BA\01\00\10\00\10\08\00\00\00\00\00\00\02\00\01\01@\00\00\00\D0\07\00\00\00\00\00\00\CB\07\00\00\00\00\00\00\07\00\01\00P\00\00\00\00\00\00\00\00\00\00\00\11 \00\00\00\00\00\00\00\00\00\00\00\00\00\00(\13\00\00\00\00\00\00\A2\7FELF\02\01\013\07\00\01\00f\02\00\BE\00q\00\01\00#\80\12\08\00\11\0F\07\00\F5\0E\00P\05P\00@\008\00\03\00@\00\0C\00\01\00\00.shstrtab\00.\08\00'ym\08\00\F0\1B_shndx\00.nv.info\00.text.main_kColReduction_r\0A\00\F5\03e__4_1_0___8w16h_1:\00\0F4\00\1Doshared6\00\1AOrela\A0\00\1F?rel\D5\00\22\9Fconstant09\00\1A\B2debug_frame{\00\09\11\00!nv\14\00\11aE\00\0F\9E\01 \0F\8A\00\17\0F\C9\01\F4\8F$____wg_3\00\17\00\0C\00/27\02\02'o_param\09\02\1C\0F\01\00\05\8C]\00\00\00\03\00\0A\00\01\00\11\C2\18\00,\0B\00\01\00 \9C\01\18\00,\09\00\01\00\11\DC\18\00,\04\00\01\00\11\FA\18\00,\07\00\01\00g2\00\00\00\12\10x\00\03#\00f\FF\FF\FF\FF(\00\0C\00\00\01\00@\03\00\04|\08\00\D1\0F\0C\81\80\80(\00\08\FF\81\80(\08\0B\00\06(\00/0\00\01\00\03!\F0\06\07\00 \00\04\9B\00R\04\14\00\00\00E\002\04|\01\18\000/\08\00#\00\10\0E\0C\00\12#\0C\00\00\01\00'\04\12\0C\00\17\11\0C\00!7\04 \05\F1\08\015\00\00\04\0A\08\00\03\00\00\00`\01(\00\03\19(\00\04\17\0C$\00u\06\00 \00\00\F0!\10\00u\05\00\1C\00\00\F0\11\10\009\04\00\18\10\009\03\00\14\10\009\02\00\10\10\009\01\00\08P\00\01\01\00\C1\F0\11\00\03\1B\FF\00\04\1C\0C\00P\98\05\82\00\00P\06\00\00\04\1E\84\01#K\00\01\00\B1\02\02\08\10\0A/\22\00\00\00\08\10\00#\00\08\08\00\13\10\08\00\13\18\08\00\13 \08\00\13(\08\00\130\08\00\228\08\84\00\13\00\08\00\13\08\08\00\13\10\08\00\13\18\08\00\13 \08\00\13(\08\00\130\08\00\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\A2\00\00\00\14,\00\00\00H\00\01\00\11\02h\01\0F\01\00\FF\B0@$v\01\FF\7F\04\B2\FF\00\8E\07\00\C4\0F\00\19y\00\01\00\10%\8B\02a\0E\00\19y\03\00\01\00\10!-\00B\0E\00$z\B5\04\90\03\02\8E\07\00\CA\1F\00\0C\10\00\C5^\00\00p`\F0\03\00\DA\0F\00M\9B\04\F0\0C\80\03\00\EA\0F\00\06{\04\00\00_\00\00\00\90 \00\00\22\0E\00\19x\05\FF\1F\1F\00\D3\14\01\00\00\E2\0F\00$r\02\FF\FF\00\80\00 \E2\0FP\00\10\FF0\00@pP\F4\03\10\00\81\B9z\04\00\00F\00\00#\05p\E2\0F\00\11r\05\05?\00\B2\FF8\8F\07\00\C6\0F\00\08s\04\F6\04\10\10\A0\00\F1\06\1E\00\10x\03\04\FE\FF\FF\0F\FF\E0\FF\07\00\CC\1F\00\05s\03$\04\00\C0\03\01\C0\00!r\07p\00\C0\03\0A\8E\07\00\C8\1F\00$z\07\07p\00\10\FF\D0\00\81\C8\0F\00'r\07\03\07\E0\02\02\90\00@\19x\02\FF\10\00\10\05\B0\00\80\E4\0F\00\12x\05\05\80\F1\04!\C0\8E\80\00T'r\07\07\02\C0\00\10\C8\D0\00\11\09`\00\10\07`\00`\E4\0F\00$x\05\BC\03$\00\05\10\000z\03\09p\00\22\02\02@\00@\19x\00\FF\A0\00\22\05\16`\00f\0Cz\00\03\00_P\010\10\0A\03\10\00\11\80\D0\00p\E4\0F\00\10\08\07\07P\00\04\10\00\060\00\12\F20\00\1A\18 \001\12\AA\07P\01 \FF3\B0\01\00\A0\00\1B\03\A0\001\00\07\10\D1\03\05\A0\00\15\03\A0\00\10\E2\F0\00 \02\05\00\01#\FF\C0@\00Sx\09\03\08\00 \00\11\CA\80\003\09\00X\80\00\22\C8\0F\10\02p\\\00\00p\10\F2\04\90\00T$\14\03\FF\04 \01\00`\000\1A\02\000\00\13\09p\01c%\16\02\02\00ZP\02q\CC\0F\00\81\19\02\02\D0\01\C4\19\1E\0C\00\A2\00\00\10x\06\00\08\D0\00\000\02\92t\04\FF\00\00\80\FF\FF\00\A0\001\1Cx\00\01\001p\F0\F00\02E$x\07\05p\00q\E2\0F\04\0Cz\00\06\90\00#`\F6 \00\16\06 \00\80\C6\0F\00\0Cx\00\05?@\00\92D\F6\01\00\E4\0F\04\10xF\07\02@\01B\1F\04\10x\EC\04\04\80\00@!\12\04\02\81\00\01\15\00!\E2O@\00\11\07@\00 \F2\04\D0\00c\88s\00\07\04\00\EE\05f\E8\0F\00\1D{\00\01\00u\EC\0F\00\84\B9\0B\06\CE\05\84(\0E\00\84\B9\02\06\000\00qb\0E\00\0B\B2\00\0B\F6\07`\80\F4\03\00\C4\1F\10\00\11\02\10\00\A3\C0\F8\03\00\E4/\00\1C\B8\00\F0\00%p\01\00\02\01\E0\00B\F4\03\00\C8\D0\00\11\1F\90\00\C3t\01\00\CE\0F\00\08\82\0B\0B\02\00\17\07\1B\E40\01\8F\C6\0F\00\88\B3\00\06\0B\B0\00\09f\A9\03\06\00\80\00\B0\00\1B\A9\B0\001\A2\00\03`\03\22\80\F6\B0\00H\A2\00\02\03\B0\00$\A8\00p\00\04\B0\00\15\00\90\01\03\B0\00\14\0F\80\01\01\B0\00/\03\03\B0\00\0AO\A3\00\06\03`\01\0AG\07\06\00@\B0\00\13\B9\A7\06\06`\01*\07\07`\01/\00\07`\01\05\10\DA\90\00\02/\00\01\90\00\12\CA0\01\1F\07\80\00\089M\19\00\00\05G$t\02\FF\90\02A\00\84y\05\9F\08\01@\00\96&\0E\00%v\02\09\00`p\03'\84y\B0\00fh\0E\00\81y\08\10\03bb\05\00\0Br\00\01\05\22\80\F0\C0\001r\00\00\10\00\92\C0\F2\03\00\D6/\00\08\82\1F\00p\00\00\80\04\00\C8O \00\11\08\11\00 @\F0\00\03$\0EF\E0\05\00\A0\00b\E6\0F\00\08r\09 \00\00\01\00p\CC\0F\00\A9s\09\02{\00\C0\09\E1\1E\00\00\A4\0E\00\0Cr\00\09\10\00 pR@\00QO\00$r\08@\04\10\09\D0\00\80\D8\0F\00G\09\00\00\90\10\05!\FF\83\F0\00*My\00\01TGy\00\00\F0 \00f\C0\0F\00\18y\00\01\00\0F\10\00p\0F\01\00-\11\01@\0A\0E\01\00\22@\00\01\00=\9E\01\000\00\08\01\00\1F\0B@\00\04\13\DE)\00?\09\02\00@\00\0A\22\13\00\A0\04\0C\01\00\13\E8U\00\03\7F\03\01$\00\13\05W\02\00\01\00\22\18\00\01\00.q\01T\00\00\01\00\11\90u\02O\00\00p\00\80\00\0B\1F)'\00\03\03\95\02$\00\00\18\0C\04\E4\00*\04\00\01\00\1Fc@\00\04*0\05\C0\00\13\03\03\08\0C@\00!\8F\01D\01\0D@\00\13\D8@\00*\D8\00\01\00\1B\08\08\00?~\01\00N\0D\002\00\00\B0\86\03\01W\08\04\80\00\17\048\00\04\18\00\138@\01\0C\84\01\13\C0@\00\17\881\01\0F\C0\00\01\132T\01\15\06R\00\03>\03\1A\08\98\0D\11\03$\00J\00\0E\80\00\01\00\13\97\94\00*\03\00\01\00\040\12/\00\02\80\00\0B\13\06\AB\01\04h\12\0C\01\00\1B\A8\08\00\04\97\00\13\018\00\04\E8\00\0C\01\00*\C0\08\08\00\088\00\18\06\A0\00\0F\01\00\05\03\B8\00\80\08\00\00\00\00\00\00\00\00\00\00\00\00"} {
    llvm.mlir.global internal @__wg_main_kColReduction_reduce__4_1_0___8w16h_1_0() {addr_space = 3 : i32} : !llvm.array<128 x f32>
    llvm.func @__nv_fabsf(f32) -> f32
    llvm.func @main_kColReduction_reduce__4_1_0___8w16h_1(%arg0: i32, %arg1: !llvm.ptr<f32>, %arg2: i32, %arg3: i32, %arg4: i32, %arg5: i32, %arg6: !llvm.ptr<f32>) attributes {disc.elimargs = [1 : index, 3 : index, 5 : index, 6 : index, 7 : index, 8 : index, 9 : index, 13 : index, 15 : index, 16 : index, 17 : index], gpu.kernel, nvvm.kernel} {
      %0 = llvm.mlir.constant(8 : index) : i32
      %1 = llvm.mlir.constant(16 : index) : i32
      %2 = llvm.mlir.constant(2 : index) : i32
      %3 = llvm.mlir.constant(0xFF800000 : f32) : f32
      %4 = llvm.mlir.constant(64 : index) : i32
      %5 = llvm.mlir.constant(4 : index) : i32
      %6 = llvm.mlir.constant(32 : index) : i32
      %7 = llvm.mlir.constant(128 : index) : i32
      %8 = llvm.mlir.constant(0 : index) : i32
      %9 = llvm.mlir.addressof @__wg_main_kColReduction_reduce__4_1_0___8w16h_1_0 : !llvm.ptr<array<128 x f32>, 3>
      %10 = llvm.getelementptr %9[0, 0] : (!llvm.ptr<array<128 x f32>, 3>) -> !llvm.ptr<f32, 3>
      %11 = nvvm.read.ptx.sreg.ctaid.x : i32
      %12 = nvvm.read.ptx.sreg.tid.x : i32
      llvm.br ^bb1
    ^bb1:  // pred: ^bb0
      %13 = llvm.mul %11, %arg3  : i32
      %14 = llvm.add %12, %13  : i32
      %15 = llvm.icmp "ult" %14, %arg4 : i32
      llvm.cond_br %15, ^bb2, ^bb16
    ^bb2:  // pred: ^bb1
      %16 = llvm.srem %14, %7  : i32
      %17 = llvm.sdiv %14, %7  : i32
      %18 = llvm.udiv %16, %0  : i32
      %19 = llvm.urem %16, %0  : i32
      %20 = llvm.udiv %17, %arg5  : i32
      %21 = llvm.urem %17, %arg5  : i32
      %22 = llvm.mul %20, %1  : i32
      %23 = llvm.add %22, %18  : i32
      %24 = llvm.mul %21, %0  : i32
      %25 = llvm.add %24, %19  : i32
      %26 = llvm.icmp "ult" %23, %arg2 : i32
      %27 = llvm.icmp "ult" %25, %arg0 : i32
      %28 = llvm.and %26, %27  : i1
      llvm.cond_br %28, ^bb3, ^bb4
    ^bb3:  // pred: ^bb2
      %29 = llvm.mul %23, %arg0  : i32
      %30 = llvm.add %29, %25  : i32
      %31 = llvm.getelementptr %arg1[%30] : (!llvm.ptr<f32>, i32) -> !llvm.ptr<f32>
      %32 = llvm.load %31 : !llvm.ptr<f32>
      %33 = llvm.call @__nv_fabsf(%32) : (f32) -> f32
      %34 = llvm.getelementptr %10[%16] : (!llvm.ptr<f32, 3>, i32) -> !llvm.ptr<f32, 3>
      llvm.store %33, %34 : !llvm.ptr<f32, 3>
      llvm.br ^bb5
    ^bb4:  // pred: ^bb2
      %35 = llvm.getelementptr %10[%16] : (!llvm.ptr<f32, 3>, i32) -> !llvm.ptr<f32, 3>
      llvm.store %3, %35 : !llvm.ptr<f32, 3>
      llvm.br ^bb5
    ^bb5:  // 2 preds: ^bb3, ^bb4
      nvvm.barrier0
      %36 = llvm.icmp "ult" %18, %0 : i32
      %37 = llvm.add %23, %0  : i32
      %38 = llvm.icmp "ult" %37, %arg2 : i32
      %39 = llvm.and %36, %38  : i1
      llvm.cond_br %39, ^bb6, ^bb7
    ^bb6:  // pred: ^bb5
      %40 = llvm.add %16, %4  : i32
      %41 = llvm.getelementptr %10[%16] : (!llvm.ptr<f32, 3>, i32) -> !llvm.ptr<f32, 3>
      %42 = llvm.load %41 : !llvm.ptr<f32, 3>
      %43 = llvm.getelementptr %10[%40] : (!llvm.ptr<f32, 3>, i32) -> !llvm.ptr<f32, 3>
      %44 = llvm.load %43 : !llvm.ptr<f32, 3>
      %45 = llvm.fcmp "ugt" %42, %44 : f32
      %46 = llvm.select %45, %42, %44 : i1, f32
      %47 = llvm.fcmp "uno" %44, %44 : f32
      %48 = llvm.select %47, %44, %46 : i1, f32
      %49 = llvm.getelementptr %10[%16] : (!llvm.ptr<f32, 3>, i32) -> !llvm.ptr<f32, 3>
      llvm.store %48, %49 : !llvm.ptr<f32, 3>
      llvm.br ^bb7
    ^bb7:  // 2 preds: ^bb5, ^bb6
      nvvm.barrier0
      %50 = llvm.icmp "ult" %18, %5 : i32
      %51 = llvm.add %23, %5  : i32
      %52 = llvm.icmp "ult" %51, %arg2 : i32
      %53 = llvm.and %50, %52  : i1
      llvm.cond_br %53, ^bb8, ^bb9
    ^bb8:  // pred: ^bb7
      %54 = llvm.add %16, %6  : i32
      %55 = llvm.getelementptr %10[%16] : (!llvm.ptr<f32, 3>, i32) -> !llvm.ptr<f32, 3>
      %56 = llvm.load %55 : !llvm.ptr<f32, 3>
      %57 = llvm.getelementptr %10[%54] : (!llvm.ptr<f32, 3>, i32) -> !llvm.ptr<f32, 3>
      %58 = llvm.load %57 : !llvm.ptr<f32, 3>
      %59 = llvm.fcmp "ugt" %56, %58 : f32
      %60 = llvm.select %59, %56, %58 : i1, f32
      %61 = llvm.fcmp "uno" %58, %58 : f32
      %62 = llvm.select %61, %58, %60 : i1, f32
      %63 = llvm.getelementptr %10[%16] : (!llvm.ptr<f32, 3>, i32) -> !llvm.ptr<f32, 3>
      llvm.store %62, %63 : !llvm.ptr<f32, 3>
      llvm.br ^bb9
    ^bb9:  // 2 preds: ^bb7, ^bb8
      nvvm.barrier0
      %64 = llvm.icmp "ult" %18, %2 : i32
      %65 = llvm.add %23, %2  : i32
      %66 = llvm.icmp "ult" %65, %arg2 : i32
      %67 = llvm.and %64, %66  : i1
      llvm.cond_br %67, ^bb10, ^bb11
    ^bb10:  // pred: ^bb9
      %68 = llvm.add %16, %1  : i32
      %69 = llvm.getelementptr %10[%16] : (!llvm.ptr<f32, 3>, i32) -> !llvm.ptr<f32, 3>
      %70 = llvm.load %69 : !llvm.ptr<f32, 3>
      %71 = llvm.getelementptr %10[%68] : (!llvm.ptr<f32, 3>, i32) -> !llvm.ptr<f32, 3>
      %72 = llvm.load %71 : !llvm.ptr<f32, 3>
      %73 = llvm.fcmp "ugt" %70, %72 : f32
      %74 = llvm.select %73, %70, %72 : i1, f32
      %75 = llvm.fcmp "uno" %72, %72 : f32
      %76 = llvm.select %75, %72, %74 : i1, f32
      %77 = llvm.getelementptr %10[%16] : (!llvm.ptr<f32, 3>, i32) -> !llvm.ptr<f32, 3>
      llvm.store %76, %77 : !llvm.ptr<f32, 3>
      llvm.br ^bb11
    ^bb11:  // 2 preds: ^bb9, ^bb10
      nvvm.barrier0
      %78 = llvm.icmp "eq" %18, %8 : i32
      %79 = llvm.and %78, %28  : i1
      llvm.cond_br %79, ^bb12, ^bb15
    ^bb12:  // pred: ^bb11
      %80 = llvm.add %16, %0  : i32
      %81 = llvm.getelementptr %10[%16] : (!llvm.ptr<f32, 3>, i32) -> !llvm.ptr<f32, 3>
      %82 = llvm.load %81 : !llvm.ptr<f32, 3>
      %83 = llvm.getelementptr %10[%80] : (!llvm.ptr<f32, 3>, i32) -> !llvm.ptr<f32, 3>
      %84 = llvm.load %83 : !llvm.ptr<f32, 3>
      %85 = llvm.fcmp "ugt" %82, %84 : f32
      %86 = llvm.select %85, %82, %84 : i1, f32
      %87 = llvm.fcmp "uno" %84, %84 : f32
      %88 = llvm.select %87, %84, %86 : i1, f32
      %89 = llvm.getelementptr %arg6[%25] : (!llvm.ptr<f32>, i32) -> !llvm.ptr<f32>
      %90 = llvm.load %89 : !llvm.ptr<f32>
      llvm.br ^bb13(%90 : f32)
    ^bb13(%91: f32):  // 2 preds: ^bb12, ^bb13
      %92 = llvm.fcmp "ogt" %91, %88 : f32
      %93 = llvm.select %92, %91, %88 : i1, f32
      %94 = llvm.bitcast %89 : !llvm.ptr<f32> to !llvm.ptr<i32>
      %95 = llvm.bitcast %91 : f32 to i32
      %96 = llvm.bitcast %93 : f32 to i32
      %97 = llvm.cmpxchg %94, %95, %96 acq_rel monotonic : !llvm.ptr<i32>, i32
      %98 = llvm.extractvalue %97[0] : !llvm.struct<(i32, i1)> 
      %99 = llvm.bitcast %98 : i32 to f32
      %100 = llvm.extractvalue %97[1] : !llvm.struct<(i32, i1)> 
      llvm.cond_br %100, ^bb14, ^bb13(%99 : f32)
    ^bb14:  // pred: ^bb13
      llvm.br ^bb15
    ^bb15:  // 2 preds: ^bb11, ^bb14
      llvm.br ^bb16
    ^bb16:  // 2 preds: ^bb1, ^bb15
      llvm.return
    }
  }
  "disc_shape.SymbolicDim"() {knownNegativeOne = false, knownNonNegative = true, knownNonSizeOne = false, knownNonSizeZero = false, sym_name = "S0", value = -9223372036854775808 : i64} : () -> ()
  "disc_shape.SymbolicDim"() {knownNegativeOne = false, knownNonNegative = true, knownNonSizeOne = false, knownNonSizeZero = false, sym_name = "S1", value = -9223372036854775808 : i64} : () -> ()
  "disc_shape.SymbolicDim"() {knownNegativeOne = false, knownNonNegative = true, knownNonSizeOne = false, knownNonSizeZero = false, sym_name = "S2", value = -9223372036854775808 : i64} : () -> ()
  "disc_shape.SymbolicDim"() {knownNegativeOne = false, knownNonNegative = true, knownNonSizeOne = false, knownNonSizeZero = false, sym_name = "S3", value = -9223372036854775808 : i64} : () -> ()
  func.func @shape_constraint_graph() {
    %0 = "disc_shape.dim"() {name = @S3} : () -> index
    %1 = "disc_shape.dim"() {name = @S1} : () -> index
    %2 = "disc_shape.dim"() {name = @S2} : () -> index
    "disc_shape.tie_product_equal"(%0, %1, %2) {operand_segment_sizes = array<i32: 1, 2>} : (index, index, index) -> ()
    return
  }
}


// -----// IR Dump After DiscStripShapeConstraintOpsPass (disc-strip-shape-constraint-ops) //----- //
module attributes {gpu.container_module} {
  func.func @main(%arg0: !disc_ral.context) attributes {tf.entry_function = {input_placements = "gpu", inputs = "input0", output_placements = "gpu", outputs = "output0"}} {
    %c-1 = arith.constant -1 : index
    %c128 = arith.constant 128 : index
    %c16 = arith.constant 16 : index
    %c32 = arith.constant 32 : index
    %c8 = arith.constant 8 : index
    %0 = llvm.mlir.constant(0 : i32) : i32
    %c0 = arith.constant 0 : index
    %c1 = arith.constant 1 : index
    %c2 = arith.constant 2 : index
    %c256 = arith.constant 256 : index
    %c108 = arith.constant 108 : index
    %1 = "disc_ral.dispatch"(%arg0, %c0) {backend_config = "", call_target_name = "ral_recv_input", device = "cpu", has_side_effect = false} : (!disc_ral.context, index) -> memref<?x?x?xf32, "gpu">
    %dim = memref.dim %1, %c2 : memref<?x?x?xf32, "gpu">
    %dim_0 = memref.dim %1, %c1 : memref<?x?x?xf32, "gpu">
    %dim_1 = memref.dim %1, %c0 : memref<?x?x?xf32, "gpu">
    %2 = arith.index_cast %dim_0 : index to i32
    %3 = arith.index_cast %dim : index to i32
    %4 = arith.muli %2, %3 : i32
    %5 = arith.index_cast %4 : i32 to index
    %alloc = memref.alloc(%5) {kDiscSymbolicDimAttr = [@S3]} : memref<?xf32, "gpu">
    %6 = arith.muli %dim_1, %5 : index
    %7 = arith.addi %6, %c-1 : index
    %8 = arith.divsi %7, %c256 : index
    %9 = arith.addi %8, %c1 : index
    %10 = arith.subi %c0, %6 : index
    %11 = arith.divsi %10, %c256 : index
    %12 = arith.subi %c0, %11 : index
    %13 = arith.cmpi sgt, %6, %c0 : index
    %14 = arith.select %13, %9, %12 : index
    %15 = arith.cmpi sgt, %14, %c108 : index
    cf.cond_br %15, ^bb1, ^bb2
  ^bb1:  // pred: ^bb0
    %c256_2 = arith.constant 256 : index
    %c0_3 = arith.constant 0 : index
    %c1_4 = arith.constant 1 : index
    %16 = arith.cmpi sle, %5, %c0_3 : index
    %17 = arith.subi %c0_3, %5 : index
    %18 = arith.subi %5, %c1_4 : index
    %19 = arith.select %16, %17, %18 : index
    %20 = arith.divsi %19, %c256_2 : index
    %21 = arith.subi %c0_3, %20 : index
    %22 = arith.addi %20, %c1_4 : index
    %23 = arith.select %16, %21, %22 : index
    gpu.launch_func  @main_kernel::@main_kColReduction_reduce__4_1_0___8w32h blocks in (%23, %c1, %c1) threads in (%c256, %c1, %c1) args(%c256 : index, %5 : index, %alloc : memref<?xf32, "gpu">)
    %24 = arith.addi %5, %c-1 : index
    %25 = arith.divsi %24, %c8 : index
    %26 = arith.addi %25, %c1 : index
    %27 = arith.subi %c0, %5 : index
    %28 = arith.divsi %27, %c8 : index
    %29 = arith.subi %c0, %28 : index
    %30 = arith.cmpi sgt, %5, %c0 : index
    %31 = arith.select %30, %26, %29 : index
    %32 = arith.addi %dim_1, %c-1 : index
    %33 = arith.divsi %32, %c32 : index
    %34 = arith.addi %33, %c1 : index
    %35 = arith.subi %c0, %dim_1 : index
    %36 = arith.divsi %35, %c32 : index
    %37 = arith.subi %c0, %36 : index
    %38 = arith.cmpi sgt, %dim_1, %c0 : index
    %39 = arith.select %38, %34, %37 : index
    %40 = arith.muli %31, %39 : index
    %41 = arith.muli %40, %c256 : index
    %c256_5 = arith.constant 256 : index
    %c0_6 = arith.constant 0 : index
    %c1_7 = arith.constant 1 : index
    %42 = arith.cmpi sle, %41, %c0_6 : index
    %43 = arith.subi %c0_6, %41 : index
    %44 = arith.subi %41, %c1_7 : index
    %45 = arith.select %42, %43, %44 : index
    %46 = arith.divsi %45, %c256_5 : index
    %47 = arith.subi %c0_6, %46 : index
    %48 = arith.addi %46, %c1_7 : index
    %49 = arith.select %42, %47, %48 : index
    gpu.launch_func  @main_kernel_0::@main_kColReduction_reduce__4_1_0___8w32h_1 blocks in (%49, %c1, %c1) threads in (%c256, %c1, %c1) args(%5 : index, %1 : memref<?x?x?xf32, "gpu">, %c256 : index, %41 : index, %31 : index, %alloc : memref<?xf32, "gpu">)
    cf.br ^bb3
  ^bb2:  // pred: ^bb0
    %c128_8 = arith.constant 128 : index
    %c0_9 = arith.constant 0 : index
    %c1_10 = arith.constant 1 : index
    %50 = arith.cmpi sle, %5, %c0_9 : index
    %51 = arith.subi %c0_9, %5 : index
    %52 = arith.subi %5, %c1_10 : index
    %53 = arith.select %50, %51, %52 : index
    %54 = arith.divsi %53, %c128_8 : index
    %55 = arith.subi %c0_9, %54 : index
    %56 = arith.addi %54, %c1_10 : index
    %57 = arith.select %50, %55, %56 : index
    gpu.launch_func  @main_kernel_1::@main_kColReduction_reduce__4_1_0___8w16h blocks in (%57, %c1, %c1) threads in (%c128, %c1, %c1) args(%c128 : index, %5 : index, %alloc : memref<?xf32, "gpu">)
    %58 = arith.addi %5, %c-1 : index
    %59 = arith.divsi %58, %c8 : index
    %60 = arith.addi %59, %c1 : index
    %61 = arith.subi %c0, %5 : index
    %62 = arith.divsi %61, %c8 : index
    %63 = arith.subi %c0, %62 : index
    %64 = arith.cmpi sgt, %5, %c0 : index
    %65 = arith.select %64, %60, %63 : index
    %66 = arith.addi %dim_1, %c-1 : index
    %67 = arith.divsi %66, %c16 : index
    %68 = arith.addi %67, %c1 : index
    %69 = arith.subi %c0, %dim_1 : index
    %70 = arith.divsi %69, %c16 : index
    %71 = arith.subi %c0, %70 : index
    %72 = arith.cmpi sgt, %dim_1, %c0 : index
    %73 = arith.select %72, %68, %71 : index
    %74 = arith.muli %65, %73 : index
    %75 = arith.muli %74, %c128 : index
    %c128_11 = arith.constant 128 : index
    %c0_12 = arith.constant 0 : index
    %c1_13 = arith.constant 1 : index
    %76 = arith.cmpi sle, %75, %c0_12 : index
    %77 = arith.subi %c0_12, %75 : index
    %78 = arith.subi %75, %c1_13 : index
    %79 = arith.select %76, %77, %78 : index
    %80 = arith.divsi %79, %c128_11 : index
    %81 = arith.subi %c0_12, %80 : index
    %82 = arith.addi %80, %c1_13 : index
    %83 = arith.select %76, %81, %82 : index
    gpu.launch_func  @main_kernel_2::@main_kColReduction_reduce__4_1_0___8w16h_1 blocks in (%83, %c1, %c1) threads in (%c128, %c1, %c1) args(%5 : index, %1 : memref<?x?x?xf32, "gpu">, %c128 : index, %75 : index, %65 : index, %alloc : memref<?xf32, "gpu">)
    cf.br ^bb3
  ^bb3:  // 2 preds: ^bb1, ^bb2
    %84 = llvm.inttoptr %0 : i32 to !llvm.ptr<i8>
    %alloca = memref.alloca() : memref<2xindex, "cpu">
    memref.store %dim_0, %alloca[%c0] : memref<2xindex, "cpu">
    memref.store %dim, %alloca[%c1] : memref<2xindex, "cpu">
    %85 = "disc_ral.dispatch"(%arg0, %84, %alloc, %alloca) {backend_config = "", call_target_name = "inc_ref", device = "gpu", has_side_effect = false} : (!disc_ral.context, !llvm.ptr<i8>, memref<?xf32, "gpu">, memref<2xindex, "cpu">) -> memref<?x?xf32, "gpu">
    %reinterpret_cast = memref.reinterpret_cast %85 to offset: [0], sizes: [%dim_0, %dim], strides: [%dim, 1] {kDiscSymbolicDimAttr = [@S1, @S2]} : memref<?x?xf32, "gpu"> to memref<?x?xf32, "gpu">
    memref.dealloc %alloc : memref<?xf32, "gpu">
    "disc_ral.dispatch"(%arg0, %c0, %reinterpret_cast) {backend_config = "", call_target_name = "ral_send_output", device = "cpu", has_side_effect = false} : (!disc_ral.context, index, memref<?x?xf32, "gpu">) -> ()
    return
  }
  gpu.module @main_kernel attributes {gpu.binary = "P\EDU\BA\01\00\10\008\04\00\00\00\00\00\00\02\00\01\01@\00\00\00\F8\03\00\00\00\00\00\00\F8\03\00\00\00\00\00\00\07\00\01\00P\00\00\00\00\00\00\00\00\00\00\00\11 \00\00\00\00\00\00\00\00\00\00\00\00\00\00\E8\0B\00\00\00\00\00\00\A2\7FELF\02\01\013\07\00\01\00f\02\00\BE\00q\00\01\00!@\0B\07\001\00\80\08\07\00\F5\0E\00P\05P\00@\008\00\03\00@\00\0B\00\01\00\00.shstrtab\00.\08\00'ym\08\00\F0\1B_shndx\00.nv.info\00.text.main_kColReduction_r\0A\00\F5\01e__4_1_0___8w32h8\00\0F2\00\1Boshared4\00\1B\9Fconstant07\00\18\FA\01debug_frame\00.rel\11\00!nv\14\00\11aC\00\0F+\01 \0F\88\00\15\0FT\01\BAo_param[\01\1C\0F\01\00\06\8C[\00\00\00\03\00\0A\00\01\00\11\F0\18\00,\09\00\01\00 .\01\18\00,\04\00\01\00\11L\18\00,\07\00\01\00f2\00\00\00\12\10`\00!\80\01\07\00v\00\FF\FF\FF\FF(\00\0C\00\00\01\00@\03\00\04|\08\00\D1\0F\0C\81\80\80(\00\08\FF\81\80(\08\0B\00\06(\00/0\00\01\00\03\13pX\00\10\04\9B\00R\04\14\00\00\00E\00\01\0B\00\00\13\00p/\08\00\05\00\00\00\A7\03\22\04#\0C\00\00\01\00'\04\12\0C\00\17\11\0C\00!7\04\E8\03\F3\08\015\00\00\04\0A\08\00\02\00\00\00`\01\10\00\03\19\10\00\04\17\0C\0C\04U\08\00\00\F0!\10\00\10\01\18\01%\F0\11\10\00\01\01\00\F2\02\F0\11\00\03\1B\FF\00\04\1C\08\00P\00\00\00\B0\00\01\00#K\00\01\00s\02\02\08\10\0A/\22\9B\00\00\07\00\03\08\00\13\10\08\00\13\18\08\00\13 \08\00\13(\08\00\130\08\00\228\08|\00\13\00\08\00\13\08\08\00\13\10\08\00\13\18\08\00\13 \08\00\13(\08\00\130\08\00\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\A2\00\00\00\14,\00\00\00H\00\01\00\000\01/\05\00\01\00\FF\B0A\02z\01\00\1F\04\B1\0F\00\00\00\C4\0F\00\19y\02\00\01\00\10%\8B\02Q\0E\00\19y\03\0F\00\F5\1A\00!\00\00\00$\0E\00$z\02\02\00X\00\00\03\02\8E\07\00\CA\1F\00\0Cz\00\02\00Y\00\00p`\F0\03\00\DA\0F\00MS\04\A0\80\03\00\EA\0F\005t\03\FF\B3\03\10\FF\C0\03P\E2\0F\00\02x6\02B\80\FF\00\0F\10\00r\B9z\04\00\00F\00\84\00\94\D0\0F\00%v\02\02\00Z`\00`\0F\00\86y\00\022\00@\04\19\10\0C0\009My\00`\00PGy\00\00\F09\04\A6\FF\83\03\00\C0\0F\00\18y\00\01\00\0F\10\00\90\0F\01\00-\00W\01.\03\00\01\00\22@\00\01\00=+\01\000\00\08\01\00\1F\0B@\00\04\13k)\00\1F[@\00\0C\13\13\0C\04\0C\01\00\13\C8\15\00&\90\000\04#\04\00\85\04\00\F6\04\12\00\01\00\1F\FET\00\00\00\01\00\13X\95\00/p\00\80\00\0B\1F)'\00\03#\00\C8@\00\04P\06\04\E4\00*\04\00\01\00\1Fa@\00\04\13\F81\00&\\\00@\00\1F\0A@\00\00!\1C\01D\01\0D@\00\13X)\00*\D8\00\01\00\1B\08\08\00?\0B\01\00\86\07\00Q\00\000\05\00\01\00&\10\00\80\00\17\048\00\04\18\00\13\C7\14\01\0C\84\01*@\058\07\1F\00\C0\00\04\132@\00+\06\00\01\00\1A\07\D0\07\12\03\F0\05:\08\80\00\01\00\13\06\08\06\04(\0B\0C\01\00*\A8\00\08\00\04\F8\00\14\018\00/\05\00\01\00\029@\03\00\08\00\088\00/\06\00\01\00\17\80\08\00\00\00\00\00\00\00"} {
    llvm.func @main_kColReduction_reduce__4_1_0___8w32h(%arg0: i32, %arg1: i32, %arg2: !llvm.ptr<f32>) attributes {disc.elimargs = [2 : index, 4 : index, 5 : index, 6 : index], gpu.kernel, nvvm.kernel} {
      %0 = llvm.mlir.constant(0xFF800000 : f32) : f32
      %1 = nvvm.read.ptx.sreg.ctaid.x : i32
      %2 = nvvm.read.ptx.sreg.tid.x : i32
      llvm.br ^bb1
    ^bb1:  // pred: ^bb0
      %3 = llvm.mul %1, %arg0  : i32
      %4 = llvm.add %2, %3  : i32
      %5 = llvm.icmp "ult" %4, %arg1 : i32
      llvm.cond_br %5, ^bb2, ^bb3
    ^bb2:  // pred: ^bb1
      %6 = llvm.getelementptr %arg2[%4] : (!llvm.ptr<f32>, i32) -> !llvm.ptr<f32>
      llvm.store %0, %6 : !llvm.ptr<f32>
      llvm.br ^bb3
    ^bb3:  // 2 preds: ^bb1, ^bb2
      llvm.return
    }
  }
  gpu.module @main_kernel_0 attributes {gpu.binary = "P\EDU\BA\01\00\10\00@\08\00\00\00\00\00\00\02\00\01\01@\00\00\00\00\08\00\00\00\00\00\00\F9\07\00\00\00\00\00\00\07\00\01\00P\00\00\00\00\00\00\00\00\00\00\00\11 \00\00\00\00\00\00\00\00\00\00\00\00\00\00(\14\00\00\00\00\00\00\A2\7FELF\02\01\013\07\00\01\00f\02\00\BE\00q\00\01\00#\80\13\08\00\11\10\07\00\F5\0E\00P\05P\00@\008\00\03\00@\00\0C\00\01\00\00.shstrtab\00.\08\00'ym\08\00\F0\1B_shndx\00.nv.info\00.text.main_kColReduction_r\0A\00\F5\03e__4_1_0___8w32h_1:\00\0F4\00\1Doshared6\00\1AOrela\A0\00\1F?rel\D5\00\22\9Fconstant09\00\1A\B2debug_frame{\00\09\11\00!nv\14\00\11aE\00\0F\9E\01 \0F\8A\00\17\0F\C9\01\F4\8F$____wg_3\00\17\00\0C\00/27\02\02'o_param\09\02\1C\0F\01\00\05\8C]\00\00\00\03\00\0A\00\01\00\11\C2\18\00,\0B\00\01\00 \9C\01\18\00,\09\00\01\00\11\DC\18\00,\04\00\01\00\11\FA\18\00,\07\00\01\00g2\00\00\00\12\10x\00\11\08\06\00v\00\FF\FF\FF\FF(\00\0C\00\00\01\00@\03\00\04|\08\00\D1\0F\0C\81\80\80(\00\08\FF\81\80(\08\0B\00\06(\00/0\00\01\00\03\13\F0{\00\10\04\9B\00R\04\14\00\00\00E\002\04\AC\01\18\00\80/\08\00\06\00\00\00\0E\0C\00\12#\0C\00\00\01\00'\04\12\0C\00\17\11\0C\00!7\04 \05\F1\08\015\00\00\04\0A\08\00\03\00\00\00`\01(\00\03\19(\00\04\17\0C$\00u\06\00 \00\00\F0!\10\00u\05\00\1C\00\00\F0\11\10\009\04\00\18\10\009\03\00\14\10\009\02\00\10\10\009\01\00\08P\00\01\01\00\F2\0A\F0\11\00\03\1B\FF\00\04\1C\0C\00P\00\00\00\10\06\00\00\10\07\00\00\04\1E\84\01#K\00\01\00v\02\02\08\10\0A/\22b\01\13\08\08\00\13\10\08\00\13\18\08\00\13 \08\00\13(\08\00\130\08\00\228\08\84\00\13\00\08\00\13\08\08\00\13\10\08\00\13\18\08\00\13 \08\00\13(\08\00\130\08\00\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\A2\00\00\00\14,\00\00\00H\00\01\00\11\02h\01\0F\01\00\FF\B0@$v\01\FF\7F\04\B2\FF\00\8E\07\00\C4\0F\00\19y\00\01\00\10%\8B\02a\0E\00\19y\03\00\01\00\10!-\00B\0E\00$z\B5\04\90\03\02\8E\07\00\CA\1F\00\0C\10\00\C5^\00\00p`\F0\03\00\DA\0F\00M\9B\04\F0\0C\80\03\00\EA\0F\00\06{\04\00\00_\00\00\00\90 \00\00\22\0E\00\19x\05\FF\1F\1F\00\D3\14\01\00\00\E2\0F\00$r\02\FF\FF\00\80\00 \E2\0FP\00\10\FF0\00@pP\F4\03\10\00\81\B9z\04\00\00F\00\00#\05p\E2\0F\00\11r\05\05?\00\B2\FF@\8F\07\00\C6\0F\00\08s\04\F6\04\00 \09\F1\07$\1E\00\10x\03\04\FE\FF\FF\0F\FF\E0\FF\07\00\CC\1F\00\05s\03$\04\00\C0\03\01\C0\00!r\07p\00\C0\03\0A\8E\07\00\C8\1F\00$z\07\07p\00\10\FF\D0\00\81\C8\0F\00'r\07\03\07\E0\02\02\90\00@\19x\02\FF\01\03\10\05\B0\00q\E4\0F\00\12x\05\05\F1\04!\C0\8E\80\00T'r\07\07\02\C0\00\10\C8\D0\00\11\09`\00\10\07`\00`\E4\0F\00$x\05\BC\03$\00\05\10\000z\03\09p\00\22\02\02@\00@\19x\00\FF\A0\00\22\05\16`\00f\0Cz\00\03\00_P\010\10\0A\03\10\00\11\80\D0\00p\E4\0F\00\10\08\07\07P\00\04\10\00\060\00\12\F20\00\1A\18 \001\12\AA\07P\01 \FF3\B0\01\00\A0\00\1B\03\A0\000\00\07  \01\15\02\A0\00\15\03\A0\00\10\E2\F0\00 \02\05\10\01#\FF\C0@\00Sx\09\03\08\00 \00\11\CA\80\003\09\00X\80\00\22\C8\0F\10\02p\\\00\00p\10\F2\04\90\00T$\14\03\FF\04 \01\00`\000\1A\02\000\00\13\09p\01c%\16\02\02\00ZP\02q\CC\0F\00\81\19\02\02\D0\01\C4\19\1E\0C\00\A2\00\00\10x\06\00\10\D0\00\000\02\92t\04\FF\00\00\80\FF\FF\00\A0\001\1Cx\00\01\001p\F0\F00\02E$x\07\05p\00q\E2\0F\04\0Cz\00\06\90\00#`\F6 \00\16\06 \00\80\C6\0F\00\0Cx\00\05\7F@\00\C5D\F6\01\00\E4\0F\04\10x\03\00\08p\00`\1F\00!\12\04\02q\00\01\F1\04!\E2O0\00\11\070\00 \F2\04\C0\00c\88s\00\07\04\00\DE\05f\E8\0F\00\1D{\00\01\00u\EC\0F\00\84\B9\0B\06~\05R\22\0E\00\10x\B0\02\01`\00Q\C4\0F\00\10x\1E\00\03p\00\000\004\02\06\00P\00qb\0E\00\0B\B2\00\0B\06\08\B1\80\F4\03\00\E4\1F\08\0B\B2\00\02\10\00\A3\C0\F8\03\00\E4/\00\1C\B8\00\00\01%p\01\10\02\01\F0\00B\F4\03\00\C8\E0\00\11?\B0\00\C3t\01\00\CE\0F\00\08\82\0B\0B\02\00'\07\1B\E4@\01\8F\C6\0F\00\88\B3\00\06\0B\D0\00\095\A9\03\06\8E\06[(\0E\00\84\A9\B0\001\A2\00\03p\03`\80\F6\03\00\C4\1F\10\00(\02\03\B0\00$\A8\00p\00\04\B0\00\15\04\A0\01\03\B0\00\14\1F\90\01\01\B0\00/\03\03\B0\00\0AO\A3\00\06\03\80\01\0AV\07\06\00\80\00\B0\00\0E`\01$\07\07`\01\00\B0\00O\B2\00\02\07`\01\0B\1C\00`\01\19\0F`\01\02\10\04\0F`\01\09\1F\07`\01\0D\17@\B0\00\13\A9g\07\0F`\01\07\1F\00`\01\06\12\DA@\01\14\00\90\00\1F\CA0\01\0F9M\19\00\C0\05G$t\02\FFP\03A\00\84y\05_\09\01@\00\96&\0E\00%v\02\09\00`0\04'\84y\B0\00fh\0E\00\81y\08\D0\03bb\05\00\0Br\00\C1\05\22\80\F0\C0\001r\00\00\10\00\92\C0\F2\03\00\D6/\00\08\82\1F\00p\00\00\80\04\00\C8O \00\11\08\11\00 @\F0\C0\03$\0EF\A0\06\00\A0\00b\E6\0F\00\08r\09 \00\00\01\00p\CC\0F\00\A9s\09\02{\00\C0\09\E1\1E\00\00\A4\0E\00\0Cr\00\09\10\00 pR@\00QO\00$r\08\00\05\10\09\D0\00\80\D8\0F\00G\09\00\00\90\D0\05!\FF\83\F0\00*My\00\01TGy\00\00\F0 \00f\C0\0F\00\18y\00\01\00\0F\10\00\B0\0F\01\00-#\01\00\80\02\0B\01\00\22@\00\01\00=\9E\01\000\00\08\01\00\1F\0B@\00\04\13\DE)\00?\09\02\00@\00\0A\22\13\00@\05\0C\01\00\13\E8U\00\03\0F\03\01$\00\13\05\97\02\00\01\00\22\18\00\01\00.q\01T\00\00\01\00\11\90\B5\02O\00\00p\00\80\00\0B\1F)'\00\03\03\D5\02$\00\00\18\0D\04\E4\00*\04\00\01\00\1Fc@\00\04*0\05\C0\00\13\03\03\09\0C@\00!\8F\01D\01\0D@\00\13\D8@\00*\D8\00\01\00\1B\08\08\00?~\01\00N\0E\002\00\00\B0\C6\03\01W\09\04\80\00\17\048\00\04\18\00\138@\01\0C\84\01\13\C0@\00\17\881\01\0F\C0\00\01\132T\01\02\E6\07\06\01\00\1B\80\A9\00\11\03$\00J\00\0E\80\00\01\00\13\97#\00*\03\00\01\00\040\13/\00\04\80\00\0B\13\06\AB\01\04h\13\0C\01\00\1B\A8\08\00\17\08\08\02\17\05\E8\00\0C\01\00*\C0\09\08\00\088\00\18\06\A0\00\0F\01\00\05\03\A9\00\80\08\00\00\00\00\00\00\00\00\00\00\00\00\00\00"} {
    llvm.mlir.global internal @__wg_main_kColReduction_reduce__4_1_0___8w32h_1_0() {addr_space = 3 : i32} : !llvm.array<256 x f32>
    llvm.func @__nv_fabsf(f32) -> f32
    llvm.func @main_kColReduction_reduce__4_1_0___8w32h_1(%arg0: i32, %arg1: !llvm.ptr<f32>, %arg2: i32, %arg3: i32, %arg4: i32, %arg5: i32, %arg6: !llvm.ptr<f32>) attributes {disc.elimargs = [1 : index, 3 : index, 5 : index, 6 : index, 7 : index, 8 : index, 9 : index, 13 : index, 15 : index, 16 : index, 17 : index], gpu.kernel, nvvm.kernel} {
      %0 = llvm.mlir.constant(8 : index) : i32
      %1 = llvm.mlir.constant(32 : index) : i32
      %2 = llvm.mlir.constant(2 : index) : i32
      %3 = llvm.mlir.constant(0xFF800000 : f32) : f32
      %4 = llvm.mlir.constant(16 : index) : i32
      %5 = llvm.mlir.constant(128 : index) : i32
      %6 = llvm.mlir.constant(64 : index) : i32
      %7 = llvm.mlir.constant(4 : index) : i32
      %8 = llvm.mlir.constant(256 : index) : i32
      %9 = llvm.mlir.constant(0 : index) : i32
      %10 = llvm.mlir.addressof @__wg_main_kColReduction_reduce__4_1_0___8w32h_1_0 : !llvm.ptr<array<256 x f32>, 3>
      %11 = llvm.getelementptr %10[0, 0] : (!llvm.ptr<array<256 x f32>, 3>) -> !llvm.ptr<f32, 3>
      %12 = nvvm.read.ptx.sreg.ctaid.x : i32
      %13 = nvvm.read.ptx.sreg.tid.x : i32
      llvm.br ^bb1
    ^bb1:  // pred: ^bb0
      %14 = llvm.mul %12, %arg3  : i32
      %15 = llvm.add %13, %14  : i32
      %16 = llvm.icmp "ult" %15, %arg4 : i32
      llvm.cond_br %16, ^bb2, ^bb18
    ^bb2:  // pred: ^bb1
      %17 = llvm.srem %15, %8  : i32
      %18 = llvm.sdiv %15, %8  : i32
      %19 = llvm.udiv %17, %0  : i32
      %20 = llvm.urem %17, %0  : i32
      %21 = llvm.udiv %18, %arg5  : i32
      %22 = llvm.urem %18, %arg5  : i32
      %23 = llvm.mul %21, %1  : i32
      %24 = llvm.add %23, %19  : i32
      %25 = llvm.mul %22, %0  : i32
      %26 = llvm.add %25, %20  : i32
      %27 = llvm.icmp "ult" %24, %arg2 : i32
      %28 = llvm.icmp "ult" %26, %arg0 : i32
      %29 = llvm.and %27, %28  : i1
      llvm.cond_br %29, ^bb3, ^bb4
    ^bb3:  // pred: ^bb2
      %30 = llvm.mul %24, %arg0  : i32
      %31 = llvm.add %30, %26  : i32
      %32 = llvm.getelementptr %arg1[%31] : (!llvm.ptr<f32>, i32) -> !llvm.ptr<f32>
      %33 = llvm.load %32 : !llvm.ptr<f32>
      %34 = llvm.call @__nv_fabsf(%33) : (f32) -> f32
      %35 = llvm.getelementptr %11[%17] : (!llvm.ptr<f32, 3>, i32) -> !llvm.ptr<f32, 3>
      llvm.store %34, %35 : !llvm.ptr<f32, 3>
      llvm.br ^bb5
    ^bb4:  // pred: ^bb2
      %36 = llvm.getelementptr %11[%17] : (!llvm.ptr<f32, 3>, i32) -> !llvm.ptr<f32, 3>
      llvm.store %3, %36 : !llvm.ptr<f32, 3>
      llvm.br ^bb5
    ^bb5:  // 2 preds: ^bb3, ^bb4
      nvvm.barrier0
      %37 = llvm.icmp "ult" %19, %4 : i32
      %38 = llvm.add %24, %4  : i32
      %39 = llvm.icmp "ult" %38, %arg2 : i32
      %40 = llvm.and %37, %39  : i1
      llvm.cond_br %40, ^bb6, ^bb7
    ^bb6:  // pred: ^bb5
      %41 = llvm.add %17, %5  : i32
      %42 = llvm.getelementptr %11[%17] : (!llvm.ptr<f32, 3>, i32) -> !llvm.ptr<f32, 3>
      %43 = llvm.load %42 : !llvm.ptr<f32, 3>
      %44 = llvm.getelementptr %11[%41] : (!llvm.ptr<f32, 3>, i32) -> !llvm.ptr<f32, 3>
      %45 = llvm.load %44 : !llvm.ptr<f32, 3>
      %46 = llvm.fcmp "ugt" %43, %45 : f32
      %47 = llvm.select %46, %43, %45 : i1, f32
      %48 = llvm.fcmp "uno" %45, %45 : f32
      %49 = llvm.select %48, %45, %47 : i1, f32
      %50 = llvm.getelementptr %11[%17] : (!llvm.ptr<f32, 3>, i32) -> !llvm.ptr<f32, 3>
      llvm.store %49, %50 : !llvm.ptr<f32, 3>
      llvm.br ^bb7
    ^bb7:  // 2 preds: ^bb5, ^bb6
      nvvm.barrier0
      %51 = llvm.icmp "ult" %19, %0 : i32
      %52 = llvm.add %24, %0  : i32
      %53 = llvm.icmp "ult" %52, %arg2 : i32
      %54 = llvm.and %51, %53  : i1
      llvm.cond_br %54, ^bb8, ^bb9
    ^bb8:  // pred: ^bb7
      %55 = llvm.add %17, %6  : i32
      %56 = llvm.getelementptr %11[%17] : (!llvm.ptr<f32, 3>, i32) -> !llvm.ptr<f32, 3>
      %57 = llvm.load %56 : !llvm.ptr<f32, 3>
      %58 = llvm.getelementptr %11[%55] : (!llvm.ptr<f32, 3>, i32) -> !llvm.ptr<f32, 3>
      %59 = llvm.load %58 : !llvm.ptr<f32, 3>
      %60 = llvm.fcmp "ugt" %57, %59 : f32
      %61 = llvm.select %60, %57, %59 : i1, f32
      %62 = llvm.fcmp "uno" %59, %59 : f32
      %63 = llvm.select %62, %59, %61 : i1, f32
      %64 = llvm.getelementptr %11[%17] : (!llvm.ptr<f32, 3>, i32) -> !llvm.ptr<f32, 3>
      llvm.store %63, %64 : !llvm.ptr<f32, 3>
      llvm.br ^bb9
    ^bb9:  // 2 preds: ^bb7, ^bb8
      nvvm.barrier0
      %65 = llvm.icmp "ult" %19, %7 : i32
      %66 = llvm.add %24, %7  : i32
      %67 = llvm.icmp "ult" %66, %arg2 : i32
      %68 = llvm.and %65, %67  : i1
      llvm.cond_br %68, ^bb10, ^bb11
    ^bb10:  // pred: ^bb9
      %69 = llvm.add %17, %1  : i32
      %70 = llvm.getelementptr %11[%17] : (!llvm.ptr<f32, 3>, i32) -> !llvm.ptr<f32, 3>
      %71 = llvm.load %70 : !llvm.ptr<f32, 3>
      %72 = llvm.getelementptr %11[%69] : (!llvm.ptr<f32, 3>, i32) -> !llvm.ptr<f32, 3>
      %73 = llvm.load %72 : !llvm.ptr<f32, 3>
      %74 = llvm.fcmp "ugt" %71, %73 : f32
      %75 = llvm.select %74, %71, %73 : i1, f32
      %76 = llvm.fcmp "uno" %73, %73 : f32
      %77 = llvm.select %76, %73, %75 : i1, f32
      %78 = llvm.getelementptr %11[%17] : (!llvm.ptr<f32, 3>, i32) -> !llvm.ptr<f32, 3>
      llvm.store %77, %78 : !llvm.ptr<f32, 3>
      llvm.br ^bb11
    ^bb11:  // 2 preds: ^bb9, ^bb10
      nvvm.barrier0
      %79 = llvm.icmp "ult" %19, %2 : i32
      %80 = llvm.add %24, %2  : i32
      %81 = llvm.icmp "ult" %80, %arg2 : i32
      %82 = llvm.and %79, %81  : i1
      llvm.cond_br %82, ^bb12, ^bb13
    ^bb12:  // pred: ^bb11
      %83 = llvm.add %17, %4  : i32
      %84 = llvm.getelementptr %11[%17] : (!llvm.ptr<f32, 3>, i32) -> !llvm.ptr<f32, 3>
      %85 = llvm.load %84 : !llvm.ptr<f32, 3>
      %86 = llvm.getelementptr %11[%83] : (!llvm.ptr<f32, 3>, i32) -> !llvm.ptr<f32, 3>
      %87 = llvm.load %86 : !llvm.ptr<f32, 3>
      %88 = llvm.fcmp "ugt" %85, %87 : f32
      %89 = llvm.select %88, %85, %87 : i1, f32
      %90 = llvm.fcmp "uno" %87, %87 : f32
      %91 = llvm.select %90, %87, %89 : i1, f32
      %92 = llvm.getelementptr %11[%17] : (!llvm.ptr<f32, 3>, i32) -> !llvm.ptr<f32, 3>
      llvm.store %91, %92 : !llvm.ptr<f32, 3>
      llvm.br ^bb13
    ^bb13:  // 2 preds: ^bb11, ^bb12
      nvvm.barrier0
      %93 = llvm.icmp "eq" %19, %9 : i32
      %94 = llvm.and %93, %29  : i1
      llvm.cond_br %94, ^bb14, ^bb17
    ^bb14:  // pred: ^bb13
      %95 = llvm.add %17, %0  : i32
      %96 = llvm.getelementptr %11[%17] : (!llvm.ptr<f32, 3>, i32) -> !llvm.ptr<f32, 3>
      %97 = llvm.load %96 : !llvm.ptr<f32, 3>
      %98 = llvm.getelementptr %11[%95] : (!llvm.ptr<f32, 3>, i32) -> !llvm.ptr<f32, 3>
      %99 = llvm.load %98 : !llvm.ptr<f32, 3>
      %100 = llvm.fcmp "ugt" %97, %99 : f32
      %101 = llvm.select %100, %97, %99 : i1, f32
      %102 = llvm.fcmp "uno" %99, %99 : f32
      %103 = llvm.select %102, %99, %101 : i1, f32
      %104 = llvm.getelementptr %arg6[%26] : (!llvm.ptr<f32>, i32) -> !llvm.ptr<f32>
      %105 = llvm.load %104 : !llvm.ptr<f32>
      llvm.br ^bb15(%105 : f32)
    ^bb15(%106: f32):  // 2 preds: ^bb14, ^bb15
      %107 = llvm.fcmp "ogt" %106, %103 : f32
      %108 = llvm.select %107, %106, %103 : i1, f32
      %109 = llvm.bitcast %104 : !llvm.ptr<f32> to !llvm.ptr<i32>
      %110 = llvm.bitcast %106 : f32 to i32
      %111 = llvm.bitcast %108 : f32 to i32
      %112 = llvm.cmpxchg %109, %110, %111 acq_rel monotonic : !llvm.ptr<i32>, i32
      %113 = llvm.extractvalue %112[0] : !llvm.struct<(i32, i1)> 
      %114 = llvm.bitcast %113 : i32 to f32
      %115 = llvm.extractvalue %112[1] : !llvm.struct<(i32, i1)> 
      llvm.cond_br %115, ^bb16, ^bb15(%114 : f32)
    ^bb16:  // pred: ^bb15
      llvm.br ^bb17
    ^bb17:  // 2 preds: ^bb13, ^bb16
      llvm.br ^bb18
    ^bb18:  // 2 preds: ^bb1, ^bb17
      llvm.return
    }
  }
  gpu.module @main_kernel_1 attributes {gpu.binary = "P\EDU\BA\01\00\10\008\04\00\00\00\00\00\00\02\00\01\01@\00\00\00\F8\03\00\00\00\00\00\00\F8\03\00\00\00\00\00\00\07\00\01\00P\00\00\00\00\00\00\00\00\00\00\00\11 \00\00\00\00\00\00\00\00\00\00\00\00\00\00\E8\0B\00\00\00\00\00\00\A2\7FELF\02\01\013\07\00\01\00f\02\00\BE\00q\00\01\00!@\0B\07\001\00\80\08\07\00\F5\0E\00P\05P\00@\008\00\03\00@\00\0B\00\01\00\00.shstrtab\00.\08\00'ym\08\00\F0\1B_shndx\00.nv.info\00.text.main_kColReduction_r\0A\00\F5\01e__4_1_0___8w16h8\00\0F2\00\1Boshared4\00\1B\9Fconstant07\00\18\FA\01debug_frame\00.rel\11\00!nv\14\00\11aC\00\0F+\01 \0F\88\00\15\0FT\01\BAo_param[\01\1C\0F\01\00\06\8C[\00\00\00\03\00\0A\00\01\00\11\F0\18\00,\09\00\01\00 .\01\18\00,\04\00\01\00\11L\18\00,\07\00\01\00f2\00\00\00\12\10`\00!\80\01\07\00v\00\FF\FF\FF\FF(\00\0C\00\00\01\00@\03\00\04|\08\00\D1\0F\0C\81\80\80(\00\08\FF\81\80(\08\0B\00\06(\00/0\00\01\00\03\13pX\00\10\04\9B\00R\04\14\00\00\00E\00\01\0B\00\00\13\00p/\08\00\05\00\00\00\A7\03\22\04#\0C\00\00\01\00'\04\12\0C\00\17\11\0C\00!7\04\E8\03\F3\08\015\00\00\04\0A\08\00\02\00\00\00`\01\10\00\03\19\10\00\04\17\0C\0C\04U\08\00\00\F0!\10\00\10\01\18\01%\F0\11\10\00\01\01\00\F2\02\F0\11\00\03\1B\FF\00\04\1C\08\00P\00\00\00\B0\00\01\00#K\00\01\00s\02\02\08\10\0A/\22\9B\00\00\07\00\03\08\00\13\10\08\00\13\18\08\00\13 \08\00\13(\08\00\130\08\00\228\08|\00\13\00\08\00\13\08\08\00\13\10\08\00\13\18\08\00\13 \08\00\13(\08\00\130\08\00\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\A2\00\00\00\14,\00\00\00H\00\01\00\000\01/\05\00\01\00\FF\B0A\02z\01\00\1F\04\B1\0F\00\00\00\C4\0F\00\19y\02\00\01\00\10%\8B\02Q\0E\00\19y\03\0F\00\F5\1A\00!\00\00\00$\0E\00$z\02\02\00X\00\00\03\02\8E\07\00\CA\1F\00\0Cz\00\02\00Y\00\00p`\F0\03\00\DA\0F\00MS\04\A0\80\03\00\EA\0F\005t\03\FF\B3\03\10\FF\C0\03P\E2\0F\00\02x6\02B\80\FF\00\0F\10\00r\B9z\04\00\00F\00\84\00\94\D0\0F\00%v\02\02\00Z`\00`\0F\00\86y\00\022\00@\04\19\10\0C0\009My\00`\00PGy\00\00\F09\04\A6\FF\83\03\00\C0\0F\00\18y\00\01\00\0F\10\00\90\0F\01\00-\00W\01.\03\00\01\00\22@\00\01\00=+\01\000\00\08\01\00\1F\0B@\00\04\13k)\00\1F[@\00\0C\13\13\0C\04\0C\01\00\13\C8\15\00&\90\000\04#\04\00\85\04\00\F6\04\12\00\01\00\1F\FET\00\00\00\01\00\13X\95\00/p\00\80\00\0B\1F)'\00\03#\00\C8@\00\04P\06\04\E4\00*\04\00\01\00\1Fa@\00\04\13\F81\00&\\\00@\00\1F\0A@\00\00!\1C\01D\01\0D@\00\13X)\00*\D8\00\01\00\1B\08\08\00?\0B\01\00\86\07\00Q\00\000\05\00\01\00&\10\00\80\00\17\048\00\04\18\00\13\C7\14\01\0C\84\01*@\058\07\1F\00\C0\00\04\132@\00+\06\00\01\00\1A\07\D0\07\12\03\F0\05:\08\80\00\01\00\13\06\08\06\04(\0B\0C\01\00*\A8\00\08\00\04\F8\00\14\018\00/\05\00\01\00\029@\03\00\08\00\088\00/\06\00\01\00\17\80\08\00\00\00\00\00\00\00"} {
    llvm.func @main_kColReduction_reduce__4_1_0___8w16h(%arg0: i32, %arg1: i32, %arg2: !llvm.ptr<f32>) attributes {disc.elimargs = [2 : index, 4 : index, 5 : index, 6 : index], gpu.kernel, nvvm.kernel} {
      %0 = llvm.mlir.constant(0xFF800000 : f32) : f32
      %1 = nvvm.read.ptx.sreg.ctaid.x : i32
      %2 = nvvm.read.ptx.sreg.tid.x : i32
      llvm.br ^bb1
    ^bb1:  // pred: ^bb0
      %3 = llvm.mul %1, %arg0  : i32
      %4 = llvm.add %2, %3  : i32
      %5 = llvm.icmp "ult" %4, %arg1 : i32
      llvm.cond_br %5, ^bb2, ^bb3
    ^bb2:  // pred: ^bb1
      %6 = llvm.getelementptr %arg2[%4] : (!llvm.ptr<f32>, i32) -> !llvm.ptr<f32>
      llvm.store %0, %6 : !llvm.ptr<f32>
      llvm.br ^bb3
    ^bb3:  // 2 preds: ^bb1, ^bb2
      llvm.return
    }
  }
  gpu.module @main_kernel_2 attributes {gpu.binary = "P\EDU\BA\01\00\10\00\10\08\00\00\00\00\00\00\02\00\01\01@\00\00\00\D0\07\00\00\00\00\00\00\CB\07\00\00\00\00\00\00\07\00\01\00P\00\00\00\00\00\00\00\00\00\00\00\11 \00\00\00\00\00\00\00\00\00\00\00\00\00\00(\13\00\00\00\00\00\00\A2\7FELF\02\01\013\07\00\01\00f\02\00\BE\00q\00\01\00#\80\12\08\00\11\0F\07\00\F5\0E\00P\05P\00@\008\00\03\00@\00\0C\00\01\00\00.shstrtab\00.\08\00'ym\08\00\F0\1B_shndx\00.nv.info\00.text.main_kColReduction_r\0A\00\F5\03e__4_1_0___8w16h_1:\00\0F4\00\1Doshared6\00\1AOrela\A0\00\1F?rel\D5\00\22\9Fconstant09\00\1A\B2debug_frame{\00\09\11\00!nv\14\00\11aE\00\0F\9E\01 \0F\8A\00\17\0F\C9\01\F4\8F$____wg_3\00\17\00\0C\00/27\02\02'o_param\09\02\1C\0F\01\00\05\8C]\00\00\00\03\00\0A\00\01\00\11\C2\18\00,\0B\00\01\00 \9C\01\18\00,\09\00\01\00\11\DC\18\00,\04\00\01\00\11\FA\18\00,\07\00\01\00g2\00\00\00\12\10x\00\03#\00f\FF\FF\FF\FF(\00\0C\00\00\01\00@\03\00\04|\08\00\D1\0F\0C\81\80\80(\00\08\FF\81\80(\08\0B\00\06(\00/0\00\01\00\03!\F0\06\07\00 \00\04\9B\00R\04\14\00\00\00E\002\04|\01\18\000/\08\00#\00\10\0E\0C\00\12#\0C\00\00\01\00'\04\12\0C\00\17\11\0C\00!7\04 \05\F1\08\015\00\00\04\0A\08\00\03\00\00\00`\01(\00\03\19(\00\04\17\0C$\00u\06\00 \00\00\F0!\10\00u\05\00\1C\00\00\F0\11\10\009\04\00\18\10\009\03\00\14\10\009\02\00\10\10\009\01\00\08P\00\01\01\00\C1\F0\11\00\03\1B\FF\00\04\1C\0C\00P\98\05\82\00\00P\06\00\00\04\1E\84\01#K\00\01\00\B1\02\02\08\10\0A/\22\00\00\00\08\10\00#\00\08\08\00\13\10\08\00\13\18\08\00\13 \08\00\13(\08\00\130\08\00\228\08\84\00\13\00\08\00\13\08\08\00\13\10\08\00\13\18\08\00\13 \08\00\13(\08\00\130\08\00\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\A2\00\00\00\14,\00\00\00H\00\01\00\11\02h\01\0F\01\00\FF\B0@$v\01\FF\7F\04\B2\FF\00\8E\07\00\C4\0F\00\19y\00\01\00\10%\8B\02a\0E\00\19y\03\00\01\00\10!-\00B\0E\00$z\B5\04\90\03\02\8E\07\00\CA\1F\00\0C\10\00\C5^\00\00p`\F0\03\00\DA\0F\00M\9B\04\F0\0C\80\03\00\EA\0F\00\06{\04\00\00_\00\00\00\90 \00\00\22\0E\00\19x\05\FF\1F\1F\00\D3\14\01\00\00\E2\0F\00$r\02\FF\FF\00\80\00 \E2\0FP\00\10\FF0\00@pP\F4\03\10\00\81\B9z\04\00\00F\00\00#\05p\E2\0F\00\11r\05\05?\00\B2\FF8\8F\07\00\C6\0F\00\08s\04\F6\04\10\10\A0\00\F1\06\1E\00\10x\03\04\FE\FF\FF\0F\FF\E0\FF\07\00\CC\1F\00\05s\03$\04\00\C0\03\01\C0\00!r\07p\00\C0\03\0A\8E\07\00\C8\1F\00$z\07\07p\00\10\FF\D0\00\81\C8\0F\00'r\07\03\07\E0\02\02\90\00@\19x\02\FF\10\00\10\05\B0\00\80\E4\0F\00\12x\05\05\80\F1\04!\C0\8E\80\00T'r\07\07\02\C0\00\10\C8\D0\00\11\09`\00\10\07`\00`\E4\0F\00$x\05\BC\03$\00\05\10\000z\03\09p\00\22\02\02@\00@\19x\00\FF\A0\00\22\05\16`\00f\0Cz\00\03\00_P\010\10\0A\03\10\00\11\80\D0\00p\E4\0F\00\10\08\07\07P\00\04\10\00\060\00\12\F20\00\1A\18 \001\12\AA\07P\01 \FF3\B0\01\00\A0\00\1B\03\A0\001\00\07\10\D1\03\05\A0\00\15\03\A0\00\10\E2\F0\00 \02\05\00\01#\FF\C0@\00Sx\09\03\08\00 \00\11\CA\80\003\09\00X\80\00\22\C8\0F\10\02p\\\00\00p\10\F2\04\90\00T$\14\03\FF\04 \01\00`\000\1A\02\000\00\13\09p\01c%\16\02\02\00ZP\02q\CC\0F\00\81\19\02\02\D0\01\C4\19\1E\0C\00\A2\00\00\10x\06\00\08\D0\00\000\02\92t\04\FF\00\00\80\FF\FF\00\A0\001\1Cx\00\01\001p\F0\F00\02E$x\07\05p\00q\E2\0F\04\0Cz\00\06\90\00#`\F6 \00\16\06 \00\80\C6\0F\00\0Cx\00\05?@\00\92D\F6\01\00\E4\0F\04\10xF\07\02@\01B\1F\04\10x\EC\04\04\80\00@!\12\04\02\81\00\01\15\00!\E2O@\00\11\07@\00 \F2\04\D0\00c\88s\00\07\04\00\EE\05f\E8\0F\00\1D{\00\01\00u\EC\0F\00\84\B9\0B\06\CE\05\84(\0E\00\84\B9\02\06\000\00qb\0E\00\0B\B2\00\0B\F6\07`\80\F4\03\00\C4\1F\10\00\11\02\10\00\A3\C0\F8\03\00\E4/\00\1C\B8\00\F0\00%p\01\00\02\01\E0\00B\F4\03\00\C8\D0\00\11\1F\90\00\C3t\01\00\CE\0F\00\08\82\0B\0B\02\00\17\07\1B\E40\01\8F\C6\0F\00\88\B3\00\06\0B\B0\00\09f\A9\03\06\00\80\00\B0\00\1B\A9\B0\001\A2\00\03`\03\22\80\F6\B0\00H\A2\00\02\03\B0\00$\A8\00p\00\04\B0\00\15\00\90\01\03\B0\00\14\0F\80\01\01\B0\00/\03\03\B0\00\0AO\A3\00\06\03`\01\0AG\07\06\00@\B0\00\13\B9\A7\06\06`\01*\07\07`\01/\00\07`\01\05\10\DA\90\00\02/\00\01\90\00\12\CA0\01\1F\07\80\00\089M\19\00\00\05G$t\02\FF\90\02A\00\84y\05\9F\08\01@\00\96&\0E\00%v\02\09\00`p\03'\84y\B0\00fh\0E\00\81y\08\10\03bb\05\00\0Br\00\01\05\22\80\F0\C0\001r\00\00\10\00\92\C0\F2\03\00\D6/\00\08\82\1F\00p\00\00\80\04\00\C8O \00\11\08\11\00 @\F0\00\03$\0EF\E0\05\00\A0\00b\E6\0F\00\08r\09 \00\00\01\00p\CC\0F\00\A9s\09\02{\00\C0\09\E1\1E\00\00\A4\0E\00\0Cr\00\09\10\00 pR@\00QO\00$r\08@\04\10\09\D0\00\80\D8\0F\00G\09\00\00\90\10\05!\FF\83\F0\00*My\00\01TGy\00\00\F0 \00f\C0\0F\00\18y\00\01\00\0F\10\00p\0F\01\00-\11\01@\0A\0E\01\00\22@\00\01\00=\9E\01\000\00\08\01\00\1F\0B@\00\04\13\DE)\00?\09\02\00@\00\0A\22\13\00\A0\04\0C\01\00\13\E8U\00\03\7F\03\01$\00\13\05W\02\00\01\00\22\18\00\01\00.q\01T\00\00\01\00\11\90u\02O\00\00p\00\80\00\0B\1F)'\00\03\03\95\02$\00\00\18\0C\04\E4\00*\04\00\01\00\1Fc@\00\04*0\05\C0\00\13\03\03\08\0C@\00!\8F\01D\01\0D@\00\13\D8@\00*\D8\00\01\00\1B\08\08\00?~\01\00N\0D\002\00\00\B0\86\03\01W\08\04\80\00\17\048\00\04\18\00\138@\01\0C\84\01\13\C0@\00\17\881\01\0F\C0\00\01\132T\01\15\06R\00\03>\03\1A\08\98\0D\11\03$\00J\00\0E\80\00\01\00\13\97\94\00*\03\00\01\00\040\12/\00\02\80\00\0B\13\06\AB\01\04h\12\0C\01\00\1B\A8\08\00\04\97\00\13\018\00\04\E8\00\0C\01\00*\C0\08\08\00\088\00\18\06\A0\00\0F\01\00\05\03\B8\00\80\08\00\00\00\00\00\00\00\00\00\00\00\00"} {
    llvm.mlir.global internal @__wg_main_kColReduction_reduce__4_1_0___8w16h_1_0() {addr_space = 3 : i32} : !llvm.array<128 x f32>
    llvm.func @__nv_fabsf(f32) -> f32
    llvm.func @main_kColReduction_reduce__4_1_0___8w16h_1(%arg0: i32, %arg1: !llvm.ptr<f32>, %arg2: i32, %arg3: i32, %arg4: i32, %arg5: i32, %arg6: !llvm.ptr<f32>) attributes {disc.elimargs = [1 : index, 3 : index, 5 : index, 6 : index, 7 : index, 8 : index, 9 : index, 13 : index, 15 : index, 16 : index, 17 : index], gpu.kernel, nvvm.kernel} {
      %0 = llvm.mlir.constant(8 : index) : i32
      %1 = llvm.mlir.constant(16 : index) : i32
      %2 = llvm.mlir.constant(2 : index) : i32
      %3 = llvm.mlir.constant(0xFF800000 : f32) : f32
      %4 = llvm.mlir.constant(64 : index) : i32
      %5 = llvm.mlir.constant(4 : index) : i32
      %6 = llvm.mlir.constant(32 : index) : i32
      %7 = llvm.mlir.constant(128 : index) : i32
      %8 = llvm.mlir.constant(0 : index) : i32
      %9 = llvm.mlir.addressof @__wg_main_kColReduction_reduce__4_1_0___8w16h_1_0 : !llvm.ptr<array<128 x f32>, 3>
      %10 = llvm.getelementptr %9[0, 0] : (!llvm.ptr<array<128 x f32>, 3>) -> !llvm.ptr<f32, 3>
      %11 = nvvm.read.ptx.sreg.ctaid.x : i32
      %12 = nvvm.read.ptx.sreg.tid.x : i32
      llvm.br ^bb1
    ^bb1:  // pred: ^bb0
      %13 = llvm.mul %11, %arg3  : i32
      %14 = llvm.add %12, %13  : i32
      %15 = llvm.icmp "ult" %14, %arg4 : i32
      llvm.cond_br %15, ^bb2, ^bb16
    ^bb2:  // pred: ^bb1
      %16 = llvm.srem %14, %7  : i32
      %17 = llvm.sdiv %14, %7  : i32
      %18 = llvm.udiv %16, %0  : i32
      %19 = llvm.urem %16, %0  : i32
      %20 = llvm.udiv %17, %arg5  : i32
      %21 = llvm.urem %17, %arg5  : i32
      %22 = llvm.mul %20, %1  : i32
      %23 = llvm.add %22, %18  : i32
      %24 = llvm.mul %21, %0  : i32
      %25 = llvm.add %24, %19  : i32
      %26 = llvm.icmp "ult" %23, %arg2 : i32
      %27 = llvm.icmp "ult" %25, %arg0 : i32
      %28 = llvm.and %26, %27  : i1
      llvm.cond_br %28, ^bb3, ^bb4
    ^bb3:  // pred: ^bb2
      %29 = llvm.mul %23, %arg0  : i32
      %30 = llvm.add %29, %25  : i32
      %31 = llvm.getelementptr %arg1[%30] : (!llvm.ptr<f32>, i32) -> !llvm.ptr<f32>
      %32 = llvm.load %31 : !llvm.ptr<f32>
      %33 = llvm.call @__nv_fabsf(%32) : (f32) -> f32
      %34 = llvm.getelementptr %10[%16] : (!llvm.ptr<f32, 3>, i32) -> !llvm.ptr<f32, 3>
      llvm.store %33, %34 : !llvm.ptr<f32, 3>
      llvm.br ^bb5
    ^bb4:  // pred: ^bb2
      %35 = llvm.getelementptr %10[%16] : (!llvm.ptr<f32, 3>, i32) -> !llvm.ptr<f32, 3>
      llvm.store %3, %35 : !llvm.ptr<f32, 3>
      llvm.br ^bb5
    ^bb5:  // 2 preds: ^bb3, ^bb4
      nvvm.barrier0
      %36 = llvm.icmp "ult" %18, %0 : i32
      %37 = llvm.add %23, %0  : i32
      %38 = llvm.icmp "ult" %37, %arg2 : i32
      %39 = llvm.and %36, %38  : i1
      llvm.cond_br %39, ^bb6, ^bb7
    ^bb6:  // pred: ^bb5
      %40 = llvm.add %16, %4  : i32
      %41 = llvm.getelementptr %10[%16] : (!llvm.ptr<f32, 3>, i32) -> !llvm.ptr<f32, 3>
      %42 = llvm.load %41 : !llvm.ptr<f32, 3>
      %43 = llvm.getelementptr %10[%40] : (!llvm.ptr<f32, 3>, i32) -> !llvm.ptr<f32, 3>
      %44 = llvm.load %43 : !llvm.ptr<f32, 3>
      %45 = llvm.fcmp "ugt" %42, %44 : f32
      %46 = llvm.select %45, %42, %44 : i1, f32
      %47 = llvm.fcmp "uno" %44, %44 : f32
      %48 = llvm.select %47, %44, %46 : i1, f32
      %49 = llvm.getelementptr %10[%16] : (!llvm.ptr<f32, 3>, i32) -> !llvm.ptr<f32, 3>
      llvm.store %48, %49 : !llvm.ptr<f32, 3>
      llvm.br ^bb7
    ^bb7:  // 2 preds: ^bb5, ^bb6
      nvvm.barrier0
      %50 = llvm.icmp "ult" %18, %5 : i32
      %51 = llvm.add %23, %5  : i32
      %52 = llvm.icmp "ult" %51, %arg2 : i32
      %53 = llvm.and %50, %52  : i1
      llvm.cond_br %53, ^bb8, ^bb9
    ^bb8:  // pred: ^bb7
      %54 = llvm.add %16, %6  : i32
      %55 = llvm.getelementptr %10[%16] : (!llvm.ptr<f32, 3>, i32) -> !llvm.ptr<f32, 3>
      %56 = llvm.load %55 : !llvm.ptr<f32, 3>
      %57 = llvm.getelementptr %10[%54] : (!llvm.ptr<f32, 3>, i32) -> !llvm.ptr<f32, 3>
      %58 = llvm.load %57 : !llvm.ptr<f32, 3>
      %59 = llvm.fcmp "ugt" %56, %58 : f32
      %60 = llvm.select %59, %56, %58 : i1, f32
      %61 = llvm.fcmp "uno" %58, %58 : f32
      %62 = llvm.select %61, %58, %60 : i1, f32
      %63 = llvm.getelementptr %10[%16] : (!llvm.ptr<f32, 3>, i32) -> !llvm.ptr<f32, 3>
      llvm.store %62, %63 : !llvm.ptr<f32, 3>
      llvm.br ^bb9
    ^bb9:  // 2 preds: ^bb7, ^bb8
      nvvm.barrier0
      %64 = llvm.icmp "ult" %18, %2 : i32
      %65 = llvm.add %23, %2  : i32
      %66 = llvm.icmp "ult" %65, %arg2 : i32
      %67 = llvm.and %64, %66  : i1
      llvm.cond_br %67, ^bb10, ^bb11
    ^bb10:  // pred: ^bb9
      %68 = llvm.add %16, %1  : i32
      %69 = llvm.getelementptr %10[%16] : (!llvm.ptr<f32, 3>, i32) -> !llvm.ptr<f32, 3>
      %70 = llvm.load %69 : !llvm.ptr<f32, 3>
      %71 = llvm.getelementptr %10[%68] : (!llvm.ptr<f32, 3>, i32) -> !llvm.ptr<f32, 3>
      %72 = llvm.load %71 : !llvm.ptr<f32, 3>
      %73 = llvm.fcmp "ugt" %70, %72 : f32
      %74 = llvm.select %73, %70, %72 : i1, f32
      %75 = llvm.fcmp "uno" %72, %72 : f32
      %76 = llvm.select %75, %72, %74 : i1, f32
      %77 = llvm.getelementptr %10[%16] : (!llvm.ptr<f32, 3>, i32) -> !llvm.ptr<f32, 3>
      llvm.store %76, %77 : !llvm.ptr<f32, 3>
      llvm.br ^bb11
    ^bb11:  // 2 preds: ^bb9, ^bb10
      nvvm.barrier0
      %78 = llvm.icmp "eq" %18, %8 : i32
      %79 = llvm.and %78, %28  : i1
      llvm.cond_br %79, ^bb12, ^bb15
    ^bb12:  // pred: ^bb11
      %80 = llvm.add %16, %0  : i32
      %81 = llvm.getelementptr %10[%16] : (!llvm.ptr<f32, 3>, i32) -> !llvm.ptr<f32, 3>
      %82 = llvm.load %81 : !llvm.ptr<f32, 3>
      %83 = llvm.getelementptr %10[%80] : (!llvm.ptr<f32, 3>, i32) -> !llvm.ptr<f32, 3>
      %84 = llvm.load %83 : !llvm.ptr<f32, 3>
      %85 = llvm.fcmp "ugt" %82, %84 : f32
      %86 = llvm.select %85, %82, %84 : i1, f32
      %87 = llvm.fcmp "uno" %84, %84 : f32
      %88 = llvm.select %87, %84, %86 : i1, f32
      %89 = llvm.getelementptr %arg6[%25] : (!llvm.ptr<f32>, i32) -> !llvm.ptr<f32>
      %90 = llvm.load %89 : !llvm.ptr<f32>
      llvm.br ^bb13(%90 : f32)
    ^bb13(%91: f32):  // 2 preds: ^bb12, ^bb13
      %92 = llvm.fcmp "ogt" %91, %88 : f32
      %93 = llvm.select %92, %91, %88 : i1, f32
      %94 = llvm.bitcast %89 : !llvm.ptr<f32> to !llvm.ptr<i32>
      %95 = llvm.bitcast %91 : f32 to i32
      %96 = llvm.bitcast %93 : f32 to i32
      %97 = llvm.cmpxchg %94, %95, %96 acq_rel monotonic : !llvm.ptr<i32>, i32
      %98 = llvm.extractvalue %97[0] : !llvm.struct<(i32, i1)> 
      %99 = llvm.bitcast %98 : i32 to f32
      %100 = llvm.extractvalue %97[1] : !llvm.struct<(i32, i1)> 
      llvm.cond_br %100, ^bb14, ^bb13(%99 : f32)
    ^bb14:  // pred: ^bb13
      llvm.br ^bb15
    ^bb15:  // 2 preds: ^bb11, ^bb14
      llvm.br ^bb16
    ^bb16:  // 2 preds: ^bb1, ^bb15
      llvm.return
    }
  }
}


// -----// IR Dump After DiscToLLVMPass (disc-to-llvm) //----- //
module attributes {gpu.container_module} {
  llvm.mlir.global internal constant @main_kernel_0_main_kColReduction_reduce__4_1_0___8w32h_1_kernel_name("main_kColReduction_reduce__4_1_0___8w32h_1\00") {addr_space = 0 : i32}
  llvm.mlir.global internal constant @main_kernel_0_blob_gpu.binary("P\EDU\BA\01\00\10\00@\08\00\00\00\00\00\00\02\00\01\01@\00\00\00\00\08\00\00\00\00\00\00\F9\07\00\00\00\00\00\00\07\00\01\00P\00\00\00\00\00\00\00\00\00\00\00\11 \00\00\00\00\00\00\00\00\00\00\00\00\00\00(\14\00\00\00\00\00\00\A2\7FELF\02\01\013\07\00\01\00f\02\00\BE\00q\00\01\00#\80\13\08\00\11\10\07\00\F5\0E\00P\05P\00@\008\00\03\00@\00\0C\00\01\00\00.shstrtab\00.\08\00'ym\08\00\F0\1B_shndx\00.nv.info\00.text.main_kColReduction_r\0A\00\F5\03e__4_1_0___8w32h_1:\00\0F4\00\1Doshared6\00\1AOrela\A0\00\1F?rel\D5\00\22\9Fconstant09\00\1A\B2debug_frame{\00\09\11\00!nv\14\00\11aE\00\0F\9E\01 \0F\8A\00\17\0F\C9\01\F4\8F$____wg_3\00\17\00\0C\00/27\02\02'o_param\09\02\1C\0F\01\00\05\8C]\00\00\00\03\00\0A\00\01\00\11\C2\18\00,\0B\00\01\00 \9C\01\18\00,\09\00\01\00\11\DC\18\00,\04\00\01\00\11\FA\18\00,\07\00\01\00g2\00\00\00\12\10x\00\11\08\06\00v\00\FF\FF\FF\FF(\00\0C\00\00\01\00@\03\00\04|\08\00\D1\0F\0C\81\80\80(\00\08\FF\81\80(\08\0B\00\06(\00/0\00\01\00\03\13\F0{\00\10\04\9B\00R\04\14\00\00\00E\002\04\AC\01\18\00\80/\08\00\06\00\00\00\0E\0C\00\12#\0C\00\00\01\00'\04\12\0C\00\17\11\0C\00!7\04 \05\F1\08\015\00\00\04\0A\08\00\03\00\00\00`\01(\00\03\19(\00\04\17\0C$\00u\06\00 \00\00\F0!\10\00u\05\00\1C\00\00\F0\11\10\009\04\00\18\10\009\03\00\14\10\009\02\00\10\10\009\01\00\08P\00\01\01\00\F2\0A\F0\11\00\03\1B\FF\00\04\1C\0C\00P\00\00\00\10\06\00\00\10\07\00\00\04\1E\84\01#K\00\01\00v\02\02\08\10\0A/\22b\01\13\08\08\00\13\10\08\00\13\18\08\00\13 \08\00\13(\08\00\130\08\00\228\08\84\00\13\00\08\00\13\08\08\00\13\10\08\00\13\18\08\00\13 \08\00\13(\08\00\130\08\00\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\A2\00\00\00\14,\00\00\00H\00\01\00\11\02h\01\0F\01\00\FF\B0@$v\01\FF\7F\04\B2\FF\00\8E\07\00\C4\0F\00\19y\00\01\00\10%\8B\02a\0E\00\19y\03\00\01\00\10!-\00B\0E\00$z\B5\04\90\03\02\8E\07\00\CA\1F\00\0C\10\00\C5^\00\00p`\F0\03\00\DA\0F\00M\9B\04\F0\0C\80\03\00\EA\0F\00\06{\04\00\00_\00\00\00\90 \00\00\22\0E\00\19x\05\FF\1F\1F\00\D3\14\01\00\00\E2\0F\00$r\02\FF\FF\00\80\00 \E2\0FP\00\10\FF0\00@pP\F4\03\10\00\81\B9z\04\00\00F\00\00#\05p\E2\0F\00\11r\05\05?\00\B2\FF@\8F\07\00\C6\0F\00\08s\04\F6\04\00 \09\F1\07$\1E\00\10x\03\04\FE\FF\FF\0F\FF\E0\FF\07\00\CC\1F\00\05s\03$\04\00\C0\03\01\C0\00!r\07p\00\C0\03\0A\8E\07\00\C8\1F\00$z\07\07p\00\10\FF\D0\00\81\C8\0F\00'r\07\03\07\E0\02\02\90\00@\19x\02\FF\01\03\10\05\B0\00q\E4\0F\00\12x\05\05\F1\04!\C0\8E\80\00T'r\07\07\02\C0\00\10\C8\D0\00\11\09`\00\10\07`\00`\E4\0F\00$x\05\BC\03$\00\05\10\000z\03\09p\00\22\02\02@\00@\19x\00\FF\A0\00\22\05\16`\00f\0Cz\00\03\00_P\010\10\0A\03\10\00\11\80\D0\00p\E4\0F\00\10\08\07\07P\00\04\10\00\060\00\12\F20\00\1A\18 \001\12\AA\07P\01 \FF3\B0\01\00\A0\00\1B\03\A0\000\00\07  \01\15\02\A0\00\15\03\A0\00\10\E2\F0\00 \02\05\10\01#\FF\C0@\00Sx\09\03\08\00 \00\11\CA\80\003\09\00X\80\00\22\C8\0F\10\02p\\\00\00p\10\F2\04\90\00T$\14\03\FF\04 \01\00`\000\1A\02\000\00\13\09p\01c%\16\02\02\00ZP\02q\CC\0F\00\81\19\02\02\D0\01\C4\19\1E\0C\00\A2\00\00\10x\06\00\10\D0\00\000\02\92t\04\FF\00\00\80\FF\FF\00\A0\001\1Cx\00\01\001p\F0\F00\02E$x\07\05p\00q\E2\0F\04\0Cz\00\06\90\00#`\F6 \00\16\06 \00\80\C6\0F\00\0Cx\00\05\7F@\00\C5D\F6\01\00\E4\0F\04\10x\03\00\08p\00`\1F\00!\12\04\02q\00\01\F1\04!\E2O0\00\11\070\00 \F2\04\C0\00c\88s\00\07\04\00\DE\05f\E8\0F\00\1D{\00\01\00u\EC\0F\00\84\B9\0B\06~\05R\22\0E\00\10x\B0\02\01`\00Q\C4\0F\00\10x\1E\00\03p\00\000\004\02\06\00P\00qb\0E\00\0B\B2\00\0B\06\08\B1\80\F4\03\00\E4\1F\08\0B\B2\00\02\10\00\A3\C0\F8\03\00\E4/\00\1C\B8\00\00\01%p\01\10\02\01\F0\00B\F4\03\00\C8\E0\00\11?\B0\00\C3t\01\00\CE\0F\00\08\82\0B\0B\02\00'\07\1B\E4@\01\8F\C6\0F\00\88\B3\00\06\0B\D0\00\095\A9\03\06\8E\06[(\0E\00\84\A9\B0\001\A2\00\03p\03`\80\F6\03\00\C4\1F\10\00(\02\03\B0\00$\A8\00p\00\04\B0\00\15\04\A0\01\03\B0\00\14\1F\90\01\01\B0\00/\03\03\B0\00\0AO\A3\00\06\03\80\01\0AV\07\06\00\80\00\B0\00\0E`\01$\07\07`\01\00\B0\00O\B2\00\02\07`\01\0B\1C\00`\01\19\0F`\01\02\10\04\0F`\01\09\1F\07`\01\0D\17@\B0\00\13\A9g\07\0F`\01\07\1F\00`\01\06\12\DA@\01\14\00\90\00\1F\CA0\01\0F9M\19\00\C0\05G$t\02\FFP\03A\00\84y\05_\09\01@\00\96&\0E\00%v\02\09\00`0\04'\84y\B0\00fh\0E\00\81y\08\D0\03bb\05\00\0Br\00\C1\05\22\80\F0\C0\001r\00\00\10\00\92\C0\F2\03\00\D6/\00\08\82\1F\00p\00\00\80\04\00\C8O \00\11\08\11\00 @\F0\C0\03$\0EF\A0\06\00\A0\00b\E6\0F\00\08r\09 \00\00\01\00p\CC\0F\00\A9s\09\02{\00\C0\09\E1\1E\00\00\A4\0E\00\0Cr\00\09\10\00 pR@\00QO\00$r\08\00\05\10\09\D0\00\80\D8\0F\00G\09\00\00\90\D0\05!\FF\83\F0\00*My\00\01TGy\00\00\F0 \00f\C0\0F\00\18y\00\01\00\0F\10\00\B0\0F\01\00-#\01\00\80\02\0B\01\00\22@\00\01\00=\9E\01\000\00\08\01\00\1F\0B@\00\04\13\DE)\00?\09\02\00@\00\0A\22\13\00@\05\0C\01\00\13\E8U\00\03\0F\03\01$\00\13\05\97\02\00\01\00\22\18\00\01\00.q\01T\00\00\01\00\11\90\B5\02O\00\00p\00\80\00\0B\1F)'\00\03\03\D5\02$\00\00\18\0D\04\E4\00*\04\00\01\00\1Fc@\00\04*0\05\C0\00\13\03\03\09\0C@\00!\8F\01D\01\0D@\00\13\D8@\00*\D8\00\01\00\1B\08\08\00?~\01\00N\0E\002\00\00\B0\C6\03\01W\09\04\80\00\17\048\00\04\18\00\138@\01\0C\84\01\13\C0@\00\17\881\01\0F\C0\00\01\132T\01\02\E6\07\06\01\00\1B\80\A9\00\11\03$\00J\00\0E\80\00\01\00\13\97#\00*\03\00\01\00\040\13/\00\04\80\00\0B\13\06\AB\01\04h\13\0C\01\00\1B\A8\08\00\17\08\08\02\17\05\E8\00\0C\01\00*\C0\09\08\00\088\00\18\06\A0\00\0F\01\00\05\03\A9\00\80\08\00\00\00\00\00\00\00\00\00\00\00\00\00\00") {addr_space = 0 : i32}
  llvm.mlir.global internal constant @main_kernel_main_kColReduction_reduce__4_1_0___8w32h_kernel_name("main_kColReduction_reduce__4_1_0___8w32h\00") {addr_space = 0 : i32}
  llvm.mlir.global internal constant @main_kernel_blob_gpu.binary("P\EDU\BA\01\00\10\008\04\00\00\00\00\00\00\02\00\01\01@\00\00\00\F8\03\00\00\00\00\00\00\F8\03\00\00\00\00\00\00\07\00\01\00P\00\00\00\00\00\00\00\00\00\00\00\11 \00\00\00\00\00\00\00\00\00\00\00\00\00\00\E8\0B\00\00\00\00\00\00\A2\7FELF\02\01\013\07\00\01\00f\02\00\BE\00q\00\01\00!@\0B\07\001\00\80\08\07\00\F5\0E\00P\05P\00@\008\00\03\00@\00\0B\00\01\00\00.shstrtab\00.\08\00'ym\08\00\F0\1B_shndx\00.nv.info\00.text.main_kColReduction_r\0A\00\F5\01e__4_1_0___8w32h8\00\0F2\00\1Boshared4\00\1B\9Fconstant07\00\18\FA\01debug_frame\00.rel\11\00!nv\14\00\11aC\00\0F+\01 \0F\88\00\15\0FT\01\BAo_param[\01\1C\0F\01\00\06\8C[\00\00\00\03\00\0A\00\01\00\11\F0\18\00,\09\00\01\00 .\01\18\00,\04\00\01\00\11L\18\00,\07\00\01\00f2\00\00\00\12\10`\00!\80\01\07\00v\00\FF\FF\FF\FF(\00\0C\00\00\01\00@\03\00\04|\08\00\D1\0F\0C\81\80\80(\00\08\FF\81\80(\08\0B\00\06(\00/0\00\01\00\03\13pX\00\10\04\9B\00R\04\14\00\00\00E\00\01\0B\00\00\13\00p/\08\00\05\00\00\00\A7\03\22\04#\0C\00\00\01\00'\04\12\0C\00\17\11\0C\00!7\04\E8\03\F3\08\015\00\00\04\0A\08\00\02\00\00\00`\01\10\00\03\19\10\00\04\17\0C\0C\04U\08\00\00\F0!\10\00\10\01\18\01%\F0\11\10\00\01\01\00\F2\02\F0\11\00\03\1B\FF\00\04\1C\08\00P\00\00\00\B0\00\01\00#K\00\01\00s\02\02\08\10\0A/\22\9B\00\00\07\00\03\08\00\13\10\08\00\13\18\08\00\13 \08\00\13(\08\00\130\08\00\228\08|\00\13\00\08\00\13\08\08\00\13\10\08\00\13\18\08\00\13 \08\00\13(\08\00\130\08\00\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\A2\00\00\00\14,\00\00\00H\00\01\00\000\01/\05\00\01\00\FF\B0A\02z\01\00\1F\04\B1\0F\00\00\00\C4\0F\00\19y\02\00\01\00\10%\8B\02Q\0E\00\19y\03\0F\00\F5\1A\00!\00\00\00$\0E\00$z\02\02\00X\00\00\03\02\8E\07\00\CA\1F\00\0Cz\00\02\00Y\00\00p`\F0\03\00\DA\0F\00MS\04\A0\80\03\00\EA\0F\005t\03\FF\B3\03\10\FF\C0\03P\E2\0F\00\02x6\02B\80\FF\00\0F\10\00r\B9z\04\00\00F\00\84\00\94\D0\0F\00%v\02\02\00Z`\00`\0F\00\86y\00\022\00@\04\19\10\0C0\009My\00`\00PGy\00\00\F09\04\A6\FF\83\03\00\C0\0F\00\18y\00\01\00\0F\10\00\90\0F\01\00-\00W\01.\03\00\01\00\22@\00\01\00=+\01\000\00\08\01\00\1F\0B@\00\04\13k)\00\1F[@\00\0C\13\13\0C\04\0C\01\00\13\C8\15\00&\90\000\04#\04\00\85\04\00\F6\04\12\00\01\00\1F\FET\00\00\00\01\00\13X\95\00/p\00\80\00\0B\1F)'\00\03#\00\C8@\00\04P\06\04\E4\00*\04\00\01\00\1Fa@\00\04\13\F81\00&\\\00@\00\1F\0A@\00\00!\1C\01D\01\0D@\00\13X)\00*\D8\00\01\00\1B\08\08\00?\0B\01\00\86\07\00Q\00\000\05\00\01\00&\10\00\80\00\17\048\00\04\18\00\13\C7\14\01\0C\84\01*@\058\07\1F\00\C0\00\04\132@\00+\06\00\01\00\1A\07\D0\07\12\03\F0\05:\08\80\00\01\00\13\06\08\06\04(\0B\0C\01\00*\A8\00\08\00\04\F8\00\14\018\00/\05\00\01\00\029@\03\00\08\00\088\00/\06\00\01\00\17\80\08\00\00\00\00\00\00\00") {addr_space = 0 : i32}
  llvm.mlir.global internal constant @ral_send_output___cpu___pvoid_i64_m2df32___void("ral_send_output___cpu___pvoid_i64_m2df32___void\00") {addr_space = 0 : i32}
  llvm.mlir.global internal constant @dealloc___gpu___pvoid_pvoid___void("dealloc___gpu___pvoid_pvoid___void\00") {addr_space = 0 : i32}
  llvm.mlir.global internal constant @inc_ref___gpu___pvoid_pvoid_m1df32_m1di64___m2df32("inc_ref___gpu___pvoid_pvoid_m1df32_m1di64___m2df32\00") {addr_space = 0 : i32}
  llvm.mlir.global internal constant @main_kernel_2_main_kColReduction_reduce__4_1_0___8w16h_1_kernel_name("main_kColReduction_reduce__4_1_0___8w16h_1\00") {addr_space = 0 : i32}
  llvm.mlir.global internal constant @main_kernel_2_blob_gpu.binary("P\EDU\BA\01\00\10\00\10\08\00\00\00\00\00\00\02\00\01\01@\00\00\00\D0\07\00\00\00\00\00\00\CB\07\00\00\00\00\00\00\07\00\01\00P\00\00\00\00\00\00\00\00\00\00\00\11 \00\00\00\00\00\00\00\00\00\00\00\00\00\00(\13\00\00\00\00\00\00\A2\7FELF\02\01\013\07\00\01\00f\02\00\BE\00q\00\01\00#\80\12\08\00\11\0F\07\00\F5\0E\00P\05P\00@\008\00\03\00@\00\0C\00\01\00\00.shstrtab\00.\08\00'ym\08\00\F0\1B_shndx\00.nv.info\00.text.main_kColReduction_r\0A\00\F5\03e__4_1_0___8w16h_1:\00\0F4\00\1Doshared6\00\1AOrela\A0\00\1F?rel\D5\00\22\9Fconstant09\00\1A\B2debug_frame{\00\09\11\00!nv\14\00\11aE\00\0F\9E\01 \0F\8A\00\17\0F\C9\01\F4\8F$____wg_3\00\17\00\0C\00/27\02\02'o_param\09\02\1C\0F\01\00\05\8C]\00\00\00\03\00\0A\00\01\00\11\C2\18\00,\0B\00\01\00 \9C\01\18\00,\09\00\01\00\11\DC\18\00,\04\00\01\00\11\FA\18\00,\07\00\01\00g2\00\00\00\12\10x\00\03#\00f\FF\FF\FF\FF(\00\0C\00\00\01\00@\03\00\04|\08\00\D1\0F\0C\81\80\80(\00\08\FF\81\80(\08\0B\00\06(\00/0\00\01\00\03!\F0\06\07\00 \00\04\9B\00R\04\14\00\00\00E\002\04|\01\18\000/\08\00#\00\10\0E\0C\00\12#\0C\00\00\01\00'\04\12\0C\00\17\11\0C\00!7\04 \05\F1\08\015\00\00\04\0A\08\00\03\00\00\00`\01(\00\03\19(\00\04\17\0C$\00u\06\00 \00\00\F0!\10\00u\05\00\1C\00\00\F0\11\10\009\04\00\18\10\009\03\00\14\10\009\02\00\10\10\009\01\00\08P\00\01\01\00\C1\F0\11\00\03\1B\FF\00\04\1C\0C\00P\98\05\82\00\00P\06\00\00\04\1E\84\01#K\00\01\00\B1\02\02\08\10\0A/\22\00\00\00\08\10\00#\00\08\08\00\13\10\08\00\13\18\08\00\13 \08\00\13(\08\00\130\08\00\228\08\84\00\13\00\08\00\13\08\08\00\13\10\08\00\13\18\08\00\13 \08\00\13(\08\00\130\08\00\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\A2\00\00\00\14,\00\00\00H\00\01\00\11\02h\01\0F\01\00\FF\B0@$v\01\FF\7F\04\B2\FF\00\8E\07\00\C4\0F\00\19y\00\01\00\10%\8B\02a\0E\00\19y\03\00\01\00\10!-\00B\0E\00$z\B5\04\90\03\02\8E\07\00\CA\1F\00\0C\10\00\C5^\00\00p`\F0\03\00\DA\0F\00M\9B\04\F0\0C\80\03\00\EA\0F\00\06{\04\00\00_\00\00\00\90 \00\00\22\0E\00\19x\05\FF\1F\1F\00\D3\14\01\00\00\E2\0F\00$r\02\FF\FF\00\80\00 \E2\0FP\00\10\FF0\00@pP\F4\03\10\00\81\B9z\04\00\00F\00\00#\05p\E2\0F\00\11r\05\05?\00\B2\FF8\8F\07\00\C6\0F\00\08s\04\F6\04\10\10\A0\00\F1\06\1E\00\10x\03\04\FE\FF\FF\0F\FF\E0\FF\07\00\CC\1F\00\05s\03$\04\00\C0\03\01\C0\00!r\07p\00\C0\03\0A\8E\07\00\C8\1F\00$z\07\07p\00\10\FF\D0\00\81\C8\0F\00'r\07\03\07\E0\02\02\90\00@\19x\02\FF\10\00\10\05\B0\00\80\E4\0F\00\12x\05\05\80\F1\04!\C0\8E\80\00T'r\07\07\02\C0\00\10\C8\D0\00\11\09`\00\10\07`\00`\E4\0F\00$x\05\BC\03$\00\05\10\000z\03\09p\00\22\02\02@\00@\19x\00\FF\A0\00\22\05\16`\00f\0Cz\00\03\00_P\010\10\0A\03\10\00\11\80\D0\00p\E4\0F\00\10\08\07\07P\00\04\10\00\060\00\12\F20\00\1A\18 \001\12\AA\07P\01 \FF3\B0\01\00\A0\00\1B\03\A0\001\00\07\10\D1\03\05\A0\00\15\03\A0\00\10\E2\F0\00 \02\05\00\01#\FF\C0@\00Sx\09\03\08\00 \00\11\CA\80\003\09\00X\80\00\22\C8\0F\10\02p\\\00\00p\10\F2\04\90\00T$\14\03\FF\04 \01\00`\000\1A\02\000\00\13\09p\01c%\16\02\02\00ZP\02q\CC\0F\00\81\19\02\02\D0\01\C4\19\1E\0C\00\A2\00\00\10x\06\00\08\D0\00\000\02\92t\04\FF\00\00\80\FF\FF\00\A0\001\1Cx\00\01\001p\F0\F00\02E$x\07\05p\00q\E2\0F\04\0Cz\00\06\90\00#`\F6 \00\16\06 \00\80\C6\0F\00\0Cx\00\05?@\00\92D\F6\01\00\E4\0F\04\10xF\07\02@\01B\1F\04\10x\EC\04\04\80\00@!\12\04\02\81\00\01\15\00!\E2O@\00\11\07@\00 \F2\04\D0\00c\88s\00\07\04\00\EE\05f\E8\0F\00\1D{\00\01\00u\EC\0F\00\84\B9\0B\06\CE\05\84(\0E\00\84\B9\02\06\000\00qb\0E\00\0B\B2\00\0B\F6\07`\80\F4\03\00\C4\1F\10\00\11\02\10\00\A3\C0\F8\03\00\E4/\00\1C\B8\00\F0\00%p\01\00\02\01\E0\00B\F4\03\00\C8\D0\00\11\1F\90\00\C3t\01\00\CE\0F\00\08\82\0B\0B\02\00\17\07\1B\E40\01\8F\C6\0F\00\88\B3\00\06\0B\B0\00\09f\A9\03\06\00\80\00\B0\00\1B\A9\B0\001\A2\00\03`\03\22\80\F6\B0\00H\A2\00\02\03\B0\00$\A8\00p\00\04\B0\00\15\00\90\01\03\B0\00\14\0F\80\01\01\B0\00/\03\03\B0\00\0AO\A3\00\06\03`\01\0AG\07\06\00@\B0\00\13\B9\A7\06\06`\01*\07\07`\01/\00\07`\01\05\10\DA\90\00\02/\00\01\90\00\12\CA0\01\1F\07\80\00\089M\19\00\00\05G$t\02\FF\90\02A\00\84y\05\9F\08\01@\00\96&\0E\00%v\02\09\00`p\03'\84y\B0\00fh\0E\00\81y\08\10\03bb\05\00\0Br\00\01\05\22\80\F0\C0\001r\00\00\10\00\92\C0\F2\03\00\D6/\00\08\82\1F\00p\00\00\80\04\00\C8O \00\11\08\11\00 @\F0\00\03$\0EF\E0\05\00\A0\00b\E6\0F\00\08r\09 \00\00\01\00p\CC\0F\00\A9s\09\02{\00\C0\09\E1\1E\00\00\A4\0E\00\0Cr\00\09\10\00 pR@\00QO\00$r\08@\04\10\09\D0\00\80\D8\0F\00G\09\00\00\90\10\05!\FF\83\F0\00*My\00\01TGy\00\00\F0 \00f\C0\0F\00\18y\00\01\00\0F\10\00p\0F\01\00-\11\01@\0A\0E\01\00\22@\00\01\00=\9E\01\000\00\08\01\00\1F\0B@\00\04\13\DE)\00?\09\02\00@\00\0A\22\13\00\A0\04\0C\01\00\13\E8U\00\03\7F\03\01$\00\13\05W\02\00\01\00\22\18\00\01\00.q\01T\00\00\01\00\11\90u\02O\00\00p\00\80\00\0B\1F)'\00\03\03\95\02$\00\00\18\0C\04\E4\00*\04\00\01\00\1Fc@\00\04*0\05\C0\00\13\03\03\08\0C@\00!\8F\01D\01\0D@\00\13\D8@\00*\D8\00\01\00\1B\08\08\00?~\01\00N\0D\002\00\00\B0\86\03\01W\08\04\80\00\17\048\00\04\18\00\138@\01\0C\84\01\13\C0@\00\17\881\01\0F\C0\00\01\132T\01\15\06R\00\03>\03\1A\08\98\0D\11\03$\00J\00\0E\80\00\01\00\13\97\94\00*\03\00\01\00\040\12/\00\02\80\00\0B\13\06\AB\01\04h\12\0C\01\00\1B\A8\08\00\04\97\00\13\018\00\04\E8\00\0C\01\00*\C0\08\08\00\088\00\18\06\A0\00\0F\01\00\05\03\B8\00\80\08\00\00\00\00\00\00\00\00\00\00\00\00") {addr_space = 0 : i32}
  llvm.mlir.global internal constant @ral_kernel_launch___gpu___pvoid_ppvoid_i64_pvoid_i64_i64_i64_i64_i64_i64_i32_pvoid_i32_ppvoid___void("ral_kernel_launch___gpu___pvoid_ppvoid_i64_pvoid_i64_i64_i64_i64_i64_i64_i32_pvoid_i32_ppvoid___void\00") {addr_space = 0 : i32}
  llvm.mlir.global internal constant @main_kernel_1_main_kColReduction_reduce__4_1_0___8w16h_kernel_name("main_kColReduction_reduce__4_1_0___8w16h\00") {addr_space = 0 : i32}
  llvm.mlir.global internal constant @main_kernel_1_blob_gpu.binary("P\EDU\BA\01\00\10\008\04\00\00\00\00\00\00\02\00\01\01@\00\00\00\F8\03\00\00\00\00\00\00\F8\03\00\00\00\00\00\00\07\00\01\00P\00\00\00\00\00\00\00\00\00\00\00\11 \00\00\00\00\00\00\00\00\00\00\00\00\00\00\E8\0B\00\00\00\00\00\00\A2\7FELF\02\01\013\07\00\01\00f\02\00\BE\00q\00\01\00!@\0B\07\001\00\80\08\07\00\F5\0E\00P\05P\00@\008\00\03\00@\00\0B\00\01\00\00.shstrtab\00.\08\00'ym\08\00\F0\1B_shndx\00.nv.info\00.text.main_kColReduction_r\0A\00\F5\01e__4_1_0___8w16h8\00\0F2\00\1Boshared4\00\1B\9Fconstant07\00\18\FA\01debug_frame\00.rel\11\00!nv\14\00\11aC\00\0F+\01 \0F\88\00\15\0FT\01\BAo_param[\01\1C\0F\01\00\06\8C[\00\00\00\03\00\0A\00\01\00\11\F0\18\00,\09\00\01\00 .\01\18\00,\04\00\01\00\11L\18\00,\07\00\01\00f2\00\00\00\12\10`\00!\80\01\07\00v\00\FF\FF\FF\FF(\00\0C\00\00\01\00@\03\00\04|\08\00\D1\0F\0C\81\80\80(\00\08\FF\81\80(\08\0B\00\06(\00/0\00\01\00\03\13pX\00\10\04\9B\00R\04\14\00\00\00E\00\01\0B\00\00\13\00p/\08\00\05\00\00\00\A7\03\22\04#\0C\00\00\01\00'\04\12\0C\00\17\11\0C\00!7\04\E8\03\F3\08\015\00\00\04\0A\08\00\02\00\00\00`\01\10\00\03\19\10\00\04\17\0C\0C\04U\08\00\00\F0!\10\00\10\01\18\01%\F0\11\10\00\01\01\00\F2\02\F0\11\00\03\1B\FF\00\04\1C\08\00P\00\00\00\B0\00\01\00#K\00\01\00s\02\02\08\10\0A/\22\9B\00\00\07\00\03\08\00\13\10\08\00\13\18\08\00\13 \08\00\13(\08\00\130\08\00\228\08|\00\13\00\08\00\13\08\08\00\13\10\08\00\13\18\08\00\13 \08\00\13(\08\00\130\08\00\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\A2\00\00\00\14,\00\00\00H\00\01\00\000\01/\05\00\01\00\FF\B0A\02z\01\00\1F\04\B1\0F\00\00\00\C4\0F\00\19y\02\00\01\00\10%\8B\02Q\0E\00\19y\03\0F\00\F5\1A\00!\00\00\00$\0E\00$z\02\02\00X\00\00\03\02\8E\07\00\CA\1F\00\0Cz\00\02\00Y\00\00p`\F0\03\00\DA\0F\00MS\04\A0\80\03\00\EA\0F\005t\03\FF\B3\03\10\FF\C0\03P\E2\0F\00\02x6\02B\80\FF\00\0F\10\00r\B9z\04\00\00F\00\84\00\94\D0\0F\00%v\02\02\00Z`\00`\0F\00\86y\00\022\00@\04\19\10\0C0\009My\00`\00PGy\00\00\F09\04\A6\FF\83\03\00\C0\0F\00\18y\00\01\00\0F\10\00\90\0F\01\00-\00W\01.\03\00\01\00\22@\00\01\00=+\01\000\00\08\01\00\1F\0B@\00\04\13k)\00\1F[@\00\0C\13\13\0C\04\0C\01\00\13\C8\15\00&\90\000\04#\04\00\85\04\00\F6\04\12\00\01\00\1F\FET\00\00\00\01\00\13X\95\00/p\00\80\00\0B\1F)'\00\03#\00\C8@\00\04P\06\04\E4\00*\04\00\01\00\1Fa@\00\04\13\F81\00&\\\00@\00\1F\0A@\00\00!\1C\01D\01\0D@\00\13X)\00*\D8\00\01\00\1B\08\08\00?\0B\01\00\86\07\00Q\00\000\05\00\01\00&\10\00\80\00\17\048\00\04\18\00\13\C7\14\01\0C\84\01*@\058\07\1F\00\C0\00\04\132@\00+\06\00\01\00\1A\07\D0\07\12\03\F0\05:\08\80\00\01\00\13\06\08\06\04(\0B\0C\01\00*\A8\00\08\00\04\F8\00\14\018\00/\05\00\01\00\029@\03\00\08\00\088\00/\06\00\01\00\17\80\08\00\00\00\00\00\00\00") {addr_space = 0 : i32}
  llvm.mlir.global internal constant @alloc___gpu___pvoid_i64___pvoid("alloc___gpu___pvoid_i64___pvoid\00") {addr_space = 0 : i32}
  llvm.mlir.global internal constant @ral_recv_input___cpu___pvoid_i64___m3df32("ral_recv_input___cpu___pvoid_i64___m3df32\00") {addr_space = 0 : i32}
  llvm.func @disc_ral_call(!llvm.ptr<i8>, !llvm.ptr<i8>, !llvm.ptr<ptr<i8>>)
  llvm.func @main(%arg0: !llvm.ptr<i8>) attributes {tf.entry_function = {input_placements = "gpu", inputs = "input0", output_placements = "gpu", outputs = "output0"}} {
    %0 = llvm.mlir.constant(-1 : index) : i64
    %1 = llvm.mlir.constant(128 : index) : i64
    %2 = llvm.mlir.constant(16 : index) : i64
    %3 = llvm.mlir.constant(32 : index) : i64
    %4 = llvm.mlir.constant(8 : index) : i64
    %5 = llvm.mlir.constant(0 : i32) : i32
    %6 = llvm.mlir.constant(0 : index) : i64
    %7 = llvm.mlir.constant(1 : index) : i64
    %8 = llvm.mlir.constant(2 : index) : i64
    %9 = llvm.mlir.constant(256 : index) : i64
    %10 = llvm.mlir.constant(108 : index) : i64
    %11 = llvm.mlir.constant(0 : i32) : i32
    %12 = llvm.mlir.constant(1 : i32) : i32
    %13 = llvm.alloca %12 x !llvm.struct<"", (ptr<i8>, i64, struct<(ptr<f32>, ptr<f32>, i64, array<3 x i64>, array<3 x i64>)>)> : (i32) -> !llvm.ptr<struct<"", (ptr<i8>, i64, struct<(ptr<f32>, ptr<f32>, i64, array<3 x i64>, array<3 x i64>)>)>>
    %14 = llvm.mlir.constant(3 : i32) : i32
    %15 = llvm.alloca %14 x !llvm.ptr<i8> : (i32) -> !llvm.ptr<ptr<i8>>
    %16 = llvm.mlir.constant(0 : i32) : i32
    %17 = llvm.getelementptr %13[%11, 0] : (!llvm.ptr<struct<"", (ptr<i8>, i64, struct<(ptr<f32>, ptr<f32>, i64, array<3 x i64>, array<3 x i64>)>)>>, i32) -> !llvm.ptr<ptr<i8>>
    llvm.store %arg0, %17 : !llvm.ptr<ptr<i8>>
    %18 = llvm.getelementptr %15[%16] : (!llvm.ptr<ptr<i8>>, i32) -> !llvm.ptr<ptr<i8>>
    %19 = llvm.bitcast %17 : !llvm.ptr<ptr<i8>> to !llvm.ptr<i8>
    llvm.store %19, %18 : !llvm.ptr<ptr<i8>>
    %20 = llvm.mlir.constant(1 : i32) : i32
    %21 = llvm.getelementptr %13[%11, 1] : (!llvm.ptr<struct<"", (ptr<i8>, i64, struct<(ptr<f32>, ptr<f32>, i64, array<3 x i64>, array<3 x i64>)>)>>, i32) -> !llvm.ptr<i64>
    llvm.store %6, %21 : !llvm.ptr<i64>
    %22 = llvm.getelementptr %15[%20] : (!llvm.ptr<ptr<i8>>, i32) -> !llvm.ptr<ptr<i8>>
    %23 = llvm.bitcast %21 : !llvm.ptr<i64> to !llvm.ptr<i8>
    llvm.store %23, %22 : !llvm.ptr<ptr<i8>>
    %24 = llvm.mlir.constant(2 : i32) : i32
    %25 = llvm.getelementptr %13[%11, 2] : (!llvm.ptr<struct<"", (ptr<i8>, i64, struct<(ptr<f32>, ptr<f32>, i64, array<3 x i64>, array<3 x i64>)>)>>, i32) -> !llvm.ptr<struct<(ptr<f32>, ptr<f32>, i64, array<3 x i64>, array<3 x i64>)>>
    %26 = llvm.getelementptr %15[%24] : (!llvm.ptr<ptr<i8>>, i32) -> !llvm.ptr<ptr<i8>>
    %27 = llvm.bitcast %25 : !llvm.ptr<struct<(ptr<f32>, ptr<f32>, i64, array<3 x i64>, array<3 x i64>)>> to !llvm.ptr<i8>
    llvm.store %27, %26 : !llvm.ptr<ptr<i8>>
    %28 = llvm.mlir.addressof @ral_recv_input___cpu___pvoid_i64___m3df32 : !llvm.ptr<array<42 x i8>>
    %29 = llvm.mlir.constant(0 : index) : i64
    %30 = llvm.getelementptr %28[%29, %29] : (!llvm.ptr<array<42 x i8>>, i64, i64) -> !llvm.ptr<i8>
    llvm.call @disc_ral_call(%arg0, %30, %15) : (!llvm.ptr<i8>, !llvm.ptr<i8>, !llvm.ptr<ptr<i8>>) -> ()
    %31 = llvm.load %25 : !llvm.ptr<struct<(ptr<f32>, ptr<f32>, i64, array<3 x i64>, array<3 x i64>)>>
    %32 = llvm.extractvalue %31[3, 2] : !llvm.struct<(ptr<f32>, ptr<f32>, i64, array<3 x i64>, array<3 x i64>)> 
    %33 = llvm.extractvalue %31[3, 1] : !llvm.struct<(ptr<f32>, ptr<f32>, i64, array<3 x i64>, array<3 x i64>)> 
    %34 = llvm.extractvalue %31[3, 0] : !llvm.struct<(ptr<f32>, ptr<f32>, i64, array<3 x i64>, array<3 x i64>)> 
    %35 = llvm.trunc %33 : i64 to i32
    %36 = llvm.trunc %32 : i64 to i32
    %37 = llvm.mul %35, %36  : i32
    %38 = llvm.sext %37 : i32 to i64
    %39 = llvm.mlir.constant(1 : index) : i64
    %40 = llvm.mlir.null : !llvm.ptr<f32>
    %41 = llvm.getelementptr %40[%38] : (!llvm.ptr<f32>, i64) -> !llvm.ptr<f32>
    %42 = llvm.ptrtoint %41 : !llvm.ptr<f32> to i64
    %43 = llvm.mlir.constant(0 : i32) : i32
    %44 = llvm.mlir.constant(1 : i32) : i32
    %45 = llvm.alloca %44 x !llvm.struct<".1", (ptr<i8>, i64, ptr<i8>)> : (i32) -> !llvm.ptr<struct<".1", (ptr<i8>, i64, ptr<i8>)>>
    %46 = llvm.mlir.constant(3 : i32) : i32
    %47 = llvm.alloca %46 x !llvm.ptr<i8> : (i32) -> !llvm.ptr<ptr<i8>>
    %48 = llvm.mlir.constant(0 : i32) : i32
    %49 = llvm.getelementptr %45[%43, 0] : (!llvm.ptr<struct<".1", (ptr<i8>, i64, ptr<i8>)>>, i32) -> !llvm.ptr<ptr<i8>>
    llvm.store %arg0, %49 : !llvm.ptr<ptr<i8>>
    %50 = llvm.getelementptr %47[%48] : (!llvm.ptr<ptr<i8>>, i32) -> !llvm.ptr<ptr<i8>>
    %51 = llvm.bitcast %49 : !llvm.ptr<ptr<i8>> to !llvm.ptr<i8>
    llvm.store %51, %50 : !llvm.ptr<ptr<i8>>
    %52 = llvm.mlir.constant(1 : i32) : i32
    %53 = llvm.getelementptr %45[%43, 1] : (!llvm.ptr<struct<".1", (ptr<i8>, i64, ptr<i8>)>>, i32) -> !llvm.ptr<i64>
    llvm.store %42, %53 : !llvm.ptr<i64>
    %54 = llvm.getelementptr %47[%52] : (!llvm.ptr<ptr<i8>>, i32) -> !llvm.ptr<ptr<i8>>
    %55 = llvm.bitcast %53 : !llvm.ptr<i64> to !llvm.ptr<i8>
    llvm.store %55, %54 : !llvm.ptr<ptr<i8>>
    %56 = llvm.mlir.constant(2 : i32) : i32
    %57 = llvm.getelementptr %45[%43, 2] : (!llvm.ptr<struct<".1", (ptr<i8>, i64, ptr<i8>)>>, i32) -> !llvm.ptr<ptr<i8>>
    %58 = llvm.getelementptr %47[%56] : (!llvm.ptr<ptr<i8>>, i32) -> !llvm.ptr<ptr<i8>>
    %59 = llvm.bitcast %57 : !llvm.ptr<ptr<i8>> to !llvm.ptr<i8>
    llvm.store %59, %58 : !llvm.ptr<ptr<i8>>
    %60 = llvm.mlir.addressof @alloc___gpu___pvoid_i64___pvoid : !llvm.ptr<array<32 x i8>>
    %61 = llvm.mlir.constant(0 : index) : i64
    %62 = llvm.getelementptr %60[%61, %61] : (!llvm.ptr<array<32 x i8>>, i64, i64) -> !llvm.ptr<i8>
    llvm.call @disc_ral_call(%arg0, %62, %47) : (!llvm.ptr<i8>, !llvm.ptr<i8>, !llvm.ptr<ptr<i8>>) -> ()
    %63 = llvm.load %57 : !llvm.ptr<ptr<i8>>
    %64 = llvm.mlir.undef : !llvm.struct<(ptr<f32>, ptr<f32>, i64, array<1 x i64>, array<1 x i64>)>
    %65 = llvm.bitcast %63 : !llvm.ptr<i8> to !llvm.ptr<f32>
    %66 = llvm.insertvalue %65, %64[0] : !llvm.struct<(ptr<f32>, ptr<f32>, i64, array<1 x i64>, array<1 x i64>)> 
    %67 = llvm.insertvalue %65, %66[1] : !llvm.struct<(ptr<f32>, ptr<f32>, i64, array<1 x i64>, array<1 x i64>)> 
    %68 = llvm.mlir.constant(0 : index) : i64
    %69 = llvm.insertvalue %68, %67[2] : !llvm.struct<(ptr<f32>, ptr<f32>, i64, array<1 x i64>, array<1 x i64>)> 
    %70 = llvm.mlir.constant(1 : index) : i64
    %71 = llvm.insertvalue %38, %69[3, 0] : !llvm.struct<(ptr<f32>, ptr<f32>, i64, array<1 x i64>, array<1 x i64>)> 
    %72 = llvm.insertvalue %70, %71[4, 0] : !llvm.struct<(ptr<f32>, ptr<f32>, i64, array<1 x i64>, array<1 x i64>)> 
    %73 = llvm.mul %34, %38  : i64
    %74 = llvm.add %73, %0  : i64
    %75 = llvm.sdiv %74, %9  : i64
    %76 = llvm.add %75, %7  : i64
    %77 = llvm.sub %6, %73  : i64
    %78 = llvm.sdiv %77, %9  : i64
    %79 = llvm.sub %6, %78  : i64
    %80 = llvm.icmp "sgt" %73, %6 : i64
    %81 = llvm.select %80, %76, %79 : i1, i64
    %82 = llvm.icmp "sgt" %81, %10 : i64
    llvm.cond_br %82, ^bb1, ^bb2
  ^bb1:  // pred: ^bb0
    %83 = llvm.icmp "sle" %38, %6 : i64
    %84 = llvm.sub %6, %38  : i64
    %85 = llvm.sub %38, %7  : i64
    %86 = llvm.select %83, %84, %85 : i1, i64
    %87 = llvm.sdiv %86, %9  : i64
    %88 = llvm.sub %6, %87  : i64
    %89 = llvm.add %87, %7  : i64
    %90 = llvm.select %83, %88, %89 : i1, i64
    %91 = llvm.mlir.addressof @main_kernel_blob_gpu.binary : !llvm.ptr<array<1096 x i8>>
    %92 = llvm.mlir.constant(0 : index) : i64
    %93 = llvm.getelementptr %91[%92, %92] : (!llvm.ptr<array<1096 x i8>>, i64, i64) -> !llvm.ptr<i8>
    %94 = llvm.mlir.constant(1 : i32) : i32
    %95 = llvm.alloca %94 x !llvm.ptr<i8> : (i32) -> !llvm.ptr<ptr<i8>>
    %96 = llvm.mlir.constant(0 : i32) : i32
    %97 = llvm.getelementptr %95[%96] : (!llvm.ptr<ptr<i8>>, i32) -> !llvm.ptr<ptr<i8>>
    llvm.store %93, %97 : !llvm.ptr<ptr<i8>>
    %98 = llvm.mlir.constant(1 : i64) : i64
    %99 = llvm.mlir.addressof @main_kernel_main_kColReduction_reduce__4_1_0___8w32h_kernel_name : !llvm.ptr<array<41 x i8>>
    %100 = llvm.mlir.constant(0 : index) : i64
    %101 = llvm.getelementptr %99[%100, %100] : (!llvm.ptr<array<41 x i8>>, i64, i64) -> !llvm.ptr<i8>
    %102 = llvm.extractvalue %72[0] : !llvm.struct<(ptr<f32>, ptr<f32>, i64, array<1 x i64>, array<1 x i64>)> 
    %103 = llvm.extractvalue %72[1] : !llvm.struct<(ptr<f32>, ptr<f32>, i64, array<1 x i64>, array<1 x i64>)> 
    %104 = llvm.extractvalue %72[2] : !llvm.struct<(ptr<f32>, ptr<f32>, i64, array<1 x i64>, array<1 x i64>)> 
    %105 = llvm.extractvalue %72[3, 0] : !llvm.struct<(ptr<f32>, ptr<f32>, i64, array<1 x i64>, array<1 x i64>)> 
    %106 = llvm.extractvalue %72[4, 0] : !llvm.struct<(ptr<f32>, ptr<f32>, i64, array<1 x i64>, array<1 x i64>)> 
    %107 = llvm.mlir.constant(1 : i32) : i32
    %108 = llvm.alloca %107 x !llvm.struct<".9", (i64, i64, ptr<f32>)> : (i32) -> !llvm.ptr<struct<".9", (i64, i64, ptr<f32>)>>
    %109 = llvm.mlir.constant(3 : i32) : i32
    %110 = llvm.alloca %109 x !llvm.ptr<i8> : (i32) -> !llvm.ptr<ptr<i8>>
    %111 = llvm.mlir.constant(0 : i32) : i32
    %112 = llvm.mlir.constant(0 : i32) : i32
    %113 = llvm.getelementptr %108[%111, 0] : (!llvm.ptr<struct<".9", (i64, i64, ptr<f32>)>>, i32) -> !llvm.ptr<i64>
    llvm.store %9, %113 : !llvm.ptr<i64>
    %114 = llvm.getelementptr %110[%112] : (!llvm.ptr<ptr<i8>>, i32) -> !llvm.ptr<ptr<i8>>
    %115 = llvm.bitcast %113 : !llvm.ptr<i64> to !llvm.ptr<i8>
    llvm.store %115, %114 : !llvm.ptr<ptr<i8>>
    %116 = llvm.mlir.constant(1 : i32) : i32
    %117 = llvm.getelementptr %108[%111, 1] : (!llvm.ptr<struct<".9", (i64, i64, ptr<f32>)>>, i32) -> !llvm.ptr<i64>
    llvm.store %38, %117 : !llvm.ptr<i64>
    %118 = llvm.getelementptr %110[%116] : (!llvm.ptr<ptr<i8>>, i32) -> !llvm.ptr<ptr<i8>>
    %119 = llvm.bitcast %117 : !llvm.ptr<i64> to !llvm.ptr<i8>
    llvm.store %119, %118 : !llvm.ptr<ptr<i8>>
    %120 = llvm.mlir.constant(2 : i32) : i32
    %121 = llvm.getelementptr %108[%111, 2] : (!llvm.ptr<struct<".9", (i64, i64, ptr<f32>)>>, i32) -> !llvm.ptr<ptr<f32>>
    llvm.store %103, %121 : !llvm.ptr<ptr<f32>>
    %122 = llvm.getelementptr %110[%120] : (!llvm.ptr<ptr<i8>>, i32) -> !llvm.ptr<ptr<i8>>
    %123 = llvm.bitcast %121 : !llvm.ptr<ptr<f32>> to !llvm.ptr<i8>
    llvm.store %123, %122 : !llvm.ptr<ptr<i8>>
    %124 = llvm.mlir.constant(0 : i32) : i32
    %125 = llvm.mlir.constant(3 : i32) : i32
    %126 = llvm.inttoptr %124 : i32 to !llvm.ptr<i8>
    %127 = llvm.mlir.constant(0 : i32) : i32
    %128 = llvm.mlir.constant(1 : i32) : i32
    %129 = llvm.alloca %128 x !llvm.struct<".10", (ptr<i8>, ptr<ptr<i8>>, i64, ptr<i8>, i64, i64, i64, i64, i64, i64, i32, ptr<i8>, i32, ptr<ptr<i8>>)> : (i32) -> !llvm.ptr<struct<".10", (ptr<i8>, ptr<ptr<i8>>, i64, ptr<i8>, i64, i64, i64, i64, i64, i64, i32, ptr<i8>, i32, ptr<ptr<i8>>)>>
    %130 = llvm.mlir.constant(14 : i32) : i32
    %131 = llvm.alloca %130 x !llvm.ptr<i8> : (i32) -> !llvm.ptr<ptr<i8>>
    %132 = llvm.mlir.constant(0 : i32) : i32
    %133 = llvm.getelementptr %129[%127, 0] : (!llvm.ptr<struct<".10", (ptr<i8>, ptr<ptr<i8>>, i64, ptr<i8>, i64, i64, i64, i64, i64, i64, i32, ptr<i8>, i32, ptr<ptr<i8>>)>>, i32) -> !llvm.ptr<ptr<i8>>
    llvm.store %arg0, %133 : !llvm.ptr<ptr<i8>>
    %134 = llvm.getelementptr %131[%132] : (!llvm.ptr<ptr<i8>>, i32) -> !llvm.ptr<ptr<i8>>
    %135 = llvm.bitcast %133 : !llvm.ptr<ptr<i8>> to !llvm.ptr<i8>
    llvm.store %135, %134 : !llvm.ptr<ptr<i8>>
    %136 = llvm.mlir.constant(1 : i32) : i32
    %137 = llvm.getelementptr %129[%127, 1] : (!llvm.ptr<struct<".10", (ptr<i8>, ptr<ptr<i8>>, i64, ptr<i8>, i64, i64, i64, i64, i64, i64, i32, ptr<i8>, i32, ptr<ptr<i8>>)>>, i32) -> !llvm.ptr<ptr<ptr<i8>>>
    llvm.store %95, %137 : !llvm.ptr<ptr<ptr<i8>>>
    %138 = llvm.getelementptr %131[%136] : (!llvm.ptr<ptr<i8>>, i32) -> !llvm.ptr<ptr<i8>>
    %139 = llvm.bitcast %137 : !llvm.ptr<ptr<ptr<i8>>> to !llvm.ptr<i8>
    llvm.store %139, %138 : !llvm.ptr<ptr<i8>>
    %140 = llvm.mlir.constant(2 : i32) : i32
    %141 = llvm.getelementptr %129[%127, 2] : (!llvm.ptr<struct<".10", (ptr<i8>, ptr<ptr<i8>>, i64, ptr<i8>, i64, i64, i64, i64, i64, i64, i32, ptr<i8>, i32, ptr<ptr<i8>>)>>, i32) -> !llvm.ptr<i64>
    llvm.store %98, %141 : !llvm.ptr<i64>
    %142 = llvm.getelementptr %131[%140] : (!llvm.ptr<ptr<i8>>, i32) -> !llvm.ptr<ptr<i8>>
    %143 = llvm.bitcast %141 : !llvm.ptr<i64> to !llvm.ptr<i8>
    llvm.store %143, %142 : !llvm.ptr<ptr<i8>>
    %144 = llvm.mlir.constant(3 : i32) : i32
    %145 = llvm.getelementptr %129[%127, 3] : (!llvm.ptr<struct<".10", (ptr<i8>, ptr<ptr<i8>>, i64, ptr<i8>, i64, i64, i64, i64, i64, i64, i32, ptr<i8>, i32, ptr<ptr<i8>>)>>, i32) -> !llvm.ptr<ptr<i8>>
    llvm.store %101, %145 : !llvm.ptr<ptr<i8>>
    %146 = llvm.getelementptr %131[%144] : (!llvm.ptr<ptr<i8>>, i32) -> !llvm.ptr<ptr<i8>>
    %147 = llvm.bitcast %145 : !llvm.ptr<ptr<i8>> to !llvm.ptr<i8>
    llvm.store %147, %146 : !llvm.ptr<ptr<i8>>
    %148 = llvm.mlir.constant(4 : i32) : i32
    %149 = llvm.getelementptr %129[%127, 4] : (!llvm.ptr<struct<".10", (ptr<i8>, ptr<ptr<i8>>, i64, ptr<i8>, i64, i64, i64, i64, i64, i64, i32, ptr<i8>, i32, ptr<ptr<i8>>)>>, i32) -> !llvm.ptr<i64>
    llvm.store %90, %149 : !llvm.ptr<i64>
    %150 = llvm.getelementptr %131[%148] : (!llvm.ptr<ptr<i8>>, i32) -> !llvm.ptr<ptr<i8>>
    %151 = llvm.bitcast %149 : !llvm.ptr<i64> to !llvm.ptr<i8>
    llvm.store %151, %150 : !llvm.ptr<ptr<i8>>
    %152 = llvm.mlir.constant(5 : i32) : i32
    %153 = llvm.getelementptr %129[%127, 5] : (!llvm.ptr<struct<".10", (ptr<i8>, ptr<ptr<i8>>, i64, ptr<i8>, i64, i64, i64, i64, i64, i64, i32, ptr<i8>, i32, ptr<ptr<i8>>)>>, i32) -> !llvm.ptr<i64>
    llvm.store %7, %153 : !llvm.ptr<i64>
    %154 = llvm.getelementptr %131[%152] : (!llvm.ptr<ptr<i8>>, i32) -> !llvm.ptr<ptr<i8>>
    %155 = llvm.bitcast %153 : !llvm.ptr<i64> to !llvm.ptr<i8>
    llvm.store %155, %154 : !llvm.ptr<ptr<i8>>
    %156 = llvm.mlir.constant(6 : i32) : i32
    %157 = llvm.getelementptr %129[%127, 6] : (!llvm.ptr<struct<".10", (ptr<i8>, ptr<ptr<i8>>, i64, ptr<i8>, i64, i64, i64, i64, i64, i64, i32, ptr<i8>, i32, ptr<ptr<i8>>)>>, i32) -> !llvm.ptr<i64>
    llvm.store %7, %157 : !llvm.ptr<i64>
    %158 = llvm.getelementptr %131[%156] : (!llvm.ptr<ptr<i8>>, i32) -> !llvm.ptr<ptr<i8>>
    %159 = llvm.bitcast %157 : !llvm.ptr<i64> to !llvm.ptr<i8>
    llvm.store %159, %158 : !llvm.ptr<ptr<i8>>
    %160 = llvm.mlir.constant(7 : i32) : i32
    %161 = llvm.getelementptr %129[%127, 7] : (!llvm.ptr<struct<".10", (ptr<i8>, ptr<ptr<i8>>, i64, ptr<i8>, i64, i64, i64, i64, i64, i64, i32, ptr<i8>, i32, ptr<ptr<i8>>)>>, i32) -> !llvm.ptr<i64>
    llvm.store %9, %161 : !llvm.ptr<i64>
    %162 = llvm.getelementptr %131[%160] : (!llvm.ptr<ptr<i8>>, i32) -> !llvm.ptr<ptr<i8>>
    %163 = llvm.bitcast %161 : !llvm.ptr<i64> to !llvm.ptr<i8>
    llvm.store %163, %162 : !llvm.ptr<ptr<i8>>
    %164 = llvm.mlir.constant(8 : i32) : i32
    %165 = llvm.getelementptr %129[%127, 8] : (!llvm.ptr<struct<".10", (ptr<i8>, ptr<ptr<i8>>, i64, ptr<i8>, i64, i64, i64, i64, i64, i64, i32, ptr<i8>, i32, ptr<ptr<i8>>)>>, i32) -> !llvm.ptr<i64>
    llvm.store %7, %165 : !llvm.ptr<i64>
    %166 = llvm.getelementptr %131[%164] : (!llvm.ptr<ptr<i8>>, i32) -> !llvm.ptr<ptr<i8>>
    %167 = llvm.bitcast %165 : !llvm.ptr<i64> to !llvm.ptr<i8>
    llvm.store %167, %166 : !llvm.ptr<ptr<i8>>
    %168 = llvm.mlir.constant(9 : i32) : i32
    %169 = llvm.getelementptr %129[%127, 9] : (!llvm.ptr<struct<".10", (ptr<i8>, ptr<ptr<i8>>, i64, ptr<i8>, i64, i64, i64, i64, i64, i64, i32, ptr<i8>, i32, ptr<ptr<i8>>)>>, i32) -> !llvm.ptr<i64>
    llvm.store %7, %169 : !llvm.ptr<i64>
    %170 = llvm.getelementptr %131[%168] : (!llvm.ptr<ptr<i8>>, i32) -> !llvm.ptr<ptr<i8>>
    %171 = llvm.bitcast %169 : !llvm.ptr<i64> to !llvm.ptr<i8>
    llvm.store %171, %170 : !llvm.ptr<ptr<i8>>
    %172 = llvm.mlir.constant(10 : i32) : i32
    %173 = llvm.getelementptr %129[%127, 10] : (!llvm.ptr<struct<".10", (ptr<i8>, ptr<ptr<i8>>, i64, ptr<i8>, i64, i64, i64, i64, i64, i64, i32, ptr<i8>, i32, ptr<ptr<i8>>)>>, i32) -> !llvm.ptr<i32>
    llvm.store %124, %173 : !llvm.ptr<i32>
    %174 = llvm.getelementptr %131[%172] : (!llvm.ptr<ptr<i8>>, i32) -> !llvm.ptr<ptr<i8>>
    %175 = llvm.bitcast %173 : !llvm.ptr<i32> to !llvm.ptr<i8>
    llvm.store %175, %174 : !llvm.ptr<ptr<i8>>
    %176 = llvm.mlir.constant(11 : i32) : i32
    %177 = llvm.getelementptr %129[%127, 11] : (!llvm.ptr<struct<".10", (ptr<i8>, ptr<ptr<i8>>, i64, ptr<i8>, i64, i64, i64, i64, i64, i64, i32, ptr<i8>, i32, ptr<ptr<i8>>)>>, i32) -> !llvm.ptr<ptr<i8>>
    llvm.store %126, %177 : !llvm.ptr<ptr<i8>>
    %178 = llvm.getelementptr %131[%176] : (!llvm.ptr<ptr<i8>>, i32) -> !llvm.ptr<ptr<i8>>
    %179 = llvm.bitcast %177 : !llvm.ptr<ptr<i8>> to !llvm.ptr<i8>
    llvm.store %179, %178 : !llvm.ptr<ptr<i8>>
    %180 = llvm.mlir.constant(12 : i32) : i32
    %181 = llvm.getelementptr %129[%127, 12] : (!llvm.ptr<struct<".10", (ptr<i8>, ptr<ptr<i8>>, i64, ptr<i8>, i64, i64, i64, i64, i64, i64, i32, ptr<i8>, i32, ptr<ptr<i8>>)>>, i32) -> !llvm.ptr<i32>
    llvm.store %125, %181 : !llvm.ptr<i32>
    %182 = llvm.getelementptr %131[%180] : (!llvm.ptr<ptr<i8>>, i32) -> !llvm.ptr<ptr<i8>>
    %183 = llvm.bitcast %181 : !llvm.ptr<i32> to !llvm.ptr<i8>
    llvm.store %183, %182 : !llvm.ptr<ptr<i8>>
    %184 = llvm.mlir.constant(13 : i32) : i32
    %185 = llvm.getelementptr %129[%127, 13] : (!llvm.ptr<struct<".10", (ptr<i8>, ptr<ptr<i8>>, i64, ptr<i8>, i64, i64, i64, i64, i64, i64, i32, ptr<i8>, i32, ptr<ptr<i8>>)>>, i32) -> !llvm.ptr<ptr<ptr<i8>>>
    llvm.store %110, %185 : !llvm.ptr<ptr<ptr<i8>>>
    %186 = llvm.getelementptr %131[%184] : (!llvm.ptr<ptr<i8>>, i32) -> !llvm.ptr<ptr<i8>>
    %187 = llvm.bitcast %185 : !llvm.ptr<ptr<ptr<i8>>> to !llvm.ptr<i8>
    llvm.store %187, %186 : !llvm.ptr<ptr<i8>>
    %188 = llvm.mlir.addressof @ral_kernel_launch___gpu___pvoid_ppvoid_i64_pvoid_i64_i64_i64_i64_i64_i64_i32_pvoid_i32_ppvoid___void : !llvm.ptr<array<101 x i8>>
    %189 = llvm.mlir.constant(0 : index) : i64
    %190 = llvm.getelementptr %188[%189, %189] : (!llvm.ptr<array<101 x i8>>, i64, i64) -> !llvm.ptr<i8>
    llvm.call @disc_ral_call(%arg0, %190, %131) : (!llvm.ptr<i8>, !llvm.ptr<i8>, !llvm.ptr<ptr<i8>>) -> ()
    %191 = llvm.add %38, %0  : i64
    %192 = llvm.sdiv %191, %4  : i64
    %193 = llvm.add %192, %7  : i64
    %194 = llvm.sub %6, %38  : i64
    %195 = llvm.sdiv %194, %4  : i64
    %196 = llvm.sub %6, %195  : i64
    %197 = llvm.icmp "sgt" %38, %6 : i64
    %198 = llvm.select %197, %193, %196 : i1, i64
    %199 = llvm.add %34, %0  : i64
    %200 = llvm.sdiv %199, %3  : i64
    %201 = llvm.add %200, %7  : i64
    %202 = llvm.sub %6, %34  : i64
    %203 = llvm.sdiv %202, %3  : i64
    %204 = llvm.sub %6, %203  : i64
    %205 = llvm.icmp "sgt" %34, %6 : i64
    %206 = llvm.select %205, %201, %204 : i1, i64
    %207 = llvm.mul %198, %206  : i64
    %208 = llvm.mul %207, %9  : i64
    %209 = llvm.icmp "sle" %208, %6 : i64
    %210 = llvm.sub %6, %208  : i64
    %211 = llvm.sub %208, %7  : i64
    %212 = llvm.select %209, %210, %211 : i1, i64
    %213 = llvm.sdiv %212, %9  : i64
    %214 = llvm.sub %6, %213  : i64
    %215 = llvm.add %213, %7  : i64
    %216 = llvm.select %209, %214, %215 : i1, i64
    %217 = llvm.mlir.addressof @main_kernel_0_blob_gpu.binary : !llvm.ptr<array<2128 x i8>>
    %218 = llvm.mlir.constant(0 : index) : i64
    %219 = llvm.getelementptr %217[%218, %218] : (!llvm.ptr<array<2128 x i8>>, i64, i64) -> !llvm.ptr<i8>
    %220 = llvm.mlir.constant(1 : i32) : i32
    %221 = llvm.alloca %220 x !llvm.ptr<i8> : (i32) -> !llvm.ptr<ptr<i8>>
    %222 = llvm.mlir.constant(0 : i32) : i32
    %223 = llvm.getelementptr %221[%222] : (!llvm.ptr<ptr<i8>>, i32) -> !llvm.ptr<ptr<i8>>
    llvm.store %219, %223 : !llvm.ptr<ptr<i8>>
    %224 = llvm.mlir.constant(1 : i64) : i64
    %225 = llvm.mlir.addressof @main_kernel_0_main_kColReduction_reduce__4_1_0___8w32h_1_kernel_name : !llvm.ptr<array<43 x i8>>
    %226 = llvm.mlir.constant(0 : index) : i64
    %227 = llvm.getelementptr %225[%226, %226] : (!llvm.ptr<array<43 x i8>>, i64, i64) -> !llvm.ptr<i8>
    %228 = llvm.extractvalue %31[0] : !llvm.struct<(ptr<f32>, ptr<f32>, i64, array<3 x i64>, array<3 x i64>)> 
    %229 = llvm.extractvalue %31[1] : !llvm.struct<(ptr<f32>, ptr<f32>, i64, array<3 x i64>, array<3 x i64>)> 
    %230 = llvm.extractvalue %31[2] : !llvm.struct<(ptr<f32>, ptr<f32>, i64, array<3 x i64>, array<3 x i64>)> 
    %231 = llvm.extractvalue %31[3, 0] : !llvm.struct<(ptr<f32>, ptr<f32>, i64, array<3 x i64>, array<3 x i64>)> 
    %232 = llvm.extractvalue %31[3, 1] : !llvm.struct<(ptr<f32>, ptr<f32>, i64, array<3 x i64>, array<3 x i64>)> 
    %233 = llvm.extractvalue %31[3, 2] : !llvm.struct<(ptr<f32>, ptr<f32>, i64, array<3 x i64>, array<3 x i64>)> 
    %234 = llvm.extractvalue %31[4, 0] : !llvm.struct<(ptr<f32>, ptr<f32>, i64, array<3 x i64>, array<3 x i64>)> 
    %235 = llvm.extractvalue %31[4, 1] : !llvm.struct<(ptr<f32>, ptr<f32>, i64, array<3 x i64>, array<3 x i64>)> 
    %236 = llvm.extractvalue %31[4, 2] : !llvm.struct<(ptr<f32>, ptr<f32>, i64, array<3 x i64>, array<3 x i64>)> 
    %237 = llvm.extractvalue %72[0] : !llvm.struct<(ptr<f32>, ptr<f32>, i64, array<1 x i64>, array<1 x i64>)> 
    %238 = llvm.extractvalue %72[1] : !llvm.struct<(ptr<f32>, ptr<f32>, i64, array<1 x i64>, array<1 x i64>)> 
    %239 = llvm.extractvalue %72[2] : !llvm.struct<(ptr<f32>, ptr<f32>, i64, array<1 x i64>, array<1 x i64>)> 
    %240 = llvm.extractvalue %72[3, 0] : !llvm.struct<(ptr<f32>, ptr<f32>, i64, array<1 x i64>, array<1 x i64>)> 
    %241 = llvm.extractvalue %72[4, 0] : !llvm.struct<(ptr<f32>, ptr<f32>, i64, array<1 x i64>, array<1 x i64>)> 
    %242 = llvm.mlir.constant(1 : i32) : i32
    %243 = llvm.alloca %242 x !llvm.struct<".11", (i64, ptr<f32>, i64, i64, i64, i64, ptr<f32>)> : (i32) -> !llvm.ptr<struct<".11", (i64, ptr<f32>, i64, i64, i64, i64, ptr<f32>)>>
    %244 = llvm.mlir.constant(7 : i32) : i32
    %245 = llvm.alloca %244 x !llvm.ptr<i8> : (i32) -> !llvm.ptr<ptr<i8>>
    %246 = llvm.mlir.constant(0 : i32) : i32
    %247 = llvm.mlir.constant(0 : i32) : i32
    %248 = llvm.getelementptr %243[%246, 0] : (!llvm.ptr<struct<".11", (i64, ptr<f32>, i64, i64, i64, i64, ptr<f32>)>>, i32) -> !llvm.ptr<i64>
    llvm.store %38, %248 : !llvm.ptr<i64>
    %249 = llvm.getelementptr %245[%247] : (!llvm.ptr<ptr<i8>>, i32) -> !llvm.ptr<ptr<i8>>
    %250 = llvm.bitcast %248 : !llvm.ptr<i64> to !llvm.ptr<i8>
    llvm.store %250, %249 : !llvm.ptr<ptr<i8>>
    %251 = llvm.mlir.constant(1 : i32) : i32
    %252 = llvm.getelementptr %243[%246, 1] : (!llvm.ptr<struct<".11", (i64, ptr<f32>, i64, i64, i64, i64, ptr<f32>)>>, i32) -> !llvm.ptr<ptr<f32>>
    llvm.store %229, %252 : !llvm.ptr<ptr<f32>>
    %253 = llvm.getelementptr %245[%251] : (!llvm.ptr<ptr<i8>>, i32) -> !llvm.ptr<ptr<i8>>
    %254 = llvm.bitcast %252 : !llvm.ptr<ptr<f32>> to !llvm.ptr<i8>
    llvm.store %254, %253 : !llvm.ptr<ptr<i8>>
    %255 = llvm.mlir.constant(2 : i32) : i32
    %256 = llvm.getelementptr %243[%246, 2] : (!llvm.ptr<struct<".11", (i64, ptr<f32>, i64, i64, i64, i64, ptr<f32>)>>, i32) -> !llvm.ptr<i64>
    llvm.store %231, %256 : !llvm.ptr<i64>
    %257 = llvm.getelementptr %245[%255] : (!llvm.ptr<ptr<i8>>, i32) -> !llvm.ptr<ptr<i8>>
    %258 = llvm.bitcast %256 : !llvm.ptr<i64> to !llvm.ptr<i8>
    llvm.store %258, %257 : !llvm.ptr<ptr<i8>>
    %259 = llvm.mlir.constant(3 : i32) : i32
    %260 = llvm.getelementptr %243[%246, 3] : (!llvm.ptr<struct<".11", (i64, ptr<f32>, i64, i64, i64, i64, ptr<f32>)>>, i32) -> !llvm.ptr<i64>
    llvm.store %9, %260 : !llvm.ptr<i64>
    %261 = llvm.getelementptr %245[%259] : (!llvm.ptr<ptr<i8>>, i32) -> !llvm.ptr<ptr<i8>>
    %262 = llvm.bitcast %260 : !llvm.ptr<i64> to !llvm.ptr<i8>
    llvm.store %262, %261 : !llvm.ptr<ptr<i8>>
    %263 = llvm.mlir.constant(4 : i32) : i32
    %264 = llvm.getelementptr %243[%246, 4] : (!llvm.ptr<struct<".11", (i64, ptr<f32>, i64, i64, i64, i64, ptr<f32>)>>, i32) -> !llvm.ptr<i64>
    llvm.store %208, %264 : !llvm.ptr<i64>
    %265 = llvm.getelementptr %245[%263] : (!llvm.ptr<ptr<i8>>, i32) -> !llvm.ptr<ptr<i8>>
    %266 = llvm.bitcast %264 : !llvm.ptr<i64> to !llvm.ptr<i8>
    llvm.store %266, %265 : !llvm.ptr<ptr<i8>>
    %267 = llvm.mlir.constant(5 : i32) : i32
    %268 = llvm.getelementptr %243[%246, 5] : (!llvm.ptr<struct<".11", (i64, ptr<f32>, i64, i64, i64, i64, ptr<f32>)>>, i32) -> !llvm.ptr<i64>
    llvm.store %198, %268 : !llvm.ptr<i64>
    %269 = llvm.getelementptr %245[%267] : (!llvm.ptr<ptr<i8>>, i32) -> !llvm.ptr<ptr<i8>>
    %270 = llvm.bitcast %268 : !llvm.ptr<i64> to !llvm.ptr<i8>
    llvm.store %270, %269 : !llvm.ptr<ptr<i8>>
    %271 = llvm.mlir.constant(6 : i32) : i32
    %272 = llvm.getelementptr %243[%246, 6] : (!llvm.ptr<struct<".11", (i64, ptr<f32>, i64, i64, i64, i64, ptr<f32>)>>, i32) -> !llvm.ptr<ptr<f32>>
    llvm.store %238, %272 : !llvm.ptr<ptr<f32>>
    %273 = llvm.getelementptr %245[%271] : (!llvm.ptr<ptr<i8>>, i32) -> !llvm.ptr<ptr<i8>>
    %274 = llvm.bitcast %272 : !llvm.ptr<ptr<f32>> to !llvm.ptr<i8>
    llvm.store %274, %273 : !llvm.ptr<ptr<i8>>
    %275 = llvm.mlir.constant(0 : i32) : i32
    %276 = llvm.mlir.constant(7 : i32) : i32
    %277 = llvm.inttoptr %275 : i32 to !llvm.ptr<i8>
    %278 = llvm.mlir.constant(0 : i32) : i32
    %279 = llvm.mlir.constant(1 : i32) : i32
    %280 = llvm.alloca %279 x !llvm.struct<".12", (ptr<i8>, ptr<ptr<i8>>, i64, ptr<i8>, i64, i64, i64, i64, i64, i64, i32, ptr<i8>, i32, ptr<ptr<i8>>)> : (i32) -> !llvm.ptr<struct<".12", (ptr<i8>, ptr<ptr<i8>>, i64, ptr<i8>, i64, i64, i64, i64, i64, i64, i32, ptr<i8>, i32, ptr<ptr<i8>>)>>
    %281 = llvm.mlir.constant(14 : i32) : i32
    %282 = llvm.alloca %281 x !llvm.ptr<i8> : (i32) -> !llvm.ptr<ptr<i8>>
    %283 = llvm.mlir.constant(0 : i32) : i32
    %284 = llvm.getelementptr %280[%278, 0] : (!llvm.ptr<struct<".12", (ptr<i8>, ptr<ptr<i8>>, i64, ptr<i8>, i64, i64, i64, i64, i64, i64, i32, ptr<i8>, i32, ptr<ptr<i8>>)>>, i32) -> !llvm.ptr<ptr<i8>>
    llvm.store %arg0, %284 : !llvm.ptr<ptr<i8>>
    %285 = llvm.getelementptr %282[%283] : (!llvm.ptr<ptr<i8>>, i32) -> !llvm.ptr<ptr<i8>>
    %286 = llvm.bitcast %284 : !llvm.ptr<ptr<i8>> to !llvm.ptr<i8>
    llvm.store %286, %285 : !llvm.ptr<ptr<i8>>
    %287 = llvm.mlir.constant(1 : i32) : i32
    %288 = llvm.getelementptr %280[%278, 1] : (!llvm.ptr<struct<".12", (ptr<i8>, ptr<ptr<i8>>, i64, ptr<i8>, i64, i64, i64, i64, i64, i64, i32, ptr<i8>, i32, ptr<ptr<i8>>)>>, i32) -> !llvm.ptr<ptr<ptr<i8>>>
    llvm.store %221, %288 : !llvm.ptr<ptr<ptr<i8>>>
    %289 = llvm.getelementptr %282[%287] : (!llvm.ptr<ptr<i8>>, i32) -> !llvm.ptr<ptr<i8>>
    %290 = llvm.bitcast %288 : !llvm.ptr<ptr<ptr<i8>>> to !llvm.ptr<i8>
    llvm.store %290, %289 : !llvm.ptr<ptr<i8>>
    %291 = llvm.mlir.constant(2 : i32) : i32
    %292 = llvm.getelementptr %280[%278, 2] : (!llvm.ptr<struct<".12", (ptr<i8>, ptr<ptr<i8>>, i64, ptr<i8>, i64, i64, i64, i64, i64, i64, i32, ptr<i8>, i32, ptr<ptr<i8>>)>>, i32) -> !llvm.ptr<i64>
    llvm.store %224, %292 : !llvm.ptr<i64>
    %293 = llvm.getelementptr %282[%291] : (!llvm.ptr<ptr<i8>>, i32) -> !llvm.ptr<ptr<i8>>
    %294 = llvm.bitcast %292 : !llvm.ptr<i64> to !llvm.ptr<i8>
    llvm.store %294, %293 : !llvm.ptr<ptr<i8>>
    %295 = llvm.mlir.constant(3 : i32) : i32
    %296 = llvm.getelementptr %280[%278, 3] : (!llvm.ptr<struct<".12", (ptr<i8>, ptr<ptr<i8>>, i64, ptr<i8>, i64, i64, i64, i64, i64, i64, i32, ptr<i8>, i32, ptr<ptr<i8>>)>>, i32) -> !llvm.ptr<ptr<i8>>
    llvm.store %227, %296 : !llvm.ptr<ptr<i8>>
    %297 = llvm.getelementptr %282[%295] : (!llvm.ptr<ptr<i8>>, i32) -> !llvm.ptr<ptr<i8>>
    %298 = llvm.bitcast %296 : !llvm.ptr<ptr<i8>> to !llvm.ptr<i8>
    llvm.store %298, %297 : !llvm.ptr<ptr<i8>>
    %299 = llvm.mlir.constant(4 : i32) : i32
    %300 = llvm.getelementptr %280[%278, 4] : (!llvm.ptr<struct<".12", (ptr<i8>, ptr<ptr<i8>>, i64, ptr<i8>, i64, i64, i64, i64, i64, i64, i32, ptr<i8>, i32, ptr<ptr<i8>>)>>, i32) -> !llvm.ptr<i64>
    llvm.store %216, %300 : !llvm.ptr<i64>
    %301 = llvm.getelementptr %282[%299] : (!llvm.ptr<ptr<i8>>, i32) -> !llvm.ptr<ptr<i8>>
    %302 = llvm.bitcast %300 : !llvm.ptr<i64> to !llvm.ptr<i8>
    llvm.store %302, %301 : !llvm.ptr<ptr<i8>>
    %303 = llvm.mlir.constant(5 : i32) : i32
    %304 = llvm.getelementptr %280[%278, 5] : (!llvm.ptr<struct<".12", (ptr<i8>, ptr<ptr<i8>>, i64, ptr<i8>, i64, i64, i64, i64, i64, i64, i32, ptr<i8>, i32, ptr<ptr<i8>>)>>, i32) -> !llvm.ptr<i64>
    llvm.store %7, %304 : !llvm.ptr<i64>
    %305 = llvm.getelementptr %282[%303] : (!llvm.ptr<ptr<i8>>, i32) -> !llvm.ptr<ptr<i8>>
    %306 = llvm.bitcast %304 : !llvm.ptr<i64> to !llvm.ptr<i8>
    llvm.store %306, %305 : !llvm.ptr<ptr<i8>>
    %307 = llvm.mlir.constant(6 : i32) : i32
    %308 = llvm.getelementptr %280[%278, 6] : (!llvm.ptr<struct<".12", (ptr<i8>, ptr<ptr<i8>>, i64, ptr<i8>, i64, i64, i64, i64, i64, i64, i32, ptr<i8>, i32, ptr<ptr<i8>>)>>, i32) -> !llvm.ptr<i64>
    llvm.store %7, %308 : !llvm.ptr<i64>
    %309 = llvm.getelementptr %282[%307] : (!llvm.ptr<ptr<i8>>, i32) -> !llvm.ptr<ptr<i8>>
    %310 = llvm.bitcast %308 : !llvm.ptr<i64> to !llvm.ptr<i8>
    llvm.store %310, %309 : !llvm.ptr<ptr<i8>>
    %311 = llvm.mlir.constant(7 : i32) : i32
    %312 = llvm.getelementptr %280[%278, 7] : (!llvm.ptr<struct<".12", (ptr<i8>, ptr<ptr<i8>>, i64, ptr<i8>, i64, i64, i64, i64, i64, i64, i32, ptr<i8>, i32, ptr<ptr<i8>>)>>, i32) -> !llvm.ptr<i64>
    llvm.store %9, %312 : !llvm.ptr<i64>
    %313 = llvm.getelementptr %282[%311] : (!llvm.ptr<ptr<i8>>, i32) -> !llvm.ptr<ptr<i8>>
    %314 = llvm.bitcast %312 : !llvm.ptr<i64> to !llvm.ptr<i8>
    llvm.store %314, %313 : !llvm.ptr<ptr<i8>>
    %315 = llvm.mlir.constant(8 : i32) : i32
    %316 = llvm.getelementptr %280[%278, 8] : (!llvm.ptr<struct<".12", (ptr<i8>, ptr<ptr<i8>>, i64, ptr<i8>, i64, i64, i64, i64, i64, i64, i32, ptr<i8>, i32, ptr<ptr<i8>>)>>, i32) -> !llvm.ptr<i64>
    llvm.store %7, %316 : !llvm.ptr<i64>
    %317 = llvm.getelementptr %282[%315] : (!llvm.ptr<ptr<i8>>, i32) -> !llvm.ptr<ptr<i8>>
    %318 = llvm.bitcast %316 : !llvm.ptr<i64> to !llvm.ptr<i8>
    llvm.store %318, %317 : !llvm.ptr<ptr<i8>>
    %319 = llvm.mlir.constant(9 : i32) : i32
    %320 = llvm.getelementptr %280[%278, 9] : (!llvm.ptr<struct<".12", (ptr<i8>, ptr<ptr<i8>>, i64, ptr<i8>, i64, i64, i64, i64, i64, i64, i32, ptr<i8>, i32, ptr<ptr<i8>>)>>, i32) -> !llvm.ptr<i64>
    llvm.store %7, %320 : !llvm.ptr<i64>
    %321 = llvm.getelementptr %282[%319] : (!llvm.ptr<ptr<i8>>, i32) -> !llvm.ptr<ptr<i8>>
    %322 = llvm.bitcast %320 : !llvm.ptr<i64> to !llvm.ptr<i8>
    llvm.store %322, %321 : !llvm.ptr<ptr<i8>>
    %323 = llvm.mlir.constant(10 : i32) : i32
    %324 = llvm.getelementptr %280[%278, 10] : (!llvm.ptr<struct<".12", (ptr<i8>, ptr<ptr<i8>>, i64, ptr<i8>, i64, i64, i64, i64, i64, i64, i32, ptr<i8>, i32, ptr<ptr<i8>>)>>, i32) -> !llvm.ptr<i32>
    llvm.store %275, %324 : !llvm.ptr<i32>
    %325 = llvm.getelementptr %282[%323] : (!llvm.ptr<ptr<i8>>, i32) -> !llvm.ptr<ptr<i8>>
    %326 = llvm.bitcast %324 : !llvm.ptr<i32> to !llvm.ptr<i8>
    llvm.store %326, %325 : !llvm.ptr<ptr<i8>>
    %327 = llvm.mlir.constant(11 : i32) : i32
    %328 = llvm.getelementptr %280[%278, 11] : (!llvm.ptr<struct<".12", (ptr<i8>, ptr<ptr<i8>>, i64, ptr<i8>, i64, i64, i64, i64, i64, i64, i32, ptr<i8>, i32, ptr<ptr<i8>>)>>, i32) -> !llvm.ptr<ptr<i8>>
    llvm.store %277, %328 : !llvm.ptr<ptr<i8>>
    %329 = llvm.getelementptr %282[%327] : (!llvm.ptr<ptr<i8>>, i32) -> !llvm.ptr<ptr<i8>>
    %330 = llvm.bitcast %328 : !llvm.ptr<ptr<i8>> to !llvm.ptr<i8>
    llvm.store %330, %329 : !llvm.ptr<ptr<i8>>
    %331 = llvm.mlir.constant(12 : i32) : i32
    %332 = llvm.getelementptr %280[%278, 12] : (!llvm.ptr<struct<".12", (ptr<i8>, ptr<ptr<i8>>, i64, ptr<i8>, i64, i64, i64, i64, i64, i64, i32, ptr<i8>, i32, ptr<ptr<i8>>)>>, i32) -> !llvm.ptr<i32>
    llvm.store %276, %332 : !llvm.ptr<i32>
    %333 = llvm.getelementptr %282[%331] : (!llvm.ptr<ptr<i8>>, i32) -> !llvm.ptr<ptr<i8>>
    %334 = llvm.bitcast %332 : !llvm.ptr<i32> to !llvm.ptr<i8>
    llvm.store %334, %333 : !llvm.ptr<ptr<i8>>
    %335 = llvm.mlir.constant(13 : i32) : i32
    %336 = llvm.getelementptr %280[%278, 13] : (!llvm.ptr<struct<".12", (ptr<i8>, ptr<ptr<i8>>, i64, ptr<i8>, i64, i64, i64, i64, i64, i64, i32, ptr<i8>, i32, ptr<ptr<i8>>)>>, i32) -> !llvm.ptr<ptr<ptr<i8>>>
    llvm.store %245, %336 : !llvm.ptr<ptr<ptr<i8>>>
    %337 = llvm.getelementptr %282[%335] : (!llvm.ptr<ptr<i8>>, i32) -> !llvm.ptr<ptr<i8>>
    %338 = llvm.bitcast %336 : !llvm.ptr<ptr<ptr<i8>>> to !llvm.ptr<i8>
    llvm.store %338, %337 : !llvm.ptr<ptr<i8>>
    %339 = llvm.mlir.addressof @ral_kernel_launch___gpu___pvoid_ppvoid_i64_pvoid_i64_i64_i64_i64_i64_i64_i32_pvoid_i32_ppvoid___void : !llvm.ptr<array<101 x i8>>
    %340 = llvm.mlir.constant(0 : index) : i64
    %341 = llvm.getelementptr %339[%340, %340] : (!llvm.ptr<array<101 x i8>>, i64, i64) -> !llvm.ptr<i8>
    llvm.call @disc_ral_call(%arg0, %341, %282) : (!llvm.ptr<i8>, !llvm.ptr<i8>, !llvm.ptr<ptr<i8>>) -> ()
    llvm.br ^bb3
  ^bb2:  // pred: ^bb0
    %342 = llvm.icmp "sle" %38, %6 : i64
    %343 = llvm.sub %6, %38  : i64
    %344 = llvm.sub %38, %7  : i64
    %345 = llvm.select %342, %343, %344 : i1, i64
    %346 = llvm.sdiv %345, %1  : i64
    %347 = llvm.sub %6, %346  : i64
    %348 = llvm.add %346, %7  : i64
    %349 = llvm.select %342, %347, %348 : i1, i64
    %350 = llvm.mlir.addressof @main_kernel_1_blob_gpu.binary : !llvm.ptr<array<1096 x i8>>
    %351 = llvm.mlir.constant(0 : index) : i64
    %352 = llvm.getelementptr %350[%351, %351] : (!llvm.ptr<array<1096 x i8>>, i64, i64) -> !llvm.ptr<i8>
    %353 = llvm.mlir.constant(1 : i32) : i32
    %354 = llvm.alloca %353 x !llvm.ptr<i8> : (i32) -> !llvm.ptr<ptr<i8>>
    %355 = llvm.mlir.constant(0 : i32) : i32
    %356 = llvm.getelementptr %354[%355] : (!llvm.ptr<ptr<i8>>, i32) -> !llvm.ptr<ptr<i8>>
    llvm.store %352, %356 : !llvm.ptr<ptr<i8>>
    %357 = llvm.mlir.constant(1 : i64) : i64
    %358 = llvm.mlir.addressof @main_kernel_1_main_kColReduction_reduce__4_1_0___8w16h_kernel_name : !llvm.ptr<array<41 x i8>>
    %359 = llvm.mlir.constant(0 : index) : i64
    %360 = llvm.getelementptr %358[%359, %359] : (!llvm.ptr<array<41 x i8>>, i64, i64) -> !llvm.ptr<i8>
    %361 = llvm.extractvalue %72[0] : !llvm.struct<(ptr<f32>, ptr<f32>, i64, array<1 x i64>, array<1 x i64>)> 
    %362 = llvm.extractvalue %72[1] : !llvm.struct<(ptr<f32>, ptr<f32>, i64, array<1 x i64>, array<1 x i64>)> 
    %363 = llvm.extractvalue %72[2] : !llvm.struct<(ptr<f32>, ptr<f32>, i64, array<1 x i64>, array<1 x i64>)> 
    %364 = llvm.extractvalue %72[3, 0] : !llvm.struct<(ptr<f32>, ptr<f32>, i64, array<1 x i64>, array<1 x i64>)> 
    %365 = llvm.extractvalue %72[4, 0] : !llvm.struct<(ptr<f32>, ptr<f32>, i64, array<1 x i64>, array<1 x i64>)> 
    %366 = llvm.mlir.constant(1 : i32) : i32
    %367 = llvm.alloca %366 x !llvm.struct<".2", (i64, i64, ptr<f32>)> : (i32) -> !llvm.ptr<struct<".2", (i64, i64, ptr<f32>)>>
    %368 = llvm.mlir.constant(3 : i32) : i32
    %369 = llvm.alloca %368 x !llvm.ptr<i8> : (i32) -> !llvm.ptr<ptr<i8>>
    %370 = llvm.mlir.constant(0 : i32) : i32
    %371 = llvm.mlir.constant(0 : i32) : i32
    %372 = llvm.getelementptr %367[%370, 0] : (!llvm.ptr<struct<".2", (i64, i64, ptr<f32>)>>, i32) -> !llvm.ptr<i64>
    llvm.store %1, %372 : !llvm.ptr<i64>
    %373 = llvm.getelementptr %369[%371] : (!llvm.ptr<ptr<i8>>, i32) -> !llvm.ptr<ptr<i8>>
    %374 = llvm.bitcast %372 : !llvm.ptr<i64> to !llvm.ptr<i8>
    llvm.store %374, %373 : !llvm.ptr<ptr<i8>>
    %375 = llvm.mlir.constant(1 : i32) : i32
    %376 = llvm.getelementptr %367[%370, 1] : (!llvm.ptr<struct<".2", (i64, i64, ptr<f32>)>>, i32) -> !llvm.ptr<i64>
    llvm.store %38, %376 : !llvm.ptr<i64>
    %377 = llvm.getelementptr %369[%375] : (!llvm.ptr<ptr<i8>>, i32) -> !llvm.ptr<ptr<i8>>
    %378 = llvm.bitcast %376 : !llvm.ptr<i64> to !llvm.ptr<i8>
    llvm.store %378, %377 : !llvm.ptr<ptr<i8>>
    %379 = llvm.mlir.constant(2 : i32) : i32
    %380 = llvm.getelementptr %367[%370, 2] : (!llvm.ptr<struct<".2", (i64, i64, ptr<f32>)>>, i32) -> !llvm.ptr<ptr<f32>>
    llvm.store %362, %380 : !llvm.ptr<ptr<f32>>
    %381 = llvm.getelementptr %369[%379] : (!llvm.ptr<ptr<i8>>, i32) -> !llvm.ptr<ptr<i8>>
    %382 = llvm.bitcast %380 : !llvm.ptr<ptr<f32>> to !llvm.ptr<i8>
    llvm.store %382, %381 : !llvm.ptr<ptr<i8>>
    %383 = llvm.mlir.constant(0 : i32) : i32
    %384 = llvm.mlir.constant(3 : i32) : i32
    %385 = llvm.inttoptr %383 : i32 to !llvm.ptr<i8>
    %386 = llvm.mlir.constant(0 : i32) : i32
    %387 = llvm.mlir.constant(1 : i32) : i32
    %388 = llvm.alloca %387 x !llvm.struct<".3", (ptr<i8>, ptr<ptr<i8>>, i64, ptr<i8>, i64, i64, i64, i64, i64, i64, i32, ptr<i8>, i32, ptr<ptr<i8>>)> : (i32) -> !llvm.ptr<struct<".3", (ptr<i8>, ptr<ptr<i8>>, i64, ptr<i8>, i64, i64, i64, i64, i64, i64, i32, ptr<i8>, i32, ptr<ptr<i8>>)>>
    %389 = llvm.mlir.constant(14 : i32) : i32
    %390 = llvm.alloca %389 x !llvm.ptr<i8> : (i32) -> !llvm.ptr<ptr<i8>>
    %391 = llvm.mlir.constant(0 : i32) : i32
    %392 = llvm.getelementptr %388[%386, 0] : (!llvm.ptr<struct<".3", (ptr<i8>, ptr<ptr<i8>>, i64, ptr<i8>, i64, i64, i64, i64, i64, i64, i32, ptr<i8>, i32, ptr<ptr<i8>>)>>, i32) -> !llvm.ptr<ptr<i8>>
    llvm.store %arg0, %392 : !llvm.ptr<ptr<i8>>
    %393 = llvm.getelementptr %390[%391] : (!llvm.ptr<ptr<i8>>, i32) -> !llvm.ptr<ptr<i8>>
    %394 = llvm.bitcast %392 : !llvm.ptr<ptr<i8>> to !llvm.ptr<i8>
    llvm.store %394, %393 : !llvm.ptr<ptr<i8>>
    %395 = llvm.mlir.constant(1 : i32) : i32
    %396 = llvm.getelementptr %388[%386, 1] : (!llvm.ptr<struct<".3", (ptr<i8>, ptr<ptr<i8>>, i64, ptr<i8>, i64, i64, i64, i64, i64, i64, i32, ptr<i8>, i32, ptr<ptr<i8>>)>>, i32) -> !llvm.ptr<ptr<ptr<i8>>>
    llvm.store %354, %396 : !llvm.ptr<ptr<ptr<i8>>>
    %397 = llvm.getelementptr %390[%395] : (!llvm.ptr<ptr<i8>>, i32) -> !llvm.ptr<ptr<i8>>
    %398 = llvm.bitcast %396 : !llvm.ptr<ptr<ptr<i8>>> to !llvm.ptr<i8>
    llvm.store %398, %397 : !llvm.ptr<ptr<i8>>
    %399 = llvm.mlir.constant(2 : i32) : i32
    %400 = llvm.getelementptr %388[%386, 2] : (!llvm.ptr<struct<".3", (ptr<i8>, ptr<ptr<i8>>, i64, ptr<i8>, i64, i64, i64, i64, i64, i64, i32, ptr<i8>, i32, ptr<ptr<i8>>)>>, i32) -> !llvm.ptr<i64>
    llvm.store %357, %400 : !llvm.ptr<i64>
    %401 = llvm.getelementptr %390[%399] : (!llvm.ptr<ptr<i8>>, i32) -> !llvm.ptr<ptr<i8>>
    %402 = llvm.bitcast %400 : !llvm.ptr<i64> to !llvm.ptr<i8>
    llvm.store %402, %401 : !llvm.ptr<ptr<i8>>
    %403 = llvm.mlir.constant(3 : i32) : i32
    %404 = llvm.getelementptr %388[%386, 3] : (!llvm.ptr<struct<".3", (ptr<i8>, ptr<ptr<i8>>, i64, ptr<i8>, i64, i64, i64, i64, i64, i64, i32, ptr<i8>, i32, ptr<ptr<i8>>)>>, i32) -> !llvm.ptr<ptr<i8>>
    llvm.store %360, %404 : !llvm.ptr<ptr<i8>>
    %405 = llvm.getelementptr %390[%403] : (!llvm.ptr<ptr<i8>>, i32) -> !llvm.ptr<ptr<i8>>
    %406 = llvm.bitcast %404 : !llvm.ptr<ptr<i8>> to !llvm.ptr<i8>
    llvm.store %406, %405 : !llvm.ptr<ptr<i8>>
    %407 = llvm.mlir.constant(4 : i32) : i32
    %408 = llvm.getelementptr %388[%386, 4] : (!llvm.ptr<struct<".3", (ptr<i8>, ptr<ptr<i8>>, i64, ptr<i8>, i64, i64, i64, i64, i64, i64, i32, ptr<i8>, i32, ptr<ptr<i8>>)>>, i32) -> !llvm.ptr<i64>
    llvm.store %349, %408 : !llvm.ptr<i64>
    %409 = llvm.getelementptr %390[%407] : (!llvm.ptr<ptr<i8>>, i32) -> !llvm.ptr<ptr<i8>>
    %410 = llvm.bitcast %408 : !llvm.ptr<i64> to !llvm.ptr<i8>
    llvm.store %410, %409 : !llvm.ptr<ptr<i8>>
    %411 = llvm.mlir.constant(5 : i32) : i32
    %412 = llvm.getelementptr %388[%386, 5] : (!llvm.ptr<struct<".3", (ptr<i8>, ptr<ptr<i8>>, i64, ptr<i8>, i64, i64, i64, i64, i64, i64, i32, ptr<i8>, i32, ptr<ptr<i8>>)>>, i32) -> !llvm.ptr<i64>
    llvm.store %7, %412 : !llvm.ptr<i64>
    %413 = llvm.getelementptr %390[%411] : (!llvm.ptr<ptr<i8>>, i32) -> !llvm.ptr<ptr<i8>>
    %414 = llvm.bitcast %412 : !llvm.ptr<i64> to !llvm.ptr<i8>
    llvm.store %414, %413 : !llvm.ptr<ptr<i8>>
    %415 = llvm.mlir.constant(6 : i32) : i32
    %416 = llvm.getelementptr %388[%386, 6] : (!llvm.ptr<struct<".3", (ptr<i8>, ptr<ptr<i8>>, i64, ptr<i8>, i64, i64, i64, i64, i64, i64, i32, ptr<i8>, i32, ptr<ptr<i8>>)>>, i32) -> !llvm.ptr<i64>
    llvm.store %7, %416 : !llvm.ptr<i64>
    %417 = llvm.getelementptr %390[%415] : (!llvm.ptr<ptr<i8>>, i32) -> !llvm.ptr<ptr<i8>>
    %418 = llvm.bitcast %416 : !llvm.ptr<i64> to !llvm.ptr<i8>
    llvm.store %418, %417 : !llvm.ptr<ptr<i8>>
    %419 = llvm.mlir.constant(7 : i32) : i32
    %420 = llvm.getelementptr %388[%386, 7] : (!llvm.ptr<struct<".3", (ptr<i8>, ptr<ptr<i8>>, i64, ptr<i8>, i64, i64, i64, i64, i64, i64, i32, ptr<i8>, i32, ptr<ptr<i8>>)>>, i32) -> !llvm.ptr<i64>
    llvm.store %1, %420 : !llvm.ptr<i64>
    %421 = llvm.getelementptr %390[%419] : (!llvm.ptr<ptr<i8>>, i32) -> !llvm.ptr<ptr<i8>>
    %422 = llvm.bitcast %420 : !llvm.ptr<i64> to !llvm.ptr<i8>
    llvm.store %422, %421 : !llvm.ptr<ptr<i8>>
    %423 = llvm.mlir.constant(8 : i32) : i32
    %424 = llvm.getelementptr %388[%386, 8] : (!llvm.ptr<struct<".3", (ptr<i8>, ptr<ptr<i8>>, i64, ptr<i8>, i64, i64, i64, i64, i64, i64, i32, ptr<i8>, i32, ptr<ptr<i8>>)>>, i32) -> !llvm.ptr<i64>
    llvm.store %7, %424 : !llvm.ptr<i64>
    %425 = llvm.getelementptr %390[%423] : (!llvm.ptr<ptr<i8>>, i32) -> !llvm.ptr<ptr<i8>>
    %426 = llvm.bitcast %424 : !llvm.ptr<i64> to !llvm.ptr<i8>
    llvm.store %426, %425 : !llvm.ptr<ptr<i8>>
    %427 = llvm.mlir.constant(9 : i32) : i32
    %428 = llvm.getelementptr %388[%386, 9] : (!llvm.ptr<struct<".3", (ptr<i8>, ptr<ptr<i8>>, i64, ptr<i8>, i64, i64, i64, i64, i64, i64, i32, ptr<i8>, i32, ptr<ptr<i8>>)>>, i32) -> !llvm.ptr<i64>
    llvm.store %7, %428 : !llvm.ptr<i64>
    %429 = llvm.getelementptr %390[%427] : (!llvm.ptr<ptr<i8>>, i32) -> !llvm.ptr<ptr<i8>>
    %430 = llvm.bitcast %428 : !llvm.ptr<i64> to !llvm.ptr<i8>
    llvm.store %430, %429 : !llvm.ptr<ptr<i8>>
    %431 = llvm.mlir.constant(10 : i32) : i32
    %432 = llvm.getelementptr %388[%386, 10] : (!llvm.ptr<struct<".3", (ptr<i8>, ptr<ptr<i8>>, i64, ptr<i8>, i64, i64, i64, i64, i64, i64, i32, ptr<i8>, i32, ptr<ptr<i8>>)>>, i32) -> !llvm.ptr<i32>
    llvm.store %383, %432 : !llvm.ptr<i32>
    %433 = llvm.getelementptr %390[%431] : (!llvm.ptr<ptr<i8>>, i32) -> !llvm.ptr<ptr<i8>>
    %434 = llvm.bitcast %432 : !llvm.ptr<i32> to !llvm.ptr<i8>
    llvm.store %434, %433 : !llvm.ptr<ptr<i8>>
    %435 = llvm.mlir.constant(11 : i32) : i32
    %436 = llvm.getelementptr %388[%386, 11] : (!llvm.ptr<struct<".3", (ptr<i8>, ptr<ptr<i8>>, i64, ptr<i8>, i64, i64, i64, i64, i64, i64, i32, ptr<i8>, i32, ptr<ptr<i8>>)>>, i32) -> !llvm.ptr<ptr<i8>>
    llvm.store %385, %436 : !llvm.ptr<ptr<i8>>
    %437 = llvm.getelementptr %390[%435] : (!llvm.ptr<ptr<i8>>, i32) -> !llvm.ptr<ptr<i8>>
    %438 = llvm.bitcast %436 : !llvm.ptr<ptr<i8>> to !llvm.ptr<i8>
    llvm.store %438, %437 : !llvm.ptr<ptr<i8>>
    %439 = llvm.mlir.constant(12 : i32) : i32
    %440 = llvm.getelementptr %388[%386, 12] : (!llvm.ptr<struct<".3", (ptr<i8>, ptr<ptr<i8>>, i64, ptr<i8>, i64, i64, i64, i64, i64, i64, i32, ptr<i8>, i32, ptr<ptr<i8>>)>>, i32) -> !llvm.ptr<i32>
    llvm.store %384, %440 : !llvm.ptr<i32>
    %441 = llvm.getelementptr %390[%439] : (!llvm.ptr<ptr<i8>>, i32) -> !llvm.ptr<ptr<i8>>
    %442 = llvm.bitcast %440 : !llvm.ptr<i32> to !llvm.ptr<i8>
    llvm.store %442, %441 : !llvm.ptr<ptr<i8>>
    %443 = llvm.mlir.constant(13 : i32) : i32
    %444 = llvm.getelementptr %388[%386, 13] : (!llvm.ptr<struct<".3", (ptr<i8>, ptr<ptr<i8>>, i64, ptr<i8>, i64, i64, i64, i64, i64, i64, i32, ptr<i8>, i32, ptr<ptr<i8>>)>>, i32) -> !llvm.ptr<ptr<ptr<i8>>>
    llvm.store %369, %444 : !llvm.ptr<ptr<ptr<i8>>>
    %445 = llvm.getelementptr %390[%443] : (!llvm.ptr<ptr<i8>>, i32) -> !llvm.ptr<ptr<i8>>
    %446 = llvm.bitcast %444 : !llvm.ptr<ptr<ptr<i8>>> to !llvm.ptr<i8>
    llvm.store %446, %445 : !llvm.ptr<ptr<i8>>
    %447 = llvm.mlir.addressof @ral_kernel_launch___gpu___pvoid_ppvoid_i64_pvoid_i64_i64_i64_i64_i64_i64_i32_pvoid_i32_ppvoid___void : !llvm.ptr<array<101 x i8>>
    %448 = llvm.mlir.constant(0 : index) : i64
    %449 = llvm.getelementptr %447[%448, %448] : (!llvm.ptr<array<101 x i8>>, i64, i64) -> !llvm.ptr<i8>
    llvm.call @disc_ral_call(%arg0, %449, %390) : (!llvm.ptr<i8>, !llvm.ptr<i8>, !llvm.ptr<ptr<i8>>) -> ()
    %450 = llvm.add %38, %0  : i64
    %451 = llvm.sdiv %450, %4  : i64
    %452 = llvm.add %451, %7  : i64
    %453 = llvm.sub %6, %38  : i64
    %454 = llvm.sdiv %453, %4  : i64
    %455 = llvm.sub %6, %454  : i64
    %456 = llvm.icmp "sgt" %38, %6 : i64
    %457 = llvm.select %456, %452, %455 : i1, i64
    %458 = llvm.add %34, %0  : i64
    %459 = llvm.sdiv %458, %2  : i64
    %460 = llvm.add %459, %7  : i64
    %461 = llvm.sub %6, %34  : i64
    %462 = llvm.sdiv %461, %2  : i64
    %463 = llvm.sub %6, %462  : i64
    %464 = llvm.icmp "sgt" %34, %6 : i64
    %465 = llvm.select %464, %460, %463 : i1, i64
    %466 = llvm.mul %457, %465  : i64
    %467 = llvm.mul %466, %1  : i64
    %468 = llvm.icmp "sle" %467, %6 : i64
    %469 = llvm.sub %6, %467  : i64
    %470 = llvm.sub %467, %7  : i64
    %471 = llvm.select %468, %469, %470 : i1, i64
    %472 = llvm.sdiv %471, %1  : i64
    %473 = llvm.sub %6, %472  : i64
    %474 = llvm.add %472, %7  : i64
    %475 = llvm.select %468, %473, %474 : i1, i64
    %476 = llvm.mlir.addressof @main_kernel_2_blob_gpu.binary : !llvm.ptr<array<2080 x i8>>
    %477 = llvm.mlir.constant(0 : index) : i64
    %478 = llvm.getelementptr %476[%477, %477] : (!llvm.ptr<array<2080 x i8>>, i64, i64) -> !llvm.ptr<i8>
    %479 = llvm.mlir.constant(1 : i32) : i32
    %480 = llvm.alloca %479 x !llvm.ptr<i8> : (i32) -> !llvm.ptr<ptr<i8>>
    %481 = llvm.mlir.constant(0 : i32) : i32
    %482 = llvm.getelementptr %480[%481] : (!llvm.ptr<ptr<i8>>, i32) -> !llvm.ptr<ptr<i8>>
    llvm.store %478, %482 : !llvm.ptr<ptr<i8>>
    %483 = llvm.mlir.constant(1 : i64) : i64
    %484 = llvm.mlir.addressof @main_kernel_2_main_kColReduction_reduce__4_1_0___8w16h_1_kernel_name : !llvm.ptr<array<43 x i8>>
    %485 = llvm.mlir.constant(0 : index) : i64
    %486 = llvm.getelementptr %484[%485, %485] : (!llvm.ptr<array<43 x i8>>, i64, i64) -> !llvm.ptr<i8>
    %487 = llvm.extractvalue %31[0] : !llvm.struct<(ptr<f32>, ptr<f32>, i64, array<3 x i64>, array<3 x i64>)> 
    %488 = llvm.extractvalue %31[1] : !llvm.struct<(ptr<f32>, ptr<f32>, i64, array<3 x i64>, array<3 x i64>)> 
    %489 = llvm.extractvalue %31[2] : !llvm.struct<(ptr<f32>, ptr<f32>, i64, array<3 x i64>, array<3 x i64>)> 
    %490 = llvm.extractvalue %31[3, 0] : !llvm.struct<(ptr<f32>, ptr<f32>, i64, array<3 x i64>, array<3 x i64>)> 
    %491 = llvm.extractvalue %31[3, 1] : !llvm.struct<(ptr<f32>, ptr<f32>, i64, array<3 x i64>, array<3 x i64>)> 
    %492 = llvm.extractvalue %31[3, 2] : !llvm.struct<(ptr<f32>, ptr<f32>, i64, array<3 x i64>, array<3 x i64>)> 
    %493 = llvm.extractvalue %31[4, 0] : !llvm.struct<(ptr<f32>, ptr<f32>, i64, array<3 x i64>, array<3 x i64>)> 
    %494 = llvm.extractvalue %31[4, 1] : !llvm.struct<(ptr<f32>, ptr<f32>, i64, array<3 x i64>, array<3 x i64>)> 
    %495 = llvm.extractvalue %31[4, 2] : !llvm.struct<(ptr<f32>, ptr<f32>, i64, array<3 x i64>, array<3 x i64>)> 
    %496 = llvm.extractvalue %72[0] : !llvm.struct<(ptr<f32>, ptr<f32>, i64, array<1 x i64>, array<1 x i64>)> 
    %497 = llvm.extractvalue %72[1] : !llvm.struct<(ptr<f32>, ptr<f32>, i64, array<1 x i64>, array<1 x i64>)> 
    %498 = llvm.extractvalue %72[2] : !llvm.struct<(ptr<f32>, ptr<f32>, i64, array<1 x i64>, array<1 x i64>)> 
    %499 = llvm.extractvalue %72[3, 0] : !llvm.struct<(ptr<f32>, ptr<f32>, i64, array<1 x i64>, array<1 x i64>)> 
    %500 = llvm.extractvalue %72[4, 0] : !llvm.struct<(ptr<f32>, ptr<f32>, i64, array<1 x i64>, array<1 x i64>)> 
    %501 = llvm.mlir.constant(1 : i32) : i32
    %502 = llvm.alloca %501 x !llvm.struct<".4", (i64, ptr<f32>, i64, i64, i64, i64, ptr<f32>)> : (i32) -> !llvm.ptr<struct<".4", (i64, ptr<f32>, i64, i64, i64, i64, ptr<f32>)>>
    %503 = llvm.mlir.constant(7 : i32) : i32
    %504 = llvm.alloca %503 x !llvm.ptr<i8> : (i32) -> !llvm.ptr<ptr<i8>>
    %505 = llvm.mlir.constant(0 : i32) : i32
    %506 = llvm.mlir.constant(0 : i32) : i32
    %507 = llvm.getelementptr %502[%505, 0] : (!llvm.ptr<struct<".4", (i64, ptr<f32>, i64, i64, i64, i64, ptr<f32>)>>, i32) -> !llvm.ptr<i64>
    llvm.store %38, %507 : !llvm.ptr<i64>
    %508 = llvm.getelementptr %504[%506] : (!llvm.ptr<ptr<i8>>, i32) -> !llvm.ptr<ptr<i8>>
    %509 = llvm.bitcast %507 : !llvm.ptr<i64> to !llvm.ptr<i8>
    llvm.store %509, %508 : !llvm.ptr<ptr<i8>>
    %510 = llvm.mlir.constant(1 : i32) : i32
    %511 = llvm.getelementptr %502[%505, 1] : (!llvm.ptr<struct<".4", (i64, ptr<f32>, i64, i64, i64, i64, ptr<f32>)>>, i32) -> !llvm.ptr<ptr<f32>>
    llvm.store %488, %511 : !llvm.ptr<ptr<f32>>
    %512 = llvm.getelementptr %504[%510] : (!llvm.ptr<ptr<i8>>, i32) -> !llvm.ptr<ptr<i8>>
    %513 = llvm.bitcast %511 : !llvm.ptr<ptr<f32>> to !llvm.ptr<i8>
    llvm.store %513, %512 : !llvm.ptr<ptr<i8>>
    %514 = llvm.mlir.constant(2 : i32) : i32
    %515 = llvm.getelementptr %502[%505, 2] : (!llvm.ptr<struct<".4", (i64, ptr<f32>, i64, i64, i64, i64, ptr<f32>)>>, i32) -> !llvm.ptr<i64>
    llvm.store %490, %515 : !llvm.ptr<i64>
    %516 = llvm.getelementptr %504[%514] : (!llvm.ptr<ptr<i8>>, i32) -> !llvm.ptr<ptr<i8>>
    %517 = llvm.bitcast %515 : !llvm.ptr<i64> to !llvm.ptr<i8>
    llvm.store %517, %516 : !llvm.ptr<ptr<i8>>
    %518 = llvm.mlir.constant(3 : i32) : i32
    %519 = llvm.getelementptr %502[%505, 3] : (!llvm.ptr<struct<".4", (i64, ptr<f32>, i64, i64, i64, i64, ptr<f32>)>>, i32) -> !llvm.ptr<i64>
    llvm.store %1, %519 : !llvm.ptr<i64>
    %520 = llvm.getelementptr %504[%518] : (!llvm.ptr<ptr<i8>>, i32) -> !llvm.ptr<ptr<i8>>
    %521 = llvm.bitcast %519 : !llvm.ptr<i64> to !llvm.ptr<i8>
    llvm.store %521, %520 : !llvm.ptr<ptr<i8>>
    %522 = llvm.mlir.constant(4 : i32) : i32
    %523 = llvm.getelementptr %502[%505, 4] : (!llvm.ptr<struct<".4", (i64, ptr<f32>, i64, i64, i64, i64, ptr<f32>)>>, i32) -> !llvm.ptr<i64>
    llvm.store %467, %523 : !llvm.ptr<i64>
    %524 = llvm.getelementptr %504[%522] : (!llvm.ptr<ptr<i8>>, i32) -> !llvm.ptr<ptr<i8>>
    %525 = llvm.bitcast %523 : !llvm.ptr<i64> to !llvm.ptr<i8>
    llvm.store %525, %524 : !llvm.ptr<ptr<i8>>
    %526 = llvm.mlir.constant(5 : i32) : i32
    %527 = llvm.getelementptr %502[%505, 5] : (!llvm.ptr<struct<".4", (i64, ptr<f32>, i64, i64, i64, i64, ptr<f32>)>>, i32) -> !llvm.ptr<i64>
    llvm.store %457, %527 : !llvm.ptr<i64>
    %528 = llvm.getelementptr %504[%526] : (!llvm.ptr<ptr<i8>>, i32) -> !llvm.ptr<ptr<i8>>
    %529 = llvm.bitcast %527 : !llvm.ptr<i64> to !llvm.ptr<i8>
    llvm.store %529, %528 : !llvm.ptr<ptr<i8>>
    %530 = llvm.mlir.constant(6 : i32) : i32
    %531 = llvm.getelementptr %502[%505, 6] : (!llvm.ptr<struct<".4", (i64, ptr<f32>, i64, i64, i64, i64, ptr<f32>)>>, i32) -> !llvm.ptr<ptr<f32>>
    llvm.store %497, %531 : !llvm.ptr<ptr<f32>>
    %532 = llvm.getelementptr %504[%530] : (!llvm.ptr<ptr<i8>>, i32) -> !llvm.ptr<ptr<i8>>
    %533 = llvm.bitcast %531 : !llvm.ptr<ptr<f32>> to !llvm.ptr<i8>
    llvm.store %533, %532 : !llvm.ptr<ptr<i8>>
    %534 = llvm.mlir.constant(0 : i32) : i32
    %535 = llvm.mlir.constant(7 : i32) : i32
    %536 = llvm.inttoptr %534 : i32 to !llvm.ptr<i8>
    %537 = llvm.mlir.constant(0 : i32) : i32
    %538 = llvm.mlir.constant(1 : i32) : i32
    %539 = llvm.alloca %538 x !llvm.struct<".5", (ptr<i8>, ptr<ptr<i8>>, i64, ptr<i8>, i64, i64, i64, i64, i64, i64, i32, ptr<i8>, i32, ptr<ptr<i8>>)> : (i32) -> !llvm.ptr<struct<".5", (ptr<i8>, ptr<ptr<i8>>, i64, ptr<i8>, i64, i64, i64, i64, i64, i64, i32, ptr<i8>, i32, ptr<ptr<i8>>)>>
    %540 = llvm.mlir.constant(14 : i32) : i32
    %541 = llvm.alloca %540 x !llvm.ptr<i8> : (i32) -> !llvm.ptr<ptr<i8>>
    %542 = llvm.mlir.constant(0 : i32) : i32
    %543 = llvm.getelementptr %539[%537, 0] : (!llvm.ptr<struct<".5", (ptr<i8>, ptr<ptr<i8>>, i64, ptr<i8>, i64, i64, i64, i64, i64, i64, i32, ptr<i8>, i32, ptr<ptr<i8>>)>>, i32) -> !llvm.ptr<ptr<i8>>
    llvm.store %arg0, %543 : !llvm.ptr<ptr<i8>>
    %544 = llvm.getelementptr %541[%542] : (!llvm.ptr<ptr<i8>>, i32) -> !llvm.ptr<ptr<i8>>
    %545 = llvm.bitcast %543 : !llvm.ptr<ptr<i8>> to !llvm.ptr<i8>
    llvm.store %545, %544 : !llvm.ptr<ptr<i8>>
    %546 = llvm.mlir.constant(1 : i32) : i32
    %547 = llvm.getelementptr %539[%537, 1] : (!llvm.ptr<struct<".5", (ptr<i8>, ptr<ptr<i8>>, i64, ptr<i8>, i64, i64, i64, i64, i64, i64, i32, ptr<i8>, i32, ptr<ptr<i8>>)>>, i32) -> !llvm.ptr<ptr<ptr<i8>>>
    llvm.store %480, %547 : !llvm.ptr<ptr<ptr<i8>>>
    %548 = llvm.getelementptr %541[%546] : (!llvm.ptr<ptr<i8>>, i32) -> !llvm.ptr<ptr<i8>>
    %549 = llvm.bitcast %547 : !llvm.ptr<ptr<ptr<i8>>> to !llvm.ptr<i8>
    llvm.store %549, %548 : !llvm.ptr<ptr<i8>>
    %550 = llvm.mlir.constant(2 : i32) : i32
    %551 = llvm.getelementptr %539[%537, 2] : (!llvm.ptr<struct<".5", (ptr<i8>, ptr<ptr<i8>>, i64, ptr<i8>, i64, i64, i64, i64, i64, i64, i32, ptr<i8>, i32, ptr<ptr<i8>>)>>, i32) -> !llvm.ptr<i64>
    llvm.store %483, %551 : !llvm.ptr<i64>
    %552 = llvm.getelementptr %541[%550] : (!llvm.ptr<ptr<i8>>, i32) -> !llvm.ptr<ptr<i8>>
    %553 = llvm.bitcast %551 : !llvm.ptr<i64> to !llvm.ptr<i8>
    llvm.store %553, %552 : !llvm.ptr<ptr<i8>>
    %554 = llvm.mlir.constant(3 : i32) : i32
    %555 = llvm.getelementptr %539[%537, 3] : (!llvm.ptr<struct<".5", (ptr<i8>, ptr<ptr<i8>>, i64, ptr<i8>, i64, i64, i64, i64, i64, i64, i32, ptr<i8>, i32, ptr<ptr<i8>>)>>, i32) -> !llvm.ptr<ptr<i8>>
    llvm.store %486, %555 : !llvm.ptr<ptr<i8>>
    %556 = llvm.getelementptr %541[%554] : (!llvm.ptr<ptr<i8>>, i32) -> !llvm.ptr<ptr<i8>>
    %557 = llvm.bitcast %555 : !llvm.ptr<ptr<i8>> to !llvm.ptr<i8>
    llvm.store %557, %556 : !llvm.ptr<ptr<i8>>
    %558 = llvm.mlir.constant(4 : i32) : i32
    %559 = llvm.getelementptr %539[%537, 4] : (!llvm.ptr<struct<".5", (ptr<i8>, ptr<ptr<i8>>, i64, ptr<i8>, i64, i64, i64, i64, i64, i64, i32, ptr<i8>, i32, ptr<ptr<i8>>)>>, i32) -> !llvm.ptr<i64>
    llvm.store %475, %559 : !llvm.ptr<i64>
    %560 = llvm.getelementptr %541[%558] : (!llvm.ptr<ptr<i8>>, i32) -> !llvm.ptr<ptr<i8>>
    %561 = llvm.bitcast %559 : !llvm.ptr<i64> to !llvm.ptr<i8>
    llvm.store %561, %560 : !llvm.ptr<ptr<i8>>
    %562 = llvm.mlir.constant(5 : i32) : i32
    %563 = llvm.getelementptr %539[%537, 5] : (!llvm.ptr<struct<".5", (ptr<i8>, ptr<ptr<i8>>, i64, ptr<i8>, i64, i64, i64, i64, i64, i64, i32, ptr<i8>, i32, ptr<ptr<i8>>)>>, i32) -> !llvm.ptr<i64>
    llvm.store %7, %563 : !llvm.ptr<i64>
    %564 = llvm.getelementptr %541[%562] : (!llvm.ptr<ptr<i8>>, i32) -> !llvm.ptr<ptr<i8>>
    %565 = llvm.bitcast %563 : !llvm.ptr<i64> to !llvm.ptr<i8>
    llvm.store %565, %564 : !llvm.ptr<ptr<i8>>
    %566 = llvm.mlir.constant(6 : i32) : i32
    %567 = llvm.getelementptr %539[%537, 6] : (!llvm.ptr<struct<".5", (ptr<i8>, ptr<ptr<i8>>, i64, ptr<i8>, i64, i64, i64, i64, i64, i64, i32, ptr<i8>, i32, ptr<ptr<i8>>)>>, i32) -> !llvm.ptr<i64>
    llvm.store %7, %567 : !llvm.ptr<i64>
    %568 = llvm.getelementptr %541[%566] : (!llvm.ptr<ptr<i8>>, i32) -> !llvm.ptr<ptr<i8>>
    %569 = llvm.bitcast %567 : !llvm.ptr<i64> to !llvm.ptr<i8>
    llvm.store %569, %568 : !llvm.ptr<ptr<i8>>
    %570 = llvm.mlir.constant(7 : i32) : i32
    %571 = llvm.getelementptr %539[%537, 7] : (!llvm.ptr<struct<".5", (ptr<i8>, ptr<ptr<i8>>, i64, ptr<i8>, i64, i64, i64, i64, i64, i64, i32, ptr<i8>, i32, ptr<ptr<i8>>)>>, i32) -> !llvm.ptr<i64>
    llvm.store %1, %571 : !llvm.ptr<i64>
    %572 = llvm.getelementptr %541[%570] : (!llvm.ptr<ptr<i8>>, i32) -> !llvm.ptr<ptr<i8>>
    %573 = llvm.bitcast %571 : !llvm.ptr<i64> to !llvm.ptr<i8>
    llvm.store %573, %572 : !llvm.ptr<ptr<i8>>
    %574 = llvm.mlir.constant(8 : i32) : i32
    %575 = llvm.getelementptr %539[%537, 8] : (!llvm.ptr<struct<".5", (ptr<i8>, ptr<ptr<i8>>, i64, ptr<i8>, i64, i64, i64, i64, i64, i64, i32, ptr<i8>, i32, ptr<ptr<i8>>)>>, i32) -> !llvm.ptr<i64>
    llvm.store %7, %575 : !llvm.ptr<i64>
    %576 = llvm.getelementptr %541[%574] : (!llvm.ptr<ptr<i8>>, i32) -> !llvm.ptr<ptr<i8>>
    %577 = llvm.bitcast %575 : !llvm.ptr<i64> to !llvm.ptr<i8>
    llvm.store %577, %576 : !llvm.ptr<ptr<i8>>
    %578 = llvm.mlir.constant(9 : i32) : i32
    %579 = llvm.getelementptr %539[%537, 9] : (!llvm.ptr<struct<".5", (ptr<i8>, ptr<ptr<i8>>, i64, ptr<i8>, i64, i64, i64, i64, i64, i64, i32, ptr<i8>, i32, ptr<ptr<i8>>)>>, i32) -> !llvm.ptr<i64>
    llvm.store %7, %579 : !llvm.ptr<i64>
    %580 = llvm.getelementptr %541[%578] : (!llvm.ptr<ptr<i8>>, i32) -> !llvm.ptr<ptr<i8>>
    %581 = llvm.bitcast %579 : !llvm.ptr<i64> to !llvm.ptr<i8>
    llvm.store %581, %580 : !llvm.ptr<ptr<i8>>
    %582 = llvm.mlir.constant(10 : i32) : i32
    %583 = llvm.getelementptr %539[%537, 10] : (!llvm.ptr<struct<".5", (ptr<i8>, ptr<ptr<i8>>, i64, ptr<i8>, i64, i64, i64, i64, i64, i64, i32, ptr<i8>, i32, ptr<ptr<i8>>)>>, i32) -> !llvm.ptr<i32>
    llvm.store %534, %583 : !llvm.ptr<i32>
    %584 = llvm.getelementptr %541[%582] : (!llvm.ptr<ptr<i8>>, i32) -> !llvm.ptr<ptr<i8>>
    %585 = llvm.bitcast %583 : !llvm.ptr<i32> to !llvm.ptr<i8>
    llvm.store %585, %584 : !llvm.ptr<ptr<i8>>
    %586 = llvm.mlir.constant(11 : i32) : i32
    %587 = llvm.getelementptr %539[%537, 11] : (!llvm.ptr<struct<".5", (ptr<i8>, ptr<ptr<i8>>, i64, ptr<i8>, i64, i64, i64, i64, i64, i64, i32, ptr<i8>, i32, ptr<ptr<i8>>)>>, i32) -> !llvm.ptr<ptr<i8>>
    llvm.store %536, %587 : !llvm.ptr<ptr<i8>>
    %588 = llvm.getelementptr %541[%586] : (!llvm.ptr<ptr<i8>>, i32) -> !llvm.ptr<ptr<i8>>
    %589 = llvm.bitcast %587 : !llvm.ptr<ptr<i8>> to !llvm.ptr<i8>
    llvm.store %589, %588 : !llvm.ptr<ptr<i8>>
    %590 = llvm.mlir.constant(12 : i32) : i32
    %591 = llvm.getelementptr %539[%537, 12] : (!llvm.ptr<struct<".5", (ptr<i8>, ptr<ptr<i8>>, i64, ptr<i8>, i64, i64, i64, i64, i64, i64, i32, ptr<i8>, i32, ptr<ptr<i8>>)>>, i32) -> !llvm.ptr<i32>
    llvm.store %535, %591 : !llvm.ptr<i32>
    %592 = llvm.getelementptr %541[%590] : (!llvm.ptr<ptr<i8>>, i32) -> !llvm.ptr<ptr<i8>>
    %593 = llvm.bitcast %591 : !llvm.ptr<i32> to !llvm.ptr<i8>
    llvm.store %593, %592 : !llvm.ptr<ptr<i8>>
    %594 = llvm.mlir.constant(13 : i32) : i32
    %595 = llvm.getelementptr %539[%537, 13] : (!llvm.ptr<struct<".5", (ptr<i8>, ptr<ptr<i8>>, i64, ptr<i8>, i64, i64, i64, i64, i64, i64, i32, ptr<i8>, i32, ptr<ptr<i8>>)>>, i32) -> !llvm.ptr<ptr<ptr<i8>>>
    llvm.store %504, %595 : !llvm.ptr<ptr<ptr<i8>>>
    %596 = llvm.getelementptr %541[%594] : (!llvm.ptr<ptr<i8>>, i32) -> !llvm.ptr<ptr<i8>>
    %597 = llvm.bitcast %595 : !llvm.ptr<ptr<ptr<i8>>> to !llvm.ptr<i8>
    llvm.store %597, %596 : !llvm.ptr<ptr<i8>>
    %598 = llvm.mlir.addressof @ral_kernel_launch___gpu___pvoid_ppvoid_i64_pvoid_i64_i64_i64_i64_i64_i64_i32_pvoid_i32_ppvoid___void : !llvm.ptr<array<101 x i8>>
    %599 = llvm.mlir.constant(0 : index) : i64
    %600 = llvm.getelementptr %598[%599, %599] : (!llvm.ptr<array<101 x i8>>, i64, i64) -> !llvm.ptr<i8>
    llvm.call @disc_ral_call(%arg0, %600, %541) : (!llvm.ptr<i8>, !llvm.ptr<i8>, !llvm.ptr<ptr<i8>>) -> ()
    llvm.br ^bb3
  ^bb3:  // 2 preds: ^bb1, ^bb2
    %601 = llvm.inttoptr %5 : i32 to !llvm.ptr<i8>
    %602 = llvm.mlir.constant(2 : index) : i64
    %603 = llvm.mlir.constant(1 : index) : i64
    %604 = llvm.mlir.null : !llvm.ptr<i64>
    %605 = llvm.getelementptr %604[%602] : (!llvm.ptr<i64>, i64) -> !llvm.ptr<i64>
    %606 = llvm.ptrtoint %605 : !llvm.ptr<i64> to i64
    %607 = llvm.alloca %606 x i64 : (i64) -> !llvm.ptr<i64>
    %608 = llvm.mlir.undef : !llvm.struct<(ptr<i64>, ptr<i64>, i64, array<1 x i64>, array<1 x i64>)>
    %609 = llvm.insertvalue %607, %608[0] : !llvm.struct<(ptr<i64>, ptr<i64>, i64, array<1 x i64>, array<1 x i64>)> 
    %610 = llvm.insertvalue %607, %609[1] : !llvm.struct<(ptr<i64>, ptr<i64>, i64, array<1 x i64>, array<1 x i64>)> 
    %611 = llvm.mlir.constant(0 : index) : i64
    %612 = llvm.insertvalue %611, %610[2] : !llvm.struct<(ptr<i64>, ptr<i64>, i64, array<1 x i64>, array<1 x i64>)> 
    %613 = llvm.insertvalue %602, %612[3, 0] : !llvm.struct<(ptr<i64>, ptr<i64>, i64, array<1 x i64>, array<1 x i64>)> 
    %614 = llvm.insertvalue %603, %613[4, 0] : !llvm.struct<(ptr<i64>, ptr<i64>, i64, array<1 x i64>, array<1 x i64>)> 
    %615 = llvm.extractvalue %614[1] : !llvm.struct<(ptr<i64>, ptr<i64>, i64, array<1 x i64>, array<1 x i64>)> 
    %616 = llvm.getelementptr %615[%6] : (!llvm.ptr<i64>, i64) -> !llvm.ptr<i64>
    llvm.store %33, %616 : !llvm.ptr<i64>
    %617 = llvm.extractvalue %614[1] : !llvm.struct<(ptr<i64>, ptr<i64>, i64, array<1 x i64>, array<1 x i64>)> 
    %618 = llvm.getelementptr %617[%7] : (!llvm.ptr<i64>, i64) -> !llvm.ptr<i64>
    llvm.store %32, %618 : !llvm.ptr<i64>
    %619 = llvm.mlir.constant(0 : i32) : i32
    %620 = llvm.mlir.constant(1 : i32) : i32
    %621 = llvm.extractvalue %72[0] : !llvm.struct<(ptr<f32>, ptr<f32>, i64, array<1 x i64>, array<1 x i64>)> 
    %622 = llvm.extractvalue %72[1] : !llvm.struct<(ptr<f32>, ptr<f32>, i64, array<1 x i64>, array<1 x i64>)> 
    %623 = llvm.extractvalue %72[2] : !llvm.struct<(ptr<f32>, ptr<f32>, i64, array<1 x i64>, array<1 x i64>)> 
    %624 = llvm.extractvalue %72[3, 0] : !llvm.struct<(ptr<f32>, ptr<f32>, i64, array<1 x i64>, array<1 x i64>)> 
    %625 = llvm.extractvalue %72[4, 0] : !llvm.struct<(ptr<f32>, ptr<f32>, i64, array<1 x i64>, array<1 x i64>)> 
    %626 = llvm.extractvalue %614[0] : !llvm.struct<(ptr<i64>, ptr<i64>, i64, array<1 x i64>, array<1 x i64>)> 
    %627 = llvm.extractvalue %614[1] : !llvm.struct<(ptr<i64>, ptr<i64>, i64, array<1 x i64>, array<1 x i64>)> 
    %628 = llvm.extractvalue %614[2] : !llvm.struct<(ptr<i64>, ptr<i64>, i64, array<1 x i64>, array<1 x i64>)> 
    %629 = llvm.extractvalue %614[3, 0] : !llvm.struct<(ptr<i64>, ptr<i64>, i64, array<1 x i64>, array<1 x i64>)> 
    %630 = llvm.extractvalue %614[4, 0] : !llvm.struct<(ptr<i64>, ptr<i64>, i64, array<1 x i64>, array<1 x i64>)> 
    %631 = llvm.alloca %620 x !llvm.struct<".6", (ptr<i8>, ptr<i8>, ptr<f32>, ptr<f32>, i64, i64, i64, ptr<i64>, ptr<i64>, i64, i64, i64, struct<(ptr<f32>, ptr<f32>, i64, array<2 x i64>, array<2 x i64>)>)> : (i32) -> !llvm.ptr<struct<".6", (ptr<i8>, ptr<i8>, ptr<f32>, ptr<f32>, i64, i64, i64, ptr<i64>, ptr<i64>, i64, i64, i64, struct<(ptr<f32>, ptr<f32>, i64, array<2 x i64>, array<2 x i64>)>)>>
    %632 = llvm.mlir.constant(13 : i32) : i32
    %633 = llvm.alloca %632 x !llvm.ptr<i8> : (i32) -> !llvm.ptr<ptr<i8>>
    %634 = llvm.mlir.constant(0 : i32) : i32
    %635 = llvm.getelementptr %631[%619, 0] : (!llvm.ptr<struct<".6", (ptr<i8>, ptr<i8>, ptr<f32>, ptr<f32>, i64, i64, i64, ptr<i64>, ptr<i64>, i64, i64, i64, struct<(ptr<f32>, ptr<f32>, i64, array<2 x i64>, array<2 x i64>)>)>>, i32) -> !llvm.ptr<ptr<i8>>
    llvm.store %arg0, %635 : !llvm.ptr<ptr<i8>>
    %636 = llvm.getelementptr %633[%634] : (!llvm.ptr<ptr<i8>>, i32) -> !llvm.ptr<ptr<i8>>
    %637 = llvm.bitcast %635 : !llvm.ptr<ptr<i8>> to !llvm.ptr<i8>
    llvm.store %637, %636 : !llvm.ptr<ptr<i8>>
    %638 = llvm.mlir.constant(1 : i32) : i32
    %639 = llvm.getelementptr %631[%619, 1] : (!llvm.ptr<struct<".6", (ptr<i8>, ptr<i8>, ptr<f32>, ptr<f32>, i64, i64, i64, ptr<i64>, ptr<i64>, i64, i64, i64, struct<(ptr<f32>, ptr<f32>, i64, array<2 x i64>, array<2 x i64>)>)>>, i32) -> !llvm.ptr<ptr<i8>>
    llvm.store %601, %639 : !llvm.ptr<ptr<i8>>
    %640 = llvm.getelementptr %633[%638] : (!llvm.ptr<ptr<i8>>, i32) -> !llvm.ptr<ptr<i8>>
    %641 = llvm.bitcast %639 : !llvm.ptr<ptr<i8>> to !llvm.ptr<i8>
    llvm.store %641, %640 : !llvm.ptr<ptr<i8>>
    %642 = llvm.mlir.constant(2 : i32) : i32
    %643 = llvm.getelementptr %631[%619, 2] : (!llvm.ptr<struct<".6", (ptr<i8>, ptr<i8>, ptr<f32>, ptr<f32>, i64, i64, i64, ptr<i64>, ptr<i64>, i64, i64, i64, struct<(ptr<f32>, ptr<f32>, i64, array<2 x i64>, array<2 x i64>)>)>>, i32) -> !llvm.ptr<ptr<f32>>
    llvm.store %621, %643 : !llvm.ptr<ptr<f32>>
    %644 = llvm.getelementptr %633[%642] : (!llvm.ptr<ptr<i8>>, i32) -> !llvm.ptr<ptr<i8>>
    %645 = llvm.bitcast %643 : !llvm.ptr<ptr<f32>> to !llvm.ptr<i8>
    llvm.store %645, %644 : !llvm.ptr<ptr<i8>>
    %646 = llvm.mlir.constant(3 : i32) : i32
    %647 = llvm.getelementptr %631[%619, 3] : (!llvm.ptr<struct<".6", (ptr<i8>, ptr<i8>, ptr<f32>, ptr<f32>, i64, i64, i64, ptr<i64>, ptr<i64>, i64, i64, i64, struct<(ptr<f32>, ptr<f32>, i64, array<2 x i64>, array<2 x i64>)>)>>, i32) -> !llvm.ptr<ptr<f32>>
    llvm.store %622, %647 : !llvm.ptr<ptr<f32>>
    %648 = llvm.getelementptr %633[%646] : (!llvm.ptr<ptr<i8>>, i32) -> !llvm.ptr<ptr<i8>>
    %649 = llvm.bitcast %647 : !llvm.ptr<ptr<f32>> to !llvm.ptr<i8>
    llvm.store %649, %648 : !llvm.ptr<ptr<i8>>
    %650 = llvm.mlir.constant(4 : i32) : i32
    %651 = llvm.getelementptr %631[%619, 4] : (!llvm.ptr<struct<".6", (ptr<i8>, ptr<i8>, ptr<f32>, ptr<f32>, i64, i64, i64, ptr<i64>, ptr<i64>, i64, i64, i64, struct<(ptr<f32>, ptr<f32>, i64, array<2 x i64>, array<2 x i64>)>)>>, i32) -> !llvm.ptr<i64>
    llvm.store %623, %651 : !llvm.ptr<i64>
    %652 = llvm.getelementptr %633[%650] : (!llvm.ptr<ptr<i8>>, i32) -> !llvm.ptr<ptr<i8>>
    %653 = llvm.bitcast %651 : !llvm.ptr<i64> to !llvm.ptr<i8>
    llvm.store %653, %652 : !llvm.ptr<ptr<i8>>
    %654 = llvm.mlir.constant(5 : i32) : i32
    %655 = llvm.getelementptr %631[%619, 5] : (!llvm.ptr<struct<".6", (ptr<i8>, ptr<i8>, ptr<f32>, ptr<f32>, i64, i64, i64, ptr<i64>, ptr<i64>, i64, i64, i64, struct<(ptr<f32>, ptr<f32>, i64, array<2 x i64>, array<2 x i64>)>)>>, i32) -> !llvm.ptr<i64>
    llvm.store %624, %655 : !llvm.ptr<i64>
    %656 = llvm.getelementptr %633[%654] : (!llvm.ptr<ptr<i8>>, i32) -> !llvm.ptr<ptr<i8>>
    %657 = llvm.bitcast %655 : !llvm.ptr<i64> to !llvm.ptr<i8>
    llvm.store %657, %656 : !llvm.ptr<ptr<i8>>
    %658 = llvm.mlir.constant(6 : i32) : i32
    %659 = llvm.getelementptr %631[%619, 6] : (!llvm.ptr<struct<".6", (ptr<i8>, ptr<i8>, ptr<f32>, ptr<f32>, i64, i64, i64, ptr<i64>, ptr<i64>, i64, i64, i64, struct<(ptr<f32>, ptr<f32>, i64, array<2 x i64>, array<2 x i64>)>)>>, i32) -> !llvm.ptr<i64>
    llvm.store %625, %659 : !llvm.ptr<i64>
    %660 = llvm.getelementptr %633[%658] : (!llvm.ptr<ptr<i8>>, i32) -> !llvm.ptr<ptr<i8>>
    %661 = llvm.bitcast %659 : !llvm.ptr<i64> to !llvm.ptr<i8>
    llvm.store %661, %660 : !llvm.ptr<ptr<i8>>
    %662 = llvm.mlir.constant(7 : i32) : i32
    %663 = llvm.getelementptr %631[%619, 7] : (!llvm.ptr<struct<".6", (ptr<i8>, ptr<i8>, ptr<f32>, ptr<f32>, i64, i64, i64, ptr<i64>, ptr<i64>, i64, i64, i64, struct<(ptr<f32>, ptr<f32>, i64, array<2 x i64>, array<2 x i64>)>)>>, i32) -> !llvm.ptr<ptr<i64>>
    llvm.store %626, %663 : !llvm.ptr<ptr<i64>>
    %664 = llvm.getelementptr %633[%662] : (!llvm.ptr<ptr<i8>>, i32) -> !llvm.ptr<ptr<i8>>
    %665 = llvm.bitcast %663 : !llvm.ptr<ptr<i64>> to !llvm.ptr<i8>
    llvm.store %665, %664 : !llvm.ptr<ptr<i8>>
    %666 = llvm.mlir.constant(8 : i32) : i32
    %667 = llvm.getelementptr %631[%619, 8] : (!llvm.ptr<struct<".6", (ptr<i8>, ptr<i8>, ptr<f32>, ptr<f32>, i64, i64, i64, ptr<i64>, ptr<i64>, i64, i64, i64, struct<(ptr<f32>, ptr<f32>, i64, array<2 x i64>, array<2 x i64>)>)>>, i32) -> !llvm.ptr<ptr<i64>>
    llvm.store %627, %667 : !llvm.ptr<ptr<i64>>
    %668 = llvm.getelementptr %633[%666] : (!llvm.ptr<ptr<i8>>, i32) -> !llvm.ptr<ptr<i8>>
    %669 = llvm.bitcast %667 : !llvm.ptr<ptr<i64>> to !llvm.ptr<i8>
    llvm.store %669, %668 : !llvm.ptr<ptr<i8>>
    %670 = llvm.mlir.constant(9 : i32) : i32
    %671 = llvm.getelementptr %631[%619, 9] : (!llvm.ptr<struct<".6", (ptr<i8>, ptr<i8>, ptr<f32>, ptr<f32>, i64, i64, i64, ptr<i64>, ptr<i64>, i64, i64, i64, struct<(ptr<f32>, ptr<f32>, i64, array<2 x i64>, array<2 x i64>)>)>>, i32) -> !llvm.ptr<i64>
    llvm.store %628, %671 : !llvm.ptr<i64>
    %672 = llvm.getelementptr %633[%670] : (!llvm.ptr<ptr<i8>>, i32) -> !llvm.ptr<ptr<i8>>
    %673 = llvm.bitcast %671 : !llvm.ptr<i64> to !llvm.ptr<i8>
    llvm.store %673, %672 : !llvm.ptr<ptr<i8>>
    %674 = llvm.mlir.constant(10 : i32) : i32
    %675 = llvm.getelementptr %631[%619, 10] : (!llvm.ptr<struct<".6", (ptr<i8>, ptr<i8>, ptr<f32>, ptr<f32>, i64, i64, i64, ptr<i64>, ptr<i64>, i64, i64, i64, struct<(ptr<f32>, ptr<f32>, i64, array<2 x i64>, array<2 x i64>)>)>>, i32) -> !llvm.ptr<i64>
    llvm.store %629, %675 : !llvm.ptr<i64>
    %676 = llvm.getelementptr %633[%674] : (!llvm.ptr<ptr<i8>>, i32) -> !llvm.ptr<ptr<i8>>
    %677 = llvm.bitcast %675 : !llvm.ptr<i64> to !llvm.ptr<i8>
    llvm.store %677, %676 : !llvm.ptr<ptr<i8>>
    %678 = llvm.mlir.constant(11 : i32) : i32
    %679 = llvm.getelementptr %631[%619, 11] : (!llvm.ptr<struct<".6", (ptr<i8>, ptr<i8>, ptr<f32>, ptr<f32>, i64, i64, i64, ptr<i64>, ptr<i64>, i64, i64, i64, struct<(ptr<f32>, ptr<f32>, i64, array<2 x i64>, array<2 x i64>)>)>>, i32) -> !llvm.ptr<i64>
    llvm.store %630, %679 : !llvm.ptr<i64>
    %680 = llvm.getelementptr %633[%678] : (!llvm.ptr<ptr<i8>>, i32) -> !llvm.ptr<ptr<i8>>
    %681 = llvm.bitcast %679 : !llvm.ptr<i64> to !llvm.ptr<i8>
    llvm.store %681, %680 : !llvm.ptr<ptr<i8>>
    %682 = llvm.mlir.constant(12 : i32) : i32
    %683 = llvm.getelementptr %631[%619, 12] : (!llvm.ptr<struct<".6", (ptr<i8>, ptr<i8>, ptr<f32>, ptr<f32>, i64, i64, i64, ptr<i64>, ptr<i64>, i64, i64, i64, struct<(ptr<f32>, ptr<f32>, i64, array<2 x i64>, array<2 x i64>)>)>>, i32) -> !llvm.ptr<struct<(ptr<f32>, ptr<f32>, i64, array<2 x i64>, array<2 x i64>)>>
    %684 = llvm.getelementptr %633[%682] : (!llvm.ptr<ptr<i8>>, i32) -> !llvm.ptr<ptr<i8>>
    %685 = llvm.bitcast %683 : !llvm.ptr<struct<(ptr<f32>, ptr<f32>, i64, array<2 x i64>, array<2 x i64>)>> to !llvm.ptr<i8>
    llvm.store %685, %684 : !llvm.ptr<ptr<i8>>
    %686 = llvm.mlir.addressof @inc_ref___gpu___pvoid_pvoid_m1df32_m1di64___m2df32 : !llvm.ptr<array<51 x i8>>
    %687 = llvm.mlir.constant(0 : index) : i64
    %688 = llvm.getelementptr %686[%687, %687] : (!llvm.ptr<array<51 x i8>>, i64, i64) -> !llvm.ptr<i8>
    llvm.call @disc_ral_call(%arg0, %688, %633) : (!llvm.ptr<i8>, !llvm.ptr<i8>, !llvm.ptr<ptr<i8>>) -> ()
    %689 = llvm.load %683 : !llvm.ptr<struct<(ptr<f32>, ptr<f32>, i64, array<2 x i64>, array<2 x i64>)>>
    %690 = llvm.mlir.undef : !llvm.struct<(ptr<f32>, ptr<f32>, i64, array<2 x i64>, array<2 x i64>)>
    %691 = llvm.extractvalue %689[0] : !llvm.struct<(ptr<f32>, ptr<f32>, i64, array<2 x i64>, array<2 x i64>)> 
    %692 = llvm.extractvalue %689[1] : !llvm.struct<(ptr<f32>, ptr<f32>, i64, array<2 x i64>, array<2 x i64>)> 
    %693 = llvm.insertvalue %691, %690[0] : !llvm.struct<(ptr<f32>, ptr<f32>, i64, array<2 x i64>, array<2 x i64>)> 
    %694 = llvm.insertvalue %692, %693[1] : !llvm.struct<(ptr<f32>, ptr<f32>, i64, array<2 x i64>, array<2 x i64>)> 
    %695 = llvm.mlir.constant(0 : index) : i64
    %696 = llvm.insertvalue %695, %694[2] : !llvm.struct<(ptr<f32>, ptr<f32>, i64, array<2 x i64>, array<2 x i64>)> 
    %697 = llvm.insertvalue %33, %696[3, 0] : !llvm.struct<(ptr<f32>, ptr<f32>, i64, array<2 x i64>, array<2 x i64>)> 
    %698 = llvm.insertvalue %32, %697[4, 0] : !llvm.struct<(ptr<f32>, ptr<f32>, i64, array<2 x i64>, array<2 x i64>)> 
    %699 = llvm.insertvalue %32, %698[3, 1] : !llvm.struct<(ptr<f32>, ptr<f32>, i64, array<2 x i64>, array<2 x i64>)> 
    %700 = llvm.mlir.constant(1 : index) : i64
    %701 = llvm.insertvalue %700, %699[4, 1] : !llvm.struct<(ptr<f32>, ptr<f32>, i64, array<2 x i64>, array<2 x i64>)> 
    %702 = llvm.extractvalue %72[0] : !llvm.struct<(ptr<f32>, ptr<f32>, i64, array<1 x i64>, array<1 x i64>)> 
    %703 = llvm.bitcast %702 : !llvm.ptr<f32> to !llvm.ptr<i8>
    %704 = llvm.mlir.constant(0 : i32) : i32
    %705 = llvm.mlir.constant(1 : i32) : i32
    %706 = llvm.alloca %705 x !llvm.struct<".7", (ptr<i8>, ptr<i8>)> : (i32) -> !llvm.ptr<struct<".7", (ptr<i8>, ptr<i8>)>>
    %707 = llvm.mlir.constant(2 : i32) : i32
    %708 = llvm.alloca %707 x !llvm.ptr<i8> : (i32) -> !llvm.ptr<ptr<i8>>
    %709 = llvm.mlir.constant(0 : i32) : i32
    %710 = llvm.getelementptr %706[%704, 0] : (!llvm.ptr<struct<".7", (ptr<i8>, ptr<i8>)>>, i32) -> !llvm.ptr<ptr<i8>>
    llvm.store %arg0, %710 : !llvm.ptr<ptr<i8>>
    %711 = llvm.getelementptr %708[%709] : (!llvm.ptr<ptr<i8>>, i32) -> !llvm.ptr<ptr<i8>>
    %712 = llvm.bitcast %710 : !llvm.ptr<ptr<i8>> to !llvm.ptr<i8>
    llvm.store %712, %711 : !llvm.ptr<ptr<i8>>
    %713 = llvm.mlir.constant(1 : i32) : i32
    %714 = llvm.getelementptr %706[%704, 1] : (!llvm.ptr<struct<".7", (ptr<i8>, ptr<i8>)>>, i32) -> !llvm.ptr<ptr<i8>>
    llvm.store %703, %714 : !llvm.ptr<ptr<i8>>
    %715 = llvm.getelementptr %708[%713] : (!llvm.ptr<ptr<i8>>, i32) -> !llvm.ptr<ptr<i8>>
    %716 = llvm.bitcast %714 : !llvm.ptr<ptr<i8>> to !llvm.ptr<i8>
    llvm.store %716, %715 : !llvm.ptr<ptr<i8>>
    %717 = llvm.mlir.addressof @dealloc___gpu___pvoid_pvoid___void : !llvm.ptr<array<35 x i8>>
    %718 = llvm.mlir.constant(0 : index) : i64
    %719 = llvm.getelementptr %717[%718, %718] : (!llvm.ptr<array<35 x i8>>, i64, i64) -> !llvm.ptr<i8>
    llvm.call @disc_ral_call(%arg0, %719, %708) : (!llvm.ptr<i8>, !llvm.ptr<i8>, !llvm.ptr<ptr<i8>>) -> ()
    %720 = llvm.mlir.constant(0 : i32) : i32
    %721 = llvm.mlir.constant(1 : i32) : i32
    %722 = llvm.extractvalue %701[0] : !llvm.struct<(ptr<f32>, ptr<f32>, i64, array<2 x i64>, array<2 x i64>)> 
    %723 = llvm.extractvalue %701[1] : !llvm.struct<(ptr<f32>, ptr<f32>, i64, array<2 x i64>, array<2 x i64>)> 
    %724 = llvm.extractvalue %701[2] : !llvm.struct<(ptr<f32>, ptr<f32>, i64, array<2 x i64>, array<2 x i64>)> 
    %725 = llvm.extractvalue %701[3, 0] : !llvm.struct<(ptr<f32>, ptr<f32>, i64, array<2 x i64>, array<2 x i64>)> 
    %726 = llvm.extractvalue %701[3, 1] : !llvm.struct<(ptr<f32>, ptr<f32>, i64, array<2 x i64>, array<2 x i64>)> 
    %727 = llvm.extractvalue %701[4, 0] : !llvm.struct<(ptr<f32>, ptr<f32>, i64, array<2 x i64>, array<2 x i64>)> 
    %728 = llvm.extractvalue %701[4, 1] : !llvm.struct<(ptr<f32>, ptr<f32>, i64, array<2 x i64>, array<2 x i64>)> 
    %729 = llvm.alloca %721 x !llvm.struct<".8", (ptr<i8>, i64, ptr<f32>, ptr<f32>, i64, i64, i64, i64, i64)> : (i32) -> !llvm.ptr<struct<".8", (ptr<i8>, i64, ptr<f32>, ptr<f32>, i64, i64, i64, i64, i64)>>
    %730 = llvm.mlir.constant(9 : i32) : i32
    %731 = llvm.alloca %730 x !llvm.ptr<i8> : (i32) -> !llvm.ptr<ptr<i8>>
    %732 = llvm.mlir.constant(0 : i32) : i32
    %733 = llvm.getelementptr %729[%720, 0] : (!llvm.ptr<struct<".8", (ptr<i8>, i64, ptr<f32>, ptr<f32>, i64, i64, i64, i64, i64)>>, i32) -> !llvm.ptr<ptr<i8>>
    llvm.store %arg0, %733 : !llvm.ptr<ptr<i8>>
    %734 = llvm.getelementptr %731[%732] : (!llvm.ptr<ptr<i8>>, i32) -> !llvm.ptr<ptr<i8>>
    %735 = llvm.bitcast %733 : !llvm.ptr<ptr<i8>> to !llvm.ptr<i8>
    llvm.store %735, %734 : !llvm.ptr<ptr<i8>>
    %736 = llvm.mlir.constant(1 : i32) : i32
    %737 = llvm.getelementptr %729[%720, 1] : (!llvm.ptr<struct<".8", (ptr<i8>, i64, ptr<f32>, ptr<f32>, i64, i64, i64, i64, i64)>>, i32) -> !llvm.ptr<i64>
    llvm.store %6, %737 : !llvm.ptr<i64>
    %738 = llvm.getelementptr %731[%736] : (!llvm.ptr<ptr<i8>>, i32) -> !llvm.ptr<ptr<i8>>
    %739 = llvm.bitcast %737 : !llvm.ptr<i64> to !llvm.ptr<i8>
    llvm.store %739, %738 : !llvm.ptr<ptr<i8>>
    %740 = llvm.mlir.constant(2 : i32) : i32
    %741 = llvm.getelementptr %729[%720, 2] : (!llvm.ptr<struct<".8", (ptr<i8>, i64, ptr<f32>, ptr<f32>, i64, i64, i64, i64, i64)>>, i32) -> !llvm.ptr<ptr<f32>>
    llvm.store %722, %741 : !llvm.ptr<ptr<f32>>
    %742 = llvm.getelementptr %731[%740] : (!llvm.ptr<ptr<i8>>, i32) -> !llvm.ptr<ptr<i8>>
    %743 = llvm.bitcast %741 : !llvm.ptr<ptr<f32>> to !llvm.ptr<i8>
    llvm.store %743, %742 : !llvm.ptr<ptr<i8>>
    %744 = llvm.mlir.constant(3 : i32) : i32
    %745 = llvm.getelementptr %729[%720, 3] : (!llvm.ptr<struct<".8", (ptr<i8>, i64, ptr<f32>, ptr<f32>, i64, i64, i64, i64, i64)>>, i32) -> !llvm.ptr<ptr<f32>>
    llvm.store %723, %745 : !llvm.ptr<ptr<f32>>
    %746 = llvm.getelementptr %731[%744] : (!llvm.ptr<ptr<i8>>, i32) -> !llvm.ptr<ptr<i8>>
    %747 = llvm.bitcast %745 : !llvm.ptr<ptr<f32>> to !llvm.ptr<i8>
    llvm.store %747, %746 : !llvm.ptr<ptr<i8>>
    %748 = llvm.mlir.constant(4 : i32) : i32
    %749 = llvm.getelementptr %729[%720, 4] : (!llvm.ptr<struct<".8", (ptr<i8>, i64, ptr<f32>, ptr<f32>, i64, i64, i64, i64, i64)>>, i32) -> !llvm.ptr<i64>
    llvm.store %724, %749 : !llvm.ptr<i64>
    %750 = llvm.getelementptr %731[%748] : (!llvm.ptr<ptr<i8>>, i32) -> !llvm.ptr<ptr<i8>>
    %751 = llvm.bitcast %749 : !llvm.ptr<i64> to !llvm.ptr<i8>
    llvm.store %751, %750 : !llvm.ptr<ptr<i8>>
    %752 = llvm.mlir.constant(5 : i32) : i32
    %753 = llvm.getelementptr %729[%720, 5] : (!llvm.ptr<struct<".8", (ptr<i8>, i64, ptr<f32>, ptr<f32>, i64, i64, i64, i64, i64)>>, i32) -> !llvm.ptr<i64>
    llvm.store %725, %753 : !llvm.ptr<i64>
    %754 = llvm.getelementptr %731[%752] : (!llvm.ptr<ptr<i8>>, i32) -> !llvm.ptr<ptr<i8>>
    %755 = llvm.bitcast %753 : !llvm.ptr<i64> to !llvm.ptr<i8>
    llvm.store %755, %754 : !llvm.ptr<ptr<i8>>
    %756 = llvm.mlir.constant(6 : i32) : i32
    %757 = llvm.getelementptr %729[%720, 6] : (!llvm.ptr<struct<".8", (ptr<i8>, i64, ptr<f32>, ptr<f32>, i64, i64, i64, i64, i64)>>, i32) -> !llvm.ptr<i64>
    llvm.store %726, %757 : !llvm.ptr<i64>
    %758 = llvm.getelementptr %731[%756] : (!llvm.ptr<ptr<i8>>, i32) -> !llvm.ptr<ptr<i8>>
    %759 = llvm.bitcast %757 : !llvm.ptr<i64> to !llvm.ptr<i8>
    llvm.store %759, %758 : !llvm.ptr<ptr<i8>>
    %760 = llvm.mlir.constant(7 : i32) : i32
    %761 = llvm.getelementptr %729[%720, 7] : (!llvm.ptr<struct<".8", (ptr<i8>, i64, ptr<f32>, ptr<f32>, i64, i64, i64, i64, i64)>>, i32) -> !llvm.ptr<i64>
    llvm.store %727, %761 : !llvm.ptr<i64>
    %762 = llvm.getelementptr %731[%760] : (!llvm.ptr<ptr<i8>>, i32) -> !llvm.ptr<ptr<i8>>
    %763 = llvm.bitcast %761 : !llvm.ptr<i64> to !llvm.ptr<i8>
    llvm.store %763, %762 : !llvm.ptr<ptr<i8>>
    %764 = llvm.mlir.constant(8 : i32) : i32
    %765 = llvm.getelementptr %729[%720, 8] : (!llvm.ptr<struct<".8", (ptr<i8>, i64, ptr<f32>, ptr<f32>, i64, i64, i64, i64, i64)>>, i32) -> !llvm.ptr<i64>
    llvm.store %728, %765 : !llvm.ptr<i64>
    %766 = llvm.getelementptr %731[%764] : (!llvm.ptr<ptr<i8>>, i32) -> !llvm.ptr<ptr<i8>>
    %767 = llvm.bitcast %765 : !llvm.ptr<i64> to !llvm.ptr<i8>
    llvm.store %767, %766 : !llvm.ptr<ptr<i8>>
    %768 = llvm.mlir.addressof @ral_send_output___cpu___pvoid_i64_m2df32___void : !llvm.ptr<array<48 x i8>>
    %769 = llvm.mlir.constant(0 : index) : i64
    %770 = llvm.getelementptr %768[%769, %769] : (!llvm.ptr<array<48 x i8>>, i64, i64) -> !llvm.ptr<i8>
    llvm.call @disc_ral_call(%arg0, %770, %731) : (!llvm.ptr<i8>, !llvm.ptr<i8>, !llvm.ptr<ptr<i8>>) -> ()
    llvm.return
  }
}


===-------------------------------------------------------------------------===
                         ... Execution time report ...
===-------------------------------------------------------------------------===
  Total Execution Time: 0.5633 seconds

  ----Wall Time----  ----Name----
    0.0004 (  0.1%)  Inliner
    0.0000 (  0.0%)    (A) CallGraph
    0.0001 (  0.0%)  'func.func' Pipeline
    0.0001 (  0.0%)    Canonicalizer
    0.0004 (  0.1%)  'func.func' Pipeline
    0.0000 (  0.0%)    MhloDecompositionRewriterPass
    0.0000 (  0.0%)    RemoveShapeConstraintsPass
    0.0001 (  0.0%)    Canonicalizer
    0.0000 (  0.0%)    CSE
    0.0000 (  0.0%)      (A) DominanceInfo
    0.0001 (  0.0%)    Canonicalizer
    0.0000 (  0.0%)    DiscTranformWeightDataLayoutForWeightOnlyQuantPass
    0.0001 (  0.0%)    Canonicalizer
    0.0000 (  0.0%)    DiscCustomCallRewriterPass
    0.0000 (  0.0%)    DiscConvertFakeQuantOpPass
    0.0000 (  0.0%)    DiscLowerGpuQuantizeAndDequantizePass
    0.0000 (  0.0%)    ConvertShapeToStandardPass
    0.0013 (  0.2%)  DiscShapeOptimizationPass
    0.0014 (  0.2%)  'builtin.func' Pipeline
    0.0013 (  0.2%)    CSE
    0.0000 (  0.0%)      (A) DominanceInfo
    0.0004 (  0.1%)  'func.func' Pipeline
    0.0000 (  0.0%)    ConvertTensorToStandardPass
    0.0000 (  0.0%)    ConvertHloToStandardPass
    0.0001 (  0.0%)    Canonicalizer
    0.0000 (  0.0%)    CSE
    0.0000 (  0.0%)      (A) DominanceInfo
    0.0001 (  0.0%)    Canonicalizer
    0.0000 (  0.0%)    DiscAlgebraicSimplifierPass
    0.0000 (  0.0%)    SplitLargeOpsPass
    0.0000 (  0.0%)    DotRewriterPass
    0.0011 (  0.2%)  DiscShapeOptimizationPass
    0.0001 (  0.0%)  'func.func' Pipeline
    0.0001 (  0.0%)    DiscDotMergePass
    0.0011 (  0.2%)  DiscShapeOptimizationPass
    0.0005 (  0.1%)  'func.func' Pipeline
    0.0005 (  0.1%)    HloCanonicalizeReductionPass
    0.0026 (  0.5%)  DiscShapeOptimizationPass
    0.0007 (  0.1%)  DiscMarkShapeCalculationPass
    0.0007 (  0.1%)  PlaceOpsPass
    0.0004 (  0.1%)  'func.func' Pipeline
    0.0002 (  0.0%)    Canonicalizer
    0.0000 (  0.0%)    CSE
    0.0000 (  0.0%)      (A) DominanceInfo
    0.0001 (  0.0%)    Canonicalizer
    0.0000 (  0.0%)    ElementTypeConverterPass
    0.0017 (  0.3%)  DiscShapeOptimizationPass
    0.0002 (  0.0%)  'func.func' Pipeline
    0.0001 (  0.0%)    ReductionRewriterPass
    0.0000 (  0.0%)    ConvRewriterPass
    0.0000 (  0.0%)    ConvRewriterPass
    0.0000 (  0.0%)    QuantizedDotRewriterPass
    0.0017 (  0.3%)  DiscShapeOptimizationPass
    0.0017 (  0.3%)  'func.func' Pipeline
    0.0002 (  0.0%)    Canonicalizer
    0.0000 (  0.0%)    CSE
    0.0000 (  0.0%)      (A) DominanceInfo
    0.0001 (  0.0%)    Canonicalizer
    0.0013 (  0.2%)    TransposeSimplifierPass
    0.0000 (  0.0%)    GpuConvPaddingLegalizationPass
    0.0017 (  0.3%)  DiscShapeOptimizationPass
    0.0001 (  0.0%)  'func.func' Pipeline
    0.0001 (  0.0%)    DiscAlgebraicSimplifierPass
    0.0019 (  0.3%)  DiscShapeOptimizationPass
    0.0011 (  0.2%)  'func.func' Pipeline
    0.0008 (  0.1%)    Canonicalizer
    0.0000 (  0.0%)    CSE
    0.0000 (  0.0%)      (A) DominanceInfo
    0.0003 (  0.0%)    Canonicalizer
    0.0010 (  0.2%)  FuncBufferize
    0.0013 (  0.2%)  DiscHloLegalizeToLhloPass
    0.0020 (  0.4%)  HloLegalizeToLhloPass
    0.0012 (  0.2%)  'func.func' Pipeline
    0.0012 (  0.2%)    Canonicalizer
    0.0001 (  0.0%)  DiscLhloRewriterPass
    0.0034 (  0.6%)  'func.func' Pipeline
    0.0002 (  0.0%)    Canonicalizer
    0.0001 (  0.0%)    ConvertShapeToStandardPass
    0.0002 (  0.0%)    Canonicalizer
    0.0009 (  0.2%)    CSE
    0.0000 (  0.0%)      (A) DominanceInfo
    0.0002 (  0.0%)    Canonicalizer
    0.0009 (  0.2%)    LegalizeToTensorOpPass
    0.0009 (  0.2%)    Canonicalizer
    0.0001 (  0.0%)    StdBufferizePass
    0.0001 (  0.0%)  ArithBufferize
    0.0037 (  0.6%)  'func.func' Pipeline
    0.0009 (  0.2%)    TensorBufferize
    0.0001 (  0.0%)    FinalizingBufferize
    0.0010 (  0.2%)    Canonicalizer
    0.0008 (  0.1%)    CSE
    0.0000 (  0.0%)      (A) DominanceInfo
    0.0002 (  0.0%)    Canonicalizer
    0.0007 (  0.1%)    DiscMemrefCanonicalizer
    0.0011 (  0.2%)  DiscAssignMemorySpacePass
    0.0072 (  1.3%)  'func.func' Pipeline
    0.0001 (  0.0%)    DiscDuplicateComputationForFusionPass
    0.0008 (  0.1%)    PromoteBuffersToStack
    0.0001 (  0.0%)    DiscMemRefLoadStoreSimplifierPass
    0.0012 (  0.2%)    DiscFusionPass
    0.0000 (  0.0%)    DiscFuseSplatConstPass
    0.0014 (  0.3%)    DiscSpecializeFusionWithSpeculationPass
    0.0015 (  0.3%)    Canonicalizer
    0.0000 (  0.0%)    CSE
    0.0000 (  0.0%)      (A) DominanceInfo
    0.0002 (  0.0%)    Canonicalizer
    0.0002 (  0.0%)    Canonicalizer
    0.0000 (  0.0%)    CSE
    0.0000 (  0.0%)      (A) DominanceInfo
    0.0002 (  0.0%)    Canonicalizer
    0.0013 (  0.2%)    BufferDeallocation
    0.0001 (  0.0%)    DiscBufferDeallocationPass
    0.0016 (  0.3%)  RalInjectExecutionContextPass
    0.0017 (  0.3%)  'func.func' Pipeline
    0.0017 (  0.3%)    DiscLowerToLibraryCallPass
    0.0002 (  0.0%)  DiscConstToRALPass
    0.0661 ( 11.7%)  'func.func' Pipeline
    0.0001 (  0.0%)    DiscMemRefLoadStoreSimplifierPass
    0.0038 (  0.7%)    DiscLhloLegalizeRootsToParallelLoopsPass
    0.0032 (  0.6%)    ExpandOps
    0.0004 (  0.1%)    UnhandledAtomicRMWConverterPass
    0.0035 (  0.6%)    InputInlineFusionPass
    0.0002 (  0.0%)    ForLoopUnrollInterleave
    0.0042 (  0.7%)    ArithExpandOps
    0.0042 (  0.7%)    DiscBF16ExpansionPass
    0.0005 (  0.1%)    FoldMemRefAliasOps
    0.0066 (  1.2%)    DiscFlattenMemrefAccessPass
    0.0061 (  1.1%)    Canonicalizer
    0.0048 (  0.8%)    CSE
    0.0000 (  0.0%)      (A) DominanceInfo
    0.0006 (  0.1%)    Canonicalizer
    0.0008 (  0.1%)    DiscMemRefCSEPass
    0.0041 (  0.7%)    ConvertShapeToStandardPass
    0.0043 (  0.8%)    Canonicalizer
    0.0003 (  0.0%)    CSE
    0.0000 (  0.0%)      (A) DominanceInfo
    0.0006 (  0.1%)    Canonicalizer
    0.0040 (  0.7%)    ParallelLoopCollapsing
    0.0043 (  0.8%)    SCFParallelLoopTiling
    0.0045 (  0.8%)    GpuMapParallelLoopsPass
    0.0052 (  0.9%)    ConvertParallelLoopToGpu
    0.0006 (  0.1%)  'func' Pipeline
    0.0006 (  0.1%)    CSE
    0.0000 (  0.0%)      (A) DominanceInfo
    0.0056 (  1.0%)  GpuLaunchSinkIndexComputations
    0.0064 (  1.1%)  GpuKernelOutlining
    0.0061 (  1.1%)  AssignKernelNamePass
    0.0016 (  0.3%)  'func.func' Pipeline
    0.0016 (  0.3%)    LhloFusionInlinerPass
    0.0004 (  0.1%)  DiscCompIntensFusionToCUDASourcePass
    0.0058 (  1.0%)  ReviseGpuKernelOutliningPass
    0.3347 ( 59.4%)  'gpu.module' Pipeline
    0.0004 (  0.1%)    LoopInvariantCodeMotion
    0.0042 (  0.7%)    'gpu.func' Pipeline
    0.0042 (  0.7%)      SideEffectLoopInvariantCodeMotionPass
    0.0003 (  0.0%)    LoopInvariantCodeMotion
    0.0038 (  0.7%)    CSE
    0.0000 (  0.0%)      (A) DominanceInfo
    0.0040 (  0.7%)    SCFToControlFlow
    0.0039 (  0.7%)    ConvertAffineToStandard
    0.0037 (  0.7%)    StripDebugInfo
    0.0098 (  1.7%)    DiscLowerGpuOpsToNVVMOpsPass
    0.0042 (  0.7%)    'llvm.func' Pipeline
    0.0042 (  0.7%)      LLVMInsertValueSimplifierPass
    0.0042 (  0.7%)    FunctionDeadArgumentEliminationPass
    0.2961 ( 52.6%)    GpuKernelToBlobPass
    0.0004 (  0.1%)  DiscGPUSourceToLibPass
    0.0024 (  0.4%)  'func.func' Pipeline
    0.0018 (  0.3%)    Canonicalizer
    0.0001 (  0.0%)    CSE
    0.0000 (  0.0%)      (A) DominanceInfo
    0.0003 (  0.1%)    Canonicalizer
    0.0001 (  0.0%)    RemoveDeadBufferPass
    0.0002 (  0.0%)    LinalgLowerToLoops
    0.0139 (  2.5%)  SCFToControlFlow
    0.0022 (  0.4%)  'func.func' Pipeline
    0.0002 (  0.0%)    ExpandStridedMetadata
    0.0017 (  0.3%)    Canonicalizer
    0.0001 (  0.0%)    CSE
    0.0000 (  0.0%)      (A) DominanceInfo
    0.0003 (  0.0%)    Canonicalizer
    0.0143 (  2.5%)  ConvertAffineToStandard
    0.0141 (  2.5%)  StripDebugInfo
    0.0139 (  2.5%)  DiscStripShapeConstraintOpsPass
    0.0234 (  4.2%)  DiscToLLVMPass
    0.0056 (  1.0%)  Rest
    0.5633 (100.0%)  Total
[DISC] LowerHLOToLLVM takes: 5.642090e-01 s.
before optimize llvm module:
; ModuleID = 'LLVMDialectModule'
source_filename = "LLVMDialectModule"

%0 = type { ptr, i64, { ptr, ptr, i64, [3 x i64], [3 x i64] } }
%.1 = type { ptr, i64, ptr }
%.9 = type { i64, i64, ptr }
%.10 = type { ptr, ptr, i64, ptr, i64, i64, i64, i64, i64, i64, i32, ptr, i32, ptr }
%.11 = type { i64, ptr, i64, i64, i64, i64, ptr }
%.12 = type { ptr, ptr, i64, ptr, i64, i64, i64, i64, i64, i64, i32, ptr, i32, ptr }
%.2 = type { i64, i64, ptr }
%.3 = type { ptr, ptr, i64, ptr, i64, i64, i64, i64, i64, i64, i32, ptr, i32, ptr }
%.4 = type { i64, ptr, i64, i64, i64, i64, ptr }
%.5 = type { ptr, ptr, i64, ptr, i64, i64, i64, i64, i64, i64, i32, ptr, i32, ptr }
%.6 = type { ptr, ptr, ptr, ptr, i64, i64, i64, ptr, ptr, i64, i64, i64, { ptr, ptr, i64, [2 x i64], [2 x i64] } }
%.7 = type { ptr, ptr }
%.8 = type { ptr, i64, ptr, ptr, i64, i64, i64, i64, i64 }

@main_kernel_0_main_kColReduction_reduce__4_1_0___8w32h_1_kernel_name = internal constant [43 x i8] c"main_kColReduction_reduce__4_1_0___8w32h_1\00"
@main_kernel_0_blob_gpu.binary = internal constant [2128 x i8] c"P\EDU\BA\01\00\10\00@\08\00\00\00\00\00\00\02\00\01\01@\00\00\00\00\08\00\00\00\00\00\00\F9\07\00\00\00\00\00\00\07\00\01\00P\00\00\00\00\00\00\00\00\00\00\00\11 \00\00\00\00\00\00\00\00\00\00\00\00\00\00(\14\00\00\00\00\00\00\A2\7FELF\02\01\013\07\00\01\00f\02\00\BE\00q\00\01\00#\80\13\08\00\11\10\07\00\F5\0E\00P\05P\00@\008\00\03\00@\00\0C\00\01\00\00.shstrtab\00.\08\00'ym\08\00\F0\1B_shndx\00.nv.info\00.text.main_kColReduction_r\0A\00\F5\03e__4_1_0___8w32h_1:\00\0F4\00\1Doshared6\00\1AOrela\A0\00\1F?rel\D5\00\22\9Fconstant09\00\1A\B2debug_frame{\00\09\11\00!nv\14\00\11aE\00\0F\9E\01 \0F\8A\00\17\0F\C9\01\F4\8F$____wg_3\00\17\00\0C\00/27\02\02'o_param\09\02\1C\0F\01\00\05\8C]\00\00\00\03\00\0A\00\01\00\11\C2\18\00,\0B\00\01\00 \9C\01\18\00,\09\00\01\00\11\DC\18\00,\04\00\01\00\11\FA\18\00,\07\00\01\00g2\00\00\00\12\10x\00\11\08\06\00v\00\FF\FF\FF\FF(\00\0C\00\00\01\00@\03\00\04|\08\00\D1\0F\0C\81\80\80(\00\08\FF\81\80(\08\0B\00\06(\00/0\00\01\00\03\13\F0{\00\10\04\9B\00R\04\14\00\00\00E\002\04\AC\01\18\00\80/\08\00\06\00\00\00\0E\0C\00\12#\0C\00\00\01\00'\04\12\0C\00\17\11\0C\00!7\04 \05\F1\08\015\00\00\04\0A\08\00\03\00\00\00`\01(\00\03\19(\00\04\17\0C$\00u\06\00 \00\00\F0!\10\00u\05\00\1C\00\00\F0\11\10\009\04\00\18\10\009\03\00\14\10\009\02\00\10\10\009\01\00\08P\00\01\01\00\F2\0A\F0\11\00\03\1B\FF\00\04\1C\0C\00P\00\00\00\10\06\00\00\10\07\00\00\04\1E\84\01#K\00\01\00v\02\02\08\10\0A/\22b\01\13\08\08\00\13\10\08\00\13\18\08\00\13 \08\00\13(\08\00\130\08\00\228\08\84\00\13\00\08\00\13\08\08\00\13\10\08\00\13\18\08\00\13 \08\00\13(\08\00\130\08\00\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\A2\00\00\00\14,\00\00\00H\00\01\00\11\02h\01\0F\01\00\FF\B0@$v\01\FF\7F\04\B2\FF\00\8E\07\00\C4\0F\00\19y\00\01\00\10%\8B\02a\0E\00\19y\03\00\01\00\10!-\00B\0E\00$z\B5\04\90\03\02\8E\07\00\CA\1F\00\0C\10\00\C5^\00\00p`\F0\03\00\DA\0F\00M\9B\04\F0\0C\80\03\00\EA\0F\00\06{\04\00\00_\00\00\00\90 \00\00\22\0E\00\19x\05\FF\1F\1F\00\D3\14\01\00\00\E2\0F\00$r\02\FF\FF\00\80\00 \E2\0FP\00\10\FF0\00@pP\F4\03\10\00\81\B9z\04\00\00F\00\00#\05p\E2\0F\00\11r\05\05?\00\B2\FF@\8F\07\00\C6\0F\00\08s\04\F6\04\00 \09\F1\07$\1E\00\10x\03\04\FE\FF\FF\0F\FF\E0\FF\07\00\CC\1F\00\05s\03$\04\00\C0\03\01\C0\00!r\07p\00\C0\03\0A\8E\07\00\C8\1F\00$z\07\07p\00\10\FF\D0\00\81\C8\0F\00'r\07\03\07\E0\02\02\90\00@\19x\02\FF\01\03\10\05\B0\00q\E4\0F\00\12x\05\05\F1\04!\C0\8E\80\00T'r\07\07\02\C0\00\10\C8\D0\00\11\09`\00\10\07`\00`\E4\0F\00$x\05\BC\03$\00\05\10\000z\03\09p\00\22\02\02@\00@\19x\00\FF\A0\00\22\05\16`\00f\0Cz\00\03\00_P\010\10\0A\03\10\00\11\80\D0\00p\E4\0F\00\10\08\07\07P\00\04\10\00\060\00\12\F20\00\1A\18 \001\12\AA\07P\01 \FF3\B0\01\00\A0\00\1B\03\A0\000\00\07  \01\15\02\A0\00\15\03\A0\00\10\E2\F0\00 \02\05\10\01#\FF\C0@\00Sx\09\03\08\00 \00\11\CA\80\003\09\00X\80\00\22\C8\0F\10\02p\\\00\00p\10\F2\04\90\00T$\14\03\FF\04 \01\00`\000\1A\02\000\00\13\09p\01c%\16\02\02\00ZP\02q\CC\0F\00\81\19\02\02\D0\01\C4\19\1E\0C\00\A2\00\00\10x\06\00\10\D0\00\000\02\92t\04\FF\00\00\80\FF\FF\00\A0\001\1Cx\00\01\001p\F0\F00\02E$x\07\05p\00q\E2\0F\04\0Cz\00\06\90\00#`\F6 \00\16\06 \00\80\C6\0F\00\0Cx\00\05\7F@\00\C5D\F6\01\00\E4\0F\04\10x\03\00\08p\00`\1F\00!\12\04\02q\00\01\F1\04!\E2O0\00\11\070\00 \F2\04\C0\00c\88s\00\07\04\00\DE\05f\E8\0F\00\1D{\00\01\00u\EC\0F\00\84\B9\0B\06~\05R\22\0E\00\10x\B0\02\01`\00Q\C4\0F\00\10x\1E\00\03p\00\000\004\02\06\00P\00qb\0E\00\0B\B2\00\0B\06\08\B1\80\F4\03\00\E4\1F\08\0B\B2\00\02\10\00\A3\C0\F8\03\00\E4/\00\1C\B8\00\00\01%p\01\10\02\01\F0\00B\F4\03\00\C8\E0\00\11?\B0\00\C3t\01\00\CE\0F\00\08\82\0B\0B\02\00'\07\1B\E4@\01\8F\C6\0F\00\88\B3\00\06\0B\D0\00\095\A9\03\06\8E\06[(\0E\00\84\A9\B0\001\A2\00\03p\03`\80\F6\03\00\C4\1F\10\00(\02\03\B0\00$\A8\00p\00\04\B0\00\15\04\A0\01\03\B0\00\14\1F\90\01\01\B0\00/\03\03\B0\00\0AO\A3\00\06\03\80\01\0AV\07\06\00\80\00\B0\00\0E`\01$\07\07`\01\00\B0\00O\B2\00\02\07`\01\0B\1C\00`\01\19\0F`\01\02\10\04\0F`\01\09\1F\07`\01\0D\17@\B0\00\13\A9g\07\0F`\01\07\1F\00`\01\06\12\DA@\01\14\00\90\00\1F\CA0\01\0F9M\19\00\C0\05G$t\02\FFP\03A\00\84y\05_\09\01@\00\96&\0E\00%v\02\09\00`0\04'\84y\B0\00fh\0E\00\81y\08\D0\03bb\05\00\0Br\00\C1\05\22\80\F0\C0\001r\00\00\10\00\92\C0\F2\03\00\D6/\00\08\82\1F\00p\00\00\80\04\00\C8O \00\11\08\11\00 @\F0\C0\03$\0EF\A0\06\00\A0\00b\E6\0F\00\08r\09 \00\00\01\00p\CC\0F\00\A9s\09\02{\00\C0\09\E1\1E\00\00\A4\0E\00\0Cr\00\09\10\00 pR@\00QO\00$r\08\00\05\10\09\D0\00\80\D8\0F\00G\09\00\00\90\D0\05!\FF\83\F0\00*My\00\01TGy\00\00\F0 \00f\C0\0F\00\18y\00\01\00\0F\10\00\B0\0F\01\00-#\01\00\80\02\0B\01\00\22@\00\01\00=\9E\01\000\00\08\01\00\1F\0B@\00\04\13\DE)\00?\09\02\00@\00\0A\22\13\00@\05\0C\01\00\13\E8U\00\03\0F\03\01$\00\13\05\97\02\00\01\00\22\18\00\01\00.q\01T\00\00\01\00\11\90\B5\02O\00\00p\00\80\00\0B\1F)'\00\03\03\D5\02$\00\00\18\0D\04\E4\00*\04\00\01\00\1Fc@\00\04*0\05\C0\00\13\03\03\09\0C@\00!\8F\01D\01\0D@\00\13\D8@\00*\D8\00\01\00\1B\08\08\00?~\01\00N\0E\002\00\00\B0\C6\03\01W\09\04\80\00\17\048\00\04\18\00\138@\01\0C\84\01\13\C0@\00\17\881\01\0F\C0\00\01\132T\01\02\E6\07\06\01\00\1B\80\A9\00\11\03$\00J\00\0E\80\00\01\00\13\97#\00*\03\00\01\00\040\13/\00\04\80\00\0B\13\06\AB\01\04h\13\0C\01\00\1B\A8\08\00\17\08\08\02\17\05\E8\00\0C\01\00*\C0\09\08\00\088\00\18\06\A0\00\0F\01\00\05\03\A9\00\80\08\00\00\00\00\00\00\00\00\00\00\00\00\00\00"
@main_kernel_main_kColReduction_reduce__4_1_0___8w32h_kernel_name = internal constant [41 x i8] c"main_kColReduction_reduce__4_1_0___8w32h\00"
@main_kernel_blob_gpu.binary = internal constant [1096 x i8] c"P\EDU\BA\01\00\10\008\04\00\00\00\00\00\00\02\00\01\01@\00\00\00\F8\03\00\00\00\00\00\00\F8\03\00\00\00\00\00\00\07\00\01\00P\00\00\00\00\00\00\00\00\00\00\00\11 \00\00\00\00\00\00\00\00\00\00\00\00\00\00\E8\0B\00\00\00\00\00\00\A2\7FELF\02\01\013\07\00\01\00f\02\00\BE\00q\00\01\00!@\0B\07\001\00\80\08\07\00\F5\0E\00P\05P\00@\008\00\03\00@\00\0B\00\01\00\00.shstrtab\00.\08\00'ym\08\00\F0\1B_shndx\00.nv.info\00.text.main_kColReduction_r\0A\00\F5\01e__4_1_0___8w32h8\00\0F2\00\1Boshared4\00\1B\9Fconstant07\00\18\FA\01debug_frame\00.rel\11\00!nv\14\00\11aC\00\0F+\01 \0F\88\00\15\0FT\01\BAo_param[\01\1C\0F\01\00\06\8C[\00\00\00\03\00\0A\00\01\00\11\F0\18\00,\09\00\01\00 .\01\18\00,\04\00\01\00\11L\18\00,\07\00\01\00f2\00\00\00\12\10`\00!\80\01\07\00v\00\FF\FF\FF\FF(\00\0C\00\00\01\00@\03\00\04|\08\00\D1\0F\0C\81\80\80(\00\08\FF\81\80(\08\0B\00\06(\00/0\00\01\00\03\13pX\00\10\04\9B\00R\04\14\00\00\00E\00\01\0B\00\00\13\00p/\08\00\05\00\00\00\A7\03\22\04#\0C\00\00\01\00'\04\12\0C\00\17\11\0C\00!7\04\E8\03\F3\08\015\00\00\04\0A\08\00\02\00\00\00`\01\10\00\03\19\10\00\04\17\0C\0C\04U\08\00\00\F0!\10\00\10\01\18\01%\F0\11\10\00\01\01\00\F2\02\F0\11\00\03\1B\FF\00\04\1C\08\00P\00\00\00\B0\00\01\00#K\00\01\00s\02\02\08\10\0A/\22\9B\00\00\07\00\03\08\00\13\10\08\00\13\18\08\00\13 \08\00\13(\08\00\130\08\00\228\08|\00\13\00\08\00\13\08\08\00\13\10\08\00\13\18\08\00\13 \08\00\13(\08\00\130\08\00\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\A2\00\00\00\14,\00\00\00H\00\01\00\000\01/\05\00\01\00\FF\B0A\02z\01\00\1F\04\B1\0F\00\00\00\C4\0F\00\19y\02\00\01\00\10%\8B\02Q\0E\00\19y\03\0F\00\F5\1A\00!\00\00\00$\0E\00$z\02\02\00X\00\00\03\02\8E\07\00\CA\1F\00\0Cz\00\02\00Y\00\00p`\F0\03\00\DA\0F\00MS\04\A0\80\03\00\EA\0F\005t\03\FF\B3\03\10\FF\C0\03P\E2\0F\00\02x6\02B\80\FF\00\0F\10\00r\B9z\04\00\00F\00\84\00\94\D0\0F\00%v\02\02\00Z`\00`\0F\00\86y\00\022\00@\04\19\10\0C0\009My\00`\00PGy\00\00\F09\04\A6\FF\83\03\00\C0\0F\00\18y\00\01\00\0F\10\00\90\0F\01\00-\00W\01.\03\00\01\00\22@\00\01\00=+\01\000\00\08\01\00\1F\0B@\00\04\13k)\00\1F[@\00\0C\13\13\0C\04\0C\01\00\13\C8\15\00&\90\000\04#\04\00\85\04\00\F6\04\12\00\01\00\1F\FET\00\00\00\01\00\13X\95\00/p\00\80\00\0B\1F)'\00\03#\00\C8@\00\04P\06\04\E4\00*\04\00\01\00\1Fa@\00\04\13\F81\00&\\\00@\00\1F\0A@\00\00!\1C\01D\01\0D@\00\13X)\00*\D8\00\01\00\1B\08\08\00?\0B\01\00\86\07\00Q\00\000\05\00\01\00&\10\00\80\00\17\048\00\04\18\00\13\C7\14\01\0C\84\01*@\058\07\1F\00\C0\00\04\132@\00+\06\00\01\00\1A\07\D0\07\12\03\F0\05:\08\80\00\01\00\13\06\08\06\04(\0B\0C\01\00*\A8\00\08\00\04\F8\00\14\018\00/\05\00\01\00\029@\03\00\08\00\088\00/\06\00\01\00\17\80\08\00\00\00\00\00\00\00"
@ral_send_output___cpu___pvoid_i64_m2df32___void = internal constant [48 x i8] c"ral_send_output___cpu___pvoid_i64_m2df32___void\00"
@dealloc___gpu___pvoid_pvoid___void = internal constant [35 x i8] c"dealloc___gpu___pvoid_pvoid___void\00"
@inc_ref___gpu___pvoid_pvoid_m1df32_m1di64___m2df32 = internal constant [51 x i8] c"inc_ref___gpu___pvoid_pvoid_m1df32_m1di64___m2df32\00"
@main_kernel_2_main_kColReduction_reduce__4_1_0___8w16h_1_kernel_name = internal constant [43 x i8] c"main_kColReduction_reduce__4_1_0___8w16h_1\00"
@main_kernel_2_blob_gpu.binary = internal constant [2080 x i8] c"P\EDU\BA\01\00\10\00\10\08\00\00\00\00\00\00\02\00\01\01@\00\00\00\D0\07\00\00\00\00\00\00\CB\07\00\00\00\00\00\00\07\00\01\00P\00\00\00\00\00\00\00\00\00\00\00\11 \00\00\00\00\00\00\00\00\00\00\00\00\00\00(\13\00\00\00\00\00\00\A2\7FELF\02\01\013\07\00\01\00f\02\00\BE\00q\00\01\00#\80\12\08\00\11\0F\07\00\F5\0E\00P\05P\00@\008\00\03\00@\00\0C\00\01\00\00.shstrtab\00.\08\00'ym\08\00\F0\1B_shndx\00.nv.info\00.text.main_kColReduction_r\0A\00\F5\03e__4_1_0___8w16h_1:\00\0F4\00\1Doshared6\00\1AOrela\A0\00\1F?rel\D5\00\22\9Fconstant09\00\1A\B2debug_frame{\00\09\11\00!nv\14\00\11aE\00\0F\9E\01 \0F\8A\00\17\0F\C9\01\F4\8F$____wg_3\00\17\00\0C\00/27\02\02'o_param\09\02\1C\0F\01\00\05\8C]\00\00\00\03\00\0A\00\01\00\11\C2\18\00,\0B\00\01\00 \9C\01\18\00,\09\00\01\00\11\DC\18\00,\04\00\01\00\11\FA\18\00,\07\00\01\00g2\00\00\00\12\10x\00\03#\00f\FF\FF\FF\FF(\00\0C\00\00\01\00@\03\00\04|\08\00\D1\0F\0C\81\80\80(\00\08\FF\81\80(\08\0B\00\06(\00/0\00\01\00\03!\F0\06\07\00 \00\04\9B\00R\04\14\00\00\00E\002\04|\01\18\000/\08\00#\00\10\0E\0C\00\12#\0C\00\00\01\00'\04\12\0C\00\17\11\0C\00!7\04 \05\F1\08\015\00\00\04\0A\08\00\03\00\00\00`\01(\00\03\19(\00\04\17\0C$\00u\06\00 \00\00\F0!\10\00u\05\00\1C\00\00\F0\11\10\009\04\00\18\10\009\03\00\14\10\009\02\00\10\10\009\01\00\08P\00\01\01\00\C1\F0\11\00\03\1B\FF\00\04\1C\0C\00P\98\05\82\00\00P\06\00\00\04\1E\84\01#K\00\01\00\B1\02\02\08\10\0A/\22\00\00\00\08\10\00#\00\08\08\00\13\10\08\00\13\18\08\00\13 \08\00\13(\08\00\130\08\00\228\08\84\00\13\00\08\00\13\08\08\00\13\10\08\00\13\18\08\00\13 \08\00\13(\08\00\130\08\00\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\A2\00\00\00\14,\00\00\00H\00\01\00\11\02h\01\0F\01\00\FF\B0@$v\01\FF\7F\04\B2\FF\00\8E\07\00\C4\0F\00\19y\00\01\00\10%\8B\02a\0E\00\19y\03\00\01\00\10!-\00B\0E\00$z\B5\04\90\03\02\8E\07\00\CA\1F\00\0C\10\00\C5^\00\00p`\F0\03\00\DA\0F\00M\9B\04\F0\0C\80\03\00\EA\0F\00\06{\04\00\00_\00\00\00\90 \00\00\22\0E\00\19x\05\FF\1F\1F\00\D3\14\01\00\00\E2\0F\00$r\02\FF\FF\00\80\00 \E2\0FP\00\10\FF0\00@pP\F4\03\10\00\81\B9z\04\00\00F\00\00#\05p\E2\0F\00\11r\05\05?\00\B2\FF8\8F\07\00\C6\0F\00\08s\04\F6\04\10\10\A0\00\F1\06\1E\00\10x\03\04\FE\FF\FF\0F\FF\E0\FF\07\00\CC\1F\00\05s\03$\04\00\C0\03\01\C0\00!r\07p\00\C0\03\0A\8E\07\00\C8\1F\00$z\07\07p\00\10\FF\D0\00\81\C8\0F\00'r\07\03\07\E0\02\02\90\00@\19x\02\FF\10\00\10\05\B0\00\80\E4\0F\00\12x\05\05\80\F1\04!\C0\8E\80\00T'r\07\07\02\C0\00\10\C8\D0\00\11\09`\00\10\07`\00`\E4\0F\00$x\05\BC\03$\00\05\10\000z\03\09p\00\22\02\02@\00@\19x\00\FF\A0\00\22\05\16`\00f\0Cz\00\03\00_P\010\10\0A\03\10\00\11\80\D0\00p\E4\0F\00\10\08\07\07P\00\04\10\00\060\00\12\F20\00\1A\18 \001\12\AA\07P\01 \FF3\B0\01\00\A0\00\1B\03\A0\001\00\07\10\D1\03\05\A0\00\15\03\A0\00\10\E2\F0\00 \02\05\00\01#\FF\C0@\00Sx\09\03\08\00 \00\11\CA\80\003\09\00X\80\00\22\C8\0F\10\02p\\\00\00p\10\F2\04\90\00T$\14\03\FF\04 \01\00`\000\1A\02\000\00\13\09p\01c%\16\02\02\00ZP\02q\CC\0F\00\81\19\02\02\D0\01\C4\19\1E\0C\00\A2\00\00\10x\06\00\08\D0\00\000\02\92t\04\FF\00\00\80\FF\FF\00\A0\001\1Cx\00\01\001p\F0\F00\02E$x\07\05p\00q\E2\0F\04\0Cz\00\06\90\00#`\F6 \00\16\06 \00\80\C6\0F\00\0Cx\00\05?@\00\92D\F6\01\00\E4\0F\04\10xF\07\02@\01B\1F\04\10x\EC\04\04\80\00@!\12\04\02\81\00\01\15\00!\E2O@\00\11\07@\00 \F2\04\D0\00c\88s\00\07\04\00\EE\05f\E8\0F\00\1D{\00\01\00u\EC\0F\00\84\B9\0B\06\CE\05\84(\0E\00\84\B9\02\06\000\00qb\0E\00\0B\B2\00\0B\F6\07`\80\F4\03\00\C4\1F\10\00\11\02\10\00\A3\C0\F8\03\00\E4/\00\1C\B8\00\F0\00%p\01\00\02\01\E0\00B\F4\03\00\C8\D0\00\11\1F\90\00\C3t\01\00\CE\0F\00\08\82\0B\0B\02\00\17\07\1B\E40\01\8F\C6\0F\00\88\B3\00\06\0B\B0\00\09f\A9\03\06\00\80\00\B0\00\1B\A9\B0\001\A2\00\03`\03\22\80\F6\B0\00H\A2\00\02\03\B0\00$\A8\00p\00\04\B0\00\15\00\90\01\03\B0\00\14\0F\80\01\01\B0\00/\03\03\B0\00\0AO\A3\00\06\03`\01\0AG\07\06\00@\B0\00\13\B9\A7\06\06`\01*\07\07`\01/\00\07`\01\05\10\DA\90\00\02/\00\01\90\00\12\CA0\01\1F\07\80\00\089M\19\00\00\05G$t\02\FF\90\02A\00\84y\05\9F\08\01@\00\96&\0E\00%v\02\09\00`p\03'\84y\B0\00fh\0E\00\81y\08\10\03bb\05\00\0Br\00\01\05\22\80\F0\C0\001r\00\00\10\00\92\C0\F2\03\00\D6/\00\08\82\1F\00p\00\00\80\04\00\C8O \00\11\08\11\00 @\F0\00\03$\0EF\E0\05\00\A0\00b\E6\0F\00\08r\09 \00\00\01\00p\CC\0F\00\A9s\09\02{\00\C0\09\E1\1E\00\00\A4\0E\00\0Cr\00\09\10\00 pR@\00QO\00$r\08@\04\10\09\D0\00\80\D8\0F\00G\09\00\00\90\10\05!\FF\83\F0\00*My\00\01TGy\00\00\F0 \00f\C0\0F\00\18y\00\01\00\0F\10\00p\0F\01\00-\11\01@\0A\0E\01\00\22@\00\01\00=\9E\01\000\00\08\01\00\1F\0B@\00\04\13\DE)\00?\09\02\00@\00\0A\22\13\00\A0\04\0C\01\00\13\E8U\00\03\7F\03\01$\00\13\05W\02\00\01\00\22\18\00\01\00.q\01T\00\00\01\00\11\90u\02O\00\00p\00\80\00\0B\1F)'\00\03\03\95\02$\00\00\18\0C\04\E4\00*\04\00\01\00\1Fc@\00\04*0\05\C0\00\13\03\03\08\0C@\00!\8F\01D\01\0D@\00\13\D8@\00*\D8\00\01\00\1B\08\08\00?~\01\00N\0D\002\00\00\B0\86\03\01W\08\04\80\00\17\048\00\04\18\00\138@\01\0C\84\01\13\C0@\00\17\881\01\0F\C0\00\01\132T\01\15\06R\00\03>\03\1A\08\98\0D\11\03$\00J\00\0E\80\00\01\00\13\97\94\00*\03\00\01\00\040\12/\00\02\80\00\0B\13\06\AB\01\04h\12\0C\01\00\1B\A8\08\00\04\97\00\13\018\00\04\E8\00\0C\01\00*\C0\08\08\00\088\00\18\06\A0\00\0F\01\00\05\03\B8\00\80\08\00\00\00\00\00\00\00\00\00\00\00\00"
@ral_kernel_launch___gpu___pvoid_ppvoid_i64_pvoid_i64_i64_i64_i64_i64_i64_i32_pvoid_i32_ppvoid___void = internal constant [101 x i8] c"ral_kernel_launch___gpu___pvoid_ppvoid_i64_pvoid_i64_i64_i64_i64_i64_i64_i32_pvoid_i32_ppvoid___void\00"
@main_kernel_1_main_kColReduction_reduce__4_1_0___8w16h_kernel_name = internal constant [41 x i8] c"main_kColReduction_reduce__4_1_0___8w16h\00"
@main_kernel_1_blob_gpu.binary = internal constant [1096 x i8] c"P\EDU\BA\01\00\10\008\04\00\00\00\00\00\00\02\00\01\01@\00\00\00\F8\03\00\00\00\00\00\00\F8\03\00\00\00\00\00\00\07\00\01\00P\00\00\00\00\00\00\00\00\00\00\00\11 \00\00\00\00\00\00\00\00\00\00\00\00\00\00\E8\0B\00\00\00\00\00\00\A2\7FELF\02\01\013\07\00\01\00f\02\00\BE\00q\00\01\00!@\0B\07\001\00\80\08\07\00\F5\0E\00P\05P\00@\008\00\03\00@\00\0B\00\01\00\00.shstrtab\00.\08\00'ym\08\00\F0\1B_shndx\00.nv.info\00.text.main_kColReduction_r\0A\00\F5\01e__4_1_0___8w16h8\00\0F2\00\1Boshared4\00\1B\9Fconstant07\00\18\FA\01debug_frame\00.rel\11\00!nv\14\00\11aC\00\0F+\01 \0F\88\00\15\0FT\01\BAo_param[\01\1C\0F\01\00\06\8C[\00\00\00\03\00\0A\00\01\00\11\F0\18\00,\09\00\01\00 .\01\18\00,\04\00\01\00\11L\18\00,\07\00\01\00f2\00\00\00\12\10`\00!\80\01\07\00v\00\FF\FF\FF\FF(\00\0C\00\00\01\00@\03\00\04|\08\00\D1\0F\0C\81\80\80(\00\08\FF\81\80(\08\0B\00\06(\00/0\00\01\00\03\13pX\00\10\04\9B\00R\04\14\00\00\00E\00\01\0B\00\00\13\00p/\08\00\05\00\00\00\A7\03\22\04#\0C\00\00\01\00'\04\12\0C\00\17\11\0C\00!7\04\E8\03\F3\08\015\00\00\04\0A\08\00\02\00\00\00`\01\10\00\03\19\10\00\04\17\0C\0C\04U\08\00\00\F0!\10\00\10\01\18\01%\F0\11\10\00\01\01\00\F2\02\F0\11\00\03\1B\FF\00\04\1C\08\00P\00\00\00\B0\00\01\00#K\00\01\00s\02\02\08\10\0A/\22\9B\00\00\07\00\03\08\00\13\10\08\00\13\18\08\00\13 \08\00\13(\08\00\130\08\00\228\08|\00\13\00\08\00\13\08\08\00\13\10\08\00\13\18\08\00\13 \08\00\13(\08\00\130\08\00\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\A2\00\00\00\14,\00\00\00H\00\01\00\000\01/\05\00\01\00\FF\B0A\02z\01\00\1F\04\B1\0F\00\00\00\C4\0F\00\19y\02\00\01\00\10%\8B\02Q\0E\00\19y\03\0F\00\F5\1A\00!\00\00\00$\0E\00$z\02\02\00X\00\00\03\02\8E\07\00\CA\1F\00\0Cz\00\02\00Y\00\00p`\F0\03\00\DA\0F\00MS\04\A0\80\03\00\EA\0F\005t\03\FF\B3\03\10\FF\C0\03P\E2\0F\00\02x6\02B\80\FF\00\0F\10\00r\B9z\04\00\00F\00\84\00\94\D0\0F\00%v\02\02\00Z`\00`\0F\00\86y\00\022\00@\04\19\10\0C0\009My\00`\00PGy\00\00\F09\04\A6\FF\83\03\00\C0\0F\00\18y\00\01\00\0F\10\00\90\0F\01\00-\00W\01.\03\00\01\00\22@\00\01\00=+\01\000\00\08\01\00\1F\0B@\00\04\13k)\00\1F[@\00\0C\13\13\0C\04\0C\01\00\13\C8\15\00&\90\000\04#\04\00\85\04\00\F6\04\12\00\01\00\1F\FET\00\00\00\01\00\13X\95\00/p\00\80\00\0B\1F)'\00\03#\00\C8@\00\04P\06\04\E4\00*\04\00\01\00\1Fa@\00\04\13\F81\00&\\\00@\00\1F\0A@\00\00!\1C\01D\01\0D@\00\13X)\00*\D8\00\01\00\1B\08\08\00?\0B\01\00\86\07\00Q\00\000\05\00\01\00&\10\00\80\00\17\048\00\04\18\00\13\C7\14\01\0C\84\01*@\058\07\1F\00\C0\00\04\132@\00+\06\00\01\00\1A\07\D0\07\12\03\F0\05:\08\80\00\01\00\13\06\08\06\04(\0B\0C\01\00*\A8\00\08\00\04\F8\00\14\018\00/\05\00\01\00\029@\03\00\08\00\088\00/\06\00\01\00\17\80\08\00\00\00\00\00\00\00"
@alloc___gpu___pvoid_i64___pvoid = internal constant [32 x i8] c"alloc___gpu___pvoid_i64___pvoid\00"
@ral_recv_input___cpu___pvoid_i64___m3df32 = internal constant [42 x i8] c"ral_recv_input___cpu___pvoid_i64___m3df32\00"

declare ptr @malloc(i64)

declare void @free(ptr)

declare void @disc_ral_call(ptr, ptr, ptr)

define void @main(ptr %0) {
  %2 = alloca %0, align 8
  %3 = alloca ptr, i32 3, align 8
  %4 = getelementptr %0, ptr %2, i32 0, i32 0
  store ptr %0, ptr %4, align 8
  %5 = getelementptr ptr, ptr %3, i32 0
  store ptr %4, ptr %5, align 8
  %6 = getelementptr %0, ptr %2, i32 0, i32 1
  store i64 0, ptr %6, align 4
  %7 = getelementptr ptr, ptr %3, i32 1
  store ptr %6, ptr %7, align 8
  %8 = getelementptr %0, ptr %2, i32 0, i32 2
  %9 = getelementptr ptr, ptr %3, i32 2
  store ptr %8, ptr %9, align 8
  call void @disc_ral_call(ptr %0, ptr @ral_recv_input___cpu___pvoid_i64___m3df32, ptr %3)
  %10 = load { ptr, ptr, i64, [3 x i64], [3 x i64] }, ptr %8, align 8
  %11 = extractvalue { ptr, ptr, i64, [3 x i64], [3 x i64] } %10, 3, 2
  %12 = extractvalue { ptr, ptr, i64, [3 x i64], [3 x i64] } %10, 3, 1
  %13 = extractvalue { ptr, ptr, i64, [3 x i64], [3 x i64] } %10, 3, 0
  %14 = trunc i64 %12 to i32
  %15 = trunc i64 %11 to i32
  %16 = mul i32 %14, %15
  %17 = sext i32 %16 to i64
  %18 = getelementptr float, ptr null, i64 %17
  %19 = ptrtoint ptr %18 to i64
  %20 = alloca %.1, align 8
  %21 = alloca ptr, i32 3, align 8
  %22 = getelementptr %.1, ptr %20, i32 0, i32 0
  store ptr %0, ptr %22, align 8
  %23 = getelementptr ptr, ptr %21, i32 0
  store ptr %22, ptr %23, align 8
  %24 = getelementptr %.1, ptr %20, i32 0, i32 1
  store i64 %19, ptr %24, align 4
  %25 = getelementptr ptr, ptr %21, i32 1
  store ptr %24, ptr %25, align 8
  %26 = getelementptr %.1, ptr %20, i32 0, i32 2
  %27 = getelementptr ptr, ptr %21, i32 2
  store ptr %26, ptr %27, align 8
  call void @disc_ral_call(ptr %0, ptr @alloc___gpu___pvoid_i64___pvoid, ptr %21)
  %28 = load ptr, ptr %26, align 8
  %29 = insertvalue { ptr, ptr, i64, [1 x i64], [1 x i64] } undef, ptr %28, 0
  %30 = insertvalue { ptr, ptr, i64, [1 x i64], [1 x i64] } %29, ptr %28, 1
  %31 = insertvalue { ptr, ptr, i64, [1 x i64], [1 x i64] } %30, i64 0, 2
  %32 = insertvalue { ptr, ptr, i64, [1 x i64], [1 x i64] } %31, i64 %17, 3, 0
  %33 = insertvalue { ptr, ptr, i64, [1 x i64], [1 x i64] } %32, i64 1, 4, 0
  %34 = mul i64 %13, %17
  %35 = add i64 %34, -1
  %36 = sdiv i64 %35, 256
  %37 = add i64 %36, 1
  %38 = sub i64 0, %34
  %39 = sdiv i64 %38, 256
  %40 = sub i64 0, %39
  %41 = icmp sgt i64 %34, 0
  %42 = select i1 %41, i64 %37, i64 %40
  %43 = icmp sgt i64 %42, 108
  br i1 %43, label %44, label %186

44:                                               ; preds = %1
  %45 = icmp sle i64 %17, 0
  %46 = sub i64 0, %17
  %47 = sub i64 %17, 1
  %48 = select i1 %45, i64 %46, i64 %47
  %49 = sdiv i64 %48, 256
  %50 = sub i64 0, %49
  %51 = add i64 %49, 1
  %52 = select i1 %45, i64 %50, i64 %51
  %53 = alloca ptr, align 8
  %54 = getelementptr ptr, ptr %53, i32 0
  store ptr @main_kernel_blob_gpu.binary, ptr %54, align 8
  %55 = extractvalue { ptr, ptr, i64, [1 x i64], [1 x i64] } %33, 0
  %56 = extractvalue { ptr, ptr, i64, [1 x i64], [1 x i64] } %33, 1
  %57 = extractvalue { ptr, ptr, i64, [1 x i64], [1 x i64] } %33, 2
  %58 = extractvalue { ptr, ptr, i64, [1 x i64], [1 x i64] } %33, 3, 0
  %59 = extractvalue { ptr, ptr, i64, [1 x i64], [1 x i64] } %33, 4, 0
  %60 = alloca %.9, align 8
  %61 = alloca ptr, i32 3, align 8
  %62 = getelementptr %.9, ptr %60, i32 0, i32 0
  store i64 256, ptr %62, align 4
  %63 = getelementptr ptr, ptr %61, i32 0
  store ptr %62, ptr %63, align 8
  %64 = getelementptr %.9, ptr %60, i32 0, i32 1
  store i64 %17, ptr %64, align 4
  %65 = getelementptr ptr, ptr %61, i32 1
  store ptr %64, ptr %65, align 8
  %66 = getelementptr %.9, ptr %60, i32 0, i32 2
  store ptr %56, ptr %66, align 8
  %67 = getelementptr ptr, ptr %61, i32 2
  store ptr %66, ptr %67, align 8
  %68 = alloca %.10, align 8
  %69 = alloca ptr, i32 14, align 8
  %70 = getelementptr %.10, ptr %68, i32 0, i32 0
  store ptr %0, ptr %70, align 8
  %71 = getelementptr ptr, ptr %69, i32 0
  store ptr %70, ptr %71, align 8
  %72 = getelementptr %.10, ptr %68, i32 0, i32 1
  store ptr %53, ptr %72, align 8
  %73 = getelementptr ptr, ptr %69, i32 1
  store ptr %72, ptr %73, align 8
  %74 = getelementptr %.10, ptr %68, i32 0, i32 2
  store i64 1, ptr %74, align 4
  %75 = getelementptr ptr, ptr %69, i32 2
  store ptr %74, ptr %75, align 8
  %76 = getelementptr %.10, ptr %68, i32 0, i32 3
  store ptr @main_kernel_main_kColReduction_reduce__4_1_0___8w32h_kernel_name, ptr %76, align 8
  %77 = getelementptr ptr, ptr %69, i32 3
  store ptr %76, ptr %77, align 8
  %78 = getelementptr %.10, ptr %68, i32 0, i32 4
  store i64 %52, ptr %78, align 4
  %79 = getelementptr ptr, ptr %69, i32 4
  store ptr %78, ptr %79, align 8
  %80 = getelementptr %.10, ptr %68, i32 0, i32 5
  store i64 1, ptr %80, align 4
  %81 = getelementptr ptr, ptr %69, i32 5
  store ptr %80, ptr %81, align 8
  %82 = getelementptr %.10, ptr %68, i32 0, i32 6
  store i64 1, ptr %82, align 4
  %83 = getelementptr ptr, ptr %69, i32 6
  store ptr %82, ptr %83, align 8
  %84 = getelementptr %.10, ptr %68, i32 0, i32 7
  store i64 256, ptr %84, align 4
  %85 = getelementptr ptr, ptr %69, i32 7
  store ptr %84, ptr %85, align 8
  %86 = getelementptr %.10, ptr %68, i32 0, i32 8
  store i64 1, ptr %86, align 4
  %87 = getelementptr ptr, ptr %69, i32 8
  store ptr %86, ptr %87, align 8
  %88 = getelementptr %.10, ptr %68, i32 0, i32 9
  store i64 1, ptr %88, align 4
  %89 = getelementptr ptr, ptr %69, i32 9
  store ptr %88, ptr %89, align 8
  %90 = getelementptr %.10, ptr %68, i32 0, i32 10
  store i32 0, ptr %90, align 4
  %91 = getelementptr ptr, ptr %69, i32 10
  store ptr %90, ptr %91, align 8
  %92 = getelementptr %.10, ptr %68, i32 0, i32 11
  store ptr null, ptr %92, align 8
  %93 = getelementptr ptr, ptr %69, i32 11
  store ptr %92, ptr %93, align 8
  %94 = getelementptr %.10, ptr %68, i32 0, i32 12
  store i32 3, ptr %94, align 4
  %95 = getelementptr ptr, ptr %69, i32 12
  store ptr %94, ptr %95, align 8
  %96 = getelementptr %.10, ptr %68, i32 0, i32 13
  store ptr %61, ptr %96, align 8
  %97 = getelementptr ptr, ptr %69, i32 13
  store ptr %96, ptr %97, align 8
  call void @disc_ral_call(ptr %0, ptr @ral_kernel_launch___gpu___pvoid_ppvoid_i64_pvoid_i64_i64_i64_i64_i64_i64_i32_pvoid_i32_ppvoid___void, ptr %69)
  %98 = add i64 %17, -1
  %99 = sdiv i64 %98, 8
  %100 = add i64 %99, 1
  %101 = sub i64 0, %17
  %102 = sdiv i64 %101, 8
  %103 = sub i64 0, %102
  %104 = icmp sgt i64 %17, 0
  %105 = select i1 %104, i64 %100, i64 %103
  %106 = add i64 %13, -1
  %107 = sdiv i64 %106, 32
  %108 = add i64 %107, 1
  %109 = sub i64 0, %13
  %110 = sdiv i64 %109, 32
  %111 = sub i64 0, %110
  %112 = icmp sgt i64 %13, 0
  %113 = select i1 %112, i64 %108, i64 %111
  %114 = mul i64 %105, %113
  %115 = mul i64 %114, 256
  %116 = icmp sle i64 %115, 0
  %117 = sub i64 0, %115
  %118 = sub i64 %115, 1
  %119 = select i1 %116, i64 %117, i64 %118
  %120 = sdiv i64 %119, 256
  %121 = sub i64 0, %120
  %122 = add i64 %120, 1
  %123 = select i1 %116, i64 %121, i64 %122
  %124 = alloca ptr, align 8
  %125 = getelementptr ptr, ptr %124, i32 0
  store ptr @main_kernel_0_blob_gpu.binary, ptr %125, align 8
  %126 = extractvalue { ptr, ptr, i64, [3 x i64], [3 x i64] } %10, 0
  %127 = extractvalue { ptr, ptr, i64, [3 x i64], [3 x i64] } %10, 1
  %128 = extractvalue { ptr, ptr, i64, [3 x i64], [3 x i64] } %10, 2
  %129 = extractvalue { ptr, ptr, i64, [3 x i64], [3 x i64] } %10, 3, 0
  %130 = extractvalue { ptr, ptr, i64, [3 x i64], [3 x i64] } %10, 3, 1
  %131 = extractvalue { ptr, ptr, i64, [3 x i64], [3 x i64] } %10, 3, 2
  %132 = extractvalue { ptr, ptr, i64, [3 x i64], [3 x i64] } %10, 4, 0
  %133 = extractvalue { ptr, ptr, i64, [3 x i64], [3 x i64] } %10, 4, 1
  %134 = extractvalue { ptr, ptr, i64, [3 x i64], [3 x i64] } %10, 4, 2
  %135 = extractvalue { ptr, ptr, i64, [1 x i64], [1 x i64] } %33, 0
  %136 = extractvalue { ptr, ptr, i64, [1 x i64], [1 x i64] } %33, 1
  %137 = extractvalue { ptr, ptr, i64, [1 x i64], [1 x i64] } %33, 2
  %138 = extractvalue { ptr, ptr, i64, [1 x i64], [1 x i64] } %33, 3, 0
  %139 = extractvalue { ptr, ptr, i64, [1 x i64], [1 x i64] } %33, 4, 0
  %140 = alloca %.11, align 8
  %141 = alloca ptr, i32 7, align 8
  %142 = getelementptr %.11, ptr %140, i32 0, i32 0
  store i64 %17, ptr %142, align 4
  %143 = getelementptr ptr, ptr %141, i32 0
  store ptr %142, ptr %143, align 8
  %144 = getelementptr %.11, ptr %140, i32 0, i32 1
  store ptr %127, ptr %144, align 8
  %145 = getelementptr ptr, ptr %141, i32 1
  store ptr %144, ptr %145, align 8
  %146 = getelementptr %.11, ptr %140, i32 0, i32 2
  store i64 %129, ptr %146, align 4
  %147 = getelementptr ptr, ptr %141, i32 2
  store ptr %146, ptr %147, align 8
  %148 = getelementptr %.11, ptr %140, i32 0, i32 3
  store i64 256, ptr %148, align 4
  %149 = getelementptr ptr, ptr %141, i32 3
  store ptr %148, ptr %149, align 8
  %150 = getelementptr %.11, ptr %140, i32 0, i32 4
  store i64 %115, ptr %150, align 4
  %151 = getelementptr ptr, ptr %141, i32 4
  store ptr %150, ptr %151, align 8
  %152 = getelementptr %.11, ptr %140, i32 0, i32 5
  store i64 %105, ptr %152, align 4
  %153 = getelementptr ptr, ptr %141, i32 5
  store ptr %152, ptr %153, align 8
  %154 = getelementptr %.11, ptr %140, i32 0, i32 6
  store ptr %136, ptr %154, align 8
  %155 = getelementptr ptr, ptr %141, i32 6
  store ptr %154, ptr %155, align 8
  %156 = alloca %.12, align 8
  %157 = alloca ptr, i32 14, align 8
  %158 = getelementptr %.12, ptr %156, i32 0, i32 0
  store ptr %0, ptr %158, align 8
  %159 = getelementptr ptr, ptr %157, i32 0
  store ptr %158, ptr %159, align 8
  %160 = getelementptr %.12, ptr %156, i32 0, i32 1
  store ptr %124, ptr %160, align 8
  %161 = getelementptr ptr, ptr %157, i32 1
  store ptr %160, ptr %161, align 8
  %162 = getelementptr %.12, ptr %156, i32 0, i32 2
  store i64 1, ptr %162, align 4
  %163 = getelementptr ptr, ptr %157, i32 2
  store ptr %162, ptr %163, align 8
  %164 = getelementptr %.12, ptr %156, i32 0, i32 3
  store ptr @main_kernel_0_main_kColReduction_reduce__4_1_0___8w32h_1_kernel_name, ptr %164, align 8
  %165 = getelementptr ptr, ptr %157, i32 3
  store ptr %164, ptr %165, align 8
  %166 = getelementptr %.12, ptr %156, i32 0, i32 4
  store i64 %123, ptr %166, align 4
  %167 = getelementptr ptr, ptr %157, i32 4
  store ptr %166, ptr %167, align 8
  %168 = getelementptr %.12, ptr %156, i32 0, i32 5
  store i64 1, ptr %168, align 4
  %169 = getelementptr ptr, ptr %157, i32 5
  store ptr %168, ptr %169, align 8
  %170 = getelementptr %.12, ptr %156, i32 0, i32 6
  store i64 1, ptr %170, align 4
  %171 = getelementptr ptr, ptr %157, i32 6
  store ptr %170, ptr %171, align 8
  %172 = getelementptr %.12, ptr %156, i32 0, i32 7
  store i64 256, ptr %172, align 4
  %173 = getelementptr ptr, ptr %157, i32 7
  store ptr %172, ptr %173, align 8
  %174 = getelementptr %.12, ptr %156, i32 0, i32 8
  store i64 1, ptr %174, align 4
  %175 = getelementptr ptr, ptr %157, i32 8
  store ptr %174, ptr %175, align 8
  %176 = getelementptr %.12, ptr %156, i32 0, i32 9
  store i64 1, ptr %176, align 4
  %177 = getelementptr ptr, ptr %157, i32 9
  store ptr %176, ptr %177, align 8
  %178 = getelementptr %.12, ptr %156, i32 0, i32 10
  store i32 0, ptr %178, align 4
  %179 = getelementptr ptr, ptr %157, i32 10
  store ptr %178, ptr %179, align 8
  %180 = getelementptr %.12, ptr %156, i32 0, i32 11
  store ptr null, ptr %180, align 8
  %181 = getelementptr ptr, ptr %157, i32 11
  store ptr %180, ptr %181, align 8
  %182 = getelementptr %.12, ptr %156, i32 0, i32 12
  store i32 7, ptr %182, align 4
  %183 = getelementptr ptr, ptr %157, i32 12
  store ptr %182, ptr %183, align 8
  %184 = getelementptr %.12, ptr %156, i32 0, i32 13
  store ptr %141, ptr %184, align 8
  %185 = getelementptr ptr, ptr %157, i32 13
  store ptr %184, ptr %185, align 8
  call void @disc_ral_call(ptr %0, ptr @ral_kernel_launch___gpu___pvoid_ppvoid_i64_pvoid_i64_i64_i64_i64_i64_i64_i32_pvoid_i32_ppvoid___void, ptr %157)
  br label %328

186:                                              ; preds = %1
  %187 = icmp sle i64 %17, 0
  %188 = sub i64 0, %17
  %189 = sub i64 %17, 1
  %190 = select i1 %187, i64 %188, i64 %189
  %191 = sdiv i64 %190, 128
  %192 = sub i64 0, %191
  %193 = add i64 %191, 1
  %194 = select i1 %187, i64 %192, i64 %193
  %195 = alloca ptr, align 8
  %196 = getelementptr ptr, ptr %195, i32 0
  store ptr @main_kernel_1_blob_gpu.binary, ptr %196, align 8
  %197 = extractvalue { ptr, ptr, i64, [1 x i64], [1 x i64] } %33, 0
  %198 = extractvalue { ptr, ptr, i64, [1 x i64], [1 x i64] } %33, 1
  %199 = extractvalue { ptr, ptr, i64, [1 x i64], [1 x i64] } %33, 2
  %200 = extractvalue { ptr, ptr, i64, [1 x i64], [1 x i64] } %33, 3, 0
  %201 = extractvalue { ptr, ptr, i64, [1 x i64], [1 x i64] } %33, 4, 0
  %202 = alloca %.2, align 8
  %203 = alloca ptr, i32 3, align 8
  %204 = getelementptr %.2, ptr %202, i32 0, i32 0
  store i64 128, ptr %204, align 4
  %205 = getelementptr ptr, ptr %203, i32 0
  store ptr %204, ptr %205, align 8
  %206 = getelementptr %.2, ptr %202, i32 0, i32 1
  store i64 %17, ptr %206, align 4
  %207 = getelementptr ptr, ptr %203, i32 1
  store ptr %206, ptr %207, align 8
  %208 = getelementptr %.2, ptr %202, i32 0, i32 2
  store ptr %198, ptr %208, align 8
  %209 = getelementptr ptr, ptr %203, i32 2
  store ptr %208, ptr %209, align 8
  %210 = alloca %.3, align 8
  %211 = alloca ptr, i32 14, align 8
  %212 = getelementptr %.3, ptr %210, i32 0, i32 0
  store ptr %0, ptr %212, align 8
  %213 = getelementptr ptr, ptr %211, i32 0
  store ptr %212, ptr %213, align 8
  %214 = getelementptr %.3, ptr %210, i32 0, i32 1
  store ptr %195, ptr %214, align 8
  %215 = getelementptr ptr, ptr %211, i32 1
  store ptr %214, ptr %215, align 8
  %216 = getelementptr %.3, ptr %210, i32 0, i32 2
  store i64 1, ptr %216, align 4
  %217 = getelementptr ptr, ptr %211, i32 2
  store ptr %216, ptr %217, align 8
  %218 = getelementptr %.3, ptr %210, i32 0, i32 3
  store ptr @main_kernel_1_main_kColReduction_reduce__4_1_0___8w16h_kernel_name, ptr %218, align 8
  %219 = getelementptr ptr, ptr %211, i32 3
  store ptr %218, ptr %219, align 8
  %220 = getelementptr %.3, ptr %210, i32 0, i32 4
  store i64 %194, ptr %220, align 4
  %221 = getelementptr ptr, ptr %211, i32 4
  store ptr %220, ptr %221, align 8
  %222 = getelementptr %.3, ptr %210, i32 0, i32 5
  store i64 1, ptr %222, align 4
  %223 = getelementptr ptr, ptr %211, i32 5
  store ptr %222, ptr %223, align 8
  %224 = getelementptr %.3, ptr %210, i32 0, i32 6
  store i64 1, ptr %224, align 4
  %225 = getelementptr ptr, ptr %211, i32 6
  store ptr %224, ptr %225, align 8
  %226 = getelementptr %.3, ptr %210, i32 0, i32 7
  store i64 128, ptr %226, align 4
  %227 = getelementptr ptr, ptr %211, i32 7
  store ptr %226, ptr %227, align 8
  %228 = getelementptr %.3, ptr %210, i32 0, i32 8
  store i64 1, ptr %228, align 4
  %229 = getelementptr ptr, ptr %211, i32 8
  store ptr %228, ptr %229, align 8
  %230 = getelementptr %.3, ptr %210, i32 0, i32 9
  store i64 1, ptr %230, align 4
  %231 = getelementptr ptr, ptr %211, i32 9
  store ptr %230, ptr %231, align 8
  %232 = getelementptr %.3, ptr %210, i32 0, i32 10
  store i32 0, ptr %232, align 4
  %233 = getelementptr ptr, ptr %211, i32 10
  store ptr %232, ptr %233, align 8
  %234 = getelementptr %.3, ptr %210, i32 0, i32 11
  store ptr null, ptr %234, align 8
  %235 = getelementptr ptr, ptr %211, i32 11
  store ptr %234, ptr %235, align 8
  %236 = getelementptr %.3, ptr %210, i32 0, i32 12
  store i32 3, ptr %236, align 4
  %237 = getelementptr ptr, ptr %211, i32 12
  store ptr %236, ptr %237, align 8
  %238 = getelementptr %.3, ptr %210, i32 0, i32 13
  store ptr %203, ptr %238, align 8
  %239 = getelementptr ptr, ptr %211, i32 13
  store ptr %238, ptr %239, align 8
  call void @disc_ral_call(ptr %0, ptr @ral_kernel_launch___gpu___pvoid_ppvoid_i64_pvoid_i64_i64_i64_i64_i64_i64_i32_pvoid_i32_ppvoid___void, ptr %211)
  %240 = add i64 %17, -1
  %241 = sdiv i64 %240, 8
  %242 = add i64 %241, 1
  %243 = sub i64 0, %17
  %244 = sdiv i64 %243, 8
  %245 = sub i64 0, %244
  %246 = icmp sgt i64 %17, 0
  %247 = select i1 %246, i64 %242, i64 %245
  %248 = add i64 %13, -1
  %249 = sdiv i64 %248, 16
  %250 = add i64 %249, 1
  %251 = sub i64 0, %13
  %252 = sdiv i64 %251, 16
  %253 = sub i64 0, %252
  %254 = icmp sgt i64 %13, 0
  %255 = select i1 %254, i64 %250, i64 %253
  %256 = mul i64 %247, %255
  %257 = mul i64 %256, 128
  %258 = icmp sle i64 %257, 0
  %259 = sub i64 0, %257
  %260 = sub i64 %257, 1
  %261 = select i1 %258, i64 %259, i64 %260
  %262 = sdiv i64 %261, 128
  %263 = sub i64 0, %262
  %264 = add i64 %262, 1
  %265 = select i1 %258, i64 %263, i64 %264
  %266 = alloca ptr, align 8
  %267 = getelementptr ptr, ptr %266, i32 0
  store ptr @main_kernel_2_blob_gpu.binary, ptr %267, align 8
  %268 = extractvalue { ptr, ptr, i64, [3 x i64], [3 x i64] } %10, 0
  %269 = extractvalue { ptr, ptr, i64, [3 x i64], [3 x i64] } %10, 1
  %270 = extractvalue { ptr, ptr, i64, [3 x i64], [3 x i64] } %10, 2
  %271 = extractvalue { ptr, ptr, i64, [3 x i64], [3 x i64] } %10, 3, 0
  %272 = extractvalue { ptr, ptr, i64, [3 x i64], [3 x i64] } %10, 3, 1
  %273 = extractvalue { ptr, ptr, i64, [3 x i64], [3 x i64] } %10, 3, 2
  %274 = extractvalue { ptr, ptr, i64, [3 x i64], [3 x i64] } %10, 4, 0
  %275 = extractvalue { ptr, ptr, i64, [3 x i64], [3 x i64] } %10, 4, 1
  %276 = extractvalue { ptr, ptr, i64, [3 x i64], [3 x i64] } %10, 4, 2
  %277 = extractvalue { ptr, ptr, i64, [1 x i64], [1 x i64] } %33, 0
  %278 = extractvalue { ptr, ptr, i64, [1 x i64], [1 x i64] } %33, 1
  %279 = extractvalue { ptr, ptr, i64, [1 x i64], [1 x i64] } %33, 2
  %280 = extractvalue { ptr, ptr, i64, [1 x i64], [1 x i64] } %33, 3, 0
  %281 = extractvalue { ptr, ptr, i64, [1 x i64], [1 x i64] } %33, 4, 0
  %282 = alloca %.4, align 8
  %283 = alloca ptr, i32 7, align 8
  %284 = getelementptr %.4, ptr %282, i32 0, i32 0
  store i64 %17, ptr %284, align 4
  %285 = getelementptr ptr, ptr %283, i32 0
  store ptr %284, ptr %285, align 8
  %286 = getelementptr %.4, ptr %282, i32 0, i32 1
  store ptr %269, ptr %286, align 8
  %287 = getelementptr ptr, ptr %283, i32 1
  store ptr %286, ptr %287, align 8
  %288 = getelementptr %.4, ptr %282, i32 0, i32 2
  store i64 %271, ptr %288, align 4
  %289 = getelementptr ptr, ptr %283, i32 2
  store ptr %288, ptr %289, align 8
  %290 = getelementptr %.4, ptr %282, i32 0, i32 3
  store i64 128, ptr %290, align 4
  %291 = getelementptr ptr, ptr %283, i32 3
  store ptr %290, ptr %291, align 8
  %292 = getelementptr %.4, ptr %282, i32 0, i32 4
  store i64 %257, ptr %292, align 4
  %293 = getelementptr ptr, ptr %283, i32 4
  store ptr %292, ptr %293, align 8
  %294 = getelementptr %.4, ptr %282, i32 0, i32 5
  store i64 %247, ptr %294, align 4
  %295 = getelementptr ptr, ptr %283, i32 5
  store ptr %294, ptr %295, align 8
  %296 = getelementptr %.4, ptr %282, i32 0, i32 6
  store ptr %278, ptr %296, align 8
  %297 = getelementptr ptr, ptr %283, i32 6
  store ptr %296, ptr %297, align 8
  %298 = alloca %.5, align 8
  %299 = alloca ptr, i32 14, align 8
  %300 = getelementptr %.5, ptr %298, i32 0, i32 0
  store ptr %0, ptr %300, align 8
  %301 = getelementptr ptr, ptr %299, i32 0
  store ptr %300, ptr %301, align 8
  %302 = getelementptr %.5, ptr %298, i32 0, i32 1
  store ptr %266, ptr %302, align 8
  %303 = getelementptr ptr, ptr %299, i32 1
  store ptr %302, ptr %303, align 8
  %304 = getelementptr %.5, ptr %298, i32 0, i32 2
  store i64 1, ptr %304, align 4
  %305 = getelementptr ptr, ptr %299, i32 2
  store ptr %304, ptr %305, align 8
  %306 = getelementptr %.5, ptr %298, i32 0, i32 3
  store ptr @main_kernel_2_main_kColReduction_reduce__4_1_0___8w16h_1_kernel_name, ptr %306, align 8
  %307 = getelementptr ptr, ptr %299, i32 3
  store ptr %306, ptr %307, align 8
  %308 = getelementptr %.5, ptr %298, i32 0, i32 4
  store i64 %265, ptr %308, align 4
  %309 = getelementptr ptr, ptr %299, i32 4
  store ptr %308, ptr %309, align 8
  %310 = getelementptr %.5, ptr %298, i32 0, i32 5
  store i64 1, ptr %310, align 4
  %311 = getelementptr ptr, ptr %299, i32 5
  store ptr %310, ptr %311, align 8
  %312 = getelementptr %.5, ptr %298, i32 0, i32 6
  store i64 1, ptr %312, align 4
  %313 = getelementptr ptr, ptr %299, i32 6
  store ptr %312, ptr %313, align 8
  %314 = getelementptr %.5, ptr %298, i32 0, i32 7
  store i64 128, ptr %314, align 4
  %315 = getelementptr ptr, ptr %299, i32 7
  store ptr %314, ptr %315, align 8
  %316 = getelementptr %.5, ptr %298, i32 0, i32 8
  store i64 1, ptr %316, align 4
  %317 = getelementptr ptr, ptr %299, i32 8
  store ptr %316, ptr %317, align 8
  %318 = getelementptr %.5, ptr %298, i32 0, i32 9
  store i64 1, ptr %318, align 4
  %319 = getelementptr ptr, ptr %299, i32 9
  store ptr %318, ptr %319, align 8
  %320 = getelementptr %.5, ptr %298, i32 0, i32 10
  store i32 0, ptr %320, align 4
  %321 = getelementptr ptr, ptr %299, i32 10
  store ptr %320, ptr %321, align 8
  %322 = getelementptr %.5, ptr %298, i32 0, i32 11
  store ptr null, ptr %322, align 8
  %323 = getelementptr ptr, ptr %299, i32 11
  store ptr %322, ptr %323, align 8
  %324 = getelementptr %.5, ptr %298, i32 0, i32 12
  store i32 7, ptr %324, align 4
  %325 = getelementptr ptr, ptr %299, i32 12
  store ptr %324, ptr %325, align 8
  %326 = getelementptr %.5, ptr %298, i32 0, i32 13
  store ptr %283, ptr %326, align 8
  %327 = getelementptr ptr, ptr %299, i32 13
  store ptr %326, ptr %327, align 8
  call void @disc_ral_call(ptr %0, ptr @ral_kernel_launch___gpu___pvoid_ppvoid_i64_pvoid_i64_i64_i64_i64_i64_i64_i32_pvoid_i32_ppvoid___void, ptr %299)
  br label %328

328:                                              ; preds = %44, %186
  %329 = alloca i64, i64 ptrtoint (ptr getelementptr (i64, ptr null, i64 2) to i64), align 8
  %330 = insertvalue { ptr, ptr, i64, [1 x i64], [1 x i64] } undef, ptr %329, 0
  %331 = insertvalue { ptr, ptr, i64, [1 x i64], [1 x i64] } %330, ptr %329, 1
  %332 = insertvalue { ptr, ptr, i64, [1 x i64], [1 x i64] } %331, i64 0, 2
  %333 = insertvalue { ptr, ptr, i64, [1 x i64], [1 x i64] } %332, i64 2, 3, 0
  %334 = insertvalue { ptr, ptr, i64, [1 x i64], [1 x i64] } %333, i64 1, 4, 0
  %335 = extractvalue { ptr, ptr, i64, [1 x i64], [1 x i64] } %334, 1
  %336 = getelementptr i64, ptr %335, i64 0
  store i64 %12, ptr %336, align 4
  %337 = extractvalue { ptr, ptr, i64, [1 x i64], [1 x i64] } %334, 1
  %338 = getelementptr i64, ptr %337, i64 1
  store i64 %11, ptr %338, align 4
  %339 = extractvalue { ptr, ptr, i64, [1 x i64], [1 x i64] } %33, 0
  %340 = extractvalue { ptr, ptr, i64, [1 x i64], [1 x i64] } %33, 1
  %341 = extractvalue { ptr, ptr, i64, [1 x i64], [1 x i64] } %33, 2
  %342 = extractvalue { ptr, ptr, i64, [1 x i64], [1 x i64] } %33, 3, 0
  %343 = extractvalue { ptr, ptr, i64, [1 x i64], [1 x i64] } %33, 4, 0
  %344 = extractvalue { ptr, ptr, i64, [1 x i64], [1 x i64] } %334, 0
  %345 = extractvalue { ptr, ptr, i64, [1 x i64], [1 x i64] } %334, 1
  %346 = extractvalue { ptr, ptr, i64, [1 x i64], [1 x i64] } %334, 2
  %347 = extractvalue { ptr, ptr, i64, [1 x i64], [1 x i64] } %334, 3, 0
  %348 = extractvalue { ptr, ptr, i64, [1 x i64], [1 x i64] } %334, 4, 0
  %349 = alloca %.6, align 8
  %350 = alloca ptr, i32 13, align 8
  %351 = getelementptr %.6, ptr %349, i32 0, i32 0
  store ptr %0, ptr %351, align 8
  %352 = getelementptr ptr, ptr %350, i32 0
  store ptr %351, ptr %352, align 8
  %353 = getelementptr %.6, ptr %349, i32 0, i32 1
  store ptr null, ptr %353, align 8
  %354 = getelementptr ptr, ptr %350, i32 1
  store ptr %353, ptr %354, align 8
  %355 = getelementptr %.6, ptr %349, i32 0, i32 2
  store ptr %339, ptr %355, align 8
  %356 = getelementptr ptr, ptr %350, i32 2
  store ptr %355, ptr %356, align 8
  %357 = getelementptr %.6, ptr %349, i32 0, i32 3
  store ptr %340, ptr %357, align 8
  %358 = getelementptr ptr, ptr %350, i32 3
  store ptr %357, ptr %358, align 8
  %359 = getelementptr %.6, ptr %349, i32 0, i32 4
  store i64 %341, ptr %359, align 4
  %360 = getelementptr ptr, ptr %350, i32 4
  store ptr %359, ptr %360, align 8
  %361 = getelementptr %.6, ptr %349, i32 0, i32 5
  store i64 %342, ptr %361, align 4
  %362 = getelementptr ptr, ptr %350, i32 5
  store ptr %361, ptr %362, align 8
  %363 = getelementptr %.6, ptr %349, i32 0, i32 6
  store i64 %343, ptr %363, align 4
  %364 = getelementptr ptr, ptr %350, i32 6
  store ptr %363, ptr %364, align 8
  %365 = getelementptr %.6, ptr %349, i32 0, i32 7
  store ptr %344, ptr %365, align 8
  %366 = getelementptr ptr, ptr %350, i32 7
  store ptr %365, ptr %366, align 8
  %367 = getelementptr %.6, ptr %349, i32 0, i32 8
  store ptr %345, ptr %367, align 8
  %368 = getelementptr ptr, ptr %350, i32 8
  store ptr %367, ptr %368, align 8
  %369 = getelementptr %.6, ptr %349, i32 0, i32 9
  store i64 %346, ptr %369, align 4
  %370 = getelementptr ptr, ptr %350, i32 9
  store ptr %369, ptr %370, align 8
  %371 = getelementptr %.6, ptr %349, i32 0, i32 10
  store i64 %347, ptr %371, align 4
  %372 = getelementptr ptr, ptr %350, i32 10
  store ptr %371, ptr %372, align 8
  %373 = getelementptr %.6, ptr %349, i32 0, i32 11
  store i64 %348, ptr %373, align 4
  %374 = getelementptr ptr, ptr %350, i32 11
  store ptr %373, ptr %374, align 8
  %375 = getelementptr %.6, ptr %349, i32 0, i32 12
  %376 = getelementptr ptr, ptr %350, i32 12
  store ptr %375, ptr %376, align 8
  call void @disc_ral_call(ptr %0, ptr @inc_ref___gpu___pvoid_pvoid_m1df32_m1di64___m2df32, ptr %350)
  %377 = load { ptr, ptr, i64, [2 x i64], [2 x i64] }, ptr %375, align 8
  %378 = extractvalue { ptr, ptr, i64, [2 x i64], [2 x i64] } %377, 0
  %379 = extractvalue { ptr, ptr, i64, [2 x i64], [2 x i64] } %377, 1
  %380 = insertvalue { ptr, ptr, i64, [2 x i64], [2 x i64] } undef, ptr %378, 0
  %381 = insertvalue { ptr, ptr, i64, [2 x i64], [2 x i64] } %380, ptr %379, 1
  %382 = insertvalue { ptr, ptr, i64, [2 x i64], [2 x i64] } %381, i64 0, 2
  %383 = insertvalue { ptr, ptr, i64, [2 x i64], [2 x i64] } %382, i64 %12, 3, 0
  %384 = insertvalue { ptr, ptr, i64, [2 x i64], [2 x i64] } %383, i64 %11, 4, 0
  %385 = insertvalue { ptr, ptr, i64, [2 x i64], [2 x i64] } %384, i64 %11, 3, 1
  %386 = insertvalue { ptr, ptr, i64, [2 x i64], [2 x i64] } %385, i64 1, 4, 1
  %387 = extractvalue { ptr, ptr, i64, [1 x i64], [1 x i64] } %33, 0
  %388 = alloca %.7, align 8
  %389 = alloca ptr, i32 2, align 8
  %390 = getelementptr %.7, ptr %388, i32 0, i32 0
  store ptr %0, ptr %390, align 8
  %391 = getelementptr ptr, ptr %389, i32 0
  store ptr %390, ptr %391, align 8
  %392 = getelementptr %.7, ptr %388, i32 0, i32 1
  store ptr %387, ptr %392, align 8
  %393 = getelementptr ptr, ptr %389, i32 1
  store ptr %392, ptr %393, align 8
  call void @disc_ral_call(ptr %0, ptr @dealloc___gpu___pvoid_pvoid___void, ptr %389)
  %394 = extractvalue { ptr, ptr, i64, [2 x i64], [2 x i64] } %386, 0
  %395 = extractvalue { ptr, ptr, i64, [2 x i64], [2 x i64] } %386, 1
  %396 = extractvalue { ptr, ptr, i64, [2 x i64], [2 x i64] } %386, 2
  %397 = extractvalue { ptr, ptr, i64, [2 x i64], [2 x i64] } %386, 3, 0
  %398 = extractvalue { ptr, ptr, i64, [2 x i64], [2 x i64] } %386, 3, 1
  %399 = extractvalue { ptr, ptr, i64, [2 x i64], [2 x i64] } %386, 4, 0
  %400 = extractvalue { ptr, ptr, i64, [2 x i64], [2 x i64] } %386, 4, 1
  %401 = alloca %.8, align 8
  %402 = alloca ptr, i32 9, align 8
  %403 = getelementptr %.8, ptr %401, i32 0, i32 0
  store ptr %0, ptr %403, align 8
  %404 = getelementptr ptr, ptr %402, i32 0
  store ptr %403, ptr %404, align 8
  %405 = getelementptr %.8, ptr %401, i32 0, i32 1
  store i64 0, ptr %405, align 4
  %406 = getelementptr ptr, ptr %402, i32 1
  store ptr %405, ptr %406, align 8
  %407 = getelementptr %.8, ptr %401, i32 0, i32 2
  store ptr %394, ptr %407, align 8
  %408 = getelementptr ptr, ptr %402, i32 2
  store ptr %407, ptr %408, align 8
  %409 = getelementptr %.8, ptr %401, i32 0, i32 3
  store ptr %395, ptr %409, align 8
  %410 = getelementptr ptr, ptr %402, i32 3
  store ptr %409, ptr %410, align 8
  %411 = getelementptr %.8, ptr %401, i32 0, i32 4
  store i64 %396, ptr %411, align 4
  %412 = getelementptr ptr, ptr %402, i32 4
  store ptr %411, ptr %412, align 8
  %413 = getelementptr %.8, ptr %401, i32 0, i32 5
  store i64 %397, ptr %413, align 4
  %414 = getelementptr ptr, ptr %402, i32 5
  store ptr %413, ptr %414, align 8
  %415 = getelementptr %.8, ptr %401, i32 0, i32 6
  store i64 %398, ptr %415, align 4
  %416 = getelementptr ptr, ptr %402, i32 6
  store ptr %415, ptr %416, align 8
  %417 = getelementptr %.8, ptr %401, i32 0, i32 7
  store i64 %399, ptr %417, align 4
  %418 = getelementptr ptr, ptr %402, i32 7
  store ptr %417, ptr %418, align 8
  %419 = getelementptr %.8, ptr %401, i32 0, i32 8
  store i64 %400, ptr %419, align 4
  %420 = getelementptr ptr, ptr %402, i32 8
  store ptr %419, ptr %420, align 8
  call void @disc_ral_call(ptr %0, ptr @ral_send_output___cpu___pvoid_i64_m2df32___void, ptr %402)
  ret void
}

host default target triple: x86_64-unknown-linux-gnu
host cpu name: icelake-server
host cpu features: -avx512pf,-tsxldtrk,+cx16,+sahf,-tbm,+avx512ifma,+sha,+crc32,-fma4,+vpclmulqdq,+prfchw,+bmi2,-cldemote,+fsgsbase,-avx512bf16,-amx-tile,-raoint,-uintr,+gfni,+popcnt,-ptwrite,+aes,+avx512bitalg,-movdiri,-widekl,+xsaves,-avx512er,-avxvnni,-avx512fp16,+avx512vnni,-amx-bf16,-avxvnniint8,+avx512vpopcntdq,-pconfig,+clwb,-cmpccxadd,+avx512f,+xsavec,-clzero,-pku,-amx-fp16,+mmx,-lwp,+rdpid,-xop,+rdseed,-waitpkg,-prefetchi,-kl,-movdir64b,-sse4a,+avx512bw,-avxneconvert,+clflushopt,+xsave,+avx512vbmi2,+64bit,+avx512vl,-serialize,-hreset,+invpcid,+avx512cd,+avx,+vaes,-amx-int8,+cx8,+fma,-rtm,+bmi,-enqcmd,+rdrnd,-mwaitx,+sse4.1,+sse4.2,+avx2,+fxsr,+wbnoinvd,+sse,+lzcnt,+pclmul,-rdpru,-avxifma,+f16c,+ssse3,-sgx,-prefetchwt1,+cmov,+avx512vbmi,-shstk,+movbe,-avx512vp2intersect,+xsaveopt,+avx512dq,+sse2,+adx,+sse3
after optimize llvm module:
; ModuleID = 'LLVMDialectModule'
source_filename = "LLVMDialectModule"
target datalayout = "e-m:e-p270:32:32-p271:32:32-p272:64:64-i64:64-f80:128-n8:16:32:64-S128"
target triple = "x86_64-unknown-linux-gnu"

%0 = type { ptr, i64, { ptr, ptr, i64, [3 x i64], [3 x i64] } }
%.1 = type { ptr, i64, ptr }
%.9 = type { i64, i64, ptr }
%.10 = type { ptr, ptr, i64, ptr, i64, i64, i64, i64, i64, i64, i32, ptr, i32, ptr }
%.11 = type { i64, ptr, i64, i64, i64, i64, ptr }
%.12 = type { ptr, ptr, i64, ptr, i64, i64, i64, i64, i64, i64, i32, ptr, i32, ptr }
%.2 = type { i64, i64, ptr }
%.3 = type { ptr, ptr, i64, ptr, i64, i64, i64, i64, i64, i64, i32, ptr, i32, ptr }
%.4 = type { i64, ptr, i64, i64, i64, i64, ptr }
%.5 = type { ptr, ptr, i64, ptr, i64, i64, i64, i64, i64, i64, i32, ptr, i32, ptr }
%.6 = type { ptr, ptr, ptr, ptr, i64, i64, i64, ptr, ptr, i64, i64, i64, { ptr, ptr, i64, [2 x i64], [2 x i64] } }
%.7 = type { ptr, ptr }
%.8 = type { ptr, i64, ptr, ptr, i64, i64, i64, i64, i64 }

@main_kernel_0_main_kColReduction_reduce__4_1_0___8w32h_1_kernel_name = internal constant [43 x i8] c"main_kColReduction_reduce__4_1_0___8w32h_1\00"
@main_kernel_0_blob_gpu.binary = internal constant [2128 x i8] c"P\EDU\BA\01\00\10\00@\08\00\00\00\00\00\00\02\00\01\01@\00\00\00\00\08\00\00\00\00\00\00\F9\07\00\00\00\00\00\00\07\00\01\00P\00\00\00\00\00\00\00\00\00\00\00\11 \00\00\00\00\00\00\00\00\00\00\00\00\00\00(\14\00\00\00\00\00\00\A2\7FELF\02\01\013\07\00\01\00f\02\00\BE\00q\00\01\00#\80\13\08\00\11\10\07\00\F5\0E\00P\05P\00@\008\00\03\00@\00\0C\00\01\00\00.shstrtab\00.\08\00'ym\08\00\F0\1B_shndx\00.nv.info\00.text.main_kColReduction_r\0A\00\F5\03e__4_1_0___8w32h_1:\00\0F4\00\1Doshared6\00\1AOrela\A0\00\1F?rel\D5\00\22\9Fconstant09\00\1A\B2debug_frame{\00\09\11\00!nv\14\00\11aE\00\0F\9E\01 \0F\8A\00\17\0F\C9\01\F4\8F$____wg_3\00\17\00\0C\00/27\02\02'o_param\09\02\1C\0F\01\00\05\8C]\00\00\00\03\00\0A\00\01\00\11\C2\18\00,\0B\00\01\00 \9C\01\18\00,\09\00\01\00\11\DC\18\00,\04\00\01\00\11\FA\18\00,\07\00\01\00g2\00\00\00\12\10x\00\11\08\06\00v\00\FF\FF\FF\FF(\00\0C\00\00\01\00@\03\00\04|\08\00\D1\0F\0C\81\80\80(\00\08\FF\81\80(\08\0B\00\06(\00/0\00\01\00\03\13\F0{\00\10\04\9B\00R\04\14\00\00\00E\002\04\AC\01\18\00\80/\08\00\06\00\00\00\0E\0C\00\12#\0C\00\00\01\00'\04\12\0C\00\17\11\0C\00!7\04 \05\F1\08\015\00\00\04\0A\08\00\03\00\00\00`\01(\00\03\19(\00\04\17\0C$\00u\06\00 \00\00\F0!\10\00u\05\00\1C\00\00\F0\11\10\009\04\00\18\10\009\03\00\14\10\009\02\00\10\10\009\01\00\08P\00\01\01\00\F2\0A\F0\11\00\03\1B\FF\00\04\1C\0C\00P\00\00\00\10\06\00\00\10\07\00\00\04\1E\84\01#K\00\01\00v\02\02\08\10\0A/\22b\01\13\08\08\00\13\10\08\00\13\18\08\00\13 \08\00\13(\08\00\130\08\00\228\08\84\00\13\00\08\00\13\08\08\00\13\10\08\00\13\18\08\00\13 \08\00\13(\08\00\130\08\00\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\A2\00\00\00\14,\00\00\00H\00\01\00\11\02h\01\0F\01\00\FF\B0@$v\01\FF\7F\04\B2\FF\00\8E\07\00\C4\0F\00\19y\00\01\00\10%\8B\02a\0E\00\19y\03\00\01\00\10!-\00B\0E\00$z\B5\04\90\03\02\8E\07\00\CA\1F\00\0C\10\00\C5^\00\00p`\F0\03\00\DA\0F\00M\9B\04\F0\0C\80\03\00\EA\0F\00\06{\04\00\00_\00\00\00\90 \00\00\22\0E\00\19x\05\FF\1F\1F\00\D3\14\01\00\00\E2\0F\00$r\02\FF\FF\00\80\00 \E2\0FP\00\10\FF0\00@pP\F4\03\10\00\81\B9z\04\00\00F\00\00#\05p\E2\0F\00\11r\05\05?\00\B2\FF@\8F\07\00\C6\0F\00\08s\04\F6\04\00 \09\F1\07$\1E\00\10x\03\04\FE\FF\FF\0F\FF\E0\FF\07\00\CC\1F\00\05s\03$\04\00\C0\03\01\C0\00!r\07p\00\C0\03\0A\8E\07\00\C8\1F\00$z\07\07p\00\10\FF\D0\00\81\C8\0F\00'r\07\03\07\E0\02\02\90\00@\19x\02\FF\01\03\10\05\B0\00q\E4\0F\00\12x\05\05\F1\04!\C0\8E\80\00T'r\07\07\02\C0\00\10\C8\D0\00\11\09`\00\10\07`\00`\E4\0F\00$x\05\BC\03$\00\05\10\000z\03\09p\00\22\02\02@\00@\19x\00\FF\A0\00\22\05\16`\00f\0Cz\00\03\00_P\010\10\0A\03\10\00\11\80\D0\00p\E4\0F\00\10\08\07\07P\00\04\10\00\060\00\12\F20\00\1A\18 \001\12\AA\07P\01 \FF3\B0\01\00\A0\00\1B\03\A0\000\00\07  \01\15\02\A0\00\15\03\A0\00\10\E2\F0\00 \02\05\10\01#\FF\C0@\00Sx\09\03\08\00 \00\11\CA\80\003\09\00X\80\00\22\C8\0F\10\02p\\\00\00p\10\F2\04\90\00T$\14\03\FF\04 \01\00`\000\1A\02\000\00\13\09p\01c%\16\02\02\00ZP\02q\CC\0F\00\81\19\02\02\D0\01\C4\19\1E\0C\00\A2\00\00\10x\06\00\10\D0\00\000\02\92t\04\FF\00\00\80\FF\FF\00\A0\001\1Cx\00\01\001p\F0\F00\02E$x\07\05p\00q\E2\0F\04\0Cz\00\06\90\00#`\F6 \00\16\06 \00\80\C6\0F\00\0Cx\00\05\7F@\00\C5D\F6\01\00\E4\0F\04\10x\03\00\08p\00`\1F\00!\12\04\02q\00\01\F1\04!\E2O0\00\11\070\00 \F2\04\C0\00c\88s\00\07\04\00\DE\05f\E8\0F\00\1D{\00\01\00u\EC\0F\00\84\B9\0B\06~\05R\22\0E\00\10x\B0\02\01`\00Q\C4\0F\00\10x\1E\00\03p\00\000\004\02\06\00P\00qb\0E\00\0B\B2\00\0B\06\08\B1\80\F4\03\00\E4\1F\08\0B\B2\00\02\10\00\A3\C0\F8\03\00\E4/\00\1C\B8\00\00\01%p\01\10\02\01\F0\00B\F4\03\00\C8\E0\00\11?\B0\00\C3t\01\00\CE\0F\00\08\82\0B\0B\02\00'\07\1B\E4@\01\8F\C6\0F\00\88\B3\00\06\0B\D0\00\095\A9\03\06\8E\06[(\0E\00\84\A9\B0\001\A2\00\03p\03`\80\F6\03\00\C4\1F\10\00(\02\03\B0\00$\A8\00p\00\04\B0\00\15\04\A0\01\03\B0\00\14\1F\90\01\01\B0\00/\03\03\B0\00\0AO\A3\00\06\03\80\01\0AV\07\06\00\80\00\B0\00\0E`\01$\07\07`\01\00\B0\00O\B2\00\02\07`\01\0B\1C\00`\01\19\0F`\01\02\10\04\0F`\01\09\1F\07`\01\0D\17@\B0\00\13\A9g\07\0F`\01\07\1F\00`\01\06\12\DA@\01\14\00\90\00\1F\CA0\01\0F9M\19\00\C0\05G$t\02\FFP\03A\00\84y\05_\09\01@\00\96&\0E\00%v\02\09\00`0\04'\84y\B0\00fh\0E\00\81y\08\D0\03bb\05\00\0Br\00\C1\05\22\80\F0\C0\001r\00\00\10\00\92\C0\F2\03\00\D6/\00\08\82\1F\00p\00\00\80\04\00\C8O \00\11\08\11\00 @\F0\C0\03$\0EF\A0\06\00\A0\00b\E6\0F\00\08r\09 \00\00\01\00p\CC\0F\00\A9s\09\02{\00\C0\09\E1\1E\00\00\A4\0E\00\0Cr\00\09\10\00 pR@\00QO\00$r\08\00\05\10\09\D0\00\80\D8\0F\00G\09\00\00\90\D0\05!\FF\83\F0\00*My\00\01TGy\00\00\F0 \00f\C0\0F\00\18y\00\01\00\0F\10\00\B0\0F\01\00-#\01\00\80\02\0B\01\00\22@\00\01\00=\9E\01\000\00\08\01\00\1F\0B@\00\04\13\DE)\00?\09\02\00@\00\0A\22\13\00@\05\0C\01\00\13\E8U\00\03\0F\03\01$\00\13\05\97\02\00\01\00\22\18\00\01\00.q\01T\00\00\01\00\11\90\B5\02O\00\00p\00\80\00\0B\1F)'\00\03\03\D5\02$\00\00\18\0D\04\E4\00*\04\00\01\00\1Fc@\00\04*0\05\C0\00\13\03\03\09\0C@\00!\8F\01D\01\0D@\00\13\D8@\00*\D8\00\01\00\1B\08\08\00?~\01\00N\0E\002\00\00\B0\C6\03\01W\09\04\80\00\17\048\00\04\18\00\138@\01\0C\84\01\13\C0@\00\17\881\01\0F\C0\00\01\132T\01\02\E6\07\06\01\00\1B\80\A9\00\11\03$\00J\00\0E\80\00\01\00\13\97#\00*\03\00\01\00\040\13/\00\04\80\00\0B\13\06\AB\01\04h\13\0C\01\00\1B\A8\08\00\17\08\08\02\17\05\E8\00\0C\01\00*\C0\09\08\00\088\00\18\06\A0\00\0F\01\00\05\03\A9\00\80\08\00\00\00\00\00\00\00\00\00\00\00\00\00\00"
@main_kernel_main_kColReduction_reduce__4_1_0___8w32h_kernel_name = internal constant [41 x i8] c"main_kColReduction_reduce__4_1_0___8w32h\00"
@main_kernel_blob_gpu.binary = internal constant [1096 x i8] c"P\EDU\BA\01\00\10\008\04\00\00\00\00\00\00\02\00\01\01@\00\00\00\F8\03\00\00\00\00\00\00\F8\03\00\00\00\00\00\00\07\00\01\00P\00\00\00\00\00\00\00\00\00\00\00\11 \00\00\00\00\00\00\00\00\00\00\00\00\00\00\E8\0B\00\00\00\00\00\00\A2\7FELF\02\01\013\07\00\01\00f\02\00\BE\00q\00\01\00!@\0B\07\001\00\80\08\07\00\F5\0E\00P\05P\00@\008\00\03\00@\00\0B\00\01\00\00.shstrtab\00.\08\00'ym\08\00\F0\1B_shndx\00.nv.info\00.text.main_kColReduction_r\0A\00\F5\01e__4_1_0___8w32h8\00\0F2\00\1Boshared4\00\1B\9Fconstant07\00\18\FA\01debug_frame\00.rel\11\00!nv\14\00\11aC\00\0F+\01 \0F\88\00\15\0FT\01\BAo_param[\01\1C\0F\01\00\06\8C[\00\00\00\03\00\0A\00\01\00\11\F0\18\00,\09\00\01\00 .\01\18\00,\04\00\01\00\11L\18\00,\07\00\01\00f2\00\00\00\12\10`\00!\80\01\07\00v\00\FF\FF\FF\FF(\00\0C\00\00\01\00@\03\00\04|\08\00\D1\0F\0C\81\80\80(\00\08\FF\81\80(\08\0B\00\06(\00/0\00\01\00\03\13pX\00\10\04\9B\00R\04\14\00\00\00E\00\01\0B\00\00\13\00p/\08\00\05\00\00\00\A7\03\22\04#\0C\00\00\01\00'\04\12\0C\00\17\11\0C\00!7\04\E8\03\F3\08\015\00\00\04\0A\08\00\02\00\00\00`\01\10\00\03\19\10\00\04\17\0C\0C\04U\08\00\00\F0!\10\00\10\01\18\01%\F0\11\10\00\01\01\00\F2\02\F0\11\00\03\1B\FF\00\04\1C\08\00P\00\00\00\B0\00\01\00#K\00\01\00s\02\02\08\10\0A/\22\9B\00\00\07\00\03\08\00\13\10\08\00\13\18\08\00\13 \08\00\13(\08\00\130\08\00\228\08|\00\13\00\08\00\13\08\08\00\13\10\08\00\13\18\08\00\13 \08\00\13(\08\00\130\08\00\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\A2\00\00\00\14,\00\00\00H\00\01\00\000\01/\05\00\01\00\FF\B0A\02z\01\00\1F\04\B1\0F\00\00\00\C4\0F\00\19y\02\00\01\00\10%\8B\02Q\0E\00\19y\03\0F\00\F5\1A\00!\00\00\00$\0E\00$z\02\02\00X\00\00\03\02\8E\07\00\CA\1F\00\0Cz\00\02\00Y\00\00p`\F0\03\00\DA\0F\00MS\04\A0\80\03\00\EA\0F\005t\03\FF\B3\03\10\FF\C0\03P\E2\0F\00\02x6\02B\80\FF\00\0F\10\00r\B9z\04\00\00F\00\84\00\94\D0\0F\00%v\02\02\00Z`\00`\0F\00\86y\00\022\00@\04\19\10\0C0\009My\00`\00PGy\00\00\F09\04\A6\FF\83\03\00\C0\0F\00\18y\00\01\00\0F\10\00\90\0F\01\00-\00W\01.\03\00\01\00\22@\00\01\00=+\01\000\00\08\01\00\1F\0B@\00\04\13k)\00\1F[@\00\0C\13\13\0C\04\0C\01\00\13\C8\15\00&\90\000\04#\04\00\85\04\00\F6\04\12\00\01\00\1F\FET\00\00\00\01\00\13X\95\00/p\00\80\00\0B\1F)'\00\03#\00\C8@\00\04P\06\04\E4\00*\04\00\01\00\1Fa@\00\04\13\F81\00&\\\00@\00\1F\0A@\00\00!\1C\01D\01\0D@\00\13X)\00*\D8\00\01\00\1B\08\08\00?\0B\01\00\86\07\00Q\00\000\05\00\01\00&\10\00\80\00\17\048\00\04\18\00\13\C7\14\01\0C\84\01*@\058\07\1F\00\C0\00\04\132@\00+\06\00\01\00\1A\07\D0\07\12\03\F0\05:\08\80\00\01\00\13\06\08\06\04(\0B\0C\01\00*\A8\00\08\00\04\F8\00\14\018\00/\05\00\01\00\029@\03\00\08\00\088\00/\06\00\01\00\17\80\08\00\00\00\00\00\00\00"
@ral_send_output___cpu___pvoid_i64_m2df32___void = internal constant [48 x i8] c"ral_send_output___cpu___pvoid_i64_m2df32___void\00"
@dealloc___gpu___pvoid_pvoid___void = internal constant [35 x i8] c"dealloc___gpu___pvoid_pvoid___void\00"
@inc_ref___gpu___pvoid_pvoid_m1df32_m1di64___m2df32 = internal constant [51 x i8] c"inc_ref___gpu___pvoid_pvoid_m1df32_m1di64___m2df32\00"
@main_kernel_2_main_kColReduction_reduce__4_1_0___8w16h_1_kernel_name = internal constant [43 x i8] c"main_kColReduction_reduce__4_1_0___8w16h_1\00"
@main_kernel_2_blob_gpu.binary = internal constant [2080 x i8] c"P\EDU\BA\01\00\10\00\10\08\00\00\00\00\00\00\02\00\01\01@\00\00\00\D0\07\00\00\00\00\00\00\CB\07\00\00\00\00\00\00\07\00\01\00P\00\00\00\00\00\00\00\00\00\00\00\11 \00\00\00\00\00\00\00\00\00\00\00\00\00\00(\13\00\00\00\00\00\00\A2\7FELF\02\01\013\07\00\01\00f\02\00\BE\00q\00\01\00#\80\12\08\00\11\0F\07\00\F5\0E\00P\05P\00@\008\00\03\00@\00\0C\00\01\00\00.shstrtab\00.\08\00'ym\08\00\F0\1B_shndx\00.nv.info\00.text.main_kColReduction_r\0A\00\F5\03e__4_1_0___8w16h_1:\00\0F4\00\1Doshared6\00\1AOrela\A0\00\1F?rel\D5\00\22\9Fconstant09\00\1A\B2debug_frame{\00\09\11\00!nv\14\00\11aE\00\0F\9E\01 \0F\8A\00\17\0F\C9\01\F4\8F$____wg_3\00\17\00\0C\00/27\02\02'o_param\09\02\1C\0F\01\00\05\8C]\00\00\00\03\00\0A\00\01\00\11\C2\18\00,\0B\00\01\00 \9C\01\18\00,\09\00\01\00\11\DC\18\00,\04\00\01\00\11\FA\18\00,\07\00\01\00g2\00\00\00\12\10x\00\03#\00f\FF\FF\FF\FF(\00\0C\00\00\01\00@\03\00\04|\08\00\D1\0F\0C\81\80\80(\00\08\FF\81\80(\08\0B\00\06(\00/0\00\01\00\03!\F0\06\07\00 \00\04\9B\00R\04\14\00\00\00E\002\04|\01\18\000/\08\00#\00\10\0E\0C\00\12#\0C\00\00\01\00'\04\12\0C\00\17\11\0C\00!7\04 \05\F1\08\015\00\00\04\0A\08\00\03\00\00\00`\01(\00\03\19(\00\04\17\0C$\00u\06\00 \00\00\F0!\10\00u\05\00\1C\00\00\F0\11\10\009\04\00\18\10\009\03\00\14\10\009\02\00\10\10\009\01\00\08P\00\01\01\00\C1\F0\11\00\03\1B\FF\00\04\1C\0C\00P\98\05\82\00\00P\06\00\00\04\1E\84\01#K\00\01\00\B1\02\02\08\10\0A/\22\00\00\00\08\10\00#\00\08\08\00\13\10\08\00\13\18\08\00\13 \08\00\13(\08\00\130\08\00\228\08\84\00\13\00\08\00\13\08\08\00\13\10\08\00\13\18\08\00\13 \08\00\13(\08\00\130\08\00\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\A2\00\00\00\14,\00\00\00H\00\01\00\11\02h\01\0F\01\00\FF\B0@$v\01\FF\7F\04\B2\FF\00\8E\07\00\C4\0F\00\19y\00\01\00\10%\8B\02a\0E\00\19y\03\00\01\00\10!-\00B\0E\00$z\B5\04\90\03\02\8E\07\00\CA\1F\00\0C\10\00\C5^\00\00p`\F0\03\00\DA\0F\00M\9B\04\F0\0C\80\03\00\EA\0F\00\06{\04\00\00_\00\00\00\90 \00\00\22\0E\00\19x\05\FF\1F\1F\00\D3\14\01\00\00\E2\0F\00$r\02\FF\FF\00\80\00 \E2\0FP\00\10\FF0\00@pP\F4\03\10\00\81\B9z\04\00\00F\00\00#\05p\E2\0F\00\11r\05\05?\00\B2\FF8\8F\07\00\C6\0F\00\08s\04\F6\04\10\10\A0\00\F1\06\1E\00\10x\03\04\FE\FF\FF\0F\FF\E0\FF\07\00\CC\1F\00\05s\03$\04\00\C0\03\01\C0\00!r\07p\00\C0\03\0A\8E\07\00\C8\1F\00$z\07\07p\00\10\FF\D0\00\81\C8\0F\00'r\07\03\07\E0\02\02\90\00@\19x\02\FF\10\00\10\05\B0\00\80\E4\0F\00\12x\05\05\80\F1\04!\C0\8E\80\00T'r\07\07\02\C0\00\10\C8\D0\00\11\09`\00\10\07`\00`\E4\0F\00$x\05\BC\03$\00\05\10\000z\03\09p\00\22\02\02@\00@\19x\00\FF\A0\00\22\05\16`\00f\0Cz\00\03\00_P\010\10\0A\03\10\00\11\80\D0\00p\E4\0F\00\10\08\07\07P\00\04\10\00\060\00\12\F20\00\1A\18 \001\12\AA\07P\01 \FF3\B0\01\00\A0\00\1B\03\A0\001\00\07\10\D1\03\05\A0\00\15\03\A0\00\10\E2\F0\00 \02\05\00\01#\FF\C0@\00Sx\09\03\08\00 \00\11\CA\80\003\09\00X\80\00\22\C8\0F\10\02p\\\00\00p\10\F2\04\90\00T$\14\03\FF\04 \01\00`\000\1A\02\000\00\13\09p\01c%\16\02\02\00ZP\02q\CC\0F\00\81\19\02\02\D0\01\C4\19\1E\0C\00\A2\00\00\10x\06\00\08\D0\00\000\02\92t\04\FF\00\00\80\FF\FF\00\A0\001\1Cx\00\01\001p\F0\F00\02E$x\07\05p\00q\E2\0F\04\0Cz\00\06\90\00#`\F6 \00\16\06 \00\80\C6\0F\00\0Cx\00\05?@\00\92D\F6\01\00\E4\0F\04\10xF\07\02@\01B\1F\04\10x\EC\04\04\80\00@!\12\04\02\81\00\01\15\00!\E2O@\00\11\07@\00 \F2\04\D0\00c\88s\00\07\04\00\EE\05f\E8\0F\00\1D{\00\01\00u\EC\0F\00\84\B9\0B\06\CE\05\84(\0E\00\84\B9\02\06\000\00qb\0E\00\0B\B2\00\0B\F6\07`\80\F4\03\00\C4\1F\10\00\11\02\10\00\A3\C0\F8\03\00\E4/\00\1C\B8\00\F0\00%p\01\00\02\01\E0\00B\F4\03\00\C8\D0\00\11\1F\90\00\C3t\01\00\CE\0F\00\08\82\0B\0B\02\00\17\07\1B\E40\01\8F\C6\0F\00\88\B3\00\06\0B\B0\00\09f\A9\03\06\00\80\00\B0\00\1B\A9\B0\001\A2\00\03`\03\22\80\F6\B0\00H\A2\00\02\03\B0\00$\A8\00p\00\04\B0\00\15\00\90\01\03\B0\00\14\0F\80\01\01\B0\00/\03\03\B0\00\0AO\A3\00\06\03`\01\0AG\07\06\00@\B0\00\13\B9\A7\06\06`\01*\07\07`\01/\00\07`\01\05\10\DA\90\00\02/\00\01\90\00\12\CA0\01\1F\07\80\00\089M\19\00\00\05G$t\02\FF\90\02A\00\84y\05\9F\08\01@\00\96&\0E\00%v\02\09\00`p\03'\84y\B0\00fh\0E\00\81y\08\10\03bb\05\00\0Br\00\01\05\22\80\F0\C0\001r\00\00\10\00\92\C0\F2\03\00\D6/\00\08\82\1F\00p\00\00\80\04\00\C8O \00\11\08\11\00 @\F0\00\03$\0EF\E0\05\00\A0\00b\E6\0F\00\08r\09 \00\00\01\00p\CC\0F\00\A9s\09\02{\00\C0\09\E1\1E\00\00\A4\0E\00\0Cr\00\09\10\00 pR@\00QO\00$r\08@\04\10\09\D0\00\80\D8\0F\00G\09\00\00\90\10\05!\FF\83\F0\00*My\00\01TGy\00\00\F0 \00f\C0\0F\00\18y\00\01\00\0F\10\00p\0F\01\00-\11\01@\0A\0E\01\00\22@\00\01\00=\9E\01\000\00\08\01\00\1F\0B@\00\04\13\DE)\00?\09\02\00@\00\0A\22\13\00\A0\04\0C\01\00\13\E8U\00\03\7F\03\01$\00\13\05W\02\00\01\00\22\18\00\01\00.q\01T\00\00\01\00\11\90u\02O\00\00p\00\80\00\0B\1F)'\00\03\03\95\02$\00\00\18\0C\04\E4\00*\04\00\01\00\1Fc@\00\04*0\05\C0\00\13\03\03\08\0C@\00!\8F\01D\01\0D@\00\13\D8@\00*\D8\00\01\00\1B\08\08\00?~\01\00N\0D\002\00\00\B0\86\03\01W\08\04\80\00\17\048\00\04\18\00\138@\01\0C\84\01\13\C0@\00\17\881\01\0F\C0\00\01\132T\01\15\06R\00\03>\03\1A\08\98\0D\11\03$\00J\00\0E\80\00\01\00\13\97\94\00*\03\00\01\00\040\12/\00\02\80\00\0B\13\06\AB\01\04h\12\0C\01\00\1B\A8\08\00\04\97\00\13\018\00\04\E8\00\0C\01\00*\C0\08\08\00\088\00\18\06\A0\00\0F\01\00\05\03\B8\00\80\08\00\00\00\00\00\00\00\00\00\00\00\00"
@ral_kernel_launch___gpu___pvoid_ppvoid_i64_pvoid_i64_i64_i64_i64_i64_i64_i32_pvoid_i32_ppvoid___void = internal constant [101 x i8] c"ral_kernel_launch___gpu___pvoid_ppvoid_i64_pvoid_i64_i64_i64_i64_i64_i64_i32_pvoid_i32_ppvoid___void\00"
@main_kernel_1_main_kColReduction_reduce__4_1_0___8w16h_kernel_name = internal constant [41 x i8] c"main_kColReduction_reduce__4_1_0___8w16h\00"
@main_kernel_1_blob_gpu.binary = internal constant [1096 x i8] c"P\EDU\BA\01\00\10\008\04\00\00\00\00\00\00\02\00\01\01@\00\00\00\F8\03\00\00\00\00\00\00\F8\03\00\00\00\00\00\00\07\00\01\00P\00\00\00\00\00\00\00\00\00\00\00\11 \00\00\00\00\00\00\00\00\00\00\00\00\00\00\E8\0B\00\00\00\00\00\00\A2\7FELF\02\01\013\07\00\01\00f\02\00\BE\00q\00\01\00!@\0B\07\001\00\80\08\07\00\F5\0E\00P\05P\00@\008\00\03\00@\00\0B\00\01\00\00.shstrtab\00.\08\00'ym\08\00\F0\1B_shndx\00.nv.info\00.text.main_kColReduction_r\0A\00\F5\01e__4_1_0___8w16h8\00\0F2\00\1Boshared4\00\1B\9Fconstant07\00\18\FA\01debug_frame\00.rel\11\00!nv\14\00\11aC\00\0F+\01 \0F\88\00\15\0FT\01\BAo_param[\01\1C\0F\01\00\06\8C[\00\00\00\03\00\0A\00\01\00\11\F0\18\00,\09\00\01\00 .\01\18\00,\04\00\01\00\11L\18\00,\07\00\01\00f2\00\00\00\12\10`\00!\80\01\07\00v\00\FF\FF\FF\FF(\00\0C\00\00\01\00@\03\00\04|\08\00\D1\0F\0C\81\80\80(\00\08\FF\81\80(\08\0B\00\06(\00/0\00\01\00\03\13pX\00\10\04\9B\00R\04\14\00\00\00E\00\01\0B\00\00\13\00p/\08\00\05\00\00\00\A7\03\22\04#\0C\00\00\01\00'\04\12\0C\00\17\11\0C\00!7\04\E8\03\F3\08\015\00\00\04\0A\08\00\02\00\00\00`\01\10\00\03\19\10\00\04\17\0C\0C\04U\08\00\00\F0!\10\00\10\01\18\01%\F0\11\10\00\01\01\00\F2\02\F0\11\00\03\1B\FF\00\04\1C\08\00P\00\00\00\B0\00\01\00#K\00\01\00s\02\02\08\10\0A/\22\9B\00\00\07\00\03\08\00\13\10\08\00\13\18\08\00\13 \08\00\13(\08\00\130\08\00\228\08|\00\13\00\08\00\13\08\08\00\13\10\08\00\13\18\08\00\13 \08\00\13(\08\00\130\08\00\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\A2\00\00\00\14,\00\00\00H\00\01\00\000\01/\05\00\01\00\FF\B0A\02z\01\00\1F\04\B1\0F\00\00\00\C4\0F\00\19y\02\00\01\00\10%\8B\02Q\0E\00\19y\03\0F\00\F5\1A\00!\00\00\00$\0E\00$z\02\02\00X\00\00\03\02\8E\07\00\CA\1F\00\0Cz\00\02\00Y\00\00p`\F0\03\00\DA\0F\00MS\04\A0\80\03\00\EA\0F\005t\03\FF\B3\03\10\FF\C0\03P\E2\0F\00\02x6\02B\80\FF\00\0F\10\00r\B9z\04\00\00F\00\84\00\94\D0\0F\00%v\02\02\00Z`\00`\0F\00\86y\00\022\00@\04\19\10\0C0\009My\00`\00PGy\00\00\F09\04\A6\FF\83\03\00\C0\0F\00\18y\00\01\00\0F\10\00\90\0F\01\00-\00W\01.\03\00\01\00\22@\00\01\00=+\01\000\00\08\01\00\1F\0B@\00\04\13k)\00\1F[@\00\0C\13\13\0C\04\0C\01\00\13\C8\15\00&\90\000\04#\04\00\85\04\00\F6\04\12\00\01\00\1F\FET\00\00\00\01\00\13X\95\00/p\00\80\00\0B\1F)'\00\03#\00\C8@\00\04P\06\04\E4\00*\04\00\01\00\1Fa@\00\04\13\F81\00&\\\00@\00\1F\0A@\00\00!\1C\01D\01\0D@\00\13X)\00*\D8\00\01\00\1B\08\08\00?\0B\01\00\86\07\00Q\00\000\05\00\01\00&\10\00\80\00\17\048\00\04\18\00\13\C7\14\01\0C\84\01*@\058\07\1F\00\C0\00\04\132@\00+\06\00\01\00\1A\07\D0\07\12\03\F0\05:\08\80\00\01\00\13\06\08\06\04(\0B\0C\01\00*\A8\00\08\00\04\F8\00\14\018\00/\05\00\01\00\029@\03\00\08\00\088\00/\06\00\01\00\17\80\08\00\00\00\00\00\00\00"
@alloc___gpu___pvoid_i64___pvoid = internal constant [32 x i8] c"alloc___gpu___pvoid_i64___pvoid\00"
@ral_recv_input___cpu___pvoid_i64___m3df32 = internal constant [42 x i8] c"ral_recv_input___cpu___pvoid_i64___m3df32\00"

define void @disc_ral_call(ptr nocapture readonly %0, ptr %1, ptr %2) local_unnamed_addr {
entry:
  %3 = load ptr, ptr %0, align 8
  %4 = getelementptr ptr, ptr %0, i64 1
  %5 = load ptr, ptr %4, align 8
  %6 = load ptr, ptr %2, align 8
  store ptr %3, ptr %6, align 8
  tail call void %5(ptr %3, ptr %1, ptr nonnull %2)
  ret void
}

define void @main(ptr %0) local_unnamed_addr {
  %2 = alloca %0, align 8
  %3 = alloca [3 x ptr], align 8
  store ptr %2, ptr %3, align 8
  %4 = getelementptr inbounds %0, ptr %2, i64 0, i32 1
  store i64 0, ptr %4, align 8
  %5 = getelementptr inbounds ptr, ptr %3, i64 1
  store ptr %4, ptr %5, align 8
  %6 = getelementptr inbounds %0, ptr %2, i64 0, i32 2
  %7 = getelementptr inbounds ptr, ptr %3, i64 2
  store ptr %6, ptr %7, align 8
  %8 = load ptr, ptr %0, align 8
  %9 = getelementptr ptr, ptr %0, i64 1
  %10 = load ptr, ptr %9, align 8
  store ptr %8, ptr %2, align 8
  call void %10(ptr %8, ptr nonnull @ral_recv_input___cpu___pvoid_i64___m3df32, ptr nonnull %3)
  %.fca.1.gep = getelementptr inbounds %0, ptr %2, i64 0, i32 2, i32 1
  %.fca.1.load = load ptr, ptr %.fca.1.gep, align 8
  %.fca.3.0.gep = getelementptr inbounds %0, ptr %2, i64 0, i32 2, i32 3
  %.fca.3.0.load = load i64, ptr %.fca.3.0.gep, align 8
  %.fca.3.1.gep = getelementptr inbounds %0, ptr %2, i64 0, i32 2, i32 3, i64 1
  %.fca.3.1.load = load i64, ptr %.fca.3.1.gep, align 8
  %.fca.3.2.gep = getelementptr inbounds %0, ptr %2, i64 0, i32 2, i32 3, i64 2
  %.fca.3.2.load = load i64, ptr %.fca.3.2.gep, align 8
  %11 = mul i64 %.fca.3.2.load, %.fca.3.1.load
  %sext = shl i64 %11, 32
  %12 = ashr exact i64 %sext, 32
  %.idx = ashr exact i64 %sext, 30
  %13 = alloca %.1, align 8
  %14 = alloca [3 x ptr], align 8
  store ptr %13, ptr %14, align 8
  %15 = getelementptr inbounds %.1, ptr %13, i64 0, i32 1
  store i64 %.idx, ptr %15, align 8
  %16 = getelementptr inbounds ptr, ptr %14, i64 1
  store ptr %15, ptr %16, align 8
  %17 = getelementptr inbounds %.1, ptr %13, i64 0, i32 2
  %18 = getelementptr inbounds ptr, ptr %14, i64 2
  store ptr %17, ptr %18, align 8
  %19 = load ptr, ptr %0, align 8
  %20 = load ptr, ptr %9, align 8
  store ptr %19, ptr %13, align 8
  call void %20(ptr %19, ptr nonnull @alloc___gpu___pvoid_i64___pvoid, ptr nonnull %14)
  %21 = load ptr, ptr %17, align 8
  %22 = mul i64 %12, %.fca.3.0.load
  %23 = add i64 %22, -1
  %24 = lshr i64 %23, 8
  %25 = add nuw nsw i64 %24, 1
  %26 = sub i64 0, %22
  %.neg = sdiv i64 %26, -256
  %27 = icmp sgt i64 %22, 0
  %28 = select i1 %27, i64 %25, i64 %.neg
  %29 = icmp sgt i64 %28, 108
  %30 = icmp slt i64 %sext, 4294967296
  %31 = sub nsw i64 0, %12
  %32 = add nsw i64 %12, -1
  %33 = select i1 %30, i64 %31, i64 %32
  %34 = alloca ptr, align 8
  %35 = alloca [3 x ptr], align 8
  %36 = getelementptr inbounds ptr, ptr %35, i64 1
  %37 = getelementptr inbounds ptr, ptr %35, i64 2
  %38 = alloca [14 x ptr], align 8
  %39 = getelementptr inbounds ptr, ptr %38, i64 1
  br i1 %29, label %40, label %141

40:                                               ; preds = %1
  %41 = sdiv i64 %33, 256
  %42 = sub nsw i64 0, %41
  %43 = add nsw i64 %41, 1
  %44 = select i1 %30, i64 %42, i64 %43
  store ptr @main_kernel_blob_gpu.binary, ptr %34, align 8
  %45 = alloca %.9, align 8
  store i64 256, ptr %45, align 8
  store ptr %45, ptr %35, align 8
  %46 = getelementptr inbounds %.9, ptr %45, i64 0, i32 1
  store i64 %12, ptr %46, align 8
  store ptr %46, ptr %36, align 8
  %47 = getelementptr inbounds %.9, ptr %45, i64 0, i32 2
  store ptr %21, ptr %47, align 8
  store ptr %47, ptr %37, align 8
  %48 = alloca %.10, align 8
  store ptr %48, ptr %38, align 8
  %49 = getelementptr inbounds %.10, ptr %48, i64 0, i32 1
  store ptr %34, ptr %49, align 8
  store ptr %49, ptr %39, align 8
  %50 = getelementptr inbounds %.10, ptr %48, i64 0, i32 2
  store i64 1, ptr %50, align 8
  %51 = getelementptr inbounds ptr, ptr %38, i64 2
  store ptr %50, ptr %51, align 8
  %52 = getelementptr inbounds %.10, ptr %48, i64 0, i32 3
  store ptr @main_kernel_main_kColReduction_reduce__4_1_0___8w32h_kernel_name, ptr %52, align 8
  %53 = getelementptr inbounds ptr, ptr %38, i64 3
  store ptr %52, ptr %53, align 8
  %54 = getelementptr inbounds %.10, ptr %48, i64 0, i32 4
  store i64 %44, ptr %54, align 8
  %55 = getelementptr inbounds ptr, ptr %38, i64 4
  store ptr %54, ptr %55, align 8
  %56 = getelementptr inbounds %.10, ptr %48, i64 0, i32 5
  store i64 1, ptr %56, align 8
  %57 = getelementptr inbounds ptr, ptr %38, i64 5
  store ptr %56, ptr %57, align 8
  %58 = getelementptr inbounds %.10, ptr %48, i64 0, i32 6
  store i64 1, ptr %58, align 8
  %59 = getelementptr inbounds ptr, ptr %38, i64 6
  store ptr %58, ptr %59, align 8
  %60 = getelementptr inbounds %.10, ptr %48, i64 0, i32 7
  store i64 256, ptr %60, align 8
  %61 = getelementptr inbounds ptr, ptr %38, i64 7
  store ptr %60, ptr %61, align 8
  %62 = getelementptr inbounds %.10, ptr %48, i64 0, i32 8
  store i64 1, ptr %62, align 8
  %63 = getelementptr inbounds ptr, ptr %38, i64 8
  store ptr %62, ptr %63, align 8
  %64 = getelementptr inbounds %.10, ptr %48, i64 0, i32 9
  store i64 1, ptr %64, align 8
  %65 = getelementptr inbounds ptr, ptr %38, i64 9
  store ptr %64, ptr %65, align 8
  %66 = getelementptr inbounds %.10, ptr %48, i64 0, i32 10
  store i32 0, ptr %66, align 8
  %67 = getelementptr inbounds ptr, ptr %38, i64 10
  store ptr %66, ptr %67, align 8
  %68 = getelementptr inbounds %.10, ptr %48, i64 0, i32 11
  store ptr null, ptr %68, align 8
  %69 = getelementptr inbounds ptr, ptr %38, i64 11
  store ptr %68, ptr %69, align 8
  %70 = getelementptr inbounds %.10, ptr %48, i64 0, i32 12
  store i32 3, ptr %70, align 8
  %71 = getelementptr inbounds ptr, ptr %38, i64 12
  store ptr %70, ptr %71, align 8
  %72 = getelementptr inbounds %.10, ptr %48, i64 0, i32 13
  store ptr %35, ptr %72, align 8
  %73 = getelementptr inbounds ptr, ptr %38, i64 13
  store ptr %72, ptr %73, align 8
  %74 = load ptr, ptr %0, align 8
  %75 = load ptr, ptr %9, align 8
  store ptr %74, ptr %48, align 8
  call void %75(ptr %74, ptr nonnull @ral_kernel_launch___gpu___pvoid_ppvoid_i64_pvoid_i64_i64_i64_i64_i64_i64_i32_pvoid_i32_ppvoid___void, ptr nonnull %38)
  %76 = sdiv i64 %32, 8
  %77 = add nsw i64 %76, 1
  %.neg17.lhs.trunc = trunc i64 %11 to i32
  %.neg1719 = sdiv i32 %.neg17.lhs.trunc, 8
  %.neg17.sext = sext i32 %.neg1719 to i64
  %78 = icmp sgt i64 %sext, 0
  %79 = select i1 %78, i64 %77, i64 %.neg17.sext
  %80 = add i64 %.fca.3.0.load, 2305843009213693951
  %81 = lshr i64 %80, 5
  %82 = add nuw nsw i64 %81, 1
  %83 = sub i64 0, %.fca.3.0.load
  %.neg18 = sdiv i64 %83, -32
  %84 = icmp sgt i64 %.fca.3.0.load, 0
  %85 = select i1 %84, i64 %82, i64 %.neg18
  %86 = shl i64 %85, 8
  %87 = mul i64 %86, %79
  %88 = icmp slt i64 %87, 1
  %89 = sub i64 0, %87
  %90 = add i64 %87, -1
  %91 = select i1 %88, i64 %89, i64 %90
  %92 = sdiv i64 %91, 256
  %93 = sub nsw i64 0, %92
  %94 = add nsw i64 %92, 1
  %95 = select i1 %88, i64 %93, i64 %94
  %96 = alloca ptr, align 8
  store ptr @main_kernel_0_blob_gpu.binary, ptr %96, align 8
  %97 = alloca %.11, align 8
  %98 = alloca [7 x ptr], align 8
  store i64 %12, ptr %97, align 8
  store ptr %97, ptr %98, align 8
  %99 = getelementptr inbounds %.11, ptr %97, i64 0, i32 1
  store ptr %.fca.1.load, ptr %99, align 8
  %100 = getelementptr inbounds ptr, ptr %98, i64 1
  store ptr %99, ptr %100, align 8
  %101 = getelementptr inbounds %.11, ptr %97, i64 0, i32 2
  store i64 %.fca.3.0.load, ptr %101, align 8
  %102 = getelementptr inbounds ptr, ptr %98, i64 2
  store ptr %101, ptr %102, align 8
  %103 = getelementptr inbounds %.11, ptr %97, i64 0, i32 3
  store i64 256, ptr %103, align 8
  %104 = getelementptr inbounds ptr, ptr %98, i64 3
  store ptr %103, ptr %104, align 8
  %105 = getelementptr inbounds %.11, ptr %97, i64 0, i32 4
  store i64 %87, ptr %105, align 8
  %106 = getelementptr inbounds ptr, ptr %98, i64 4
  store ptr %105, ptr %106, align 8
  %107 = getelementptr inbounds %.11, ptr %97, i64 0, i32 5
  store i64 %79, ptr %107, align 8
  %108 = getelementptr inbounds ptr, ptr %98, i64 5
  store ptr %107, ptr %108, align 8
  %109 = getelementptr inbounds %.11, ptr %97, i64 0, i32 6
  store ptr %21, ptr %109, align 8
  %110 = getelementptr inbounds ptr, ptr %98, i64 6
  store ptr %109, ptr %110, align 8
  %111 = alloca %.12, align 8
  %112 = alloca [14 x ptr], align 8
  store ptr %111, ptr %112, align 8
  %113 = getelementptr inbounds %.12, ptr %111, i64 0, i32 1
  store ptr %96, ptr %113, align 8
  %114 = getelementptr inbounds ptr, ptr %112, i64 1
  store ptr %113, ptr %114, align 8
  %115 = getelementptr inbounds %.12, ptr %111, i64 0, i32 2
  store i64 1, ptr %115, align 8
  %116 = getelementptr inbounds ptr, ptr %112, i64 2
  store ptr %115, ptr %116, align 8
  %117 = getelementptr inbounds %.12, ptr %111, i64 0, i32 3
  store ptr @main_kernel_0_main_kColReduction_reduce__4_1_0___8w32h_1_kernel_name, ptr %117, align 8
  %118 = getelementptr inbounds ptr, ptr %112, i64 3
  store ptr %117, ptr %118, align 8
  %119 = getelementptr inbounds %.12, ptr %111, i64 0, i32 4
  store i64 %95, ptr %119, align 8
  %120 = getelementptr inbounds ptr, ptr %112, i64 4
  store ptr %119, ptr %120, align 8
  %121 = getelementptr inbounds %.12, ptr %111, i64 0, i32 5
  store i64 1, ptr %121, align 8
  %122 = getelementptr inbounds ptr, ptr %112, i64 5
  store ptr %121, ptr %122, align 8
  %123 = getelementptr inbounds %.12, ptr %111, i64 0, i32 6
  store i64 1, ptr %123, align 8
  %124 = getelementptr inbounds ptr, ptr %112, i64 6
  store ptr %123, ptr %124, align 8
  %125 = getelementptr inbounds %.12, ptr %111, i64 0, i32 7
  store i64 256, ptr %125, align 8
  %126 = getelementptr inbounds ptr, ptr %112, i64 7
  store ptr %125, ptr %126, align 8
  %127 = getelementptr inbounds %.12, ptr %111, i64 0, i32 8
  store i64 1, ptr %127, align 8
  %128 = getelementptr inbounds ptr, ptr %112, i64 8
  store ptr %127, ptr %128, align 8
  %129 = getelementptr inbounds %.12, ptr %111, i64 0, i32 9
  store i64 1, ptr %129, align 8
  %130 = getelementptr inbounds ptr, ptr %112, i64 9
  store ptr %129, ptr %130, align 8
  %131 = getelementptr inbounds %.12, ptr %111, i64 0, i32 10
  store i32 0, ptr %131, align 8
  %132 = getelementptr inbounds ptr, ptr %112, i64 10
  store ptr %131, ptr %132, align 8
  %133 = getelementptr inbounds %.12, ptr %111, i64 0, i32 11
  store ptr null, ptr %133, align 8
  %134 = getelementptr inbounds ptr, ptr %112, i64 11
  store ptr %133, ptr %134, align 8
  %135 = getelementptr inbounds %.12, ptr %111, i64 0, i32 12
  store i32 7, ptr %135, align 8
  %136 = getelementptr inbounds ptr, ptr %112, i64 12
  store ptr %135, ptr %136, align 8
  %137 = getelementptr inbounds %.12, ptr %111, i64 0, i32 13
  store ptr %98, ptr %137, align 8
  %138 = getelementptr inbounds ptr, ptr %112, i64 13
  store ptr %137, ptr %138, align 8
  %139 = load ptr, ptr %0, align 8
  %140 = load ptr, ptr %9, align 8
  store ptr %139, ptr %111, align 8
  call void %140(ptr %139, ptr nonnull @ral_kernel_launch___gpu___pvoid_ppvoid_i64_pvoid_i64_i64_i64_i64_i64_i64_i32_pvoid_i32_ppvoid___void, ptr nonnull %112)
  br label %242

141:                                              ; preds = %1
  %142 = sdiv i64 %33, 128
  %143 = sub nsw i64 0, %142
  %144 = add nsw i64 %142, 1
  %145 = select i1 %30, i64 %143, i64 %144
  store ptr @main_kernel_1_blob_gpu.binary, ptr %34, align 8
  %146 = alloca %.2, align 8
  store i64 128, ptr %146, align 8
  store ptr %146, ptr %35, align 8
  %147 = getelementptr inbounds %.2, ptr %146, i64 0, i32 1
  store i64 %12, ptr %147, align 8
  store ptr %147, ptr %36, align 8
  %148 = getelementptr inbounds %.2, ptr %146, i64 0, i32 2
  store ptr %21, ptr %148, align 8
  store ptr %148, ptr %37, align 8
  %149 = alloca %.3, align 8
  store ptr %149, ptr %38, align 8
  %150 = getelementptr inbounds %.3, ptr %149, i64 0, i32 1
  store ptr %34, ptr %150, align 8
  store ptr %150, ptr %39, align 8
  %151 = getelementptr inbounds %.3, ptr %149, i64 0, i32 2
  store i64 1, ptr %151, align 8
  %152 = getelementptr inbounds ptr, ptr %38, i64 2
  store ptr %151, ptr %152, align 8
  %153 = getelementptr inbounds %.3, ptr %149, i64 0, i32 3
  store ptr @main_kernel_1_main_kColReduction_reduce__4_1_0___8w16h_kernel_name, ptr %153, align 8
  %154 = getelementptr inbounds ptr, ptr %38, i64 3
  store ptr %153, ptr %154, align 8
  %155 = getelementptr inbounds %.3, ptr %149, i64 0, i32 4
  store i64 %145, ptr %155, align 8
  %156 = getelementptr inbounds ptr, ptr %38, i64 4
  store ptr %155, ptr %156, align 8
  %157 = getelementptr inbounds %.3, ptr %149, i64 0, i32 5
  store i64 1, ptr %157, align 8
  %158 = getelementptr inbounds ptr, ptr %38, i64 5
  store ptr %157, ptr %158, align 8
  %159 = getelementptr inbounds %.3, ptr %149, i64 0, i32 6
  store i64 1, ptr %159, align 8
  %160 = getelementptr inbounds ptr, ptr %38, i64 6
  store ptr %159, ptr %160, align 8
  %161 = getelementptr inbounds %.3, ptr %149, i64 0, i32 7
  store i64 128, ptr %161, align 8
  %162 = getelementptr inbounds ptr, ptr %38, i64 7
  store ptr %161, ptr %162, align 8
  %163 = getelementptr inbounds %.3, ptr %149, i64 0, i32 8
  store i64 1, ptr %163, align 8
  %164 = getelementptr inbounds ptr, ptr %38, i64 8
  store ptr %163, ptr %164, align 8
  %165 = getelementptr inbounds %.3, ptr %149, i64 0, i32 9
  store i64 1, ptr %165, align 8
  %166 = getelementptr inbounds ptr, ptr %38, i64 9
  store ptr %165, ptr %166, align 8
  %167 = getelementptr inbounds %.3, ptr %149, i64 0, i32 10
  store i32 0, ptr %167, align 8
  %168 = getelementptr inbounds ptr, ptr %38, i64 10
  store ptr %167, ptr %168, align 8
  %169 = getelementptr inbounds %.3, ptr %149, i64 0, i32 11
  store ptr null, ptr %169, align 8
  %170 = getelementptr inbounds ptr, ptr %38, i64 11
  store ptr %169, ptr %170, align 8
  %171 = getelementptr inbounds %.3, ptr %149, i64 0, i32 12
  store i32 3, ptr %171, align 8
  %172 = getelementptr inbounds ptr, ptr %38, i64 12
  store ptr %171, ptr %172, align 8
  %173 = getelementptr inbounds %.3, ptr %149, i64 0, i32 13
  store ptr %35, ptr %173, align 8
  %174 = getelementptr inbounds ptr, ptr %38, i64 13
  store ptr %173, ptr %174, align 8
  %175 = load ptr, ptr %0, align 8
  %176 = load ptr, ptr %9, align 8
  store ptr %175, ptr %149, align 8
  call void %176(ptr %175, ptr nonnull @ral_kernel_launch___gpu___pvoid_ppvoid_i64_pvoid_i64_i64_i64_i64_i64_i64_i32_pvoid_i32_ppvoid___void, ptr nonnull %38)
  %177 = sdiv i64 %32, 8
  %178 = add nsw i64 %177, 1
  %.neg1.lhs.trunc = trunc i64 %11 to i32
  %.neg120 = sdiv i32 %.neg1.lhs.trunc, 8
  %.neg1.sext = sext i32 %.neg120 to i64
  %179 = icmp sgt i64 %sext, 0
  %180 = select i1 %179, i64 %178, i64 %.neg1.sext
  %181 = add i64 %.fca.3.0.load, 2305843009213693951
  %182 = lshr i64 %181, 4
  %183 = add nuw nsw i64 %182, 1
  %184 = sub i64 0, %.fca.3.0.load
  %.neg2 = sdiv i64 %184, -16
  %185 = icmp sgt i64 %.fca.3.0.load, 0
  %186 = select i1 %185, i64 %183, i64 %.neg2
  %187 = shl i64 %186, 7
  %188 = mul i64 %187, %180
  %189 = icmp slt i64 %188, 1
  %190 = sub i64 0, %188
  %191 = add i64 %188, -1
  %192 = select i1 %189, i64 %190, i64 %191
  %193 = sdiv i64 %192, 128
  %194 = sub nsw i64 0, %193
  %195 = add nsw i64 %193, 1
  %196 = select i1 %189, i64 %194, i64 %195
  %197 = alloca ptr, align 8
  store ptr @main_kernel_2_blob_gpu.binary, ptr %197, align 8
  %198 = alloca %.4, align 8
  %199 = alloca [7 x ptr], align 8
  store i64 %12, ptr %198, align 8
  store ptr %198, ptr %199, align 8
  %200 = getelementptr inbounds %.4, ptr %198, i64 0, i32 1
  store ptr %.fca.1.load, ptr %200, align 8
  %201 = getelementptr inbounds ptr, ptr %199, i64 1
  store ptr %200, ptr %201, align 8
  %202 = getelementptr inbounds %.4, ptr %198, i64 0, i32 2
  store i64 %.fca.3.0.load, ptr %202, align 8
  %203 = getelementptr inbounds ptr, ptr %199, i64 2
  store ptr %202, ptr %203, align 8
  %204 = getelementptr inbounds %.4, ptr %198, i64 0, i32 3
  store i64 128, ptr %204, align 8
  %205 = getelementptr inbounds ptr, ptr %199, i64 3
  store ptr %204, ptr %205, align 8
  %206 = getelementptr inbounds %.4, ptr %198, i64 0, i32 4
  store i64 %188, ptr %206, align 8
  %207 = getelementptr inbounds ptr, ptr %199, i64 4
  store ptr %206, ptr %207, align 8
  %208 = getelementptr inbounds %.4, ptr %198, i64 0, i32 5
  store i64 %180, ptr %208, align 8
  %209 = getelementptr inbounds ptr, ptr %199, i64 5
  store ptr %208, ptr %209, align 8
  %210 = getelementptr inbounds %.4, ptr %198, i64 0, i32 6
  store ptr %21, ptr %210, align 8
  %211 = getelementptr inbounds ptr, ptr %199, i64 6
  store ptr %210, ptr %211, align 8
  %212 = alloca %.5, align 8
  %213 = alloca [14 x ptr], align 8
  store ptr %212, ptr %213, align 8
  %214 = getelementptr inbounds %.5, ptr %212, i64 0, i32 1
  store ptr %197, ptr %214, align 8
  %215 = getelementptr inbounds ptr, ptr %213, i64 1
  store ptr %214, ptr %215, align 8
  %216 = getelementptr inbounds %.5, ptr %212, i64 0, i32 2
  store i64 1, ptr %216, align 8
  %217 = getelementptr inbounds ptr, ptr %213, i64 2
  store ptr %216, ptr %217, align 8
  %218 = getelementptr inbounds %.5, ptr %212, i64 0, i32 3
  store ptr @main_kernel_2_main_kColReduction_reduce__4_1_0___8w16h_1_kernel_name, ptr %218, align 8
  %219 = getelementptr inbounds ptr, ptr %213, i64 3
  store ptr %218, ptr %219, align 8
  %220 = getelementptr inbounds %.5, ptr %212, i64 0, i32 4
  store i64 %196, ptr %220, align 8
  %221 = getelementptr inbounds ptr, ptr %213, i64 4
  store ptr %220, ptr %221, align 8
  %222 = getelementptr inbounds %.5, ptr %212, i64 0, i32 5
  store i64 1, ptr %222, align 8
  %223 = getelementptr inbounds ptr, ptr %213, i64 5
  store ptr %222, ptr %223, align 8
  %224 = getelementptr inbounds %.5, ptr %212, i64 0, i32 6
  store i64 1, ptr %224, align 8
  %225 = getelementptr inbounds ptr, ptr %213, i64 6
  store ptr %224, ptr %225, align 8
  %226 = getelementptr inbounds %.5, ptr %212, i64 0, i32 7
  store i64 128, ptr %226, align 8
  %227 = getelementptr inbounds ptr, ptr %213, i64 7
  store ptr %226, ptr %227, align 8
  %228 = getelementptr inbounds %.5, ptr %212, i64 0, i32 8
  store i64 1, ptr %228, align 8
  %229 = getelementptr inbounds ptr, ptr %213, i64 8
  store ptr %228, ptr %229, align 8
  %230 = getelementptr inbounds %.5, ptr %212, i64 0, i32 9
  store i64 1, ptr %230, align 8
  %231 = getelementptr inbounds ptr, ptr %213, i64 9
  store ptr %230, ptr %231, align 8
  %232 = getelementptr inbounds %.5, ptr %212, i64 0, i32 10
  store i32 0, ptr %232, align 8
  %233 = getelementptr inbounds ptr, ptr %213, i64 10
  store ptr %232, ptr %233, align 8
  %234 = getelementptr inbounds %.5, ptr %212, i64 0, i32 11
  store ptr null, ptr %234, align 8
  %235 = getelementptr inbounds ptr, ptr %213, i64 11
  store ptr %234, ptr %235, align 8
  %236 = getelementptr inbounds %.5, ptr %212, i64 0, i32 12
  store i32 7, ptr %236, align 8
  %237 = getelementptr inbounds ptr, ptr %213, i64 12
  store ptr %236, ptr %237, align 8
  %238 = getelementptr inbounds %.5, ptr %212, i64 0, i32 13
  store ptr %199, ptr %238, align 8
  %239 = getelementptr inbounds ptr, ptr %213, i64 13
  store ptr %238, ptr %239, align 8
  %240 = load ptr, ptr %0, align 8
  %241 = load ptr, ptr %9, align 8
  store ptr %240, ptr %212, align 8
  call void %241(ptr %240, ptr nonnull @ral_kernel_launch___gpu___pvoid_ppvoid_i64_pvoid_i64_i64_i64_i64_i64_i64_i32_pvoid_i32_ppvoid___void, ptr nonnull %213)
  br label %242

242:                                              ; preds = %40, %141
  %243 = alloca [16 x i64], align 8
  store i64 %.fca.3.1.load, ptr %243, align 8
  %244 = getelementptr inbounds i64, ptr %243, i64 1
  store i64 %.fca.3.2.load, ptr %244, align 8
  %245 = alloca %.6, align 8
  %246 = alloca [13 x ptr], align 8
  store ptr %245, ptr %246, align 8
  %247 = getelementptr inbounds %.6, ptr %245, i64 0, i32 1
  store ptr null, ptr %247, align 8
  %248 = getelementptr inbounds ptr, ptr %246, i64 1
  store ptr %247, ptr %248, align 8
  %249 = getelementptr inbounds %.6, ptr %245, i64 0, i32 2
  store ptr %21, ptr %249, align 8
  %250 = getelementptr inbounds ptr, ptr %246, i64 2
  store ptr %249, ptr %250, align 8
  %251 = getelementptr inbounds %.6, ptr %245, i64 0, i32 3
  store ptr %21, ptr %251, align 8
  %252 = getelementptr inbounds ptr, ptr %246, i64 3
  store ptr %251, ptr %252, align 8
  %253 = getelementptr inbounds %.6, ptr %245, i64 0, i32 4
  store i64 0, ptr %253, align 8
  %254 = getelementptr inbounds ptr, ptr %246, i64 4
  store ptr %253, ptr %254, align 8
  %255 = getelementptr inbounds %.6, ptr %245, i64 0, i32 5
  store i64 %12, ptr %255, align 8
  %256 = getelementptr inbounds ptr, ptr %246, i64 5
  store ptr %255, ptr %256, align 8
  %257 = getelementptr inbounds %.6, ptr %245, i64 0, i32 6
  store i64 1, ptr %257, align 8
  %258 = getelementptr inbounds ptr, ptr %246, i64 6
  store ptr %257, ptr %258, align 8
  %259 = getelementptr inbounds %.6, ptr %245, i64 0, i32 7
  store ptr %243, ptr %259, align 8
  %260 = getelementptr inbounds ptr, ptr %246, i64 7
  store ptr %259, ptr %260, align 8
  %261 = getelementptr inbounds %.6, ptr %245, i64 0, i32 8
  store ptr %243, ptr %261, align 8
  %262 = getelementptr inbounds ptr, ptr %246, i64 8
  store ptr %261, ptr %262, align 8
  %263 = getelementptr inbounds %.6, ptr %245, i64 0, i32 9
  store i64 0, ptr %263, align 8
  %264 = getelementptr inbounds ptr, ptr %246, i64 9
  store ptr %263, ptr %264, align 8
  %265 = getelementptr inbounds %.6, ptr %245, i64 0, i32 10
  store i64 2, ptr %265, align 8
  %266 = getelementptr inbounds ptr, ptr %246, i64 10
  store ptr %265, ptr %266, align 8
  %267 = getelementptr inbounds %.6, ptr %245, i64 0, i32 11
  store i64 1, ptr %267, align 8
  %268 = getelementptr inbounds ptr, ptr %246, i64 11
  store ptr %267, ptr %268, align 8
  %269 = getelementptr inbounds %.6, ptr %245, i64 0, i32 12
  %270 = getelementptr inbounds ptr, ptr %246, i64 12
  store ptr %269, ptr %270, align 8
  %271 = load ptr, ptr %0, align 8
  %272 = load ptr, ptr %9, align 8
  store ptr %271, ptr %245, align 8
  call void %272(ptr %271, ptr nonnull @inc_ref___gpu___pvoid_pvoid_m1df32_m1di64___m2df32, ptr nonnull %246)
  %.unpack = load ptr, ptr %269, align 8
  %.elt3 = getelementptr inbounds %.6, ptr %245, i64 0, i32 12, i32 1
  %.unpack4 = load ptr, ptr %.elt3, align 8
  %273 = alloca %.7, align 8
  %274 = alloca [2 x ptr], align 8
  store ptr %273, ptr %274, align 8
  %275 = getelementptr inbounds %.7, ptr %273, i64 0, i32 1
  store ptr %21, ptr %275, align 8
  %276 = getelementptr inbounds ptr, ptr %274, i64 1
  store ptr %275, ptr %276, align 8
  %277 = load ptr, ptr %0, align 8
  %278 = load ptr, ptr %9, align 8
  store ptr %277, ptr %273, align 8
  call void %278(ptr %277, ptr nonnull @dealloc___gpu___pvoid_pvoid___void, ptr nonnull %274)
  %279 = alloca %.8, align 8
  %280 = alloca [9 x ptr], align 8
  store ptr %279, ptr %280, align 8
  %281 = getelementptr inbounds %.8, ptr %279, i64 0, i32 1
  store i64 0, ptr %281, align 8
  %282 = getelementptr inbounds ptr, ptr %280, i64 1
  store ptr %281, ptr %282, align 8
  %283 = getelementptr inbounds %.8, ptr %279, i64 0, i32 2
  store ptr %.unpack, ptr %283, align 8
  %284 = getelementptr inbounds ptr, ptr %280, i64 2
  store ptr %283, ptr %284, align 8
  %285 = getelementptr inbounds %.8, ptr %279, i64 0, i32 3
  store ptr %.unpack4, ptr %285, align 8
  %286 = getelementptr inbounds ptr, ptr %280, i64 3
  store ptr %285, ptr %286, align 8
  %287 = getelementptr inbounds %.8, ptr %279, i64 0, i32 4
  store i64 0, ptr %287, align 8
  %288 = getelementptr inbounds ptr, ptr %280, i64 4
  store ptr %287, ptr %288, align 8
  %289 = getelementptr inbounds %.8, ptr %279, i64 0, i32 5
  store i64 %.fca.3.1.load, ptr %289, align 8
  %290 = getelementptr inbounds ptr, ptr %280, i64 5
  store ptr %289, ptr %290, align 8
  %291 = getelementptr inbounds %.8, ptr %279, i64 0, i32 6
  store i64 %.fca.3.2.load, ptr %291, align 8
  %292 = getelementptr inbounds ptr, ptr %280, i64 6
  store ptr %291, ptr %292, align 8
  %293 = getelementptr inbounds %.8, ptr %279, i64 0, i32 7
  store i64 %.fca.3.2.load, ptr %293, align 8
  %294 = getelementptr inbounds ptr, ptr %280, i64 7
  store ptr %293, ptr %294, align 8
  %295 = getelementptr inbounds %.8, ptr %279, i64 0, i32 8
  store i64 1, ptr %295, align 8
  %296 = getelementptr inbounds ptr, ptr %280, i64 8
  store ptr %295, ptr %296, align 8
  %297 = load ptr, ptr %0, align 8
  %298 = load ptr, ptr %9, align 8
  store ptr %297, ptr %279, align 8
  call void %298(ptr %297, ptr nonnull @ral_send_output___cpu___pvoid_i64_m2df32___void, ptr nonnull %280)
  ret void
}

[DISC] LowerLLVMToBinary takes: 2.571000e-02 s.
object file to shared library command: gcc --shared -o /root/.cache/bazel/_bazel_root/54ece412abe75fa85ab728a2c061e33c/execroot/org_disc_compiler/_tmp/b247c1654ecfc81b23b002417f617beaColReduceFullyDynamicShape3DF32_0.so /root/.cache/bazel/_bazel_root/54ece412abe75fa85ab728a2c061e33c/execroot/org_disc_compiler/_tmp/b247c1654ecfc81b23b002417f617beaColReduceFullyDynamicShape3DF32_0.so.o
save shared lib file to : /root/.cache/bazel/_bazel_root/54ece412abe75fa85ab728a2c061e33c/execroot/org_disc_compiler/_tmp/b247c1654ecfc81b23b002417f617beaColReduceFullyDynamicShape3DF32_0.so
[DISC] BinaryStrToSharedLibrary takes: 9.361000e-03 s.
[DISC] LowerHLOToSharedLibrary takes: 7.740070e-01 s.

============ END ============

2023-06-25 03:54:50.135116: I mlir/disc/tests/mlir_test.cc:275] ret: 0

2023-06-25 03:54:50.135189: I mlir/disc/tests/mlir_test.cc:241] run compiled program

2023-06-25 03:54:50.332083: I external/org_tensorflow/tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-06-25 03:54:50.334920: I mlir/disc/tests/mlir_test.cc:932] --- MLIR Execution uses: 2.854 ms
2023-06-25 03:54:50.335017: I mlir/disc/tests/mlir_test.cc:183] out buffer = 0x7f143b28bc00
2023-06-25 03:54:50.335025: I mlir/disc/tests/mlir_test.cc:184] out shape:
2023-06-25 03:54:50.335029: I mlir/disc/tests/mlir_test.cc:186]   dim #0: 100
2023-06-25 03:54:50.335032: I mlir/disc/tests/mlir_test.cc:186]   dim #1: 13
2023-06-25 03:54:50.335080: I mlir/disc/tests/mlir_test.cc:244] run golden tf

2023-06-25 03:54:50.335088: I mlir/disc/tests/mlir_test.cc:257] program_path: external/org_tensorflow/tensorflow/compiler/mlir/tf-mlir-translate

2023-06-25 03:54:50.393096: I mlir/disc/tests/mlir_test.cc:269] Executed: external/org_tensorflow/tensorflow/compiler/mlir/tf-mlir-translate -mlir-to-graphdef /root/.cache/bazel/_bazel_root/54ece412abe75fa85ab728a2c061e33c/execroot/org_disc_compiler/_tmp/b247c1654ecfc81b23b002417f617bea/tempfile-adf59ec6ac82-7602f1e3-2553507-5feec320fcb82 -o /root/.cache/bazel/_bazel_root/54ece412abe75fa85ab728a2c061e33c/execroot/org_disc_compiler/_tmp/b247c1654ecfc81b23b002417f617beaColReduceFullyDynamicShape3DF32_0.pbtxt 
2023-06-25 03:54:50.393107: I mlir/disc/tests/mlir_test.cc:270] external/org_tensorflow/tensorflow/compiler/mlir/tf-mlir-translate: 0
2023-06-25 03:54:50.393110: I mlir/disc/tests/mlir_test.cc:271] -- stdout:

============ END ============

2023-06-25 03:54:50.393112: I mlir/disc/tests/mlir_test.cc:273] -- stderr:

============ END ============

2023-06-25 03:54:50.393115: I mlir/disc/tests/mlir_test.cc:275] ret: 0

2023-06-25 03:54:50.393123: I mlir/disc/tests/mlir_test.cc:391] graphdef_path: /root/.cache/bazel/_bazel_root/54ece412abe75fa85ab728a2c061e33c/execroot/org_disc_compiler/_tmp/b247c1654ecfc81b23b002417f617beaColReduceFullyDynamicShape3DF32_0.pbtxt
2023-06-25 03:54:50.394635: I external/org_tensorflow/tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: SSE3 SSE4.1 SSE4.2 AVX AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-06-25 03:54:51.192339: I external/org_tensorflow/tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-06-25 03:54:51.194875: I external/org_tensorflow/tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-06-25 03:54:51.197243: I external/org_tensorflow/tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-06-25 03:54:51.199619: I external/org_tensorflow/tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-06-25 03:54:51.201932: I external/org_tensorflow/tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-06-25 03:54:51.204248: I external/org_tensorflow/tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-06-25 03:54:51.206557: I external/org_tensorflow/tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-06-25 03:54:51.208883: I external/org_tensorflow/tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-06-25 03:54:51.323204: I external/org_tensorflow/tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-06-25 03:54:51.325587: I external/org_tensorflow/tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-06-25 03:54:51.327938: I external/org_tensorflow/tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-06-25 03:54:51.330273: I external/org_tensorflow/tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-06-25 03:54:51.332594: I external/org_tensorflow/tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-06-25 03:54:51.334906: I external/org_tensorflow/tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-06-25 03:54:51.337221: I external/org_tensorflow/tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-06-25 03:54:51.339533: I external/org_tensorflow/tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-06-25 03:54:51.341844: I external/org_tensorflow/tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-06-25 03:54:51.344155: I external/org_tensorflow/tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-06-25 03:54:51.346460: I external/org_tensorflow/tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-06-25 03:54:51.348787: I external/org_tensorflow/tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-06-25 03:54:51.351099: I external/org_tensorflow/tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-06-25 03:54:51.353399: I external/org_tensorflow/tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-06-25 03:54:51.355713: I external/org_tensorflow/tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-06-25 03:54:51.358012: I external/org_tensorflow/tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-06-25 03:54:53.875099: I external/org_tensorflow/tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-06-25 03:54:53.877662: I external/org_tensorflow/tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-06-25 03:54:53.880067: I external/org_tensorflow/tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-06-25 03:54:53.882466: I external/org_tensorflow/tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-06-25 03:54:53.884825: I external/org_tensorflow/tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-06-25 03:54:53.887182: I external/org_tensorflow/tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-06-25 03:54:53.889526: I external/org_tensorflow/tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-06-25 03:54:53.891862: I external/org_tensorflow/tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-06-25 03:54:53.894263: I external/org_tensorflow/tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-06-25 03:54:53.896614: I external/org_tensorflow/tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-06-25 03:54:53.898934: I external/org_tensorflow/tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-06-25 03:54:53.901334: I external/org_tensorflow/tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-06-25 03:54:53.903745: I external/org_tensorflow/tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-06-25 03:54:53.906053: I external/org_tensorflow/tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-06-25 03:54:53.908369: I external/org_tensorflow/tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-06-25 03:54:53.910691: I external/org_tensorflow/tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-06-25 03:54:53.913004: I external/org_tensorflow/tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-06-25 03:54:53.915329: I external/org_tensorflow/tensorflow/core/common_runtime/gpu/gpu_device.cc:1635] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 79025 MB memory:  -> device: 0, name: NVIDIA A100-SXM4-80GB, pci bus id: 0000:00:07.0, compute capability: 8.0
2023-06-25 03:54:53.916381: I external/org_tensorflow/tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-06-25 03:54:53.918668: I external/org_tensorflow/tensorflow/core/common_runtime/gpu/gpu_device.cc:1635] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 79149 MB memory:  -> device: 1, name: NVIDIA A100-SXM4-80GB, pci bus id: 0000:00:08.0, compute capability: 8.0
2023-06-25 03:54:53.919248: I external/org_tensorflow/tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-06-25 03:54:53.921533: I external/org_tensorflow/tensorflow/core/common_runtime/gpu/gpu_device.cc:1635] Created device /job:localhost/replica:0/task:0/device:GPU:2 with 79149 MB memory:  -> device: 2, name: NVIDIA A100-SXM4-80GB, pci bus id: 0000:00:09.0, compute capability: 8.0
2023-06-25 03:54:53.922068: I external/org_tensorflow/tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-06-25 03:54:53.924342: I external/org_tensorflow/tensorflow/core/common_runtime/gpu/gpu_device.cc:1635] Created device /job:localhost/replica:0/task:0/device:GPU:3 with 79149 MB memory:  -> device: 3, name: NVIDIA A100-SXM4-80GB, pci bus id: 0000:00:0a.0, compute capability: 8.0
2023-06-25 03:54:53.924896: I external/org_tensorflow/tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-06-25 03:54:53.927170: I external/org_tensorflow/tensorflow/core/common_runtime/gpu/gpu_device.cc:1635] Created device /job:localhost/replica:0/task:0/device:GPU:4 with 79149 MB memory:  -> device: 4, name: NVIDIA A100-SXM4-80GB, pci bus id: 0000:00:0b.0, compute capability: 8.0
2023-06-25 03:54:53.927712: I external/org_tensorflow/tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-06-25 03:54:53.929974: I external/org_tensorflow/tensorflow/core/common_runtime/gpu/gpu_device.cc:1635] Created device /job:localhost/replica:0/task:0/device:GPU:5 with 79149 MB memory:  -> device: 5, name: NVIDIA A100-SXM4-80GB, pci bus id: 0000:00:0c.0, compute capability: 8.0
2023-06-25 03:54:53.930520: I external/org_tensorflow/tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-06-25 03:54:53.932789: I external/org_tensorflow/tensorflow/core/common_runtime/gpu/gpu_device.cc:1635] Created device /job:localhost/replica:0/task:0/device:GPU:6 with 79149 MB memory:  -> device: 6, name: NVIDIA A100-SXM4-80GB, pci bus id: 0000:00:0d.0, compute capability: 8.0
2023-06-25 03:54:53.933311: I external/org_tensorflow/tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-06-25 03:54:53.935589: I external/org_tensorflow/tensorflow/core/common_runtime/gpu/gpu_device.cc:1635] Created device /job:localhost/replica:0/task:0/device:GPU:7 with 79149 MB memory:  -> device: 7, name: NVIDIA A100-SXM4-80GB, pci bus id: 0000:00:0e.0, compute capability: 8.0
2023-06-25 03:54:53.976928: I mlir/disc/tests/mlir_test.cc:467] --- TF Execution uses: 22.847 ms
2023-06-25 03:54:53.976954: I mlir/disc/tests/mlir_test.cc:474] 	output shape #0: [100,13]
2023-06-25 03:54:53.977276: I mlir/disc/tests/mlir_test.cc:467] --- TF Execution uses: 0.314 ms
2023-06-25 03:54:53.977281: I mlir/disc/tests/mlir_test.cc:474] 	output shape #0: [100,13]
2023-06-25 03:54:53.977634: I mlir/disc/tests/mlir_test.cc:467] --- TF Execution uses: 0.348 ms
2023-06-25 03:54:53.977638: I mlir/disc/tests/mlir_test.cc:474] 	output shape #0: [100,13]
2023-06-25 03:54:53.977984: I mlir/disc/tests/mlir_test.cc:467] --- TF Execution uses: 0.341 ms
2023-06-25 03:54:53.977988: I mlir/disc/tests/mlir_test.cc:474] 	output shape #0: [100,13]
2023-06-25 03:54:53.978224: I mlir/disc/tests/mlir_test.cc:467] --- TF Execution uses: 0.232 ms
2023-06-25 03:54:53.978228: I mlir/disc/tests/mlir_test.cc:474] 	output shape #0: [100,13]
2023-06-25 03:54:53.978238: I mlir/disc/tests/mlir_test.cc:484] processing output 0
2023-06-25 03:54:53.979839: I ./mlir/disc/tests/mlir_feature_test.h:37] Unset env setting:
[       OK ] TFMaxOpTest.ColReduceFullyDynamicShape3DF32 (4793 ms)
[----------] 1 test from TFMaxOpTest (4793 ms total)

[----------] Global test environment tear-down
[==========] 1 test from 1 test suite ran. (4794 ms total)
[  PASSED  ] 1 test.
