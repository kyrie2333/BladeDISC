exec ${PAGER:-/usr/bin/less} "$0" || exit 1
Executing tests from //mlir/disc/tests/tensorflow_ops:max.cpp.test
-----------------------------------------------------------------------------
[==========] Running 1 test from 1 test suite.
[----------] Global test environment set-up.
[----------] 1 test from TFMaxOpTest
[ RUN      ] TFMaxOpTest.ColReduceFullyDynamicShape3DF32_1
2023-07-03 08:37:02.195512: I ./mlir/disc/tests/mlir_feature_test.h:29] Apply env setting:
2023-07-03 08:37:02.195524: I ./mlir/disc/tests/mlir_feature_test.h:31] 	DISC_MEM_INTENSIVE_OPT_EXPERIMENTAL = true
2023-07-03 08:37:02.195527: I ./mlir/disc/tests/mlir_feature_test.h:31] 	DISC_ENABLE_STITCH = true
2023-07-03 08:37:02.195530: I mlir/disc/tests/mlir_feature_test.cc:271] Testing for CUDA backend
2023-07-03 08:37:02.195596: I mlir/disc/tests/mlir_feature_test.cc:145] Original TF code: func.func @main(%arg0: tensor<?x?x?xf32>) -> tensor<?x?xf32> attributes {tf.entry_function = {inputs = "{{INPUTS}}", outputs = "{{OUTPUTS}}", input_placements="{{INPUT_PLACEMENTS}}", output_placements="{{OUTPUT_PLACEMENTS}}"}} {
  %graph = tf_executor.graph {
    %1:2 = tf_executor.island wraps "tf.Const"() {value = dense<[0]> : tensor<1xi32>} : () -> tensor<1xi32>
    %2:2 = tf_executor.island wraps "tf.Abs"(%arg0) : (tensor<?x?x?xf32>) -> tensor<?x?x?xf32>
    %3:2 = tf_executor.island wraps "tf.Max"(%2, %1) : (tensor<?x?x?xf32>, tensor<1xi32>) -> tensor<?x?xf32>
    tf_executor.fetch %3 : tensor<?x?xf32>
  }
  return %graph : tensor<?x?xf32>
}
2023-07-03 08:37:02.195611: I mlir/disc/tests/mlir_feature_test.cc:149] New TF code: func.func @main(%arg0: tensor<?x?x?xf32>) -> tensor<?x?xf32> attributes {tf.entry_function = {inputs = "input0", outputs = "output0", input_placements="gpu", output_placements="gpu"}} {
  %graph = tf_executor.graph {
    %1:2 = tf_executor.island wraps "tf.Const"() {value = dense<[0]> : tensor<1xi32>} : () -> tensor<1xi32>
    %2:2 = tf_executor.island wraps "tf.Abs"(%arg0) : (tensor<?x?x?xf32>) -> tensor<?x?x?xf32>
    %3:2 = tf_executor.island wraps "tf.Max"(%2, %1) : (tensor<?x?x?xf32>, tensor<1xi32>) -> tensor<?x?xf32>
    tf_executor.fetch %3 : tensor<?x?xf32>
  }
  return %graph : tensor<?x?xf32>
}
2023-07-03 08:37:02.195718: I mlir/disc/tests/mlir_test.cc:233] tf_opt_pat: external/org_tensorflow/tensorflow/compiler/mlir/tf-opt
2023-07-03 08:37:02.195723: I mlir/disc/tests/mlir_test.cc:234] mlir_file_path: /root/.cache/bazel/_bazel_root/54ece412abe75fa85ab728a2c061e33c/execroot/org_disc_compiler/_tmp/b247c1654ecfc81b23b002417f617bea/tempfile-adf59ec6ac82-b79f4076-1052541-5ff91121403af
2023-07-03 08:37:02.195726: I mlir/disc/tests/mlir_test.cc:235] tmp_dir: /root/.cache/bazel/_bazel_root/54ece412abe75fa85ab728a2c061e33c/execroot/org_disc_compiler/_tmp/b247c1654ecfc81b23b002417f617bea
2023-07-03 08:37:02.195728: I mlir/disc/tests/mlir_test.cc:236] test_name: ColReduceFullyDynamicShape3DF32_1_0
2023-07-03 08:37:02.195734: I mlir/disc/tests/mlir_test.cc:284] tf_opt_path: external/org_tensorflow/tensorflow/compiler/mlir/tf-opt

2023-07-03 08:37:02.195740: I mlir/disc/tests/mlir_test.cc:257] program_path: external/org_tensorflow/tensorflow/compiler/mlir/tf-opt

2023-07-03 08:37:02.292527: I mlir/disc/tests/mlir_test.cc:269] Executed: external/org_tensorflow/tensorflow/compiler/mlir/tf-opt --tf-standard-pipeline /root/.cache/bazel/_bazel_root/54ece412abe75fa85ab728a2c061e33c/execroot/org_disc_compiler/_tmp/b247c1654ecfc81b23b002417f617bea/tempfile-adf59ec6ac82-b79f4076-1052541-5ff91121403af -o /root/.cache/bazel/_bazel_root/54ece412abe75fa85ab728a2c061e33c/execroot/org_disc_compiler/_tmp/b247c1654ecfc81b23b002417f617beaColReduceFullyDynamicShape3DF32_1_0_tf_dialect.mlir 
2023-07-03 08:37:02.292544: I mlir/disc/tests/mlir_test.cc:270] external/org_tensorflow/tensorflow/compiler/mlir/tf-opt: 0
2023-07-03 08:37:02.292547: I mlir/disc/tests/mlir_test.cc:271] -- stdout:

============ END ============

2023-07-03 08:37:02.292549: I mlir/disc/tests/mlir_test.cc:273] -- stderr:
2023-07-03 08:37:02.279494: I external/org_tensorflow/tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: SSE3 SSE4.1 SSE4.2 AVX AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.

============ END ============

2023-07-03 08:37:02.292552: I mlir/disc/tests/mlir_test.cc:275] ret: 0

2023-07-03 08:37:02.292567: I mlir/disc/tests/mlir_test.cc:257] program_path: mlir/disc/disc_compiler_main

2023-07-03 08:37:04.233350: I mlir/disc/tests/mlir_test.cc:269] Executed: mlir/disc/disc_compiler_main --mlir-print-elementsattrs-with-hex-if-larger -1 --mlir-elide-elementsattrs-if-larger 8 /root/.cache/bazel/_bazel_root/54ece412abe75fa85ab728a2c061e33c/execroot/org_disc_compiler/_tmp/b247c1654ecfc81b23b002417f617beaColReduceFullyDynamicShape3DF32_1_0_tf_dialect.mlir /root/.cache/bazel/_bazel_root/54ece412abe75fa85ab728a2c061e33c/execroot/org_disc_compiler/_tmp/b247c1654ecfc81b23b002417f617beaColReduceFullyDynamicShape3DF32_1_0.so 
2023-07-03 08:37:04.233373: I mlir/disc/tests/mlir_test.cc:270] mlir/disc/disc_compiler_main: 0
2023-07-03 08:37:04.233375: I mlir/disc/tests/mlir_test.cc:271] -- stdout:

============ END ============

2023-07-03 08:37:04.234604: I mlir/disc/tests/mlir_test.cc:273] -- stderr:
======== BEGIN Original Module =========
module {
  func.func @main(%arg0: tensor<?x?x?xf32>) -> tensor<?x?xf32> attributes {tf.entry_function = {input_placements = "gpu", inputs = "input0", output_placements = "gpu", outputs = "output0"}} {
    %cst = "tf.Const"() {value = dense<0> : tensor<1xi32>} : () -> tensor<1xi32>
    %0 = "tf.Abs"(%arg0) : (tensor<?x?x?xf32>) -> tensor<?x?x?xf32>
    %1 = "tf.Max"(%0, %cst) : (tensor<?x?x?xf32>, tensor<1xi32>) -> tensor<?x?xf32>
    return %1 : tensor<?x?xf32>
  }
}

======= END Original Module ==========
[DISC] Load Input IR takes: 2.055000e-03 s.
[[ INFO ]] Running TF2XLA
2023-07-03 08:37:02.381636: I external/org_tensorflow/tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: SSE3 SSE4.1 SSE4.2 AVX AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
// -----// IR Dump After SCCP (sccp) //----- //
module {
  func.func @main(%arg0: tensor<?x?x?xf32>) -> tensor<?x?xf32> attributes {tf.entry_function = {input_placements = "gpu", inputs = "input0", output_placements = "gpu", outputs = "output0"}} {
    %cst = "tf.Const"() {value = dense<0> : tensor<1xi32>} : () -> tensor<1xi32>
    %0 = "tf.Abs"(%arg0) : (tensor<?x?x?xf32>) -> tensor<?x?x?xf32>
    %1 = "tf.Max"(%0, %cst) : (tensor<?x?x?xf32>, tensor<1xi32>) -> tensor<?x?xf32>
    return %1 : tensor<?x?xf32>
  }
}


// -----// IR Dump After SCCP (sccp) //----- //
module {
  func.func @main(%arg0: tensor<?x?x?xf32>) -> tensor<?x?xf32> attributes {tf.entry_function = {input_placements = "gpu", inputs = "input0", output_placements = "gpu", outputs = "output0"}} {
    %cst = "tf.Const"() {value = dense<0> : tensor<1xi32>} : () -> tensor<1xi32>
    %0 = "tf.Abs"(%arg0) : (tensor<?x?x?xf32>) -> tensor<?x?x?xf32>
    %1 = "tf.Max"(%0, %cst) : (tensor<?x?x?xf32>, tensor<1xi32>) -> tensor<?x?xf32>
    return %1 : tensor<?x?xf32>
  }
}


// -----// IR Dump After LegalizeTF (xla-legalize-tf) //----- //
func.func @main(%arg0: tensor<?x?x?xf32>) -> tensor<?x?xf32> attributes {tf.entry_function = {input_placements = "gpu", inputs = "input0", output_placements = "gpu", outputs = "output0"}} {
  %0 = mhlo.constant dense<0> : tensor<1xi32>
  %1 = mhlo.abs %arg0 : tensor<?x?x?xf32>
  %2 = mhlo.convert %1 : tensor<?x?x?xf32>
  %3 = mhlo.constant dense<0xFF800000> : tensor<f32>
  %4 = mhlo.reduce(%2 init: %3) applies mhlo.maximum across dimensions = [0] : (tensor<?x?x?xf32>, tensor<f32>) -> tensor<?x?xf32>
  %5 = mhlo.convert %4 : tensor<?x?xf32>
  return %5 : tensor<?x?xf32>
}

// -----// IR Dump After DiscLowerTfPass (disc-lower-tf) //----- //
func.func @main(%arg0: tensor<?x?x?xf32>) -> tensor<?x?xf32> attributes {tf.entry_function = {input_placements = "gpu", inputs = "input0", output_placements = "gpu", outputs = "output0"}} {
  %0 = mhlo.constant dense<0xFF800000> : tensor<f32>
  %1 = mhlo.abs %arg0 : tensor<?x?x?xf32>
  %2 = mhlo.reduce(%1 init: %0) applies mhlo.maximum across dimensions = [0] : (tensor<?x?x?xf32>, tensor<f32>) -> tensor<?x?xf32>
  return %2 : tensor<?x?xf32>
}

===-------------------------------------------------------------------------===
                         ... Execution time report ...
===-------------------------------------------------------------------------===
  Total Execution Time: 0.0097 seconds

  ----Wall Time----  ----Name----
    0.0000 (  0.4%)  ReviseArgumentsForStaticRankPass
    0.0000 (  0.1%)  FunctionalControlFlowToRegionsPass
    0.0054 ( 55.3%)  Inliner
    0.0000 (  0.2%)    (A) CallGraph
    0.0052 ( 53.0%)  'func.func' Pipeline
    0.0051 ( 53.0%)    Canonicalizer
    0.0001 (  1.2%)  'func.func' Pipeline
    0.0000 (  0.1%)    DropWhileShapeInvariantPass
    0.0000 (  0.1%)    ReplicateTensorListInitOpsPass
    0.0001 (  1.0%)    Canonicalizer
    0.0003 (  3.1%)  SCCP
    0.0000 (  0.2%)  GuaranteeAllFuncsOneUsePass
    0.0000 (  0.0%)    (A) CallGraph
    0.0000 (  0.1%)  TensorFlowShapeInferencePass
    0.0003 (  3.1%)  SCCP
    0.0000 (  0.2%)  TensorListOpsDecompositionPass
    0.0000 (  0.1%)  StackOpsDecompositionPass
    0.0000 (  0.1%)  TensorArrayOpsDecompositionPass
    0.0001 (  0.6%)  'func.func' Pipeline
    0.0001 (  0.6%)    DecomposeResourceOpsPass
    0.0000 (  0.2%)  PromoteResourcesToArgsPass
    0.0000 (  0.1%)  SymbolDCE
    0.0000 (  0.1%)  'func.func' Pipeline
    0.0000 (  0.1%)    SinkConstantsToControlFlowPass
    0.0000 (  0.1%)  TensorFlowShapeInferencePass
    0.0002 (  2.6%)  StablehloLegalizeToHloPass
    0.0000 (  0.5%)  'func.func' Pipeline
    0.0000 (  0.3%)    DiscLowerTfPass
    0.0000 (  0.2%)    LowerQuantizedPass
    0.0000 (  0.1%)  LegalizeTfTypesPass
    0.0010 ( 10.2%)  'func.func' Pipeline
    0.0008 (  8.1%)    LegalizeTF
    0.0002 (  2.0%)    DiscLowerTfPass
    0.0000 (  0.1%)    mlir::mhlo::{anonymous}::AdjustLayout
    0.0000 (  0.2%)  LegalizeTFCollective
    0.0001 (  0.8%)  'func.func' Pipeline
    0.0001 (  0.8%)    Canonicalizer
    0.0000 (  0.1%)  TensorFlowShapeInferencePass
    0.0003 (  3.5%)  'func.func' Pipeline
    0.0003 (  3.4%)    LegalizeTF
    0.0000 (  0.1%)  LegalizeTFCommunicationPass
    0.0000 (  0.4%)  'func.func' Pipeline
    0.0000 (  0.3%)    DiscDynamicSliceConverterPass
    0.0000 (  0.1%)    SinkConstantsToControlFlowPass
   -0.0036 (-36.8%)  Rest
    0.0097 (100.0%)  Total
======== BEGIN After TF2HLO =========
module {
  func.func @main(%arg0: tensor<?x?x?xf32>) -> tensor<?x?xf32> attributes {tf.entry_function = {input_placements = "gpu", inputs = "input0", output_placements = "gpu", outputs = "output0"}} {
    %0 = mhlo.constant dense<0xFF800000> : tensor<f32>
    %1 = mhlo.abs %arg0 : tensor<?x?x?xf32>
    %2 = mhlo.reduce(%1 init: %0) applies mhlo.maximum across dimensions = [0] : (tensor<?x?x?xf32>, tensor<f32>) -> tensor<?x?xf32>
    return %2 : tensor<?x?xf32>
  }
}

======= END After TF2HLO ==========
[DISC] tf2hlo takes: 1.006200e-02 s.
SymbolicDimMgr::save walkRankedTensorValue takes: 4 us
SymbolicDimMgr::save update attributes takes: 1 us
SymbolicDimMgr::updateProductEqualityMap simplifySymbolicDimProductPair takes: 0 us
productSet.size() = 0
SymbolicDimMgr::updateProductEqualityMap propagate graph takes: 1 us
SymbolicDimMgr::updateProductEqualityMap remove multiply takes: 0 us
SymbolicDimMgr::updateProductEqualityMap build toRemove  takes: 0 us
SymbolicDimMgr::updateProductEqualityMap apply toRemove  takes: 0 us
SymbolicDimMgr::save updateProductEqualityMap takes: 14 us
SymbolicDimMgr::save updateFunctionType takes: 1 us
SymbolicDimMgr::save collect symbolicDim ops takes: 4 us
SymbolicDimMgr::save remove symbolicDim ops takes: 1 us
SymbolicDimMgr::save remove unused production takes: 0 us
SymbolicDimMgr::save remove unused production #2 takes: 0 us
SymbolicDimMgr::save canonicalize the name takes: 4 us
SymbolicDimMgr::save replace the name takes: 3 us
SymbolicDimMgr::save updateFunctionType takes: 0 us
// -----// IR Dump After DiscShapeOptimizationPass (disc-shape-optimization) //----- //
module {
  func.func @main(%arg0: tensor<?x?x?xf32, [@S0, @S1, @S2]>) -> tensor<?x?xf32, [@S1, @S2]> attributes {tf.entry_function = {input_placements = "gpu", inputs = "input0", output_placements = "gpu", outputs = "output0"}} {
    %0 = mhlo.constant dense<0xFF800000> : tensor<f32>
    %1 = mhlo.abs %arg0 : tensor<?x?x?xf32, [@S0, @S1, @S2]>
    %2 = mhlo.reduce(%1 init: %0) applies mhlo.maximum across dimensions = [0] : (tensor<?x?x?xf32, [@S0, @S1, @S2]>, tensor<f32>) -> tensor<?x?xf32, [@S1, @S2]>
    return %2 : tensor<?x?xf32, [@S1, @S2]>
  }
  "disc_shape.SymbolicDim"() {knownNegativeOne = false, knownNonNegative = true, knownNonSizeOne = false, knownNonSizeZero = false, sym_name = "S0", value = -9223372036854775808 : i64} : () -> ()
  "disc_shape.SymbolicDim"() {knownNegativeOne = false, knownNonNegative = true, knownNonSizeOne = false, knownNonSizeZero = false, sym_name = "S1", value = -9223372036854775808 : i64} : () -> ()
  "disc_shape.SymbolicDim"() {knownNegativeOne = false, knownNonNegative = true, knownNonSizeOne = false, knownNonSizeZero = false, sym_name = "S2", value = -9223372036854775808 : i64} : () -> ()
  func.func @shape_constraint_graph() {
    return
  }
}


SymbolicDimMgr::save walkRankedTensorValue takes: 3 us
SymbolicDimMgr::save update attributes takes: 0 us
SymbolicDimMgr::updateProductEqualityMap simplifySymbolicDimProductPair takes: 0 us
productSet.size() = 0
SymbolicDimMgr::updateProductEqualityMap propagate graph takes: 4 us
SymbolicDimMgr::updateProductEqualityMap remove multiply takes: 0 us
SymbolicDimMgr::updateProductEqualityMap build toRemove  takes: 0 us
SymbolicDimMgr::updateProductEqualityMap apply toRemove  takes: 0 us
SymbolicDimMgr::save updateProductEqualityMap takes: 16 us
SymbolicDimMgr::save updateFunctionType takes: 1 us
SymbolicDimMgr::save collect symbolicDim ops takes: 3 us
SymbolicDimMgr::save remove symbolicDim ops takes: 0 us
SymbolicDimMgr::save remove unused production takes: 0 us
SymbolicDimMgr::save remove unused production #2 takes: 0 us
SymbolicDimMgr::save canonicalize the name takes: 2 us
SymbolicDimMgr::save replace the name takes: 3 us
SymbolicDimMgr::save updateFunctionType takes: 0 us
// -----// IR Dump After DiscShapeOptimizationPass (disc-shape-optimization) //----- //
module {
  func.func @main(%arg0: tensor<?x?x?xf32, [@S0, @S1, @S2]>) -> tensor<?x?xf32, [@S1, @S2]> attributes {tf.entry_function = {input_placements = "gpu", inputs = "input0", output_placements = "gpu", outputs = "output0"}} {
    %0 = mhlo.constant dense<0xFF800000> : tensor<f32>
    %1 = mhlo.abs %arg0 : tensor<?x?x?xf32, [@S0, @S1, @S2]>
    %2 = mhlo.reduce(%1 init: %0) applies mhlo.maximum across dimensions = [0] : (tensor<?x?x?xf32, [@S0, @S1, @S2]>, tensor<f32>) -> tensor<?x?xf32, [@S1, @S2]>
    return %2 : tensor<?x?xf32, [@S1, @S2]>
  }
  "disc_shape.SymbolicDim"() {knownNegativeOne = false, knownNonNegative = true, knownNonSizeOne = false, knownNonSizeZero = false, sym_name = "S0", value = -9223372036854775808 : i64} : () -> ()
  "disc_shape.SymbolicDim"() {knownNegativeOne = false, knownNonNegative = true, knownNonSizeOne = false, knownNonSizeZero = false, sym_name = "S1", value = -9223372036854775808 : i64} : () -> ()
  "disc_shape.SymbolicDim"() {knownNegativeOne = false, knownNonNegative = true, knownNonSizeOne = false, knownNonSizeZero = false, sym_name = "S2", value = -9223372036854775808 : i64} : () -> ()
  func.func @shape_constraint_graph() {
    return
  }
}


SymbolicDimMgr::save walkRankedTensorValue takes: 3 us
SymbolicDimMgr::save update attributes takes: 1 us
SymbolicDimMgr::updateProductEqualityMap simplifySymbolicDimProductPair takes: 0 us
productSet.size() = 0
SymbolicDimMgr::updateProductEqualityMap propagate graph takes: 2 us
SymbolicDimMgr::updateProductEqualityMap remove multiply takes: 0 us
SymbolicDimMgr::updateProductEqualityMap build toRemove  takes: 0 us
SymbolicDimMgr::updateProductEqualityMap apply toRemove  takes: 0 us
SymbolicDimMgr::save updateProductEqualityMap takes: 19 us
SymbolicDimMgr::save updateFunctionType takes: 1 us
SymbolicDimMgr::save collect symbolicDim ops takes: 3 us
SymbolicDimMgr::save remove symbolicDim ops takes: 0 us
SymbolicDimMgr::save remove unused production takes: 0 us
SymbolicDimMgr::save remove unused production #2 takes: 0 us
SymbolicDimMgr::save canonicalize the name takes: 3 us
SymbolicDimMgr::save replace the name takes: 3 us
SymbolicDimMgr::save updateFunctionType takes: 0 us
// -----// IR Dump After DiscShapeOptimizationPass (disc-shape-optimization) //----- //
module {
  func.func @main(%arg0: tensor<?x?x?xf32, [@S0, @S1, @S2]>) -> tensor<?x?xf32, [@S1, @S2]> attributes {tf.entry_function = {input_placements = "gpu", inputs = "input0", output_placements = "gpu", outputs = "output0"}} {
    %0 = mhlo.constant dense<0xFF800000> : tensor<f32>
    %1 = mhlo.abs %arg0 : tensor<?x?x?xf32, [@S0, @S1, @S2]>
    %2 = mhlo.reduce(%1 init: %0) applies mhlo.maximum across dimensions = [0] : (tensor<?x?x?xf32, [@S0, @S1, @S2]>, tensor<f32>) -> tensor<?x?xf32, [@S1, @S2]>
    return %2 : tensor<?x?xf32, [@S1, @S2]>
  }
  "disc_shape.SymbolicDim"() {knownNegativeOne = false, knownNonNegative = true, knownNonSizeOne = false, knownNonSizeZero = false, sym_name = "S0", value = -9223372036854775808 : i64} : () -> ()
  "disc_shape.SymbolicDim"() {knownNegativeOne = false, knownNonNegative = true, knownNonSizeOne = false, knownNonSizeZero = false, sym_name = "S1", value = -9223372036854775808 : i64} : () -> ()
  "disc_shape.SymbolicDim"() {knownNegativeOne = false, knownNonNegative = true, knownNonSizeOne = false, knownNonSizeZero = false, sym_name = "S2", value = -9223372036854775808 : i64} : () -> ()
  func.func @shape_constraint_graph() {
    return
  }
}


// -----// IR Dump After HloCanonicalizeReductionPass (hlo-canonicalize-reduction) //----- //
func.func @main(%arg0: tensor<?x?x?xf32, [@S0, @S1, @S2]>) -> tensor<?x?xf32, [@S1, @S2]> attributes {tf.entry_function = {input_placements = "gpu", inputs = "input0", output_placements = "gpu", outputs = "output0"}} {
  %0 = mhlo.constant dense<0xFF800000> : tensor<f32>
  %1 = mhlo.abs %arg0 : tensor<?x?x?xf32, [@S0, @S1, @S2]>
  %c1_i32 = arith.constant 1 : i32
  %c0 = arith.constant 0 : index
  %dim = tensor.dim %1, %c0 : tensor<?x?x?xf32, [@S0, @S1, @S2]>
  %2 = arith.index_cast %dim : index to i32
  %3 = arith.muli %c1_i32, %2 : i32
  %c1 = arith.constant 1 : index
  %dim_0 = tensor.dim %1, %c1 : tensor<?x?x?xf32, [@S0, @S1, @S2]>
  %4 = arith.index_cast %dim_0 : index to i32
  %5 = arith.muli %c1_i32, %4 : i32
  %c2 = arith.constant 2 : index
  %dim_1 = tensor.dim %1, %c2 : tensor<?x?x?xf32, [@S0, @S1, @S2]>
  %6 = arith.index_cast %dim_1 : index to i32
  %7 = arith.muli %5, %6 : i32
  %from_elements = tensor.from_elements %3, %7 : tensor<2xi32>
  %8 = mhlo.dynamic_reshape %1, %from_elements : (tensor<?x?x?xf32, [@S0, @S1, @S2]>, tensor<2xi32>) -> tensor<?x?xf32>
  %9 = mhlo.reduce(%8 init: %0) applies mhlo.maximum across dimensions = [0] : (tensor<?x?xf32>, tensor<f32>) -> tensor<?xf32>
  %c1_2 = arith.constant 1 : index
  %dim_3 = tensor.dim %1, %c1_2 : tensor<?x?x?xf32, [@S0, @S1, @S2]>
  %10 = arith.index_cast %dim_3 : index to i32
  %c2_4 = arith.constant 2 : index
  %dim_5 = tensor.dim %1, %c2_4 : tensor<?x?x?xf32, [@S0, @S1, @S2]>
  %11 = arith.index_cast %dim_5 : index to i32
  %from_elements_6 = tensor.from_elements %10, %11 : tensor<2xi32>
  %12 = mhlo.dynamic_reshape %9, %from_elements_6 : (tensor<?xf32>, tensor<2xi32>) -> tensor<?x?xf32, [@S1, @S2]>
  return %12 : tensor<?x?xf32, [@S1, @S2]>
}

// -----// IR Dump After CSE (cse) //----- //
module {
  func.func @main(%arg0: tensor<?x?x?xf32, [@S0, @S1, @S2]>) -> tensor<?x?xf32, [@S1, @S2]> attributes {tf.entry_function = {input_placements = "gpu", inputs = "input0", output_placements = "gpu", outputs = "output0"}} {
    %c0 = arith.constant 0 : index
    %c1 = arith.constant 1 : index
    %c2 = arith.constant 2 : index
    %0 = mhlo.constant dense<0xFF800000> : tensor<f32>
    %dim = tensor.dim %arg0, %c0 : tensor<?x?x?xf32, [@S0, @S1, @S2]>
    %dim_0 = tensor.dim %arg0, %c1 : tensor<?x?x?xf32, [@S0, @S1, @S2]>
    %dim_1 = tensor.dim %arg0, %c2 : tensor<?x?x?xf32, [@S0, @S1, @S2]>
    %1 = "disc_shape.tie_shape"(%arg0, %dim, %dim_0, %dim_1) : (tensor<?x?x?xf32, [@S0, @S1, @S2]>, index, index, index) -> tensor<?x?x?xf32, [@S0, @S1, @S2]>
    %2 = mhlo.abs %1 : tensor<?x?x?xf32, [@S0, @S1, @S2]>
    %3 = "disc_shape.tie_shape"(%2, %dim, %dim_0, %dim_1) : (tensor<?x?x?xf32, [@S0, @S1, @S2]>, index, index, index) -> tensor<?x?x?xf32, [@S0, @S1, @S2]>
    %4 = arith.index_cast %dim : index to i32
    %5 = arith.index_cast %dim_0 : index to i32
    %6 = arith.index_cast %dim_1 : index to i32
    %7 = arith.muli %5, %6 : i32
    %from_elements = tensor.from_elements %4, %7 : tensor<2xi32>
    %8 = "disc_shape.tie_shape"(%from_elements, %c2) : (tensor<2xi32>, index) -> tensor<2xi32>
    %9 = mhlo.dynamic_reshape %3, %8 : (tensor<?x?x?xf32, [@S0, @S1, @S2]>, tensor<2xi32>) -> tensor<?x?xf32>
    %10 = arith.index_cast %7 : i32 to index
    %11 = "disc_shape.tie_shape"(%9, %dim, %10) : (tensor<?x?xf32>, index, index) -> tensor<?x?xf32>
    %12 = mhlo.reduce(%11 init: %0) applies mhlo.maximum across dimensions = [0] : (tensor<?x?xf32>, tensor<f32>) -> tensor<?xf32>
    %13 = "disc_shape.tie_shape"(%12, %10) : (tensor<?xf32>, index) -> tensor<?xf32>
    %from_elements_2 = tensor.from_elements %5, %6 : tensor<2xi32>
    %14 = "disc_shape.tie_shape"(%from_elements_2, %c2) : (tensor<2xi32>, index) -> tensor<2xi32>
    %15 = mhlo.dynamic_reshape %13, %14 : (tensor<?xf32>, tensor<2xi32>) -> tensor<?x?xf32, [@S1, @S2]>
    %16 = "disc_shape.tie_shape"(%15, %dim_0, %dim_1) : (tensor<?x?xf32, [@S1, @S2]>, index, index) -> tensor<?x?xf32, [@S1, @S2]>
    return %16 : tensor<?x?xf32, [@S1, @S2]>
  }
  "disc_shape.SymbolicDim"() {knownNegativeOne = false, knownNonNegative = true, knownNonSizeOne = false, knownNonSizeZero = false, sym_name = "S0", value = -9223372036854775808 : i64} : () -> ()
  "disc_shape.SymbolicDim"() {knownNegativeOne = false, knownNonNegative = true, knownNonSizeOne = false, knownNonSizeZero = false, sym_name = "S1", value = -9223372036854775808 : i64} : () -> ()
  "disc_shape.SymbolicDim"() {knownNegativeOne = false, knownNonNegative = true, knownNonSizeOne = false, knownNonSizeZero = false, sym_name = "S2", value = -9223372036854775808 : i64} : () -> ()
  func.func @shape_constraint_graph() {
    return
  }
}


SymbolicDimMgr::save walkRankedTensorValue takes: 6 us
SymbolicDimMgr::save update attributes takes: 1 us
SymbolicDimMgr::updateProductEqualityMap simplifySymbolicDimProductPair takes: 14 us
productSet.size() = 4
SymbolicDimMgr::updateProductEqualityMap propagate graph takes: 13 us
SymbolicDimMgr::updateProductEqualityMap remove multiply takes: 37 us
SymbolicDimMgr::updateProductEqualityMap build toRemove  takes: 3 us
SymbolicDimMgr::updateProductEqualityMap apply toRemove  takes: 2 us
SymbolicDimMgr::save updateProductEqualityMap takes: 91 us
SymbolicDimMgr::save updateFunctionType takes: 1 us
SymbolicDimMgr::save collect symbolicDim ops takes: 5 us
SymbolicDimMgr::save remove symbolicDim ops takes: 1 us
SymbolicDimMgr::save remove unused production takes: 0 us
SymbolicDimMgr::save remove unused production #2 takes: 1 us
SymbolicDimMgr::save canonicalize the name takes: 4 us
SymbolicDimMgr::save replace the name takes: 7 us
SymbolicDimMgr::save updateFunctionType takes: 0 us
// -----// IR Dump After DiscShapeOptimizationPass (disc-shape-optimization) //----- //
module {
  func.func @main(%arg0: tensor<?x?x?xf32, [@S0, @S1, @S2]>) -> tensor<?x?xf32, [@S1, @S2]> attributes {tf.entry_function = {input_placements = "gpu", inputs = "input0", output_placements = "gpu", outputs = "output0"}} {
    %c0 = arith.constant 0 : index
    %c1 = arith.constant 1 : index
    %c2 = arith.constant 2 : index
    %0 = mhlo.constant dense<0xFF800000> : tensor<f32>
    %dim = tensor.dim %arg0, %c0 : tensor<?x?x?xf32, [@S0, @S1, @S2]>
    %dim_0 = tensor.dim %arg0, %c1 : tensor<?x?x?xf32, [@S0, @S1, @S2]>
    %dim_1 = tensor.dim %arg0, %c2 : tensor<?x?x?xf32, [@S0, @S1, @S2]>
    %1 = mhlo.abs %arg0 : tensor<?x?x?xf32, [@S0, @S1, @S2]>
    %2 = arith.index_cast %dim : index to i32
    %3 = arith.index_cast %dim_0 : index to i32
    %4 = arith.index_cast %dim_1 : index to i32
    %5 = arith.muli %3, %4 : i32
    %from_elements = tensor.from_elements %2, %5 : tensor<2xi32>
    %6 = mhlo.dynamic_reshape %1, %from_elements : (tensor<?x?x?xf32, [@S0, @S1, @S2]>, tensor<2xi32>) -> tensor<?x?xf32, [@S0, @S3]>
    %7 = mhlo.reduce(%6 init: %0) applies mhlo.maximum across dimensions = [0] : (tensor<?x?xf32, [@S0, @S3]>, tensor<f32>) -> tensor<?xf32, [@S3]>
    %from_elements_2 = tensor.from_elements %3, %4 : tensor<2xi32>
    %8 = mhlo.dynamic_reshape %7, %from_elements_2 : (tensor<?xf32, [@S3]>, tensor<2xi32>) -> tensor<?x?xf32, [@S1, @S2]>
    return %8 : tensor<?x?xf32, [@S1, @S2]>
  }
  "disc_shape.SymbolicDim"() {knownNegativeOne = false, knownNonNegative = true, knownNonSizeOne = false, knownNonSizeZero = false, sym_name = "S0", value = -9223372036854775808 : i64} : () -> ()
  "disc_shape.SymbolicDim"() {knownNegativeOne = false, knownNonNegative = true, knownNonSizeOne = false, knownNonSizeZero = false, sym_name = "S1", value = -9223372036854775808 : i64} : () -> ()
  "disc_shape.SymbolicDim"() {knownNegativeOne = false, knownNonNegative = true, knownNonSizeOne = false, knownNonSizeZero = false, sym_name = "S2", value = -9223372036854775808 : i64} : () -> ()
  "disc_shape.SymbolicDim"() {knownNegativeOne = false, knownNonNegative = true, knownNonSizeOne = false, knownNonSizeZero = false, sym_name = "S3", value = -9223372036854775808 : i64} : () -> ()
  func.func @shape_constraint_graph() {
    %0 = "disc_shape.dim"() {name = @S3} : () -> index
    %1 = "disc_shape.dim"() {name = @S1} : () -> index
    %2 = "disc_shape.dim"() {name = @S2} : () -> index
    "disc_shape.tie_product_equal"(%0, %1, %2) {operand_segment_sizes = array<i32: 1, 2>} : (index, index, index) -> ()
    return
  }
}


// -----// IR Dump After DiscMarkShapeCalculationPass (disc-mhlo-mark-shape-calc) //----- //
module {
  func.func @main(%arg0: tensor<?x?x?xf32, [@S0, @S1, @S2]>) -> tensor<?x?xf32, [@S1, @S2]> attributes {tf.entry_function = {input_placements = "gpu", inputs = "input0", output_placements = "gpu", outputs = "output0"}} {
    %c0 = arith.constant 0 : index
    %c1 = arith.constant 1 : index
    %c2 = arith.constant 2 : index
    %0 = mhlo.constant dense<0xFF800000> : tensor<f32>
    %dim = tensor.dim %arg0, %c0 : tensor<?x?x?xf32, [@S0, @S1, @S2]>
    %dim_0 = tensor.dim %arg0, %c1 : tensor<?x?x?xf32, [@S0, @S1, @S2]>
    %dim_1 = tensor.dim %arg0, %c2 : tensor<?x?x?xf32, [@S0, @S1, @S2]>
    %1 = mhlo.abs %arg0 : tensor<?x?x?xf32, [@S0, @S1, @S2]>
    %2 = arith.index_cast %dim : index to i32
    %3 = arith.index_cast %dim_0 : index to i32
    %4 = arith.index_cast %dim_1 : index to i32
    %5 = arith.muli %3, %4 : i32
    %from_elements = tensor.from_elements %2, %5 {disc.shape_op = true} : tensor<2xi32>
    %6 = mhlo.dynamic_reshape %1, %from_elements : (tensor<?x?x?xf32, [@S0, @S1, @S2]>, tensor<2xi32>) -> tensor<?x?xf32, [@S0, @S3]>
    %7 = mhlo.reduce(%6 init: %0) applies mhlo.maximum across dimensions = [0] : (tensor<?x?xf32, [@S0, @S3]>, tensor<f32>) -> tensor<?xf32, [@S3]>
    %from_elements_2 = tensor.from_elements %3, %4 {disc.shape_op = true} : tensor<2xi32>
    %8 = mhlo.dynamic_reshape %7, %from_elements_2 : (tensor<?xf32, [@S3]>, tensor<2xi32>) -> tensor<?x?xf32, [@S1, @S2]>
    return %8 : tensor<?x?xf32, [@S1, @S2]>
  }
  "disc_shape.SymbolicDim"() {knownNegativeOne = false, knownNonNegative = true, knownNonSizeOne = false, knownNonSizeZero = false, sym_name = "S0", value = -9223372036854775808 : i64} : () -> ()
  "disc_shape.SymbolicDim"() {knownNegativeOne = false, knownNonNegative = true, knownNonSizeOne = false, knownNonSizeZero = false, sym_name = "S1", value = -9223372036854775808 : i64} : () -> ()
  "disc_shape.SymbolicDim"() {knownNegativeOne = false, knownNonNegative = true, knownNonSizeOne = false, knownNonSizeZero = false, sym_name = "S2", value = -9223372036854775808 : i64} : () -> ()
  "disc_shape.SymbolicDim"() {knownNegativeOne = false, knownNonNegative = true, knownNonSizeOne = false, knownNonSizeZero = false, sym_name = "S3", value = -9223372036854775808 : i64} : () -> ()
  func.func @shape_constraint_graph() {
    %0 = "disc_shape.dim"() {name = @S3} : () -> index
    %1 = "disc_shape.dim"() {name = @S1} : () -> index
    %2 = "disc_shape.dim"() {name = @S2} : () -> index
    "disc_shape.tie_product_equal"(%0, %1, %2) {operand_segment_sizes = array<i32: 1, 2>} : (index, index, index) -> ()
    return
  }
}


// -----// IR Dump After PlaceOpsPass (mhlo-place-ops) //----- //
module {
  func.func @main(%arg0: tensor<?x?x?xf32, [@S0, @S1, @S2]>) -> tensor<?x?xf32, [@S1, @S2]> attributes {tf.entry_function = {input_placements = "gpu", inputs = "input0", output_placements = "gpu", outputs = "output0"}} {
    %c0 = arith.constant 0 : index
    %c1 = arith.constant 1 : index
    %c2 = arith.constant 2 : index
    %0 = mhlo.constant {disc.device = "gpu"} dense<0xFF800000> : tensor<f32>
    %dim = tensor.dim %arg0, %c0 : tensor<?x?x?xf32, [@S0, @S1, @S2]>
    %dim_0 = tensor.dim %arg0, %c1 : tensor<?x?x?xf32, [@S0, @S1, @S2]>
    %dim_1 = tensor.dim %arg0, %c2 : tensor<?x?x?xf32, [@S0, @S1, @S2]>
    %1 = mhlo.abs %arg0 {disc.device = "gpu"} : tensor<?x?x?xf32, [@S0, @S1, @S2]>
    %2 = arith.index_cast %dim : index to i32
    %3 = arith.index_cast %dim_0 : index to i32
    %4 = arith.index_cast %dim_1 : index to i32
    %5 = arith.muli %3, %4 : i32
    %from_elements = tensor.from_elements %2, %5 {disc.shape_op = true} : tensor<2xi32>
    %6 = mhlo.dynamic_reshape %1, %from_elements {disc.device = "gpu"} : (tensor<?x?x?xf32, [@S0, @S1, @S2]>, tensor<2xi32>) -> tensor<?x?xf32, [@S0, @S3]>
    %7 = mhlo.reduce(%6 init: %0) applies mhlo.maximum across dimensions = [0] : (tensor<?x?xf32, [@S0, @S3]>, tensor<f32>) -> tensor<?xf32, [@S3]>
    %from_elements_2 = tensor.from_elements %3, %4 {disc.shape_op = true} : tensor<2xi32>
    %8 = mhlo.dynamic_reshape %7, %from_elements_2 {disc.device = "gpu"} : (tensor<?xf32, [@S3]>, tensor<2xi32>) -> tensor<?x?xf32, [@S1, @S2]>
    return %8 : tensor<?x?xf32, [@S1, @S2]>
  }
  "disc_shape.SymbolicDim"() {knownNegativeOne = false, knownNonNegative = true, knownNonSizeOne = false, knownNonSizeZero = false, sym_name = "S0", value = -9223372036854775808 : i64} : () -> ()
  "disc_shape.SymbolicDim"() {knownNegativeOne = false, knownNonNegative = true, knownNonSizeOne = false, knownNonSizeZero = false, sym_name = "S1", value = -9223372036854775808 : i64} : () -> ()
  "disc_shape.SymbolicDim"() {knownNegativeOne = false, knownNonNegative = true, knownNonSizeOne = false, knownNonSizeZero = false, sym_name = "S2", value = -9223372036854775808 : i64} : () -> ()
  "disc_shape.SymbolicDim"() {knownNegativeOne = false, knownNonNegative = true, knownNonSizeOne = false, knownNonSizeZero = false, sym_name = "S3", value = -9223372036854775808 : i64} : () -> ()
  func.func @shape_constraint_graph() {
    %0 = "disc_shape.dim"() {name = @S3} : () -> index
    %1 = "disc_shape.dim"() {name = @S1} : () -> index
    %2 = "disc_shape.dim"() {name = @S2} : () -> index
    "disc_shape.tie_product_equal"(%0, %1, %2) {operand_segment_sizes = array<i32: 1, 2>} : (index, index, index) -> ()
    return
  }
}


SymbolicDimMgr::save walkRankedTensorValue takes: 6 us
SymbolicDimMgr::save update attributes takes: 1 us
SymbolicDimMgr::updateProductEqualityMap simplifySymbolicDimProductPair takes: 12 us
productSet.size() = 4
SymbolicDimMgr::updateProductEqualityMap propagate graph takes: 5 us
SymbolicDimMgr::updateProductEqualityMap remove multiply takes: 34 us
SymbolicDimMgr::updateProductEqualityMap build toRemove  takes: 3 us
SymbolicDimMgr::updateProductEqualityMap apply toRemove  takes: 2 us
SymbolicDimMgr::save updateProductEqualityMap takes: 79 us
SymbolicDimMgr::save updateFunctionType takes: 1 us
SymbolicDimMgr::save collect symbolicDim ops takes: 5 us
SymbolicDimMgr::save remove symbolicDim ops takes: 1 us
SymbolicDimMgr::save remove unused production takes: 0 us
SymbolicDimMgr::save remove unused production #2 takes: 1 us
SymbolicDimMgr::save canonicalize the name takes: 3 us
SymbolicDimMgr::save replace the name takes: 5 us
SymbolicDimMgr::save updateFunctionType takes: 0 us
// -----// IR Dump After DiscShapeOptimizationPass (disc-shape-optimization) //----- //
module {
  func.func @main(%arg0: tensor<?x?x?xf32, [@S0, @S1, @S2]>) -> tensor<?x?xf32, [@S1, @S2]> attributes {tf.entry_function = {input_placements = "gpu", inputs = "input0", output_placements = "gpu", outputs = "output0"}} {
    %c0 = arith.constant 0 : index
    %c1 = arith.constant 1 : index
    %c2 = arith.constant 2 : index
    %0 = mhlo.constant {disc.device = "gpu"} dense<0xFF800000> : tensor<f32>
    %dim = tensor.dim %arg0, %c0 : tensor<?x?x?xf32, [@S0, @S1, @S2]>
    %dim_0 = tensor.dim %arg0, %c1 : tensor<?x?x?xf32, [@S0, @S1, @S2]>
    %dim_1 = tensor.dim %arg0, %c2 : tensor<?x?x?xf32, [@S0, @S1, @S2]>
    %1 = mhlo.abs %arg0 {disc.device = "gpu"} : tensor<?x?x?xf32, [@S0, @S1, @S2]>
    %2 = arith.index_cast %dim : index to i32
    %3 = arith.index_cast %dim_0 : index to i32
    %4 = arith.index_cast %dim_1 : index to i32
    %5 = arith.muli %3, %4 : i32
    %from_elements = tensor.from_elements %2, %5 {disc.shape_op = true} : tensor<2xi32>
    %6 = mhlo.dynamic_reshape %1, %from_elements {disc.device = "gpu"} : (tensor<?x?x?xf32, [@S0, @S1, @S2]>, tensor<2xi32>) -> tensor<?x?xf32, [@S0, @S3]>
    %7 = mhlo.reduce(%6 init: %0) applies mhlo.maximum across dimensions = [0] : (tensor<?x?xf32, [@S0, @S3]>, tensor<f32>) -> tensor<?xf32, [@S3]>
    %from_elements_2 = tensor.from_elements %3, %4 {disc.shape_op = true} : tensor<2xi32>
    %8 = mhlo.dynamic_reshape %7, %from_elements_2 {disc.device = "gpu"} : (tensor<?xf32, [@S3]>, tensor<2xi32>) -> tensor<?x?xf32, [@S1, @S2]>
    return %8 : tensor<?x?xf32, [@S1, @S2]>
  }
  "disc_shape.SymbolicDim"() {knownNegativeOne = false, knownNonNegative = true, knownNonSizeOne = false, knownNonSizeZero = false, sym_name = "S0", value = -9223372036854775808 : i64} : () -> ()
  "disc_shape.SymbolicDim"() {knownNegativeOne = false, knownNonNegative = true, knownNonSizeOne = false, knownNonSizeZero = false, sym_name = "S1", value = -9223372036854775808 : i64} : () -> ()
  "disc_shape.SymbolicDim"() {knownNegativeOne = false, knownNonNegative = true, knownNonSizeOne = false, knownNonSizeZero = false, sym_name = "S2", value = -9223372036854775808 : i64} : () -> ()
  "disc_shape.SymbolicDim"() {knownNegativeOne = false, knownNonNegative = true, knownNonSizeOne = false, knownNonSizeZero = false, sym_name = "S3", value = -9223372036854775808 : i64} : () -> ()
  func.func @shape_constraint_graph() {
    %0 = "disc_shape.dim"() {name = @S3} : () -> index
    %1 = "disc_shape.dim"() {name = @S1} : () -> index
    %2 = "disc_shape.dim"() {name = @S2} : () -> index
    "disc_shape.tie_product_equal"(%0, %1, %2) {operand_segment_sizes = array<i32: 1, 2>} : (index, index, index) -> ()
    return
  }
}


SymbolicDimMgr::save walkRankedTensorValue takes: 5 us
SymbolicDimMgr::save update attributes takes: 1 us
SymbolicDimMgr::updateProductEqualityMap simplifySymbolicDimProductPair takes: 12 us
productSet.size() = 4
SymbolicDimMgr::updateProductEqualityMap propagate graph takes: 5 us
SymbolicDimMgr::updateProductEqualityMap remove multiply takes: 36 us
SymbolicDimMgr::updateProductEqualityMap build toRemove  takes: 2 us
SymbolicDimMgr::updateProductEqualityMap apply toRemove  takes: 1 us
SymbolicDimMgr::save updateProductEqualityMap takes: 80 us
SymbolicDimMgr::save updateFunctionType takes: 1 us
SymbolicDimMgr::save collect symbolicDim ops takes: 5 us
SymbolicDimMgr::save remove symbolicDim ops takes: 1 us
SymbolicDimMgr::save remove unused production takes: 0 us
SymbolicDimMgr::save remove unused production #2 takes: 1 us
SymbolicDimMgr::save canonicalize the name takes: 3 us
SymbolicDimMgr::save replace the name takes: 5 us
SymbolicDimMgr::save updateFunctionType takes: 0 us
// -----// IR Dump After DiscShapeOptimizationPass (disc-shape-optimization) //----- //
module {
  func.func @main(%arg0: tensor<?x?x?xf32, [@S0, @S1, @S2]>) -> tensor<?x?xf32, [@S1, @S2]> attributes {tf.entry_function = {input_placements = "gpu", inputs = "input0", output_placements = "gpu", outputs = "output0"}} {
    %c0 = arith.constant 0 : index
    %c1 = arith.constant 1 : index
    %c2 = arith.constant 2 : index
    %0 = mhlo.constant {disc.device = "gpu"} dense<0xFF800000> : tensor<f32>
    %dim = tensor.dim %arg0, %c0 : tensor<?x?x?xf32, [@S0, @S1, @S2]>
    %dim_0 = tensor.dim %arg0, %c1 : tensor<?x?x?xf32, [@S0, @S1, @S2]>
    %dim_1 = tensor.dim %arg0, %c2 : tensor<?x?x?xf32, [@S0, @S1, @S2]>
    %1 = mhlo.abs %arg0 {disc.device = "gpu"} : tensor<?x?x?xf32, [@S0, @S1, @S2]>
    %2 = arith.index_cast %dim : index to i32
    %3 = arith.index_cast %dim_0 : index to i32
    %4 = arith.index_cast %dim_1 : index to i32
    %5 = arith.muli %3, %4 : i32
    %from_elements = tensor.from_elements %2, %5 {disc.shape_op = true} : tensor<2xi32>
    %6 = mhlo.dynamic_reshape %1, %from_elements {disc.device = "gpu"} : (tensor<?x?x?xf32, [@S0, @S1, @S2]>, tensor<2xi32>) -> tensor<?x?xf32, [@S0, @S3]>
    %7 = mhlo.reduce(%6 init: %0) applies mhlo.maximum across dimensions = [0] : (tensor<?x?xf32, [@S0, @S3]>, tensor<f32>) -> tensor<?xf32, [@S3]>
    %from_elements_2 = tensor.from_elements %3, %4 {disc.shape_op = true} : tensor<2xi32>
    %8 = mhlo.dynamic_reshape %7, %from_elements_2 {disc.device = "gpu"} : (tensor<?xf32, [@S3]>, tensor<2xi32>) -> tensor<?x?xf32, [@S1, @S2]>
    return %8 : tensor<?x?xf32, [@S1, @S2]>
  }
  "disc_shape.SymbolicDim"() {knownNegativeOne = false, knownNonNegative = true, knownNonSizeOne = false, knownNonSizeZero = false, sym_name = "S0", value = -9223372036854775808 : i64} : () -> ()
  "disc_shape.SymbolicDim"() {knownNegativeOne = false, knownNonNegative = true, knownNonSizeOne = false, knownNonSizeZero = false, sym_name = "S1", value = -9223372036854775808 : i64} : () -> ()
  "disc_shape.SymbolicDim"() {knownNegativeOne = false, knownNonNegative = true, knownNonSizeOne = false, knownNonSizeZero = false, sym_name = "S2", value = -9223372036854775808 : i64} : () -> ()
  "disc_shape.SymbolicDim"() {knownNegativeOne = false, knownNonNegative = true, knownNonSizeOne = false, knownNonSizeZero = false, sym_name = "S3", value = -9223372036854775808 : i64} : () -> ()
  func.func @shape_constraint_graph() {
    %0 = "disc_shape.dim"() {name = @S3} : () -> index
    %1 = "disc_shape.dim"() {name = @S1} : () -> index
    %2 = "disc_shape.dim"() {name = @S2} : () -> index
    "disc_shape.tie_product_equal"(%0, %1, %2) {operand_segment_sizes = array<i32: 1, 2>} : (index, index, index) -> ()
    return
  }
}


SymbolicDimMgr::save walkRankedTensorValue takes: 5 us
SymbolicDimMgr::save update attributes takes: 2 us
SymbolicDimMgr::updateProductEqualityMap simplifySymbolicDimProductPair takes: 13 us
productSet.size() = 4
SymbolicDimMgr::updateProductEqualityMap propagate graph takes: 6 us
SymbolicDimMgr::updateProductEqualityMap remove multiply takes: 35 us
SymbolicDimMgr::updateProductEqualityMap build toRemove  takes: 3 us
SymbolicDimMgr::updateProductEqualityMap apply toRemove  takes: 2 us
SymbolicDimMgr::save updateProductEqualityMap takes: 88 us
SymbolicDimMgr::save updateFunctionType takes: 1 us
SymbolicDimMgr::save collect symbolicDim ops takes: 5 us
SymbolicDimMgr::save remove symbolicDim ops takes: 1 us
SymbolicDimMgr::save remove unused production takes: 0 us
SymbolicDimMgr::save remove unused production #2 takes: 1 us
SymbolicDimMgr::save canonicalize the name takes: 3 us
SymbolicDimMgr::save replace the name takes: 5 us
SymbolicDimMgr::save updateFunctionType takes: 0 us
// -----// IR Dump After DiscShapeOptimizationPass (disc-shape-optimization) //----- //
module {
  func.func @main(%arg0: tensor<?x?x?xf32, [@S0, @S1, @S2]>) -> tensor<?x?xf32, [@S1, @S2]> attributes {tf.entry_function = {input_placements = "gpu", inputs = "input0", output_placements = "gpu", outputs = "output0"}} {
    %c0 = arith.constant 0 : index
    %c1 = arith.constant 1 : index
    %c2 = arith.constant 2 : index
    %0 = mhlo.constant {disc.device = "gpu"} dense<0xFF800000> : tensor<f32>
    %dim = tensor.dim %arg0, %c0 : tensor<?x?x?xf32, [@S0, @S1, @S2]>
    %dim_0 = tensor.dim %arg0, %c1 : tensor<?x?x?xf32, [@S0, @S1, @S2]>
    %dim_1 = tensor.dim %arg0, %c2 : tensor<?x?x?xf32, [@S0, @S1, @S2]>
    %1 = mhlo.abs %arg0 {disc.device = "gpu"} : tensor<?x?x?xf32, [@S0, @S1, @S2]>
    %2 = arith.index_cast %dim : index to i32
    %3 = arith.index_cast %dim_0 : index to i32
    %4 = arith.index_cast %dim_1 : index to i32
    %5 = arith.muli %3, %4 : i32
    %from_elements = tensor.from_elements %2, %5 {disc.shape_op = true} : tensor<2xi32>
    %6 = mhlo.dynamic_reshape %1, %from_elements {disc.device = "gpu"} : (tensor<?x?x?xf32, [@S0, @S1, @S2]>, tensor<2xi32>) -> tensor<?x?xf32, [@S0, @S3]>
    %7 = mhlo.reduce(%6 init: %0) applies mhlo.maximum across dimensions = [0] : (tensor<?x?xf32, [@S0, @S3]>, tensor<f32>) -> tensor<?xf32, [@S3]>
    %from_elements_2 = tensor.from_elements %3, %4 {disc.shape_op = true} : tensor<2xi32>
    %8 = mhlo.dynamic_reshape %7, %from_elements_2 {disc.device = "gpu"} : (tensor<?xf32, [@S3]>, tensor<2xi32>) -> tensor<?x?xf32, [@S1, @S2]>
    return %8 : tensor<?x?xf32, [@S1, @S2]>
  }
  "disc_shape.SymbolicDim"() {knownNegativeOne = false, knownNonNegative = true, knownNonSizeOne = false, knownNonSizeZero = false, sym_name = "S0", value = -9223372036854775808 : i64} : () -> ()
  "disc_shape.SymbolicDim"() {knownNegativeOne = false, knownNonNegative = true, knownNonSizeOne = false, knownNonSizeZero = false, sym_name = "S1", value = -9223372036854775808 : i64} : () -> ()
  "disc_shape.SymbolicDim"() {knownNegativeOne = false, knownNonNegative = true, knownNonSizeOne = false, knownNonSizeZero = false, sym_name = "S2", value = -9223372036854775808 : i64} : () -> ()
  "disc_shape.SymbolicDim"() {knownNegativeOne = false, knownNonNegative = true, knownNonSizeOne = false, knownNonSizeZero = false, sym_name = "S3", value = -9223372036854775808 : i64} : () -> ()
  func.func @shape_constraint_graph() {
    %0 = "disc_shape.dim"() {name = @S3} : () -> index
    %1 = "disc_shape.dim"() {name = @S1} : () -> index
    %2 = "disc_shape.dim"() {name = @S2} : () -> index
    "disc_shape.tie_product_equal"(%0, %1, %2) {operand_segment_sizes = array<i32: 1, 2>} : (index, index, index) -> ()
    return
  }
}


SymbolicDimMgr::save walkRankedTensorValue takes: 5 us
SymbolicDimMgr::save update attributes takes: 1 us
SymbolicDimMgr::updateProductEqualityMap simplifySymbolicDimProductPair takes: 12 us
productSet.size() = 4
SymbolicDimMgr::updateProductEqualityMap propagate graph takes: 6 us
SymbolicDimMgr::updateProductEqualityMap remove multiply takes: 35 us
SymbolicDimMgr::updateProductEqualityMap build toRemove  takes: 3 us
SymbolicDimMgr::updateProductEqualityMap apply toRemove  takes: 2 us
SymbolicDimMgr::save updateProductEqualityMap takes: 80 us
SymbolicDimMgr::save updateFunctionType takes: 1 us
SymbolicDimMgr::save collect symbolicDim ops takes: 5 us
SymbolicDimMgr::save remove symbolicDim ops takes: 1 us
SymbolicDimMgr::save remove unused production takes: 0 us
SymbolicDimMgr::save remove unused production #2 takes: 1 us
SymbolicDimMgr::save canonicalize the name takes: 3 us
SymbolicDimMgr::save replace the name takes: 5 us
SymbolicDimMgr::save updateFunctionType takes: 0 us
// -----// IR Dump After DiscShapeOptimizationPass (disc-shape-optimization) //----- //
module {
  func.func @main(%arg0: tensor<?x?x?xf32>) -> tensor<?x?xf32> attributes {tf.entry_function = {input_placements = "gpu", inputs = "input0", output_placements = "gpu", outputs = "output0"}} {
    %c0 = arith.constant 0 : index
    %c1 = arith.constant 1 : index
    %c2 = arith.constant 2 : index
    %0 = mhlo.constant {disc.device = "gpu"} dense<0xFF800000> : tensor<f32>
    %dim = tensor.dim %arg0, %c0 : tensor<?x?x?xf32>
    %dim_0 = tensor.dim %arg0, %c1 : tensor<?x?x?xf32>
    %dim_1 = tensor.dim %arg0, %c2 : tensor<?x?x?xf32>
    %1 = "disc_shape.tie_shape"(%arg0, %dim, %dim_0, %dim_1) {kDiscSymbolicDimAttr = [@S0, @S1, @S2]} : (tensor<?x?x?xf32>, index, index, index) -> tensor<?x?x?xf32>
    %2 = mhlo.abs %1 {disc.device = "gpu"} : tensor<?x?x?xf32>
    %3 = "disc_shape.tie_shape"(%2, %dim, %dim_0, %dim_1) {kDiscSymbolicDimAttr = [@S0, @S1, @S2]} : (tensor<?x?x?xf32>, index, index, index) -> tensor<?x?x?xf32>
    %4 = arith.index_cast %dim : index to i32
    %5 = arith.index_cast %dim_0 : index to i32
    %6 = arith.index_cast %dim_1 : index to i32
    %7 = arith.muli %5, %6 : i32
    %from_elements = tensor.from_elements %4, %7 {disc.shape_op = true} : tensor<2xi32>
    %8 = "disc_shape.tie_shape"(%from_elements, %c2) : (tensor<2xi32>, index) -> tensor<2xi32>
    %9 = arith.index_cast %7 : i32 to index
    %10 = mhlo.dynamic_reshape %3, %8 {disc.device = "gpu"} : (tensor<?x?x?xf32>, tensor<2xi32>) -> tensor<?x?xf32>
    %11 = "disc_shape.tie_shape"(%10, %dim, %9) {kDiscSymbolicDimAttr = [@S0, @S3]} : (tensor<?x?xf32>, index, index) -> tensor<?x?xf32>
    %12 = mhlo.reduce(%11 init: %0) applies mhlo.maximum across dimensions = [0] : (tensor<?x?xf32>, tensor<f32>) -> tensor<?xf32>
    %13 = "disc_shape.tie_shape"(%12, %9) {kDiscSymbolicDimAttr = [@S3]} : (tensor<?xf32>, index) -> tensor<?xf32>
    %from_elements_2 = tensor.from_elements %5, %6 {disc.shape_op = true} : tensor<2xi32>
    %14 = "disc_shape.tie_shape"(%from_elements_2, %c2) : (tensor<2xi32>, index) -> tensor<2xi32>
    %15 = mhlo.dynamic_reshape %13, %14 {disc.device = "gpu"} : (tensor<?xf32>, tensor<2xi32>) -> tensor<?x?xf32>
    %16 = "disc_shape.tie_shape"(%15, %dim_0, %dim_1) {kDiscSymbolicDimAttr = [@S1, @S2]} : (tensor<?x?xf32>, index, index) -> tensor<?x?xf32>
    return %16 : tensor<?x?xf32>
  }
  "disc_shape.SymbolicDim"() {knownNegativeOne = false, knownNonNegative = true, knownNonSizeOne = false, knownNonSizeZero = false, sym_name = "S0", value = -9223372036854775808 : i64} : () -> ()
  "disc_shape.SymbolicDim"() {knownNegativeOne = false, knownNonNegative = true, knownNonSizeOne = false, knownNonSizeZero = false, sym_name = "S1", value = -9223372036854775808 : i64} : () -> ()
  "disc_shape.SymbolicDim"() {knownNegativeOne = false, knownNonNegative = true, knownNonSizeOne = false, knownNonSizeZero = false, sym_name = "S2", value = -9223372036854775808 : i64} : () -> ()
  "disc_shape.SymbolicDim"() {knownNegativeOne = false, knownNonNegative = true, knownNonSizeOne = false, knownNonSizeZero = false, sym_name = "S3", value = -9223372036854775808 : i64} : () -> ()
  func.func @shape_constraint_graph() {
    %0 = "disc_shape.dim"() {name = @S3} : () -> index
    %1 = "disc_shape.dim"() {name = @S1} : () -> index
    %2 = "disc_shape.dim"() {name = @S2} : () -> index
    "disc_shape.tie_product_equal"(%0, %1, %2) {operand_segment_sizes = array<i32: 1, 2>} : (index, index, index) -> ()
    return
  }
}


// -----// IR Dump After Canonicalizer (canonicalize) //----- //
func.func @main(%arg0: tensor<?x?x?xf32>) -> tensor<?x?xf32> attributes {tf.entry_function = {input_placements = "gpu", inputs = "input0", output_placements = "gpu", outputs = "output0"}} {
  %c0 = arith.constant 0 : index
  %c1 = arith.constant 1 : index
  %c2 = arith.constant 2 : index
  %0 = mhlo.constant {disc.device = "gpu"} dense<0xFF800000> : tensor<f32>
  %dim = tensor.dim %arg0, %c0 : tensor<?x?x?xf32>
  %dim_0 = tensor.dim %arg0, %c1 : tensor<?x?x?xf32>
  %dim_1 = tensor.dim %arg0, %c2 : tensor<?x?x?xf32>
  %1 = "disc_shape.tie_shape"(%arg0, %dim, %dim_0, %dim_1) {kDiscSymbolicDimAttr = [@S0, @S1, @S2]} : (tensor<?x?x?xf32>, index, index, index) -> tensor<?x?x?xf32>
  %2 = mhlo.abs %1 {disc.device = "gpu"} : tensor<?x?x?xf32>
  %3 = "disc_shape.tie_shape"(%2, %dim, %dim_0, %dim_1) {kDiscSymbolicDimAttr = [@S0, @S1, @S2]} : (tensor<?x?x?xf32>, index, index, index) -> tensor<?x?x?xf32>
  %4 = arith.index_cast %dim : index to i32
  %5 = arith.index_cast %dim_0 : index to i32
  %6 = arith.index_cast %dim_1 : index to i32
  %7 = arith.muli %5, %6 : i32
  %from_elements = tensor.from_elements %4, %7 {disc.shape_op = true} : tensor<2xi32>
  %8 = arith.index_cast %7 : i32 to index
  %9 = mhlo.dynamic_reshape %3, %from_elements {disc.device = "gpu"} : (tensor<?x?x?xf32>, tensor<2xi32>) -> tensor<?x?xf32>
  %10 = "disc_shape.tie_shape"(%9, %dim, %8) {kDiscSymbolicDimAttr = [@S0, @S3]} : (tensor<?x?xf32>, index, index) -> tensor<?x?xf32>
  %11 = mhlo.reduce(%10 init: %0) applies mhlo.maximum across dimensions = [0] : (tensor<?x?xf32>, tensor<f32>) -> tensor<?xf32>
  %12 = "disc_shape.tie_shape"(%11, %8) {kDiscSymbolicDimAttr = [@S3]} : (tensor<?xf32>, index) -> tensor<?xf32>
  %from_elements_2 = tensor.from_elements %5, %6 {disc.shape_op = true} : tensor<2xi32>
  %13 = mhlo.dynamic_reshape %12, %from_elements_2 {disc.device = "gpu"} : (tensor<?xf32>, tensor<2xi32>) -> tensor<?x?xf32>
  %14 = "disc_shape.tie_shape"(%13, %dim_0, %dim_1) {kDiscSymbolicDimAttr = [@S1, @S2]} : (tensor<?x?xf32>, index, index) -> tensor<?x?xf32>
  return %14 : tensor<?x?xf32>
}

// -----// IR Dump After FuncBufferize (func-bufferize) //----- //
module {
  func.func @main(%arg0: memref<?x?x?xf32>) -> memref<?x?xf32> attributes {tf.entry_function = {input_placements = "gpu", inputs = "input0", output_placements = "gpu", outputs = "output0"}} {
    %0 = bufferization.to_tensor %arg0 : memref<?x?x?xf32>
    %c0 = arith.constant 0 : index
    %c1 = arith.constant 1 : index
    %c2 = arith.constant 2 : index
    %1 = mhlo.constant {disc.device = "gpu"} dense<0xFF800000> : tensor<f32>
    %dim = tensor.dim %0, %c0 : tensor<?x?x?xf32>
    %dim_0 = tensor.dim %0, %c1 : tensor<?x?x?xf32>
    %dim_1 = tensor.dim %0, %c2 : tensor<?x?x?xf32>
    %2 = "disc_shape.tie_shape"(%0, %dim, %dim_0, %dim_1) {kDiscSymbolicDimAttr = [@S0, @S1, @S2]} : (tensor<?x?x?xf32>, index, index, index) -> tensor<?x?x?xf32>
    %3 = mhlo.abs %2 {disc.device = "gpu"} : tensor<?x?x?xf32>
    %4 = "disc_shape.tie_shape"(%3, %dim, %dim_0, %dim_1) {kDiscSymbolicDimAttr = [@S0, @S1, @S2]} : (tensor<?x?x?xf32>, index, index, index) -> tensor<?x?x?xf32>
    %5 = arith.index_cast %dim : index to i32
    %6 = arith.index_cast %dim_0 : index to i32
    %7 = arith.index_cast %dim_1 : index to i32
    %8 = arith.muli %6, %7 : i32
    %from_elements = tensor.from_elements %5, %8 {disc.shape_op = true} : tensor<2xi32>
    %9 = arith.index_cast %8 : i32 to index
    %10 = mhlo.dynamic_reshape %4, %from_elements {disc.device = "gpu"} : (tensor<?x?x?xf32>, tensor<2xi32>) -> tensor<?x?xf32>
    %11 = "disc_shape.tie_shape"(%10, %dim, %9) {kDiscSymbolicDimAttr = [@S0, @S3]} : (tensor<?x?xf32>, index, index) -> tensor<?x?xf32>
    %12 = mhlo.reduce(%11 init: %1) applies mhlo.maximum across dimensions = [0] : (tensor<?x?xf32>, tensor<f32>) -> tensor<?xf32>
    %13 = "disc_shape.tie_shape"(%12, %9) {kDiscSymbolicDimAttr = [@S3]} : (tensor<?xf32>, index) -> tensor<?xf32>
    %from_elements_2 = tensor.from_elements %6, %7 {disc.shape_op = true} : tensor<2xi32>
    %14 = mhlo.dynamic_reshape %13, %from_elements_2 {disc.device = "gpu"} : (tensor<?xf32>, tensor<2xi32>) -> tensor<?x?xf32>
    %15 = "disc_shape.tie_shape"(%14, %dim_0, %dim_1) {kDiscSymbolicDimAttr = [@S1, @S2]} : (tensor<?x?xf32>, index, index) -> tensor<?x?xf32>
    %16 = bufferization.to_memref %15 : memref<?x?xf32>
    return %16 : memref<?x?xf32>
  }
  "disc_shape.SymbolicDim"() {knownNegativeOne = false, knownNonNegative = true, knownNonSizeOne = false, knownNonSizeZero = false, sym_name = "S0", value = -9223372036854775808 : i64} : () -> ()
  "disc_shape.SymbolicDim"() {knownNegativeOne = false, knownNonNegative = true, knownNonSizeOne = false, knownNonSizeZero = false, sym_name = "S1", value = -9223372036854775808 : i64} : () -> ()
  "disc_shape.SymbolicDim"() {knownNegativeOne = false, knownNonNegative = true, knownNonSizeOne = false, knownNonSizeZero = false, sym_name = "S2", value = -9223372036854775808 : i64} : () -> ()
  "disc_shape.SymbolicDim"() {knownNegativeOne = false, knownNonNegative = true, knownNonSizeOne = false, knownNonSizeZero = false, sym_name = "S3", value = -9223372036854775808 : i64} : () -> ()
  func.func @shape_constraint_graph() {
    %0 = "disc_shape.dim"() {name = @S3} : () -> index
    %1 = "disc_shape.dim"() {name = @S1} : () -> index
    %2 = "disc_shape.dim"() {name = @S2} : () -> index
    "disc_shape.tie_product_equal"(%0, %1, %2) {operand_segment_sizes = array<i32: 1, 2>} : (index, index, index) -> ()
    return
  }
}


// -----// IR Dump After DiscHloLegalizeToLhloPass (disc-hlo-legalize-to-lhlo) //----- //
module {
  func.func @main(%arg0: memref<?x?x?xf32>) -> memref<?x?xf32> attributes {tf.entry_function = {input_placements = "gpu", inputs = "input0", output_placements = "gpu", outputs = "output0"}} {
    %0 = bufferization.to_tensor %arg0 : memref<?x?x?xf32>
    %1 = bufferization.to_memref %0 : memref<?x?x?xf32>
    %c0 = arith.constant 0 : index
    %c1 = arith.constant 1 : index
    %c2 = arith.constant 2 : index
    %2 = mhlo.constant {disc.device = "gpu"} dense<0xFF800000> : tensor<f32>
    %dim = tensor.dim %0, %c0 : tensor<?x?x?xf32>
    %dim_0 = tensor.dim %0, %c1 : tensor<?x?x?xf32>
    %dim_1 = tensor.dim %0, %c2 : tensor<?x?x?xf32>
    %c1_2 = arith.constant 1 : index
    %3 = arith.muli %c1_2, %dim_1 : index
    %4 = arith.muli %3, %dim_0 : index
    %5 = arith.muli %4, %dim : index
    %reinterpret_cast = memref.reinterpret_cast %1 to offset: [0], sizes: [%dim, %dim_0, %dim_1], strides: [%4, %3, 1] {kDiscSymbolicDimAttr = [@S0, @S1, @S2]} : memref<?x?x?xf32> to memref<?x?x?xf32>
    %6 = bufferization.to_tensor %reinterpret_cast : memref<?x?x?xf32>
    %7 = mhlo.abs %6 {disc.device = "gpu"} : tensor<?x?x?xf32>
    %8 = bufferization.to_memref %7 : memref<?x?x?xf32>
    %c1_3 = arith.constant 1 : index
    %9 = arith.muli %c1_3, %dim_1 : index
    %10 = arith.muli %9, %dim_0 : index
    %11 = arith.muli %10, %dim : index
    %reinterpret_cast_4 = memref.reinterpret_cast %8 to offset: [0], sizes: [%dim, %dim_0, %dim_1], strides: [%10, %9, 1] {kDiscSymbolicDimAttr = [@S0, @S1, @S2]} : memref<?x?x?xf32> to memref<?x?x?xf32>
    %12 = bufferization.to_tensor %reinterpret_cast_4 : memref<?x?x?xf32>
    %13 = arith.index_cast %dim : index to i32
    %14 = arith.index_cast %dim_0 : index to i32
    %15 = arith.index_cast %dim_1 : index to i32
    %16 = arith.muli %14, %15 : i32
    %from_elements = tensor.from_elements %13, %16 {disc.shape_op = true} : tensor<2xi32>
    %17 = arith.index_cast %16 : i32 to index
    %18 = mhlo.dynamic_reshape %12, %from_elements {disc.device = "gpu"} : (tensor<?x?x?xf32>, tensor<2xi32>) -> tensor<?x?xf32>
    %19 = bufferization.to_memref %18 : memref<?x?xf32>
    %c1_5 = arith.constant 1 : index
    %20 = arith.muli %c1_5, %17 : index
    %21 = arith.muli %20, %dim : index
    %reinterpret_cast_6 = memref.reinterpret_cast %19 to offset: [0], sizes: [%dim, %17], strides: [%20, 1] {kDiscSymbolicDimAttr = [@S0, @S3]} : memref<?x?xf32> to memref<?x?xf32>
    %22 = bufferization.to_tensor %reinterpret_cast_6 : memref<?x?xf32>
    %23 = mhlo.reduce(%22 init: %2) applies mhlo.maximum across dimensions = [0] : (tensor<?x?xf32>, tensor<f32>) -> tensor<?xf32>
    %24 = bufferization.to_memref %23 : memref<?xf32>
    %c1_7 = arith.constant 1 : index
    %25 = arith.muli %c1_7, %17 : index
    %reinterpret_cast_8 = memref.reinterpret_cast %24 to offset: [0], sizes: [%17], strides: [1] {kDiscSymbolicDimAttr = [@S3]} : memref<?xf32> to memref<?xf32>
    %26 = bufferization.to_tensor %reinterpret_cast_8 : memref<?xf32>
    %from_elements_9 = tensor.from_elements %14, %15 {disc.shape_op = true} : tensor<2xi32>
    %27 = mhlo.dynamic_reshape %26, %from_elements_9 {disc.device = "gpu"} : (tensor<?xf32>, tensor<2xi32>) -> tensor<?x?xf32>
    %28 = bufferization.to_memref %27 : memref<?x?xf32>
    %c1_10 = arith.constant 1 : index
    %29 = arith.muli %c1_10, %dim_1 : index
    %30 = arith.muli %29, %dim_0 : index
    %reinterpret_cast_11 = memref.reinterpret_cast %28 to offset: [0], sizes: [%dim_0, %dim_1], strides: [%29, 1] {kDiscSymbolicDimAttr = [@S1, @S2]} : memref<?x?xf32> to memref<?x?xf32>
    %31 = bufferization.to_tensor %reinterpret_cast_11 : memref<?x?xf32>
    %32 = bufferization.to_memref %31 : memref<?x?xf32>
    return %32 : memref<?x?xf32>
  }
  "disc_shape.SymbolicDim"() {knownNegativeOne = false, knownNonNegative = true, knownNonSizeOne = false, knownNonSizeZero = false, sym_name = "S0", value = -9223372036854775808 : i64} : () -> ()
  "disc_shape.SymbolicDim"() {knownNegativeOne = false, knownNonNegative = true, knownNonSizeOne = false, knownNonSizeZero = false, sym_name = "S1", value = -9223372036854775808 : i64} : () -> ()
  "disc_shape.SymbolicDim"() {knownNegativeOne = false, knownNonNegative = true, knownNonSizeOne = false, knownNonSizeZero = false, sym_name = "S2", value = -9223372036854775808 : i64} : () -> ()
  "disc_shape.SymbolicDim"() {knownNegativeOne = false, knownNonNegative = true, knownNonSizeOne = false, knownNonSizeZero = false, sym_name = "S3", value = -9223372036854775808 : i64} : () -> ()
  func.func @shape_constraint_graph() {
    %0 = "disc_shape.dim"() {name = @S3} : () -> index
    %1 = "disc_shape.dim"() {name = @S1} : () -> index
    %2 = "disc_shape.dim"() {name = @S2} : () -> index
    "disc_shape.tie_product_equal"(%0, %1, %2) {operand_segment_sizes = array<i32: 1, 2>} : (index, index, index) -> ()
    return
  }
}


// -----// IR Dump After HloLegalizeToLhloPass (hlo-legalize-to-lhlo) //----- //
module {
  func.func @main(%arg0: memref<?x?x?xf32>) -> memref<?x?xf32> attributes {tf.entry_function = {input_placements = "gpu", inputs = "input0", output_placements = "gpu", outputs = "output0"}} {
    %0 = bufferization.to_tensor %arg0 : memref<?x?x?xf32>
    %c0 = arith.constant 0 : index
    %c1 = arith.constant 1 : index
    %c2 = arith.constant 2 : index
    %alloc = memref.alloc() : memref<f32>
    "lmhlo.constant"(%alloc) {disc.device = "gpu", value = dense<0xFF800000> : tensor<f32>} : (memref<f32>) -> ()
    %dim = tensor.dim %0, %c0 : tensor<?x?x?xf32>
    %dim_0 = tensor.dim %0, %c1 : tensor<?x?x?xf32>
    %dim_1 = tensor.dim %0, %c2 : tensor<?x?x?xf32>
    %c1_2 = arith.constant 1 : index
    %1 = arith.muli %c1_2, %dim_1 : index
    %2 = arith.muli %1, %dim_0 : index
    %3 = arith.muli %2, %dim : index
    %reinterpret_cast = memref.reinterpret_cast %arg0 to offset: [0], sizes: [%dim, %dim_0, %dim_1], strides: [%2, %1, 1] {kDiscSymbolicDimAttr = [@S0, @S1, @S2]} : memref<?x?x?xf32> to memref<?x?x?xf32>
    %4 = bufferization.to_tensor %reinterpret_cast : memref<?x?x?xf32>
    %5 = bufferization.to_memref %4 : memref<?x?x?xf32>
    %6 = bufferization.to_tensor %5 : memref<?x?x?xf32>
    %7 = shape.shape_of %6 : tensor<?x?x?xf32> -> tensor<3xindex>
    %c0_3 = arith.constant 0 : index
    %extracted = tensor.extract %7[%c0_3] : tensor<3xindex>
    %c1_4 = arith.constant 1 : index
    %extracted_5 = tensor.extract %7[%c1_4] : tensor<3xindex>
    %c2_6 = arith.constant 2 : index
    %extracted_7 = tensor.extract %7[%c2_6] : tensor<3xindex>
    %alloc_8 = memref.alloc(%extracted, %extracted_5, %extracted_7) : memref<?x?x?xf32>
    "lmhlo.abs"(%5, %alloc_8) {disc.device = "gpu"} : (memref<?x?x?xf32>, memref<?x?x?xf32>) -> ()
    %8 = bufferization.to_tensor %alloc_8 : memref<?x?x?xf32>
    %9 = bufferization.to_memref %8 : memref<?x?x?xf32>
    %c1_9 = arith.constant 1 : index
    %10 = arith.muli %c1_9, %dim_1 : index
    %11 = arith.muli %10, %dim_0 : index
    %12 = arith.muli %11, %dim : index
    %reinterpret_cast_10 = memref.reinterpret_cast %9 to offset: [0], sizes: [%dim, %dim_0, %dim_1], strides: [%11, %10, 1] {kDiscSymbolicDimAttr = [@S0, @S1, @S2]} : memref<?x?x?xf32> to memref<?x?x?xf32>
    %13 = bufferization.to_tensor %reinterpret_cast_10 : memref<?x?x?xf32>
    %14 = bufferization.to_memref %13 : memref<?x?x?xf32>
    %15 = arith.index_cast %dim : index to i32
    %16 = arith.index_cast %dim_0 : index to i32
    %17 = arith.index_cast %dim_1 : index to i32
    %18 = arith.muli %16, %17 : i32
    %from_elements = tensor.from_elements %15, %18 {disc.shape_op = true} : tensor<2xi32>
    %19 = bufferization.to_memref %from_elements : memref<2xi32>
    %20 = arith.index_cast %18 : i32 to index
    %21 = bufferization.to_tensor %14 : memref<?x?x?xf32>
    %22 = bufferization.to_tensor %19 : memref<2xi32>
    %23 = arith.index_cast %22 : tensor<2xi32> to tensor<2xindex>
    %c0_11 = arith.constant 0 : index
    %extracted_12 = tensor.extract %23[%c0_11] : tensor<2xindex>
    %c1_13 = arith.constant 1 : index
    %extracted_14 = tensor.extract %23[%c1_13] : tensor<2xindex>
    %alloc_15 = memref.alloc(%extracted_12, %extracted_14) : memref<?x?xf32>
    "lmhlo.dynamic_reshape"(%14, %19, %alloc_15) {disc.device = "gpu"} : (memref<?x?x?xf32>, memref<2xi32>, memref<?x?xf32>) -> ()
    %24 = bufferization.to_tensor %alloc_15 : memref<?x?xf32>
    %25 = bufferization.to_memref %24 : memref<?x?xf32>
    %c1_16 = arith.constant 1 : index
    %26 = arith.muli %c1_16, %20 : index
    %27 = arith.muli %26, %dim : index
    %reinterpret_cast_17 = memref.reinterpret_cast %25 to offset: [0], sizes: [%dim, %20], strides: [%26, 1] {kDiscSymbolicDimAttr = [@S0, @S3]} : memref<?x?xf32> to memref<?x?xf32>
    %28 = bufferization.to_tensor %reinterpret_cast_17 : memref<?x?xf32>
    %29 = bufferization.to_memref %28 : memref<?x?xf32>
    %30 = bufferization.to_tensor %29 : memref<?x?xf32>
    %31 = bufferization.to_tensor %alloc : memref<f32>
    %c1_18 = arith.constant 1 : index
    %dim_19 = tensor.dim %30, %c1_18 : tensor<?x?xf32>
    %from_elements_20 = tensor.from_elements %dim_19 : tensor<1xindex>
    %c0_21 = arith.constant 0 : index
    %extracted_22 = tensor.extract %from_elements_20[%c0_21] : tensor<1xindex>
    %alloc_23 = memref.alloc(%extracted_22) : memref<?xf32>
    "lmhlo.reduce"(%29, %alloc, %alloc_23) ({
    ^bb0(%arg1: memref<f32>, %arg2: memref<f32>, %arg3: memref<f32>):
      %alloc_34 = memref.alloc() : memref<f32>
      "lmhlo.maximum"(%arg1, %arg2, %alloc_34) {disc.device = "gpu"} : (memref<f32>, memref<f32>, memref<f32>) -> ()
      "lmhlo.copy"(%alloc_34, %arg3) : (memref<f32>, memref<f32>) -> ()
      "lmhlo.terminator"() : () -> ()
    }) {dimensions = dense<0> : tensor<1xi64>, disc.device = "gpu"} : (memref<?x?xf32>, memref<f32>, memref<?xf32>) -> ()
    %32 = bufferization.to_tensor %alloc_23 : memref<?xf32>
    %33 = bufferization.to_memref %32 : memref<?xf32>
    %c1_24 = arith.constant 1 : index
    %34 = arith.muli %c1_24, %20 : index
    %reinterpret_cast_25 = memref.reinterpret_cast %33 to offset: [0], sizes: [%20], strides: [1] {kDiscSymbolicDimAttr = [@S3]} : memref<?xf32> to memref<?xf32>
    %35 = bufferization.to_tensor %reinterpret_cast_25 : memref<?xf32>
    %36 = bufferization.to_memref %35 : memref<?xf32>
    %from_elements_26 = tensor.from_elements %16, %17 {disc.shape_op = true} : tensor<2xi32>
    %37 = bufferization.to_memref %from_elements_26 : memref<2xi32>
    %38 = bufferization.to_tensor %36 : memref<?xf32>
    %39 = bufferization.to_tensor %37 : memref<2xi32>
    %40 = arith.index_cast %39 : tensor<2xi32> to tensor<2xindex>
    %c0_27 = arith.constant 0 : index
    %extracted_28 = tensor.extract %40[%c0_27] : tensor<2xindex>
    %c1_29 = arith.constant 1 : index
    %extracted_30 = tensor.extract %40[%c1_29] : tensor<2xindex>
    %alloc_31 = memref.alloc(%extracted_28, %extracted_30) : memref<?x?xf32>
    "lmhlo.dynamic_reshape"(%36, %37, %alloc_31) {disc.device = "gpu"} : (memref<?xf32>, memref<2xi32>, memref<?x?xf32>) -> ()
    %41 = bufferization.to_tensor %alloc_31 : memref<?x?xf32>
    %42 = bufferization.to_memref %41 : memref<?x?xf32>
    %c1_32 = arith.constant 1 : index
    %43 = arith.muli %c1_32, %dim_1 : index
    %44 = arith.muli %43, %dim_0 : index
    %reinterpret_cast_33 = memref.reinterpret_cast %42 to offset: [0], sizes: [%dim_0, %dim_1], strides: [%43, 1] {kDiscSymbolicDimAttr = [@S1, @S2]} : memref<?x?xf32> to memref<?x?xf32>
    return %reinterpret_cast_33 : memref<?x?xf32>
  }
  "disc_shape.SymbolicDim"() {knownNegativeOne = false, knownNonNegative = true, knownNonSizeOne = false, knownNonSizeZero = false, sym_name = "S0", value = -9223372036854775808 : i64} : () -> ()
  "disc_shape.SymbolicDim"() {knownNegativeOne = false, knownNonNegative = true, knownNonSizeOne = false, knownNonSizeZero = false, sym_name = "S1", value = -9223372036854775808 : i64} : () -> ()
  "disc_shape.SymbolicDim"() {knownNegativeOne = false, knownNonNegative = true, knownNonSizeOne = false, knownNonSizeZero = false, sym_name = "S2", value = -9223372036854775808 : i64} : () -> ()
  "disc_shape.SymbolicDim"() {knownNegativeOne = false, knownNonNegative = true, knownNonSizeOne = false, knownNonSizeZero = false, sym_name = "S3", value = -9223372036854775808 : i64} : () -> ()
  func.func @shape_constraint_graph() {
    %0 = "disc_shape.dim"() {name = @S3} : () -> index
    %1 = "disc_shape.dim"() {name = @S1} : () -> index
    %2 = "disc_shape.dim"() {name = @S2} : () -> index
    "disc_shape.tie_product_equal"(%0, %1, %2) {operand_segment_sizes = array<i32: 1, 2>} : (index, index, index) -> ()
    return
  }
}


// -----// IR Dump After Canonicalizer (canonicalize) //----- //
func.func @main(%arg0: memref<?x?x?xf32>) -> memref<?x?xf32> attributes {tf.entry_function = {input_placements = "gpu", inputs = "input0", output_placements = "gpu", outputs = "output0"}} {
  %c2 = arith.constant 2 : index
  %c1 = arith.constant 1 : index
  %c0 = arith.constant 0 : index
  %alloc = memref.alloc() : memref<f32>
  "lmhlo.constant"(%alloc) {disc.device = "gpu", value = dense<0xFF800000> : tensor<f32>} : (memref<f32>) -> ()
  %dim = memref.dim %arg0, %c0 : memref<?x?x?xf32>
  %dim_0 = memref.dim %arg0, %c1 : memref<?x?x?xf32>
  %dim_1 = memref.dim %arg0, %c2 : memref<?x?x?xf32>
  %0 = arith.muli %dim_1, %dim_0 : index
  %reinterpret_cast = memref.reinterpret_cast %arg0 to offset: [0], sizes: [%dim, %dim_0, %dim_1], strides: [%0, %dim_1, 1] {kDiscSymbolicDimAttr = [@S0, @S1, @S2]} : memref<?x?x?xf32> to memref<?x?x?xf32>
  %alloc_2 = memref.alloc(%dim, %dim_0, %dim_1) : memref<?x?x?xf32>
  "lmhlo.abs"(%reinterpret_cast, %alloc_2) {disc.device = "gpu"} : (memref<?x?x?xf32>, memref<?x?x?xf32>) -> ()
  %1 = arith.muli %dim_1, %dim_0 : index
  %reinterpret_cast_3 = memref.reinterpret_cast %alloc_2 to offset: [0], sizes: [%dim, %dim_0, %dim_1], strides: [%1, %dim_1, 1] {kDiscSymbolicDimAttr = [@S0, @S1, @S2]} : memref<?x?x?xf32> to memref<?x?x?xf32>
  %2 = arith.index_cast %dim : index to i32
  %3 = arith.index_cast %dim_0 : index to i32
  %4 = arith.index_cast %dim_1 : index to i32
  %5 = arith.muli %3, %4 : i32
  %from_elements = tensor.from_elements %2, %5 {disc.shape_op = true} : tensor<2xi32>
  %6 = bufferization.to_memref %from_elements : memref<2xi32>
  %7 = arith.index_cast %5 : i32 to index
  %8 = bufferization.to_tensor %6 : memref<2xi32>
  %extracted = tensor.extract %8[%c0] : tensor<2xi32>
  %9 = arith.index_cast %extracted : i32 to index
  %extracted_4 = tensor.extract %8[%c1] : tensor<2xi32>
  %10 = arith.index_cast %extracted_4 : i32 to index
  %alloc_5 = memref.alloc(%9, %10) : memref<?x?xf32>
  "lmhlo.dynamic_reshape"(%reinterpret_cast_3, %6, %alloc_5) {disc.device = "gpu"} : (memref<?x?x?xf32>, memref<2xi32>, memref<?x?xf32>) -> ()
  %reinterpret_cast_6 = memref.reinterpret_cast %alloc_5 to offset: [0], sizes: [%dim, %7], strides: [%7, 1] {kDiscSymbolicDimAttr = [@S0, @S3]} : memref<?x?xf32> to memref<?x?xf32>
  %alloc_7 = memref.alloc(%7) : memref<?xf32>
  "lmhlo.reduce"(%reinterpret_cast_6, %alloc, %alloc_7) ({
  ^bb0(%arg1: memref<f32>, %arg2: memref<f32>, %arg3: memref<f32>):
    "lmhlo.maximum"(%arg1, %arg2, %arg3) {disc.device = "gpu"} : (memref<f32>, memref<f32>, memref<f32>) -> ()
    "lmhlo.terminator"() : () -> ()
  }) {dimensions = dense<0> : tensor<1xi64>, disc.device = "gpu"} : (memref<?x?xf32>, memref<f32>, memref<?xf32>) -> ()
  %reinterpret_cast_8 = memref.reinterpret_cast %alloc_7 to offset: [0], sizes: [%7], strides: [1] {kDiscSymbolicDimAttr = [@S3]} : memref<?xf32> to memref<?xf32>
  %from_elements_9 = tensor.from_elements %3, %4 {disc.shape_op = true} : tensor<2xi32>
  %11 = bufferization.to_memref %from_elements_9 : memref<2xi32>
  %alloc_10 = memref.alloc(%dim_0, %dim_1) : memref<?x?xf32>
  "lmhlo.dynamic_reshape"(%reinterpret_cast_8, %11, %alloc_10) {disc.device = "gpu"} : (memref<?xf32>, memref<2xi32>, memref<?x?xf32>) -> ()
  %reinterpret_cast_11 = memref.reinterpret_cast %alloc_10 to offset: [0], sizes: [%dim_0, %dim_1], strides: [%dim_1, 1] {kDiscSymbolicDimAttr = [@S1, @S2]} : memref<?x?xf32> to memref<?x?xf32>
  return %reinterpret_cast_11 : memref<?x?xf32>
}

// -----// IR Dump After CSE (cse) //----- //
func.func @main(%arg0: memref<?x?x?xf32>) -> memref<?x?xf32> attributes {tf.entry_function = {input_placements = "gpu", inputs = "input0", output_placements = "gpu", outputs = "output0"}} {
  %c2 = arith.constant 2 : index
  %c1 = arith.constant 1 : index
  %c0 = arith.constant 0 : index
  %alloc = memref.alloc() : memref<f32>
  "lmhlo.constant"(%alloc) {disc.device = "gpu", value = dense<0xFF800000> : tensor<f32>} : (memref<f32>) -> ()
  %dim = memref.dim %arg0, %c0 : memref<?x?x?xf32>
  %dim_0 = memref.dim %arg0, %c1 : memref<?x?x?xf32>
  %dim_1 = memref.dim %arg0, %c2 : memref<?x?x?xf32>
  %0 = arith.muli %dim_1, %dim_0 : index
  %reinterpret_cast = memref.reinterpret_cast %arg0 to offset: [0], sizes: [%dim, %dim_0, %dim_1], strides: [%0, %dim_1, 1] {kDiscSymbolicDimAttr = [@S0, @S1, @S2]} : memref<?x?x?xf32> to memref<?x?x?xf32>
  %alloc_2 = memref.alloc(%dim, %dim_0, %dim_1) : memref<?x?x?xf32>
  "lmhlo.abs"(%reinterpret_cast, %alloc_2) {disc.device = "gpu"} : (memref<?x?x?xf32>, memref<?x?x?xf32>) -> ()
  %reinterpret_cast_3 = memref.reinterpret_cast %alloc_2 to offset: [0], sizes: [%dim, %dim_0, %dim_1], strides: [%0, %dim_1, 1] {kDiscSymbolicDimAttr = [@S0, @S1, @S2]} : memref<?x?x?xf32> to memref<?x?x?xf32>
  %1 = arith.index_cast %dim : index to i32
  %2 = arith.index_cast %dim_0 : index to i32
  %3 = arith.index_cast %dim_1 : index to i32
  %4 = arith.muli %2, %3 : i32
  %from_elements = tensor.from_elements %1, %4 {disc.shape_op = true} : tensor<2xi32>
  %5 = bufferization.to_memref %from_elements : memref<2xi32>
  %6 = arith.index_cast %4 : i32 to index
  %7 = bufferization.to_tensor %5 : memref<2xi32>
  %extracted = tensor.extract %7[%c0] : tensor<2xi32>
  %8 = arith.index_cast %extracted : i32 to index
  %extracted_4 = tensor.extract %7[%c1] : tensor<2xi32>
  %9 = arith.index_cast %extracted_4 : i32 to index
  %alloc_5 = memref.alloc(%8, %9) : memref<?x?xf32>
  "lmhlo.dynamic_reshape"(%reinterpret_cast_3, %5, %alloc_5) {disc.device = "gpu"} : (memref<?x?x?xf32>, memref<2xi32>, memref<?x?xf32>) -> ()
  %reinterpret_cast_6 = memref.reinterpret_cast %alloc_5 to offset: [0], sizes: [%dim, %6], strides: [%6, 1] {kDiscSymbolicDimAttr = [@S0, @S3]} : memref<?x?xf32> to memref<?x?xf32>
  %alloc_7 = memref.alloc(%6) : memref<?xf32>
  "lmhlo.reduce"(%reinterpret_cast_6, %alloc, %alloc_7) ({
  ^bb0(%arg1: memref<f32>, %arg2: memref<f32>, %arg3: memref<f32>):
    "lmhlo.maximum"(%arg1, %arg2, %arg3) {disc.device = "gpu"} : (memref<f32>, memref<f32>, memref<f32>) -> ()
    "lmhlo.terminator"() : () -> ()
  }) {dimensions = dense<0> : tensor<1xi64>, disc.device = "gpu"} : (memref<?x?xf32>, memref<f32>, memref<?xf32>) -> ()
  %reinterpret_cast_8 = memref.reinterpret_cast %alloc_7 to offset: [0], sizes: [%6], strides: [1] {kDiscSymbolicDimAttr = [@S3]} : memref<?xf32> to memref<?xf32>
  %from_elements_9 = tensor.from_elements %2, %3 {disc.shape_op = true} : tensor<2xi32>
  %10 = bufferization.to_memref %from_elements_9 : memref<2xi32>
  %alloc_10 = memref.alloc(%dim_0, %dim_1) : memref<?x?xf32>
  "lmhlo.dynamic_reshape"(%reinterpret_cast_8, %10, %alloc_10) {disc.device = "gpu"} : (memref<?xf32>, memref<2xi32>, memref<?x?xf32>) -> ()
  %reinterpret_cast_11 = memref.reinterpret_cast %alloc_10 to offset: [0], sizes: [%dim_0, %dim_1], strides: [%dim_1, 1] {kDiscSymbolicDimAttr = [@S1, @S2]} : memref<?x?xf32> to memref<?x?xf32>
  return %reinterpret_cast_11 : memref<?x?xf32>
}

// -----// IR Dump After LegalizeToTensorOpPass (lhlo-legalize-to-tensor-op) //----- //
func.func @main(%arg0: memref<?x?x?xf32>) -> memref<?x?xf32> attributes {tf.entry_function = {input_placements = "gpu", inputs = "input0", output_placements = "gpu", outputs = "output0"}} {
  %c2 = arith.constant 2 : index
  %c1 = arith.constant 1 : index
  %c0 = arith.constant 0 : index
  %alloc = memref.alloc() : memref<f32>
  "lmhlo.constant"(%alloc) {disc.device = "gpu", value = dense<0xFF800000> : tensor<f32>} : (memref<f32>) -> ()
  %dim = memref.dim %arg0, %c0 : memref<?x?x?xf32>
  %dim_0 = memref.dim %arg0, %c1 : memref<?x?x?xf32>
  %dim_1 = memref.dim %arg0, %c2 : memref<?x?x?xf32>
  %0 = arith.muli %dim_1, %dim_0 : index
  %reinterpret_cast = memref.reinterpret_cast %arg0 to offset: [0], sizes: [%dim, %dim_0, %dim_1], strides: [%0, %dim_1, 1] {kDiscSymbolicDimAttr = [@S0, @S1, @S2]} : memref<?x?x?xf32> to memref<?x?x?xf32>
  %alloc_2 = memref.alloc(%dim, %dim_0, %dim_1) : memref<?x?x?xf32>
  "lmhlo.abs"(%reinterpret_cast, %alloc_2) {disc.device = "gpu"} : (memref<?x?x?xf32>, memref<?x?x?xf32>) -> ()
  %reinterpret_cast_3 = memref.reinterpret_cast %alloc_2 to offset: [0], sizes: [%dim, %dim_0, %dim_1], strides: [%0, %dim_1, 1] {kDiscSymbolicDimAttr = [@S0, @S1, @S2]} : memref<?x?x?xf32> to memref<?x?x?xf32>
  %1 = arith.index_cast %dim : index to i32
  %2 = arith.index_cast %dim_0 : index to i32
  %3 = arith.index_cast %dim_1 : index to i32
  %4 = arith.muli %2, %3 : i32
  %from_elements = tensor.from_elements %1, %4 {disc.shape_op = true} : tensor<2xi32>
  %5 = bufferization.to_memref %from_elements : memref<2xi32>
  %6 = arith.index_cast %4 : i32 to index
  %7 = memref.load %5[%c0] : memref<2xi32>
  %8 = arith.index_cast %7 : i32 to index
  %9 = memref.load %5[%c1] : memref<2xi32>
  %10 = arith.index_cast %9 : i32 to index
  %alloc_4 = memref.alloc(%8, %10) : memref<?x?xf32>
  "lmhlo.dynamic_reshape"(%reinterpret_cast_3, %5, %alloc_4) {disc.device = "gpu"} : (memref<?x?x?xf32>, memref<2xi32>, memref<?x?xf32>) -> ()
  %reinterpret_cast_5 = memref.reinterpret_cast %alloc_4 to offset: [0], sizes: [%dim, %6], strides: [%6, 1] {kDiscSymbolicDimAttr = [@S0, @S3]} : memref<?x?xf32> to memref<?x?xf32>
  %alloc_6 = memref.alloc(%6) : memref<?xf32>
  "lmhlo.reduce"(%reinterpret_cast_5, %alloc, %alloc_6) ({
  ^bb0(%arg1: memref<f32>, %arg2: memref<f32>, %arg3: memref<f32>):
    "lmhlo.maximum"(%arg1, %arg2, %arg3) {disc.device = "gpu"} : (memref<f32>, memref<f32>, memref<f32>) -> ()
    "lmhlo.terminator"() : () -> ()
  }) {dimensions = dense<0> : tensor<1xi64>, disc.device = "gpu"} : (memref<?x?xf32>, memref<f32>, memref<?xf32>) -> ()
  %reinterpret_cast_7 = memref.reinterpret_cast %alloc_6 to offset: [0], sizes: [%6], strides: [1] {kDiscSymbolicDimAttr = [@S3]} : memref<?xf32> to memref<?xf32>
  %from_elements_8 = tensor.from_elements %2, %3 {disc.shape_op = true} : tensor<2xi32>
  %11 = bufferization.to_memref %from_elements_8 : memref<2xi32>
  %alloc_9 = memref.alloc(%dim_0, %dim_1) : memref<?x?xf32>
  "lmhlo.dynamic_reshape"(%reinterpret_cast_7, %11, %alloc_9) {disc.device = "gpu"} : (memref<?xf32>, memref<2xi32>, memref<?x?xf32>) -> ()
  %reinterpret_cast_10 = memref.reinterpret_cast %alloc_9 to offset: [0], sizes: [%dim_0, %dim_1], strides: [%dim_1, 1] {kDiscSymbolicDimAttr = [@S1, @S2]} : memref<?x?xf32> to memref<?x?xf32>
  return %reinterpret_cast_10 : memref<?x?xf32>
}

// -----// IR Dump After Canonicalizer (canonicalize) //----- //
func.func @main(%arg0: memref<?x?x?xf32>) -> memref<?x?xf32> attributes {tf.entry_function = {input_placements = "gpu", inputs = "input0", output_placements = "gpu", outputs = "output0"}} {
  %c2 = arith.constant 2 : index
  %c1 = arith.constant 1 : index
  %c0 = arith.constant 0 : index
  %alloc = memref.alloc() : memref<f32>
  "lmhlo.constant"(%alloc) {disc.device = "gpu", value = dense<0xFF800000> : tensor<f32>} : (memref<f32>) -> ()
  %dim = memref.dim %arg0, %c0 : memref<?x?x?xf32>
  %dim_0 = memref.dim %arg0, %c1 : memref<?x?x?xf32>
  %dim_1 = memref.dim %arg0, %c2 : memref<?x?x?xf32>
  %0 = arith.muli %dim_1, %dim_0 : index
  %reinterpret_cast = memref.reinterpret_cast %arg0 to offset: [0], sizes: [%dim, %dim_0, %dim_1], strides: [%0, %dim_1, 1] {kDiscSymbolicDimAttr = [@S0, @S1, @S2]} : memref<?x?x?xf32> to memref<?x?x?xf32>
  %alloc_2 = memref.alloc(%dim, %dim_0, %dim_1) : memref<?x?x?xf32>
  "lmhlo.abs"(%reinterpret_cast, %alloc_2) {disc.device = "gpu"} : (memref<?x?x?xf32>, memref<?x?x?xf32>) -> ()
  %reinterpret_cast_3 = memref.reinterpret_cast %alloc_2 to offset: [0], sizes: [%dim, %dim_0, %dim_1], strides: [%0, %dim_1, 1] {kDiscSymbolicDimAttr = [@S0, @S1, @S2]} : memref<?x?x?xf32> to memref<?x?x?xf32>
  %1 = arith.index_cast %dim : index to i32
  %2 = arith.index_cast %dim_0 : index to i32
  %3 = arith.index_cast %dim_1 : index to i32
  %4 = arith.muli %2, %3 : i32
  %from_elements = tensor.from_elements %1, %4 {disc.shape_op = true} : tensor<2xi32>
  %5 = bufferization.to_memref %from_elements : memref<2xi32>
  %6 = arith.index_cast %4 : i32 to index
  %7 = arith.index_cast %4 : i32 to index
  %alloc_4 = memref.alloc(%dim, %7) : memref<?x?xf32>
  "lmhlo.dynamic_reshape"(%reinterpret_cast_3, %5, %alloc_4) {disc.device = "gpu"} : (memref<?x?x?xf32>, memref<2xi32>, memref<?x?xf32>) -> ()
  %reinterpret_cast_5 = memref.reinterpret_cast %alloc_4 to offset: [0], sizes: [%dim, %6], strides: [%6, 1] {kDiscSymbolicDimAttr = [@S0, @S3]} : memref<?x?xf32> to memref<?x?xf32>
  %alloc_6 = memref.alloc(%6) : memref<?xf32>
  "lmhlo.reduce"(%reinterpret_cast_5, %alloc, %alloc_6) ({
  ^bb0(%arg1: memref<f32>, %arg2: memref<f32>, %arg3: memref<f32>):
    "lmhlo.maximum"(%arg1, %arg2, %arg3) {disc.device = "gpu"} : (memref<f32>, memref<f32>, memref<f32>) -> ()
    "lmhlo.terminator"() : () -> ()
  }) {dimensions = dense<0> : tensor<1xi64>, disc.device = "gpu"} : (memref<?x?xf32>, memref<f32>, memref<?xf32>) -> ()
  %reinterpret_cast_7 = memref.reinterpret_cast %alloc_6 to offset: [0], sizes: [%6], strides: [1] {kDiscSymbolicDimAttr = [@S3]} : memref<?xf32> to memref<?xf32>
  %from_elements_8 = tensor.from_elements %2, %3 {disc.shape_op = true} : tensor<2xi32>
  %8 = bufferization.to_memref %from_elements_8 : memref<2xi32>
  %alloc_9 = memref.alloc(%dim_0, %dim_1) : memref<?x?xf32>
  "lmhlo.dynamic_reshape"(%reinterpret_cast_7, %8, %alloc_9) {disc.device = "gpu"} : (memref<?xf32>, memref<2xi32>, memref<?x?xf32>) -> ()
  %reinterpret_cast_10 = memref.reinterpret_cast %alloc_9 to offset: [0], sizes: [%dim_0, %dim_1], strides: [%dim_1, 1] {kDiscSymbolicDimAttr = [@S1, @S2]} : memref<?x?xf32> to memref<?x?xf32>
  return %reinterpret_cast_10 : memref<?x?xf32>
}

// -----// IR Dump After TensorBufferize (tensor-bufferize) //----- //
func.func @main(%arg0: memref<?x?x?xf32>) -> memref<?x?xf32> attributes {tf.entry_function = {input_placements = "gpu", inputs = "input0", output_placements = "gpu", outputs = "output0"}} {
  %c2 = arith.constant 2 : index
  %c1 = arith.constant 1 : index
  %c0 = arith.constant 0 : index
  %alloc = memref.alloc() : memref<f32>
  "lmhlo.constant"(%alloc) {disc.device = "gpu", value = dense<0xFF800000> : tensor<f32>} : (memref<f32>) -> ()
  %dim = memref.dim %arg0, %c0 : memref<?x?x?xf32>
  %dim_0 = memref.dim %arg0, %c1 : memref<?x?x?xf32>
  %dim_1 = memref.dim %arg0, %c2 : memref<?x?x?xf32>
  %0 = arith.muli %dim_1, %dim_0 : index
  %reinterpret_cast = memref.reinterpret_cast %arg0 to offset: [0], sizes: [%dim, %dim_0, %dim_1], strides: [%0, %dim_1, 1] {kDiscSymbolicDimAttr = [@S0, @S1, @S2]} : memref<?x?x?xf32> to memref<?x?x?xf32>
  %alloc_2 = memref.alloc(%dim, %dim_0, %dim_1) : memref<?x?x?xf32>
  "lmhlo.abs"(%reinterpret_cast, %alloc_2) {disc.device = "gpu"} : (memref<?x?x?xf32>, memref<?x?x?xf32>) -> ()
  %reinterpret_cast_3 = memref.reinterpret_cast %alloc_2 to offset: [0], sizes: [%dim, %dim_0, %dim_1], strides: [%0, %dim_1, 1] {kDiscSymbolicDimAttr = [@S0, @S1, @S2]} : memref<?x?x?xf32> to memref<?x?x?xf32>
  %1 = arith.index_cast %dim : index to i32
  %2 = arith.index_cast %dim_0 : index to i32
  %3 = arith.index_cast %dim_1 : index to i32
  %4 = arith.muli %2, %3 : i32
  %alloc_4 = memref.alloc() {alignment = 64 : i64} : memref<2xi32>
  %c0_5 = arith.constant 0 : index
  %c1_6 = arith.constant 1 : index
  memref.store %1, %alloc_4[%c0_5] : memref<2xi32>
  memref.store %4, %alloc_4[%c1_6] : memref<2xi32>
  %5 = arith.index_cast %4 : i32 to index
  %6 = arith.index_cast %4 : i32 to index
  %alloc_7 = memref.alloc(%dim, %6) : memref<?x?xf32>
  "lmhlo.dynamic_reshape"(%reinterpret_cast_3, %alloc_4, %alloc_7) {disc.device = "gpu"} : (memref<?x?x?xf32>, memref<2xi32>, memref<?x?xf32>) -> ()
  %reinterpret_cast_8 = memref.reinterpret_cast %alloc_7 to offset: [0], sizes: [%dim, %5], strides: [%5, 1] {kDiscSymbolicDimAttr = [@S0, @S3]} : memref<?x?xf32> to memref<?x?xf32>
  %alloc_9 = memref.alloc(%5) : memref<?xf32>
  "lmhlo.reduce"(%reinterpret_cast_8, %alloc, %alloc_9) ({
  ^bb0(%arg1: memref<f32>, %arg2: memref<f32>, %arg3: memref<f32>):
    "lmhlo.maximum"(%arg1, %arg2, %arg3) {disc.device = "gpu"} : (memref<f32>, memref<f32>, memref<f32>) -> ()
    "lmhlo.terminator"() : () -> ()
  }) {dimensions = dense<0> : tensor<1xi64>, disc.device = "gpu"} : (memref<?x?xf32>, memref<f32>, memref<?xf32>) -> ()
  %reinterpret_cast_10 = memref.reinterpret_cast %alloc_9 to offset: [0], sizes: [%5], strides: [1] {kDiscSymbolicDimAttr = [@S3]} : memref<?xf32> to memref<?xf32>
  %alloc_11 = memref.alloc() {alignment = 64 : i64} : memref<2xi32>
  %c0_12 = arith.constant 0 : index
  %c1_13 = arith.constant 1 : index
  memref.store %2, %alloc_11[%c0_12] : memref<2xi32>
  memref.store %3, %alloc_11[%c1_13] : memref<2xi32>
  %alloc_14 = memref.alloc(%dim_0, %dim_1) : memref<?x?xf32>
  "lmhlo.dynamic_reshape"(%reinterpret_cast_10, %alloc_11, %alloc_14) {disc.device = "gpu"} : (memref<?xf32>, memref<2xi32>, memref<?x?xf32>) -> ()
  %reinterpret_cast_15 = memref.reinterpret_cast %alloc_14 to offset: [0], sizes: [%dim_0, %dim_1], strides: [%dim_1, 1] {kDiscSymbolicDimAttr = [@S1, @S2]} : memref<?x?xf32> to memref<?x?xf32>
  return %reinterpret_cast_15 : memref<?x?xf32>
}

// -----// IR Dump After Canonicalizer (canonicalize) //----- //
func.func @main(%arg0: memref<?x?x?xf32>) -> memref<?x?xf32> attributes {tf.entry_function = {input_placements = "gpu", inputs = "input0", output_placements = "gpu", outputs = "output0"}} {
  %c2 = arith.constant 2 : index
  %c1 = arith.constant 1 : index
  %c0 = arith.constant 0 : index
  %alloc = memref.alloc() : memref<f32>
  "lmhlo.constant"(%alloc) {disc.device = "gpu", value = dense<0xFF800000> : tensor<f32>} : (memref<f32>) -> ()
  %dim = memref.dim %arg0, %c0 : memref<?x?x?xf32>
  %dim_0 = memref.dim %arg0, %c1 : memref<?x?x?xf32>
  %dim_1 = memref.dim %arg0, %c2 : memref<?x?x?xf32>
  %0 = arith.muli %dim_1, %dim_0 : index
  %reinterpret_cast = memref.reinterpret_cast %arg0 to offset: [0], sizes: [%dim, %dim_0, %dim_1], strides: [%0, %dim_1, 1] {kDiscSymbolicDimAttr = [@S0, @S1, @S2]} : memref<?x?x?xf32> to memref<?x?x?xf32>
  %alloc_2 = memref.alloc(%dim, %dim_0, %dim_1) : memref<?x?x?xf32>
  "lmhlo.abs"(%reinterpret_cast, %alloc_2) {disc.device = "gpu"} : (memref<?x?x?xf32>, memref<?x?x?xf32>) -> ()
  %reinterpret_cast_3 = memref.reinterpret_cast %alloc_2 to offset: [0], sizes: [%dim, %dim_0, %dim_1], strides: [%0, %dim_1, 1] {kDiscSymbolicDimAttr = [@S0, @S1, @S2]} : memref<?x?x?xf32> to memref<?x?x?xf32>
  %1 = arith.index_cast %dim : index to i32
  %2 = arith.index_cast %dim_0 : index to i32
  %3 = arith.index_cast %dim_1 : index to i32
  %4 = arith.muli %2, %3 : i32
  %alloc_4 = memref.alloc() {alignment = 64 : i64} : memref<2xi32>
  memref.store %1, %alloc_4[%c0] : memref<2xi32>
  memref.store %4, %alloc_4[%c1] : memref<2xi32>
  %5 = arith.index_cast %4 : i32 to index
  %6 = arith.index_cast %4 : i32 to index
  %alloc_5 = memref.alloc(%dim, %6) : memref<?x?xf32>
  "lmhlo.dynamic_reshape"(%reinterpret_cast_3, %alloc_4, %alloc_5) {disc.device = "gpu"} : (memref<?x?x?xf32>, memref<2xi32>, memref<?x?xf32>) -> ()
  %reinterpret_cast_6 = memref.reinterpret_cast %alloc_5 to offset: [0], sizes: [%dim, %5], strides: [%5, 1] {kDiscSymbolicDimAttr = [@S0, @S3]} : memref<?x?xf32> to memref<?x?xf32>
  %alloc_7 = memref.alloc(%5) : memref<?xf32>
  "lmhlo.reduce"(%reinterpret_cast_6, %alloc, %alloc_7) ({
  ^bb0(%arg1: memref<f32>, %arg2: memref<f32>, %arg3: memref<f32>):
    "lmhlo.maximum"(%arg1, %arg2, %arg3) {disc.device = "gpu"} : (memref<f32>, memref<f32>, memref<f32>) -> ()
    "lmhlo.terminator"() : () -> ()
  }) {dimensions = dense<0> : tensor<1xi64>, disc.device = "gpu"} : (memref<?x?xf32>, memref<f32>, memref<?xf32>) -> ()
  %reinterpret_cast_8 = memref.reinterpret_cast %alloc_7 to offset: [0], sizes: [%5], strides: [1] {kDiscSymbolicDimAttr = [@S3]} : memref<?xf32> to memref<?xf32>
  %alloc_9 = memref.alloc() {alignment = 64 : i64} : memref<2xi32>
  memref.store %2, %alloc_9[%c0] : memref<2xi32>
  memref.store %3, %alloc_9[%c1] : memref<2xi32>
  %alloc_10 = memref.alloc(%dim_0, %dim_1) : memref<?x?xf32>
  "lmhlo.dynamic_reshape"(%reinterpret_cast_8, %alloc_9, %alloc_10) {disc.device = "gpu"} : (memref<?xf32>, memref<2xi32>, memref<?x?xf32>) -> ()
  %reinterpret_cast_11 = memref.reinterpret_cast %alloc_10 to offset: [0], sizes: [%dim_0, %dim_1], strides: [%dim_1, 1] {kDiscSymbolicDimAttr = [@S1, @S2]} : memref<?x?xf32> to memref<?x?xf32>
  return %reinterpret_cast_11 : memref<?x?xf32>
}

// -----// IR Dump After CSE (cse) //----- //
func.func @main(%arg0: memref<?x?x?xf32>) -> memref<?x?xf32> attributes {tf.entry_function = {input_placements = "gpu", inputs = "input0", output_placements = "gpu", outputs = "output0"}} {
  %c2 = arith.constant 2 : index
  %c1 = arith.constant 1 : index
  %c0 = arith.constant 0 : index
  %alloc = memref.alloc() : memref<f32>
  "lmhlo.constant"(%alloc) {disc.device = "gpu", value = dense<0xFF800000> : tensor<f32>} : (memref<f32>) -> ()
  %dim = memref.dim %arg0, %c0 : memref<?x?x?xf32>
  %dim_0 = memref.dim %arg0, %c1 : memref<?x?x?xf32>
  %dim_1 = memref.dim %arg0, %c2 : memref<?x?x?xf32>
  %0 = arith.muli %dim_1, %dim_0 : index
  %reinterpret_cast = memref.reinterpret_cast %arg0 to offset: [0], sizes: [%dim, %dim_0, %dim_1], strides: [%0, %dim_1, 1] {kDiscSymbolicDimAttr = [@S0, @S1, @S2]} : memref<?x?x?xf32> to memref<?x?x?xf32>
  %alloc_2 = memref.alloc(%dim, %dim_0, %dim_1) : memref<?x?x?xf32>
  "lmhlo.abs"(%reinterpret_cast, %alloc_2) {disc.device = "gpu"} : (memref<?x?x?xf32>, memref<?x?x?xf32>) -> ()
  %reinterpret_cast_3 = memref.reinterpret_cast %alloc_2 to offset: [0], sizes: [%dim, %dim_0, %dim_1], strides: [%0, %dim_1, 1] {kDiscSymbolicDimAttr = [@S0, @S1, @S2]} : memref<?x?x?xf32> to memref<?x?x?xf32>
  %1 = arith.index_cast %dim : index to i32
  %2 = arith.index_cast %dim_0 : index to i32
  %3 = arith.index_cast %dim_1 : index to i32
  %4 = arith.muli %2, %3 : i32
  %alloc_4 = memref.alloc() {alignment = 64 : i64} : memref<2xi32>
  memref.store %1, %alloc_4[%c0] : memref<2xi32>
  memref.store %4, %alloc_4[%c1] : memref<2xi32>
  %5 = arith.index_cast %4 : i32 to index
  %alloc_5 = memref.alloc(%dim, %5) : memref<?x?xf32>
  "lmhlo.dynamic_reshape"(%reinterpret_cast_3, %alloc_4, %alloc_5) {disc.device = "gpu"} : (memref<?x?x?xf32>, memref<2xi32>, memref<?x?xf32>) -> ()
  %reinterpret_cast_6 = memref.reinterpret_cast %alloc_5 to offset: [0], sizes: [%dim, %5], strides: [%5, 1] {kDiscSymbolicDimAttr = [@S0, @S3]} : memref<?x?xf32> to memref<?x?xf32>
  %alloc_7 = memref.alloc(%5) : memref<?xf32>
  "lmhlo.reduce"(%reinterpret_cast_6, %alloc, %alloc_7) ({
  ^bb0(%arg1: memref<f32>, %arg2: memref<f32>, %arg3: memref<f32>):
    "lmhlo.maximum"(%arg1, %arg2, %arg3) {disc.device = "gpu"} : (memref<f32>, memref<f32>, memref<f32>) -> ()
    "lmhlo.terminator"() : () -> ()
  }) {dimensions = dense<0> : tensor<1xi64>, disc.device = "gpu"} : (memref<?x?xf32>, memref<f32>, memref<?xf32>) -> ()
  %reinterpret_cast_8 = memref.reinterpret_cast %alloc_7 to offset: [0], sizes: [%5], strides: [1] {kDiscSymbolicDimAttr = [@S3]} : memref<?xf32> to memref<?xf32>
  %alloc_9 = memref.alloc() {alignment = 64 : i64} : memref<2xi32>
  memref.store %2, %alloc_9[%c0] : memref<2xi32>
  memref.store %3, %alloc_9[%c1] : memref<2xi32>
  %alloc_10 = memref.alloc(%dim_0, %dim_1) : memref<?x?xf32>
  "lmhlo.dynamic_reshape"(%reinterpret_cast_8, %alloc_9, %alloc_10) {disc.device = "gpu"} : (memref<?xf32>, memref<2xi32>, memref<?x?xf32>) -> ()
  %reinterpret_cast_11 = memref.reinterpret_cast %alloc_10 to offset: [0], sizes: [%dim_0, %dim_1], strides: [%dim_1, 1] {kDiscSymbolicDimAttr = [@S1, @S2]} : memref<?x?xf32> to memref<?x?xf32>
  return %reinterpret_cast_11 : memref<?x?xf32>
}

// -----// IR Dump After DiscMemrefCanonicalizer (disc-memref-canonicalize) //----- //
func.func @main(%arg0: memref<?x?x?xf32>) -> memref<?x?xf32> attributes {tf.entry_function = {input_placements = "gpu", inputs = "input0", output_placements = "gpu", outputs = "output0"}} {
  %c2 = arith.constant 2 : index
  %c1 = arith.constant 1 : index
  %c0 = arith.constant 0 : index
  %alloc = memref.alloc() : memref<f32>
  "lmhlo.constant"(%alloc) {disc.device = "gpu", value = dense<0xFF800000> : tensor<f32>} : (memref<f32>) -> ()
  %dim = memref.dim %arg0, %c0 : memref<?x?x?xf32>
  %dim_0 = memref.dim %arg0, %c1 : memref<?x?x?xf32>
  %dim_1 = memref.dim %arg0, %c2 : memref<?x?x?xf32>
  %0 = arith.muli %dim_1, %dim_0 : index
  %reinterpret_cast = memref.reinterpret_cast %arg0 to offset: [0], sizes: [%dim, %dim_0, %dim_1], strides: [%0, %dim_1, 1] {kDiscSymbolicDimAttr = [@S0, @S1, @S2]} : memref<?x?x?xf32> to memref<?x?x?xf32>
  %alloc_2 = memref.alloc(%dim, %dim_0, %dim_1) {kDiscSymbolicDimAttr = [@S0, @S1, @S2]} : memref<?x?x?xf32>
  "lmhlo.abs"(%reinterpret_cast, %alloc_2) {disc.device = "gpu"} : (memref<?x?x?xf32>, memref<?x?x?xf32>) -> ()
  %1 = arith.index_cast %dim : index to i32
  %2 = arith.index_cast %dim_0 : index to i32
  %3 = arith.index_cast %dim_1 : index to i32
  %4 = arith.muli %2, %3 : i32
  %alloc_3 = memref.alloc() {alignment = 64 : i64} : memref<2xi32>
  memref.store %1, %alloc_3[%c0] : memref<2xi32>
  memref.store %4, %alloc_3[%c1] : memref<2xi32>
  %5 = arith.index_cast %4 : i32 to index
  %alloc_4 = memref.alloc(%dim, %5) {kDiscSymbolicDimAttr = [@S0, @S3]} : memref<?x?xf32>
  "lmhlo.dynamic_reshape"(%alloc_2, %alloc_3, %alloc_4) {disc.device = "gpu"} : (memref<?x?x?xf32>, memref<2xi32>, memref<?x?xf32>) -> ()
  %alloc_5 = memref.alloc(%5) {kDiscSymbolicDimAttr = [@S3]} : memref<?xf32>
  "lmhlo.reduce"(%alloc_4, %alloc, %alloc_5) ({
  ^bb0(%arg1: memref<f32>, %arg2: memref<f32>, %arg3: memref<f32>):
    "lmhlo.maximum"(%arg1, %arg2, %arg3) {disc.device = "gpu"} : (memref<f32>, memref<f32>, memref<f32>) -> ()
    "lmhlo.terminator"() : () -> ()
  }) {dimensions = dense<0> : tensor<1xi64>, disc.device = "gpu"} : (memref<?x?xf32>, memref<f32>, memref<?xf32>) -> ()
  %alloc_6 = memref.alloc() {alignment = 64 : i64} : memref<2xi32>
  memref.store %2, %alloc_6[%c0] : memref<2xi32>
  memref.store %3, %alloc_6[%c1] : memref<2xi32>
  %alloc_7 = memref.alloc(%dim_0, %dim_1) {kDiscSymbolicDimAttr = [@S1, @S2]} : memref<?x?xf32>
  "lmhlo.dynamic_reshape"(%alloc_5, %alloc_6, %alloc_7) {disc.device = "gpu"} : (memref<?xf32>, memref<2xi32>, memref<?x?xf32>) -> ()
  return %alloc_7 : memref<?x?xf32>
}

// -----// IR Dump After DiscAssignMemorySpacePass (disc-assign-memory-space) //----- //
module {
  func.func @main(%arg0: memref<?x?x?xf32, "gpu">) -> memref<?x?xf32, "gpu"> attributes {tf.entry_function = {input_placements = "gpu", inputs = "input0", output_placements = "gpu", outputs = "output0"}} {
    %c2 = arith.constant 2 : index
    %c1 = arith.constant 1 : index
    %c0 = arith.constant 0 : index
    %alloc = memref.alloc() : memref<f32, "gpu">
    "lmhlo.constant"(%alloc) {disc.device = "gpu", value = dense<0xFF800000> : tensor<f32>} : (memref<f32, "gpu">) -> ()
    %dim = memref.dim %arg0, %c0 : memref<?x?x?xf32, "gpu">
    %dim_0 = memref.dim %arg0, %c1 : memref<?x?x?xf32, "gpu">
    %dim_1 = memref.dim %arg0, %c2 : memref<?x?x?xf32, "gpu">
    %0 = arith.muli %dim_1, %dim_0 : index
    %reinterpret_cast = memref.reinterpret_cast %arg0 to offset: [0], sizes: [%dim, %dim_0, %dim_1], strides: [%0, %dim_1, 1] {kDiscSymbolicDimAttr = [@S0, @S1, @S2]} : memref<?x?x?xf32, "gpu"> to memref<?x?x?xf32, "gpu">
    %alloc_2 = memref.alloc(%dim, %dim_0, %dim_1) {kDiscSymbolicDimAttr = [@S0, @S1, @S2]} : memref<?x?x?xf32, "gpu">
    "lmhlo.abs"(%reinterpret_cast, %alloc_2) {disc.device = "gpu"} : (memref<?x?x?xf32, "gpu">, memref<?x?x?xf32, "gpu">) -> ()
    %1 = arith.index_cast %dim : index to i32
    %2 = arith.index_cast %dim_0 : index to i32
    %3 = arith.index_cast %dim_1 : index to i32
    %4 = arith.muli %2, %3 : i32
    %alloc_3 = memref.alloc() {alignment = 64 : i64} : memref<2xi32, "cpu">
    memref.store %1, %alloc_3[%c0] : memref<2xi32, "cpu">
    memref.store %4, %alloc_3[%c1] : memref<2xi32, "cpu">
    %5 = arith.index_cast %4 : i32 to index
    %alloc_4 = memref.alloc(%dim, %5) {kDiscSymbolicDimAttr = [@S0, @S3]} : memref<?x?xf32, "gpu">
    "lmhlo.dynamic_reshape"(%alloc_2, %alloc_3, %alloc_4) {disc.device = "gpu"} : (memref<?x?x?xf32, "gpu">, memref<2xi32, "cpu">, memref<?x?xf32, "gpu">) -> ()
    %alloc_5 = memref.alloc(%5) {kDiscSymbolicDimAttr = [@S3]} : memref<?xf32, "gpu">
    "lmhlo.reduce"(%alloc_4, %alloc, %alloc_5) ({
    ^bb0(%arg1: memref<f32>, %arg2: memref<f32>, %arg3: memref<f32>):
      "lmhlo.maximum"(%arg1, %arg2, %arg3) {disc.device = "gpu"} : (memref<f32>, memref<f32>, memref<f32>) -> ()
      "lmhlo.terminator"() : () -> ()
    }) {dimensions = dense<0> : tensor<1xi64>, disc.device = "gpu"} : (memref<?x?xf32, "gpu">, memref<f32, "gpu">, memref<?xf32, "gpu">) -> ()
    %alloc_6 = memref.alloc() {alignment = 64 : i64} : memref<2xi32, "cpu">
    memref.store %2, %alloc_6[%c0] : memref<2xi32, "cpu">
    memref.store %3, %alloc_6[%c1] : memref<2xi32, "cpu">
    %alloc_7 = memref.alloc(%dim_0, %dim_1) {kDiscSymbolicDimAttr = [@S1, @S2]} : memref<?x?xf32, "gpu">
    "lmhlo.dynamic_reshape"(%alloc_5, %alloc_6, %alloc_7) {disc.device = "gpu"} : (memref<?xf32, "gpu">, memref<2xi32, "cpu">, memref<?x?xf32, "gpu">) -> ()
    return %alloc_7 : memref<?x?xf32, "gpu">
  }
  "disc_shape.SymbolicDim"() {knownNegativeOne = false, knownNonNegative = true, knownNonSizeOne = false, knownNonSizeZero = false, sym_name = "S0", value = -9223372036854775808 : i64} : () -> ()
  "disc_shape.SymbolicDim"() {knownNegativeOne = false, knownNonNegative = true, knownNonSizeOne = false, knownNonSizeZero = false, sym_name = "S1", value = -9223372036854775808 : i64} : () -> ()
  "disc_shape.SymbolicDim"() {knownNegativeOne = false, knownNonNegative = true, knownNonSizeOne = false, knownNonSizeZero = false, sym_name = "S2", value = -9223372036854775808 : i64} : () -> ()
  "disc_shape.SymbolicDim"() {knownNegativeOne = false, knownNonNegative = true, knownNonSizeOne = false, knownNonSizeZero = false, sym_name = "S3", value = -9223372036854775808 : i64} : () -> ()
  func.func @shape_constraint_graph() {
    %0 = "disc_shape.dim"() {name = @S3} : () -> index
    %1 = "disc_shape.dim"() {name = @S1} : () -> index
    %2 = "disc_shape.dim"() {name = @S2} : () -> index
    "disc_shape.tie_product_equal"(%0, %1, %2) {operand_segment_sizes = array<i32: 1, 2>} : (index, index, index) -> ()
    return
  }
}


// -----// IR Dump After PromoteBuffersToStack (promote-buffers-to-stack) //----- //
func.func @main(%arg0: memref<?x?x?xf32, "gpu">) -> memref<?x?xf32, "gpu"> attributes {tf.entry_function = {input_placements = "gpu", inputs = "input0", output_placements = "gpu", outputs = "output0"}} {
  %c2 = arith.constant 2 : index
  %c1 = arith.constant 1 : index
  %c0 = arith.constant 0 : index
  %alloc = memref.alloc() : memref<f32, "gpu">
  "lmhlo.constant"(%alloc) {disc.device = "gpu", value = dense<0xFF800000> : tensor<f32>} : (memref<f32, "gpu">) -> ()
  %dim = memref.dim %arg0, %c0 : memref<?x?x?xf32, "gpu">
  %dim_0 = memref.dim %arg0, %c1 : memref<?x?x?xf32, "gpu">
  %dim_1 = memref.dim %arg0, %c2 : memref<?x?x?xf32, "gpu">
  %0 = arith.muli %dim_1, %dim_0 : index
  %reinterpret_cast = memref.reinterpret_cast %arg0 to offset: [0], sizes: [%dim, %dim_0, %dim_1], strides: [%0, %dim_1, 1] {kDiscSymbolicDimAttr = [@S0, @S1, @S2]} : memref<?x?x?xf32, "gpu"> to memref<?x?x?xf32, "gpu">
  %alloc_2 = memref.alloc(%dim, %dim_0, %dim_1) {kDiscSymbolicDimAttr = [@S0, @S1, @S2]} : memref<?x?x?xf32, "gpu">
  "lmhlo.abs"(%reinterpret_cast, %alloc_2) {disc.device = "gpu"} : (memref<?x?x?xf32, "gpu">, memref<?x?x?xf32, "gpu">) -> ()
  %1 = arith.index_cast %dim : index to i32
  %2 = arith.index_cast %dim_0 : index to i32
  %3 = arith.index_cast %dim_1 : index to i32
  %4 = arith.muli %2, %3 : i32
  %alloca = memref.alloca() : memref<2xi32, "cpu">
  memref.store %1, %alloca[%c0] : memref<2xi32, "cpu">
  memref.store %4, %alloca[%c1] : memref<2xi32, "cpu">
  %5 = arith.index_cast %4 : i32 to index
  %alloc_3 = memref.alloc(%dim, %5) {kDiscSymbolicDimAttr = [@S0, @S3]} : memref<?x?xf32, "gpu">
  "lmhlo.dynamic_reshape"(%alloc_2, %alloca, %alloc_3) {disc.device = "gpu"} : (memref<?x?x?xf32, "gpu">, memref<2xi32, "cpu">, memref<?x?xf32, "gpu">) -> ()
  %alloc_4 = memref.alloc(%5) {kDiscSymbolicDimAttr = [@S3]} : memref<?xf32, "gpu">
  "lmhlo.reduce"(%alloc_3, %alloc, %alloc_4) ({
  ^bb0(%arg1: memref<f32>, %arg2: memref<f32>, %arg3: memref<f32>):
    "lmhlo.maximum"(%arg1, %arg2, %arg3) {disc.device = "gpu"} : (memref<f32>, memref<f32>, memref<f32>) -> ()
    "lmhlo.terminator"() : () -> ()
  }) {dimensions = dense<0> : tensor<1xi64>, disc.device = "gpu"} : (memref<?x?xf32, "gpu">, memref<f32, "gpu">, memref<?xf32, "gpu">) -> ()
  %alloca_5 = memref.alloca() : memref<2xi32, "cpu">
  memref.store %2, %alloca_5[%c0] : memref<2xi32, "cpu">
  memref.store %3, %alloca_5[%c1] : memref<2xi32, "cpu">
  %alloc_6 = memref.alloc(%dim_0, %dim_1) {kDiscSymbolicDimAttr = [@S1, @S2]} : memref<?x?xf32, "gpu">
  "lmhlo.dynamic_reshape"(%alloc_4, %alloca_5, %alloc_6) {disc.device = "gpu"} : (memref<?xf32, "gpu">, memref<2xi32, "cpu">, memref<?x?xf32, "gpu">) -> ()
  return %alloc_6 : memref<?x?xf32, "gpu">
}

SymbolicDimMgr::save walkRankedTensorValue takes: 1 us
SymbolicDimMgr::save update attributes takes: 5 us
SymbolicDimMgr::updateProductEqualityMap simplifySymbolicDimProductPair takes: 7 us
productSet.size() = 2
SymbolicDimMgr::updateProductEqualityMap propagate graph takes: 3 us
SymbolicDimMgr::updateProductEqualityMap remove multiply takes: 12 us
SymbolicDimMgr::updateProductEqualityMap build toRemove  takes: 0 us
SymbolicDimMgr::updateProductEqualityMap apply toRemove  takes: 0 us
SymbolicDimMgr::save updateProductEqualityMap takes: 51 us
SymbolicDimMgr::save updateFunctionType takes: 2 us
SymbolicDimMgr::save collect symbolicDim ops takes: 4 us
SymbolicDimMgr::save remove symbolicDim ops takes: 0 us
SymbolicDimMgr::save remove unused production takes: 0 us
SymbolicDimMgr::save remove unused production #2 takes: 0 us
SymbolicDimMgr::save canonicalize the name takes: 3 us
SymbolicDimMgr::save replace the name takes: 4 us
SymbolicDimMgr::save updateFunctionType takes: 0 us
// -----// IR Dump After DiscFusionPass (disc-fusion) //----- //
func.func @main(%arg0: memref<?x?x?xf32, "gpu">) -> memref<?x?xf32, "gpu"> attributes {tf.entry_function = {input_placements = "gpu", inputs = "input0", output_placements = "gpu", outputs = "output0"}} {
  %c2 = arith.constant 2 : index
  %dim = memref.dim %arg0, %c2 : memref<?x?x?xf32, "gpu">
  %c1 = arith.constant 1 : index
  %dim_0 = memref.dim %arg0, %c1 : memref<?x?x?xf32, "gpu">
  %c0 = arith.constant 0 : index
  %dim_1 = memref.dim %arg0, %c0 : memref<?x?x?xf32, "gpu">
  %alloc = memref.alloc() : memref<f32, "gpu">
  %0 = arith.muli %dim, %dim_0 : index
  %reinterpret_cast = memref.reinterpret_cast %arg0 to offset: [0], sizes: [%dim_1, %dim_0, %dim], strides: [%0, %dim, 1] {kDiscSymbolicDimAttr = [@S0, @S1, @S2]} : memref<?x?x?xf32, "gpu"> to memref<?x?x?xf32, "gpu">
  %alloc_2 = memref.alloc(%dim_1, %dim_0, %dim) {kDiscSymbolicDimAttr = [@S0, @S1, @S2]} : memref<?x?x?xf32, "gpu">
  %1 = arith.index_cast %dim_1 : index to i32
  %2 = arith.index_cast %dim_0 : index to i32
  %3 = arith.index_cast %dim : index to i32
  %4 = arith.muli %2, %3 : i32
  %alloca = memref.alloca() : memref<2xi32, "cpu">
  memref.store %1, %alloca[%c0] : memref<2xi32, "cpu">
  memref.store %4, %alloca[%c1] : memref<2xi32, "cpu">
  %5 = arith.index_cast %4 : i32 to index
  %alloc_3 = memref.alloc(%dim_1, %5) {kDiscSymbolicDimAttr = [@S0, @S3]} : memref<?x?xf32, "gpu">
  %alloc_4 = memref.alloc(%5) {kDiscSymbolicDimAttr = [@S3]} : memref<?xf32, "gpu">
  "lmhlo.fusion"() ({
    "lmhlo.constant"(%alloc) {disc.device = "gpu", value = dense<0xFF800000> : tensor<f32>} : (memref<f32, "gpu">) -> ()
    "lmhlo.abs"(%reinterpret_cast, %alloc_2) {disc.device = "gpu"} : (memref<?x?x?xf32, "gpu">, memref<?x?x?xf32, "gpu">) -> ()
    "lmhlo.dynamic_reshape"(%alloc_2, %alloca, %alloc_3) {disc.device = "gpu"} : (memref<?x?x?xf32, "gpu">, memref<2xi32, "cpu">, memref<?x?xf32, "gpu">) -> ()
    "lmhlo.reduce"(%alloc_3, %alloc, %alloc_4) ({
    ^bb0(%arg1: memref<f32>, %arg2: memref<f32>, %arg3: memref<f32>):
      "lmhlo.maximum"(%arg1, %arg2, %arg3) {disc.device = "gpu"} : (memref<f32>, memref<f32>, memref<f32>) -> ()
      "lmhlo.terminator"() : () -> ()
    }) {dimensions = dense<0> : tensor<1xi64>, disc.device = "gpu"} : (memref<?x?xf32, "gpu">, memref<f32, "gpu">, memref<?xf32, "gpu">) -> ()
    "lmhlo.terminator"() : () -> ()
  }) {disc.device = "gpu", disc.fusion.name = "main_kColReduction_reduce__4_1_0", disc.fusion_type = "kColReduction"} : () -> ()
  %alloca_5 = memref.alloca() : memref<2xi32, "cpu">
  memref.store %2, %alloca_5[%c0] : memref<2xi32, "cpu">
  memref.store %3, %alloca_5[%c1] : memref<2xi32, "cpu">
  %alloc_6 = memref.alloc(%dim_0, %dim) {kDiscSymbolicDimAttr = [@S1, @S2]} : memref<?x?xf32, "gpu">
  "lmhlo.dynamic_reshape"(%alloc_4, %alloca_5, %alloc_6) {disc.device = "gpu"} : (memref<?xf32, "gpu">, memref<2xi32, "cpu">, memref<?x?xf32, "gpu">) -> ()
  return %alloc_6 : memref<?x?xf32, "gpu">
}

SymbolicDimMgr::save walkRankedTensorValue takes: 1 us
SymbolicDimMgr::save update attributes takes: 5 us
SymbolicDimMgr::updateProductEqualityMap simplifySymbolicDimProductPair takes: 6 us
productSet.size() = 2
SymbolicDimMgr::updateProductEqualityMap propagate graph takes: 5 us
SymbolicDimMgr::updateProductEqualityMap remove multiply takes: 11 us
SymbolicDimMgr::updateProductEqualityMap build toRemove  takes: 0 us
SymbolicDimMgr::updateProductEqualityMap apply toRemove  takes: 0 us
SymbolicDimMgr::save updateProductEqualityMap takes: 39 us
SymbolicDimMgr::save updateFunctionType takes: 2 us
SymbolicDimMgr::save collect symbolicDim ops takes: 4 us
SymbolicDimMgr::save remove symbolicDim ops takes: 0 us
SymbolicDimMgr::save remove unused production takes: 0 us
SymbolicDimMgr::save remove unused production #2 takes: 0 us
SymbolicDimMgr::save canonicalize the name takes: 4 us
SymbolicDimMgr::save replace the name takes: 5 us
SymbolicDimMgr::save updateFunctionType takes: 1 us
// -----// IR Dump After DiscSpecializeFusionWithSpeculationPass (disc-specialize-fusion-with-speculation) //----- //
func.func @main(%arg0: memref<?x?x?xf32, "gpu">) -> memref<?x?xf32, "gpu"> attributes {tf.entry_function = {input_placements = "gpu", inputs = "input0", output_placements = "gpu", outputs = "output0"}} {
  %c2 = arith.constant 2 : index
  %dim = memref.dim %arg0, %c2 : memref<?x?x?xf32, "gpu">
  %c1 = arith.constant 1 : index
  %dim_0 = memref.dim %arg0, %c1 : memref<?x?x?xf32, "gpu">
  %c0 = arith.constant 0 : index
  %dim_1 = memref.dim %arg0, %c0 : memref<?x?x?xf32, "gpu">
  %alloc = memref.alloc() : memref<f32, "gpu">
  %0 = arith.muli %dim, %dim_0 : index
  %reinterpret_cast = memref.reinterpret_cast %arg0 to offset: [0], sizes: [%dim_1, %dim_0, %dim], strides: [%0, %dim, 1] {kDiscSymbolicDimAttr = [@S0, @S1, @S2]} : memref<?x?x?xf32, "gpu"> to memref<?x?x?xf32, "gpu">
  %alloc_2 = memref.alloc(%dim_1, %dim_0, %dim) {kDiscSymbolicDimAttr = [@S0, @S1, @S2]} : memref<?x?x?xf32, "gpu">
  %1 = arith.index_cast %dim_1 : index to i32
  %2 = arith.index_cast %dim_0 : index to i32
  %3 = arith.index_cast %dim : index to i32
  %4 = arith.muli %2, %3 : i32
  %alloca = memref.alloca() : memref<2xi32, "cpu">
  memref.store %1, %alloca[%c0] : memref<2xi32, "cpu">
  memref.store %4, %alloca[%c1] : memref<2xi32, "cpu">
  %5 = arith.index_cast %4 : i32 to index
  %alloc_3 = memref.alloc(%dim_1, %5) {kDiscSymbolicDimAttr = [@S0, @S3]} : memref<?x?xf32, "gpu">
  %alloc_4 = memref.alloc(%5) {kDiscSymbolicDimAttr = [@S3]} : memref<?xf32, "gpu">
  %c0_5 = arith.constant 0 : index
  %dim_6 = memref.dim %alloc_3, %c0_5 : memref<?x?xf32, "gpu">
  %c1_7 = arith.constant 1 : index
  %dim_8 = memref.dim %alloc_3, %c1_7 : memref<?x?xf32, "gpu">
  %6 = arith.muli %dim_6, %dim_8 : index
  %c256 = arith.constant 256 : index
  %7 = arith.ceildivsi %6, %c256 : index
  %c108 = arith.constant 108 : index
  %8 = arith.cmpi slt, %dim_6, %dim_8 : index
  scf.if %8 {
    "lmhlo.fusion"() ({
      "lmhlo.constant"(%alloc) {disc.device = "gpu", value = dense<0xFF800000> : tensor<f32>} : (memref<f32, "gpu">) -> ()
      "lmhlo.abs"(%reinterpret_cast, %alloc_2) {disc.device = "gpu"} : (memref<?x?x?xf32, "gpu">, memref<?x?x?xf32, "gpu">) -> ()
      "lmhlo.dynamic_reshape"(%alloc_2, %alloca, %alloc_3) {disc.device = "gpu"} : (memref<?x?x?xf32, "gpu">, memref<2xi32, "cpu">, memref<?x?xf32, "gpu">) -> ()
      "lmhlo.reduce"(%alloc_3, %alloc, %alloc_4) ({
      ^bb0(%arg1: memref<f32>, %arg2: memref<f32>, %arg3: memref<f32>):
        "lmhlo.maximum"(%arg1, %arg2, %arg3) {disc.device = "gpu"} : (memref<f32>, memref<f32>, memref<f32>) -> ()
        "lmhlo.terminator"() : () -> ()
      }) {dimensions = dense<0> : tensor<1xi64>, disc.device = "gpu"} : (memref<?x?xf32, "gpu">, memref<f32, "gpu">, memref<?xf32, "gpu">) -> ()
      "lmhlo.terminator"() : () -> ()
    }) {disc.device = "gpu", disc.fusion.name = "main_kColReduction_reduce__4_1_0", disc.fusion.tag = "flat", disc.fusion_type = "kColReduction", disc_col_reduction_schedule_hint = 7 : i32, disc_thread_per_block_hint = 512 : i32} : () -> ()
  } else {
    "lmhlo.fusion"() ({
      "lmhlo.constant"(%alloc) {disc.device = "gpu", value = dense<0xFF800000> : tensor<f32>} : (memref<f32, "gpu">) -> ()
      "lmhlo.abs"(%reinterpret_cast, %alloc_2) {disc.device = "gpu"} : (memref<?x?x?xf32, "gpu">, memref<?x?x?xf32, "gpu">) -> ()
      "lmhlo.dynamic_reshape"(%alloc_2, %alloca, %alloc_3) {disc.device = "gpu"} : (memref<?x?x?xf32, "gpu">, memref<2xi32, "cpu">, memref<?x?xf32, "gpu">) -> ()
      "lmhlo.reduce"(%alloc_3, %alloc, %alloc_4) ({
      ^bb0(%arg1: memref<f32>, %arg2: memref<f32>, %arg3: memref<f32>):
        "lmhlo.maximum"(%arg1, %arg2, %arg3) {disc.device = "gpu"} : (memref<f32>, memref<f32>, memref<f32>) -> ()
        "lmhlo.terminator"() : () -> ()
      }) {dimensions = dense<0> : tensor<1xi64>, disc.device = "gpu"} : (memref<?x?xf32, "gpu">, memref<f32, "gpu">, memref<?xf32, "gpu">) -> ()
      "lmhlo.terminator"() : () -> ()
    }) {disc.device = "gpu", disc.fusion.name = "main_kColReduction_reduce__4_1_0", disc.fusion.tag = "thin", disc.fusion_type = "kColReduction", disc_col_reduction_schedule_hint = 8 : i32, disc_thread_per_block_hint = 256 : i32} : () -> ()
  }
  %alloca_9 = memref.alloca() : memref<2xi32, "cpu">
  memref.store %2, %alloca_9[%c0] : memref<2xi32, "cpu">
  memref.store %3, %alloca_9[%c1] : memref<2xi32, "cpu">
  %alloc_10 = memref.alloc(%dim_0, %dim) {kDiscSymbolicDimAttr = [@S1, @S2]} : memref<?x?xf32, "gpu">
  "lmhlo.dynamic_reshape"(%alloc_4, %alloca_9, %alloc_10) {disc.device = "gpu"} : (memref<?xf32, "gpu">, memref<2xi32, "cpu">, memref<?x?xf32, "gpu">) -> ()
  return %alloc_10 : memref<?x?xf32, "gpu">
}

// -----// IR Dump After Canonicalizer (canonicalize) //----- //
func.func @main(%arg0: memref<?x?x?xf32, "gpu">) -> memref<?x?xf32, "gpu"> attributes {tf.entry_function = {input_placements = "gpu", inputs = "input0", output_placements = "gpu", outputs = "output0"}} {
  %c0 = arith.constant 0 : index
  %c1 = arith.constant 1 : index
  %c2 = arith.constant 2 : index
  %dim = memref.dim %arg0, %c2 : memref<?x?x?xf32, "gpu">
  %dim_0 = memref.dim %arg0, %c1 : memref<?x?x?xf32, "gpu">
  %dim_1 = memref.dim %arg0, %c0 : memref<?x?x?xf32, "gpu">
  %alloc = memref.alloc() : memref<f32, "gpu">
  %0 = arith.muli %dim, %dim_0 : index
  %reinterpret_cast = memref.reinterpret_cast %arg0 to offset: [0], sizes: [%dim_1, %dim_0, %dim], strides: [%0, %dim, 1] {kDiscSymbolicDimAttr = [@S0, @S1, @S2]} : memref<?x?x?xf32, "gpu"> to memref<?x?x?xf32, "gpu">
  %alloc_2 = memref.alloc(%dim_1, %dim_0, %dim) {kDiscSymbolicDimAttr = [@S0, @S1, @S2]} : memref<?x?x?xf32, "gpu">
  %1 = arith.index_cast %dim_1 : index to i32
  %2 = arith.index_cast %dim_0 : index to i32
  %3 = arith.index_cast %dim : index to i32
  %4 = arith.muli %2, %3 : i32
  %alloca = memref.alloca() : memref<2xi32, "cpu">
  memref.store %1, %alloca[%c0] : memref<2xi32, "cpu">
  memref.store %4, %alloca[%c1] : memref<2xi32, "cpu">
  %5 = arith.index_cast %4 : i32 to index
  %alloc_3 = memref.alloc(%dim_1, %5) {kDiscSymbolicDimAttr = [@S0, @S3]} : memref<?x?xf32, "gpu">
  %alloc_4 = memref.alloc(%5) {kDiscSymbolicDimAttr = [@S3]} : memref<?xf32, "gpu">
  %6 = arith.cmpi slt, %dim_1, %5 : index
  scf.if %6 {
    "lmhlo.fusion"() ({
      "lmhlo.constant"(%alloc) {disc.device = "gpu", value = dense<0xFF800000> : tensor<f32>} : (memref<f32, "gpu">) -> ()
      "lmhlo.abs"(%reinterpret_cast, %alloc_2) {disc.device = "gpu"} : (memref<?x?x?xf32, "gpu">, memref<?x?x?xf32, "gpu">) -> ()
      "lmhlo.dynamic_reshape"(%alloc_2, %alloca, %alloc_3) {disc.device = "gpu"} : (memref<?x?x?xf32, "gpu">, memref<2xi32, "cpu">, memref<?x?xf32, "gpu">) -> ()
      "lmhlo.reduce"(%alloc_3, %alloc, %alloc_4) ({
      ^bb0(%arg1: memref<f32>, %arg2: memref<f32>, %arg3: memref<f32>):
        "lmhlo.maximum"(%arg1, %arg2, %arg3) {disc.device = "gpu"} : (memref<f32>, memref<f32>, memref<f32>) -> ()
        "lmhlo.terminator"() : () -> ()
      }) {dimensions = dense<0> : tensor<1xi64>, disc.device = "gpu"} : (memref<?x?xf32, "gpu">, memref<f32, "gpu">, memref<?xf32, "gpu">) -> ()
      "lmhlo.terminator"() : () -> ()
    }) {disc.device = "gpu", disc.fusion.name = "main_kColReduction_reduce__4_1_0", disc.fusion.tag = "flat", disc.fusion_type = "kColReduction", disc_col_reduction_schedule_hint = 7 : i32, disc_thread_per_block_hint = 512 : i32} : () -> ()
  } else {
    "lmhlo.fusion"() ({
      "lmhlo.constant"(%alloc) {disc.device = "gpu", value = dense<0xFF800000> : tensor<f32>} : (memref<f32, "gpu">) -> ()
      "lmhlo.abs"(%reinterpret_cast, %alloc_2) {disc.device = "gpu"} : (memref<?x?x?xf32, "gpu">, memref<?x?x?xf32, "gpu">) -> ()
      "lmhlo.dynamic_reshape"(%alloc_2, %alloca, %alloc_3) {disc.device = "gpu"} : (memref<?x?x?xf32, "gpu">, memref<2xi32, "cpu">, memref<?x?xf32, "gpu">) -> ()
      "lmhlo.reduce"(%alloc_3, %alloc, %alloc_4) ({
      ^bb0(%arg1: memref<f32>, %arg2: memref<f32>, %arg3: memref<f32>):
        "lmhlo.maximum"(%arg1, %arg2, %arg3) {disc.device = "gpu"} : (memref<f32>, memref<f32>, memref<f32>) -> ()
        "lmhlo.terminator"() : () -> ()
      }) {dimensions = dense<0> : tensor<1xi64>, disc.device = "gpu"} : (memref<?x?xf32, "gpu">, memref<f32, "gpu">, memref<?xf32, "gpu">) -> ()
      "lmhlo.terminator"() : () -> ()
    }) {disc.device = "gpu", disc.fusion.name = "main_kColReduction_reduce__4_1_0", disc.fusion.tag = "thin", disc.fusion_type = "kColReduction", disc_col_reduction_schedule_hint = 8 : i32, disc_thread_per_block_hint = 256 : i32} : () -> ()
  }
  %alloca_5 = memref.alloca() : memref<2xi32, "cpu">
  memref.store %2, %alloca_5[%c0] : memref<2xi32, "cpu">
  memref.store %3, %alloca_5[%c1] : memref<2xi32, "cpu">
  %alloc_6 = memref.alloc(%dim_0, %dim) {kDiscSymbolicDimAttr = [@S1, @S2]} : memref<?x?xf32, "gpu">
  "lmhlo.dynamic_reshape"(%alloc_4, %alloca_5, %alloc_6) {disc.device = "gpu"} : (memref<?xf32, "gpu">, memref<2xi32, "cpu">, memref<?x?xf32, "gpu">) -> ()
  return %alloc_6 : memref<?x?xf32, "gpu">
}

// -----// IR Dump After BufferDeallocation (buffer-deallocation) //----- //
func.func @main(%arg0: memref<?x?x?xf32, "gpu">) -> memref<?x?xf32, "gpu"> attributes {tf.entry_function = {input_placements = "gpu", inputs = "input0", output_placements = "gpu", outputs = "output0"}} {
  %c0 = arith.constant 0 : index
  %c1 = arith.constant 1 : index
  %c2 = arith.constant 2 : index
  %dim = memref.dim %arg0, %c2 : memref<?x?x?xf32, "gpu">
  %dim_0 = memref.dim %arg0, %c1 : memref<?x?x?xf32, "gpu">
  %dim_1 = memref.dim %arg0, %c0 : memref<?x?x?xf32, "gpu">
  %alloc = memref.alloc() : memref<f32, "gpu">
  %0 = arith.muli %dim, %dim_0 : index
  %reinterpret_cast = memref.reinterpret_cast %arg0 to offset: [0], sizes: [%dim_1, %dim_0, %dim], strides: [%0, %dim, 1] {kDiscSymbolicDimAttr = [@S0, @S1, @S2]} : memref<?x?x?xf32, "gpu"> to memref<?x?x?xf32, "gpu">
  %alloc_2 = memref.alloc(%dim_1, %dim_0, %dim) {kDiscSymbolicDimAttr = [@S0, @S1, @S2]} : memref<?x?x?xf32, "gpu">
  %1 = arith.index_cast %dim_1 : index to i32
  %2 = arith.index_cast %dim_0 : index to i32
  %3 = arith.index_cast %dim : index to i32
  %4 = arith.muli %2, %3 : i32
  %alloca = memref.alloca() : memref<2xi32, "cpu">
  memref.store %1, %alloca[%c0] : memref<2xi32, "cpu">
  memref.store %4, %alloca[%c1] : memref<2xi32, "cpu">
  %5 = arith.index_cast %4 : i32 to index
  %alloc_3 = memref.alloc(%dim_1, %5) {kDiscSymbolicDimAttr = [@S0, @S3]} : memref<?x?xf32, "gpu">
  %alloc_4 = memref.alloc(%5) {kDiscSymbolicDimAttr = [@S3]} : memref<?xf32, "gpu">
  %6 = arith.cmpi slt, %dim_1, %5 : index
  scf.if %6 {
    "lmhlo.fusion"() ({
      "lmhlo.constant"(%alloc) {disc.device = "gpu", value = dense<0xFF800000> : tensor<f32>} : (memref<f32, "gpu">) -> ()
      "lmhlo.abs"(%reinterpret_cast, %alloc_2) {disc.device = "gpu"} : (memref<?x?x?xf32, "gpu">, memref<?x?x?xf32, "gpu">) -> ()
      "lmhlo.dynamic_reshape"(%alloc_2, %alloca, %alloc_3) {disc.device = "gpu"} : (memref<?x?x?xf32, "gpu">, memref<2xi32, "cpu">, memref<?x?xf32, "gpu">) -> ()
      "lmhlo.reduce"(%alloc_3, %alloc, %alloc_4) ({
      ^bb0(%arg1: memref<f32>, %arg2: memref<f32>, %arg3: memref<f32>):
        "lmhlo.maximum"(%arg1, %arg2, %arg3) {disc.device = "gpu"} : (memref<f32>, memref<f32>, memref<f32>) -> ()
        "lmhlo.terminator"() : () -> ()
      }) {dimensions = dense<0> : tensor<1xi64>, disc.device = "gpu"} : (memref<?x?xf32, "gpu">, memref<f32, "gpu">, memref<?xf32, "gpu">) -> ()
      "lmhlo.terminator"() : () -> ()
    }) {disc.device = "gpu", disc.fusion.name = "main_kColReduction_reduce__4_1_0", disc.fusion.tag = "flat", disc.fusion_type = "kColReduction", disc_col_reduction_schedule_hint = 7 : i32, disc_thread_per_block_hint = 512 : i32} : () -> ()
  } else {
    "lmhlo.fusion"() ({
      "lmhlo.constant"(%alloc) {disc.device = "gpu", value = dense<0xFF800000> : tensor<f32>} : (memref<f32, "gpu">) -> ()
      "lmhlo.abs"(%reinterpret_cast, %alloc_2) {disc.device = "gpu"} : (memref<?x?x?xf32, "gpu">, memref<?x?x?xf32, "gpu">) -> ()
      "lmhlo.dynamic_reshape"(%alloc_2, %alloca, %alloc_3) {disc.device = "gpu"} : (memref<?x?x?xf32, "gpu">, memref<2xi32, "cpu">, memref<?x?xf32, "gpu">) -> ()
      "lmhlo.reduce"(%alloc_3, %alloc, %alloc_4) ({
      ^bb0(%arg1: memref<f32>, %arg2: memref<f32>, %arg3: memref<f32>):
        "lmhlo.maximum"(%arg1, %arg2, %arg3) {disc.device = "gpu"} : (memref<f32>, memref<f32>, memref<f32>) -> ()
        "lmhlo.terminator"() : () -> ()
      }) {dimensions = dense<0> : tensor<1xi64>, disc.device = "gpu"} : (memref<?x?xf32, "gpu">, memref<f32, "gpu">, memref<?xf32, "gpu">) -> ()
      "lmhlo.terminator"() : () -> ()
    }) {disc.device = "gpu", disc.fusion.name = "main_kColReduction_reduce__4_1_0", disc.fusion.tag = "thin", disc.fusion_type = "kColReduction", disc_col_reduction_schedule_hint = 8 : i32, disc_thread_per_block_hint = 256 : i32} : () -> ()
  }
  memref.dealloc %alloc_3 : memref<?x?xf32, "gpu">
  memref.dealloc %alloc_2 : memref<?x?x?xf32, "gpu">
  memref.dealloc %alloc : memref<f32, "gpu">
  %alloca_5 = memref.alloca() : memref<2xi32, "cpu">
  memref.store %2, %alloca_5[%c0] : memref<2xi32, "cpu">
  memref.store %3, %alloca_5[%c1] : memref<2xi32, "cpu">
  %alloc_6 = memref.alloc(%dim_0, %dim) {kDiscSymbolicDimAttr = [@S1, @S2]} : memref<?x?xf32, "gpu">
  "lmhlo.dynamic_reshape"(%alloc_4, %alloca_5, %alloc_6) {disc.device = "gpu"} : (memref<?xf32, "gpu">, memref<2xi32, "cpu">, memref<?x?xf32, "gpu">) -> ()
  memref.dealloc %alloc_4 : memref<?xf32, "gpu">
  return %alloc_6 : memref<?x?xf32, "gpu">
}

// -----// IR Dump After RalInjectExecutionContextPass (disc-ral-inject-execution-context) //----- //
module {
  func.func @main(%arg0: !disc_ral.context) attributes {tf.entry_function = {input_placements = "gpu", inputs = "input0", output_placements = "gpu", outputs = "output0"}} {
    %c0 = arith.constant 0 : index
    %0 = "disc_ral.recv_input"(%arg0, %c0) : (!disc_ral.context, index) -> memref<?x?x?xf32, "gpu">
    %c0_0 = arith.constant 0 : index
    %c1 = arith.constant 1 : index
    %c2 = arith.constant 2 : index
    %dim = memref.dim %0, %c2 : memref<?x?x?xf32, "gpu">
    %dim_1 = memref.dim %0, %c1 : memref<?x?x?xf32, "gpu">
    %dim_2 = memref.dim %0, %c0_0 : memref<?x?x?xf32, "gpu">
    %alloc = memref.alloc() : memref<f32, "gpu">
    %1 = arith.muli %dim, %dim_1 : index
    %reinterpret_cast = memref.reinterpret_cast %0 to offset: [0], sizes: [%dim_2, %dim_1, %dim], strides: [%1, %dim, 1] {kDiscSymbolicDimAttr = [@S0, @S1, @S2]} : memref<?x?x?xf32, "gpu"> to memref<?x?x?xf32, "gpu">
    %alloc_3 = memref.alloc(%dim_2, %dim_1, %dim) {kDiscSymbolicDimAttr = [@S0, @S1, @S2]} : memref<?x?x?xf32, "gpu">
    %2 = arith.index_cast %dim_2 : index to i32
    %3 = arith.index_cast %dim_1 : index to i32
    %4 = arith.index_cast %dim : index to i32
    %5 = arith.muli %3, %4 : i32
    %alloca = memref.alloca() : memref<2xi32, "cpu">
    memref.store %2, %alloca[%c0_0] : memref<2xi32, "cpu">
    memref.store %5, %alloca[%c1] : memref<2xi32, "cpu">
    %6 = arith.index_cast %5 : i32 to index
    %alloc_4 = memref.alloc(%dim_2, %6) {kDiscSymbolicDimAttr = [@S0, @S3]} : memref<?x?xf32, "gpu">
    %alloc_5 = memref.alloc(%6) {kDiscSymbolicDimAttr = [@S3]} : memref<?xf32, "gpu">
    %7 = arith.cmpi slt, %dim_2, %6 : index
    scf.if %7 {
      "lmhlo.fusion"() ({
        "lmhlo.constant"(%alloc) {disc.device = "gpu", value = dense<0xFF800000> : tensor<f32>} : (memref<f32, "gpu">) -> ()
        "lmhlo.abs"(%reinterpret_cast, %alloc_3) {disc.device = "gpu"} : (memref<?x?x?xf32, "gpu">, memref<?x?x?xf32, "gpu">) -> ()
        "lmhlo.dynamic_reshape"(%alloc_3, %alloca, %alloc_4) {disc.device = "gpu"} : (memref<?x?x?xf32, "gpu">, memref<2xi32, "cpu">, memref<?x?xf32, "gpu">) -> ()
        "lmhlo.reduce"(%alloc_4, %alloc, %alloc_5) ({
        ^bb0(%arg1: memref<f32>, %arg2: memref<f32>, %arg3: memref<f32>):
          "lmhlo.maximum"(%arg1, %arg2, %arg3) {disc.device = "gpu"} : (memref<f32>, memref<f32>, memref<f32>) -> ()
          "lmhlo.terminator"() : () -> ()
        }) {dimensions = dense<0> : tensor<1xi64>, disc.device = "gpu"} : (memref<?x?xf32, "gpu">, memref<f32, "gpu">, memref<?xf32, "gpu">) -> ()
        "lmhlo.terminator"() : () -> ()
      }) {disc.device = "gpu", disc.fusion.name = "main_kColReduction_reduce__4_1_0", disc.fusion.tag = "flat", disc.fusion_type = "kColReduction", disc_col_reduction_schedule_hint = 7 : i32, disc_thread_per_block_hint = 512 : i32} : () -> ()
    } else {
      "lmhlo.fusion"() ({
        "lmhlo.constant"(%alloc) {disc.device = "gpu", value = dense<0xFF800000> : tensor<f32>} : (memref<f32, "gpu">) -> ()
        "lmhlo.abs"(%reinterpret_cast, %alloc_3) {disc.device = "gpu"} : (memref<?x?x?xf32, "gpu">, memref<?x?x?xf32, "gpu">) -> ()
        "lmhlo.dynamic_reshape"(%alloc_3, %alloca, %alloc_4) {disc.device = "gpu"} : (memref<?x?x?xf32, "gpu">, memref<2xi32, "cpu">, memref<?x?xf32, "gpu">) -> ()
        "lmhlo.reduce"(%alloc_4, %alloc, %alloc_5) ({
        ^bb0(%arg1: memref<f32>, %arg2: memref<f32>, %arg3: memref<f32>):
          "lmhlo.maximum"(%arg1, %arg2, %arg3) {disc.device = "gpu"} : (memref<f32>, memref<f32>, memref<f32>) -> ()
          "lmhlo.terminator"() : () -> ()
        }) {dimensions = dense<0> : tensor<1xi64>, disc.device = "gpu"} : (memref<?x?xf32, "gpu">, memref<f32, "gpu">, memref<?xf32, "gpu">) -> ()
        "lmhlo.terminator"() : () -> ()
      }) {disc.device = "gpu", disc.fusion.name = "main_kColReduction_reduce__4_1_0", disc.fusion.tag = "thin", disc.fusion_type = "kColReduction", disc_col_reduction_schedule_hint = 8 : i32, disc_thread_per_block_hint = 256 : i32} : () -> ()
    }
    memref.dealloc %alloc_4 : memref<?x?xf32, "gpu">
    memref.dealloc %alloc_3 : memref<?x?x?xf32, "gpu">
    memref.dealloc %alloc : memref<f32, "gpu">
    %alloca_6 = memref.alloca() : memref<2xi32, "cpu">
    memref.store %3, %alloca_6[%c0_0] : memref<2xi32, "cpu">
    memref.store %4, %alloca_6[%c1] : memref<2xi32, "cpu">
    %alloc_7 = memref.alloc(%dim_1, %dim) {kDiscSymbolicDimAttr = [@S1, @S2]} : memref<?x?xf32, "gpu">
    "lmhlo.dynamic_reshape"(%alloc_5, %alloca_6, %alloc_7) {disc.device = "gpu"} : (memref<?xf32, "gpu">, memref<2xi32, "cpu">, memref<?x?xf32, "gpu">) -> ()
    memref.dealloc %alloc_5 : memref<?xf32, "gpu">
    %c0_8 = arith.constant 0 : index
    "disc_ral.send_output"(%arg0, %c0_8, %alloc_7) : (!disc_ral.context, index, memref<?x?xf32, "gpu">) -> ()
    return
  }
  "disc_shape.SymbolicDim"() {knownNegativeOne = false, knownNonNegative = true, knownNonSizeOne = false, knownNonSizeZero = false, sym_name = "S0", value = -9223372036854775808 : i64} : () -> ()
  "disc_shape.SymbolicDim"() {knownNegativeOne = false, knownNonNegative = true, knownNonSizeOne = false, knownNonSizeZero = false, sym_name = "S1", value = -9223372036854775808 : i64} : () -> ()
  "disc_shape.SymbolicDim"() {knownNegativeOne = false, knownNonNegative = true, knownNonSizeOne = false, knownNonSizeZero = false, sym_name = "S2", value = -9223372036854775808 : i64} : () -> ()
  "disc_shape.SymbolicDim"() {knownNegativeOne = false, knownNonNegative = true, knownNonSizeOne = false, knownNonSizeZero = false, sym_name = "S3", value = -9223372036854775808 : i64} : () -> ()
  func.func @shape_constraint_graph() {
    %0 = "disc_shape.dim"() {name = @S3} : () -> index
    %1 = "disc_shape.dim"() {name = @S1} : () -> index
    %2 = "disc_shape.dim"() {name = @S2} : () -> index
    "disc_shape.tie_product_equal"(%0, %1, %2) {operand_segment_sizes = array<i32: 1, 2>} : (index, index, index) -> ()
    return
  }
}


// -----// IR Dump After DiscLowerToLibraryCallPass (disc-lower-to-library-call) //----- //
func.func @main(%arg0: !disc_ral.context) attributes {tf.entry_function = {input_placements = "gpu", inputs = "input0", output_placements = "gpu", outputs = "output0"}} {
  %0 = llvm.mlir.constant(0 : i32) : i32
  %c0 = arith.constant 0 : index
  %c1 = arith.constant 1 : index
  %c2 = arith.constant 2 : index
  %1 = "disc_ral.dispatch"(%arg0, %c0) {backend_config = "", call_target_name = "ral_recv_input", device = "cpu", has_side_effect = false} : (!disc_ral.context, index) -> memref<?x?x?xf32, "gpu">
  %dim = memref.dim %1, %c2 : memref<?x?x?xf32, "gpu">
  %dim_0 = memref.dim %1, %c1 : memref<?x?x?xf32, "gpu">
  %dim_1 = memref.dim %1, %c0 : memref<?x?x?xf32, "gpu">
  %alloc = memref.alloc() : memref<f32, "gpu">
  %2 = arith.muli %dim, %dim_0 : index
  %reinterpret_cast = memref.reinterpret_cast %1 to offset: [0], sizes: [%dim_1, %dim_0, %dim], strides: [%2, %dim, 1] {kDiscSymbolicDimAttr = [@S0, @S1, @S2]} : memref<?x?x?xf32, "gpu"> to memref<?x?x?xf32, "gpu">
  %alloc_2 = memref.alloc(%dim_1, %dim_0, %dim) {kDiscSymbolicDimAttr = [@S0, @S1, @S2]} : memref<?x?x?xf32, "gpu">
  %3 = arith.index_cast %dim_1 : index to i32
  %4 = arith.index_cast %dim_0 : index to i32
  %5 = arith.index_cast %dim : index to i32
  %6 = arith.muli %4, %5 : i32
  %alloca = memref.alloca() : memref<2xi32, "cpu">
  memref.store %3, %alloca[%c0] : memref<2xi32, "cpu">
  memref.store %6, %alloca[%c1] : memref<2xi32, "cpu">
  %7 = arith.index_cast %6 : i32 to index
  %alloc_3 = memref.alloc(%dim_1, %7) {kDiscSymbolicDimAttr = [@S0, @S3]} : memref<?x?xf32, "gpu">
  %alloc_4 = memref.alloc(%7) {kDiscSymbolicDimAttr = [@S3]} : memref<?xf32, "gpu">
  %8 = arith.cmpi slt, %dim_1, %7 : index
  scf.if %8 {
    "lmhlo.fusion"() ({
      "lmhlo.constant"(%alloc) {disc.device = "gpu", value = dense<0xFF800000> : tensor<f32>} : (memref<f32, "gpu">) -> ()
      "lmhlo.abs"(%reinterpret_cast, %alloc_2) {disc.device = "gpu"} : (memref<?x?x?xf32, "gpu">, memref<?x?x?xf32, "gpu">) -> ()
      "lmhlo.dynamic_reshape"(%alloc_2, %alloca, %alloc_3) {disc.device = "gpu"} : (memref<?x?x?xf32, "gpu">, memref<2xi32, "cpu">, memref<?x?xf32, "gpu">) -> ()
      "lmhlo.reduce"(%alloc_3, %alloc, %alloc_4) ({
      ^bb0(%arg1: memref<f32>, %arg2: memref<f32>, %arg3: memref<f32>):
        "lmhlo.maximum"(%arg1, %arg2, %arg3) {disc.device = "gpu"} : (memref<f32>, memref<f32>, memref<f32>) -> ()
        "lmhlo.terminator"() : () -> ()
      }) {dimensions = dense<0> : tensor<1xi64>, disc.device = "gpu"} : (memref<?x?xf32, "gpu">, memref<f32, "gpu">, memref<?xf32, "gpu">) -> ()
      "lmhlo.terminator"() : () -> ()
    }) {disc.device = "gpu", disc.fusion.name = "main_kColReduction_reduce__4_1_0", disc.fusion.tag = "flat", disc.fusion_type = "kColReduction", disc_col_reduction_schedule_hint = 7 : i32, disc_thread_per_block_hint = 512 : i32} : () -> ()
  } else {
    "lmhlo.fusion"() ({
      "lmhlo.constant"(%alloc) {disc.device = "gpu", value = dense<0xFF800000> : tensor<f32>} : (memref<f32, "gpu">) -> ()
      "lmhlo.abs"(%reinterpret_cast, %alloc_2) {disc.device = "gpu"} : (memref<?x?x?xf32, "gpu">, memref<?x?x?xf32, "gpu">) -> ()
      "lmhlo.dynamic_reshape"(%alloc_2, %alloca, %alloc_3) {disc.device = "gpu"} : (memref<?x?x?xf32, "gpu">, memref<2xi32, "cpu">, memref<?x?xf32, "gpu">) -> ()
      "lmhlo.reduce"(%alloc_3, %alloc, %alloc_4) ({
      ^bb0(%arg1: memref<f32>, %arg2: memref<f32>, %arg3: memref<f32>):
        "lmhlo.maximum"(%arg1, %arg2, %arg3) {disc.device = "gpu"} : (memref<f32>, memref<f32>, memref<f32>) -> ()
        "lmhlo.terminator"() : () -> ()
      }) {dimensions = dense<0> : tensor<1xi64>, disc.device = "gpu"} : (memref<?x?xf32, "gpu">, memref<f32, "gpu">, memref<?xf32, "gpu">) -> ()
      "lmhlo.terminator"() : () -> ()
    }) {disc.device = "gpu", disc.fusion.name = "main_kColReduction_reduce__4_1_0", disc.fusion.tag = "thin", disc.fusion_type = "kColReduction", disc_col_reduction_schedule_hint = 8 : i32, disc_thread_per_block_hint = 256 : i32} : () -> ()
  }
  memref.dealloc %alloc_3 : memref<?x?xf32, "gpu">
  memref.dealloc %alloc_2 : memref<?x?x?xf32, "gpu">
  memref.dealloc %alloc : memref<f32, "gpu">
  %alloca_5 = memref.alloca() : memref<2xi32, "cpu">
  memref.store %4, %alloca_5[%c0] : memref<2xi32, "cpu">
  memref.store %5, %alloca_5[%c1] : memref<2xi32, "cpu">
  %9 = llvm.inttoptr %0 : i32 to !llvm.ptr<i8>
  %alloca_6 = memref.alloca() : memref<2xindex, "cpu">
  memref.store %dim_0, %alloca_6[%c0] : memref<2xindex, "cpu">
  memref.store %dim, %alloca_6[%c1] : memref<2xindex, "cpu">
  %10 = "disc_ral.dispatch"(%arg0, %9, %alloc_4, %alloca_6) {backend_config = "", call_target_name = "inc_ref", device = "gpu", has_side_effect = false} : (!disc_ral.context, !llvm.ptr<i8>, memref<?xf32, "gpu">, memref<2xindex, "cpu">) -> memref<?x?xf32, "gpu">
  %reinterpret_cast_7 = memref.reinterpret_cast %10 to offset: [0], sizes: [%dim_0, %dim], strides: [%dim, 1] {kDiscSymbolicDimAttr = [@S1, @S2]} : memref<?x?xf32, "gpu"> to memref<?x?xf32, "gpu">
  memref.dealloc %alloc_4 : memref<?xf32, "gpu">
  "disc_ral.dispatch"(%arg0, %c0, %reinterpret_cast_7) {backend_config = "", call_target_name = "ral_send_output", device = "cpu", has_side_effect = false} : (!disc_ral.context, index, memref<?x?xf32, "gpu">) -> ()
  return
}

kColReduction <main_kColReduction_reduce__4_1_0___flat>, use_new: 0 schedule_hint: 7
kColReduction <main_kColReduction_reduce__4_1_0___thin>, use_new: 0 schedule_hint: 8
SymbolicDimMgr::save walkRankedTensorValue takes: 4 us
SymbolicDimMgr::save update attributes takes: 9 us
SymbolicDimMgr::updateProductEqualityMap simplifySymbolicDimProductPair takes: 8 us
productSet.size() = 2
SymbolicDimMgr::updateProductEqualityMap propagate graph takes: 5 us
SymbolicDimMgr::updateProductEqualityMap remove multiply takes: 13 us
SymbolicDimMgr::updateProductEqualityMap build toRemove  takes: 0 us
SymbolicDimMgr::updateProductEqualityMap apply toRemove  takes: 0 us
SymbolicDimMgr::save updateProductEqualityMap takes: 44 us
SymbolicDimMgr::save updateFunctionType takes: 4 us
SymbolicDimMgr::save collect symbolicDim ops takes: 9 us
SymbolicDimMgr::save remove symbolicDim ops takes: 0 us
SymbolicDimMgr::save remove unused production takes: 0 us
SymbolicDimMgr::save remove unused production #2 takes: 0 us
SymbolicDimMgr::save canonicalize the name takes: 3 us
SymbolicDimMgr::save replace the name takes: 9 us
SymbolicDimMgr::save updateFunctionType takes: 2 us
// -----// IR Dump After DiscLhloLegalizeRootsToParallelLoopsPass (disc-lhlo-legalize-roots-to-parallel-loops) //----- //
func.func @main(%arg0: !disc_ral.context) attributes {tf.entry_function = {input_placements = "gpu", inputs = "input0", output_placements = "gpu", outputs = "output0"}} {
  %c4 = arith.constant 4 : index
  %c256 = arith.constant 256 : index
  %c64 = arith.constant 64 : index
  %c8 = arith.constant 8 : index
  %c32 = arith.constant 32 : index
  %c512 = arith.constant 512 : index
  %0 = llvm.mlir.constant(0 : i32) : i32
  %c0 = arith.constant 0 : index
  %c1 = arith.constant 1 : index
  %c2 = arith.constant 2 : index
  %1 = "disc_ral.dispatch"(%arg0, %c0) {backend_config = "", call_target_name = "ral_recv_input", device = "cpu", has_side_effect = false} : (!disc_ral.context, index) -> memref<?x?x?xf32, "gpu">
  %dim = memref.dim %1, %c2 : memref<?x?x?xf32, "gpu">
  %dim_0 = memref.dim %1, %c1 : memref<?x?x?xf32, "gpu">
  %dim_1 = memref.dim %1, %c0 : memref<?x?x?xf32, "gpu">
  %alloc = memref.alloc() : memref<f32, "gpu">
  %2 = arith.muli %dim, %dim_0 : index
  %reinterpret_cast = memref.reinterpret_cast %1 to offset: [0], sizes: [%dim_1, %dim_0, %dim], strides: [%2, %dim, 1] {kDiscSymbolicDimAttr = [@S0, @S1, @S2]} : memref<?x?x?xf32, "gpu"> to memref<?x?x?xf32, "gpu">
  %alloc_2 = memref.alloc(%dim_1, %dim_0, %dim) {kDiscSymbolicDimAttr = [@S0, @S1, @S2]} : memref<?x?x?xf32, "gpu">
  %3 = arith.index_cast %dim_1 : index to i32
  %4 = arith.index_cast %dim_0 : index to i32
  %5 = arith.index_cast %dim : index to i32
  %6 = arith.muli %4, %5 : i32
  %alloca = memref.alloca() : memref<2xi32, "cpu">
  memref.store %3, %alloca[%c0] : memref<2xi32, "cpu">
  memref.store %6, %alloca[%c1] : memref<2xi32, "cpu">
  %7 = arith.index_cast %6 : i32 to index
  %alloc_3 = memref.alloc(%dim_1, %7) {kDiscSymbolicDimAttr = [@S0, @S3]} : memref<?x?xf32, "gpu">
  %alloc_4 = memref.alloc(%7) {kDiscSymbolicDimAttr = [@S3]} : memref<?xf32, "gpu">
  %8 = arith.cmpi slt, %dim_1, %7 : index
  scf.if %8 {
    "lmhlo.fusion"() ({
      "lmhlo.constant"(%alloc) {disc.device = "gpu", value = dense<0xFF800000> : tensor<f32>} : (memref<f32, "gpu">) -> ()
      "lmhlo.abs"(%reinterpret_cast, %alloc_2) {disc.device = "gpu"} : (memref<?x?x?xf32, "gpu">, memref<?x?x?xf32, "gpu">) -> ()
      "lmhlo.dynamic_reshape"(%alloc_2, %alloca, %alloc_3) {disc.device = "gpu"} : (memref<?x?x?xf32, "gpu">, memref<2xi32, "cpu">, memref<?x?xf32, "gpu">) -> ()
      scf.parallel (%arg1) = (%c0) to (%7) step (%c1) {
        %14 = "disc_shape.delinearize"(%arg1, %7) : (index, index) -> index
        %15 = memref.load %alloc[] : memref<f32, "gpu">
        memref.store %15, %alloc_4[%14] : memref<?xf32, "gpu">
        scf.yield
      }
      %11 = arith.ceildivui %7, %c512 : index
      %12 = arith.ceildivui %dim_1, %c32 : index
      %13 = arith.muli %11, %12 : index
      scf.parallel (%arg1, %arg2) = (%c0, %c0) to (%13, %c512) step (%c1, %c1) {
        %14 = memref.load %alloc[] : memref<f32, "gpu">
        %15 = arith.divui %arg1, %11 : index
        %16 = arith.remui %arg1, %11 : index
        %17 = arith.muli %16, %c512 : index
        %18 = arith.addi %17, %arg2 : index
        %19 = arith.cmpi ult, %18, %7 : index
        %20 = scf.if %19 -> (f32) {
          %21 = scf.for %arg3 = %c0 to %c32 step %c1 iter_args(%arg4 = %14) -> (f32) {
            %22 = arith.muli %15, %c32 : index
            %23 = arith.addi %22, %arg3 : index
            %24 = arith.cmpi slt, %23, %dim_1 : index
            %25 = scf.if %24 -> (f32) {
              %26 = memref.load %alloc_3[%23, %18] : memref<?x?xf32, "gpu">
              %27 = arith.cmpf oge, %arg4, %26 : f32
              %28 = arith.select %27, %arg4, %26 : f32
              scf.yield %28 : f32
            } else {
              scf.yield %arg4 : f32
            }
            scf.yield %25 : f32
          }
          scf.yield %21 : f32
        } else {
          scf.yield %14 : f32
        }
        scf.if %19 {
          %21 = memref.atomic_rmw maxf %20, %alloc_4[%18] : (f32, memref<?xf32, "gpu">) -> f32
        }
        scf.yield
      }
      "lmhlo.terminator"() : () -> ()
    }) {disc.device = "gpu", disc.fusion.name = "main_kColReduction_reduce__4_1_0", disc.fusion.tag = "flat", disc.fusion_type = "kColReduction", disc_col_reduction_schedule_hint = 7 : i32, disc_thread_per_block_hint = 512 : i32} : () -> ()
  } else {
    "lmhlo.fusion"() ({
      "lmhlo.constant"(%alloc) {disc.device = "gpu", value = dense<0xFF800000> : tensor<f32>} : (memref<f32, "gpu">) -> ()
      "lmhlo.abs"(%reinterpret_cast, %alloc_2) {disc.device = "gpu"} : (memref<?x?x?xf32, "gpu">, memref<?x?x?xf32, "gpu">) -> ()
      "lmhlo.dynamic_reshape"(%alloc_2, %alloca, %alloc_3) {disc.device = "gpu"} : (memref<?x?x?xf32, "gpu">, memref<2xi32, "cpu">, memref<?x?xf32, "gpu">) -> ()
      scf.parallel (%arg1) = (%c0) to (%7) step (%c1) {
        %14 = "disc_shape.delinearize"(%arg1, %7) : (index, index) -> index
        %15 = memref.load %alloc[] : memref<f32, "gpu">
        memref.store %15, %alloc_4[%14] : memref<?xf32, "gpu">
        scf.yield
      }
      %11 = arith.ceildivui %7, %c32 : index
      %12 = arith.ceildivui %dim_1, %c512 : index
      %13 = arith.muli %11, %12 : index
      scf.parallel (%arg1, %arg2) = (%c0, %c0) to (%13, %c256) step (%c1, %c1) {
        %14 = memref.load %alloc[] : memref<f32, "gpu">
        %15 = arith.divui %arg1, %11 : index
        %16 = arith.remui %arg1, %11 : index
        %17 = arith.divui %arg2, %c32 : index
        %18 = arith.remui %arg2, %c32 : index
        %19 = arith.muli %18, %c8 : index
        %20 = arith.addi %17, %19 : index
        %21 = arith.muli %16, %c32 : index
        %22 = arith.addi %18, %21 : index
        %alloc_8 = memref.alloc() : memref<256xf32, #gpu.address_space<workgroup>>
        %23 = arith.cmpi ult, %22, %7 : index
        %24 = scf.if %23 -> (f32) {
          %29 = scf.for %arg3 = %c0 to %c64 step %c1 iter_args(%arg4 = %14) -> (f32) {
            %30 = arith.muli %15, %c8 : index
            %31 = arith.addi %17, %30 : index
            %32 = arith.muli %31, %c64 : index
            %33 = arith.addi %arg3, %32 : index
            %34 = arith.cmpi slt, %33, %dim_1 : index
            %35 = scf.if %34 -> (f32) {
              %36 = memref.load %alloc_3[%33, %22] : memref<?x?xf32, "gpu">
              %37 = arith.maxf %arg4, %36 : f32
              scf.yield %37 : f32
            } else {
              scf.yield %arg4 : f32
            }
            scf.yield %35 : f32
          }
          scf.yield %29 : f32
        } else {
          scf.yield %14 : f32
        }
        memref.store %24, %alloc_8[%20] : memref<256xf32, #gpu.address_space<workgroup>>
        gpu.barrier
        %25 = arith.cmpi slt, %17, %c4 : index
        scf.if %25 {
          %29 = arith.addi %20, %c4 : index
          %30 = memref.load %alloc_8[%20] : memref<256xf32, #gpu.address_space<workgroup>>
          %31 = memref.load %alloc_8[%29] : memref<256xf32, #gpu.address_space<workgroup>>
          %32 = arith.maxf %30, %31 : f32
          memref.store %32, %alloc_8[%20] : memref<256xf32, #gpu.address_space<workgroup>>
        }
        gpu.barrier
        %26 = arith.cmpi slt, %17, %c2 : index
        scf.if %26 {
          %29 = arith.addi %20, %c2 : index
          %30 = memref.load %alloc_8[%20] : memref<256xf32, #gpu.address_space<workgroup>>
          %31 = memref.load %alloc_8[%29] : memref<256xf32, #gpu.address_space<workgroup>>
          %32 = arith.maxf %30, %31 : f32
          memref.store %32, %alloc_8[%20] : memref<256xf32, #gpu.address_space<workgroup>>
        }
        gpu.barrier
        %27 = arith.cmpi eq, %17, %c0 : index
        %28 = arith.andi %27, %23 : i1
        scf.if %28 {
          %29 = arith.addi %20, %c1 : index
          %30 = memref.load %alloc_8[%20] : memref<256xf32, #gpu.address_space<workgroup>>
          %31 = memref.load %alloc_8[%29] : memref<256xf32, #gpu.address_space<workgroup>>
          %32 = arith.maxf %30, %31 : f32
          %33 = memref.atomic_rmw maxf %32, %alloc_4[%22] : (f32, memref<?xf32, "gpu">) -> f32
        }
        scf.yield
      }
      "lmhlo.terminator"() : () -> ()
    }) {disc.device = "gpu", disc.fusion.name = "main_kColReduction_reduce__4_1_0", disc.fusion.tag = "thin", disc.fusion_type = "kColReduction", disc_col_reduction_schedule_hint = 8 : i32, disc_thread_per_block_hint = 256 : i32} : () -> ()
  }
  memref.dealloc %alloc_3 : memref<?x?xf32, "gpu">
  memref.dealloc %alloc_2 : memref<?x?x?xf32, "gpu">
  memref.dealloc %alloc : memref<f32, "gpu">
  %alloca_5 = memref.alloca() : memref<2xi32, "cpu">
  memref.store %4, %alloca_5[%c0] : memref<2xi32, "cpu">
  memref.store %5, %alloca_5[%c1] : memref<2xi32, "cpu">
  %9 = llvm.inttoptr %0 : i32 to !llvm.ptr<i8>
  %alloca_6 = memref.alloca() : memref<2xindex, "cpu">
  memref.store %dim_0, %alloca_6[%c0] : memref<2xindex, "cpu">
  memref.store %dim, %alloca_6[%c1] : memref<2xindex, "cpu">
  %10 = "disc_ral.dispatch"(%arg0, %9, %alloc_4, %alloca_6) {backend_config = "", call_target_name = "inc_ref", device = "gpu", has_side_effect = false} : (!disc_ral.context, !llvm.ptr<i8>, memref<?xf32, "gpu">, memref<2xindex, "cpu">) -> memref<?x?xf32, "gpu">
  %reinterpret_cast_7 = memref.reinterpret_cast %10 to offset: [0], sizes: [%dim_0, %dim], strides: [%dim, 1] {kDiscSymbolicDimAttr = [@S1, @S2]} : memref<?x?xf32, "gpu"> to memref<?x?xf32, "gpu">
  memref.dealloc %alloc_4 : memref<?xf32, "gpu">
  "disc_ral.dispatch"(%arg0, %c0, %reinterpret_cast_7) {backend_config = "", call_target_name = "ral_send_output", device = "cpu", has_side_effect = false} : (!disc_ral.context, index, memref<?x?xf32, "gpu">) -> ()
  return
}

// -----// IR Dump After ExpandOps (memref-expand) //----- //
func.func @main(%arg0: !disc_ral.context) attributes {tf.entry_function = {input_placements = "gpu", inputs = "input0", output_placements = "gpu", outputs = "output0"}} {
  %c4 = arith.constant 4 : index
  %c256 = arith.constant 256 : index
  %c64 = arith.constant 64 : index
  %c8 = arith.constant 8 : index
  %c32 = arith.constant 32 : index
  %c512 = arith.constant 512 : index
  %0 = llvm.mlir.constant(0 : i32) : i32
  %c0 = arith.constant 0 : index
  %c1 = arith.constant 1 : index
  %c2 = arith.constant 2 : index
  %1 = "disc_ral.dispatch"(%arg0, %c0) {backend_config = "", call_target_name = "ral_recv_input", device = "cpu", has_side_effect = false} : (!disc_ral.context, index) -> memref<?x?x?xf32, "gpu">
  %dim = memref.dim %1, %c2 : memref<?x?x?xf32, "gpu">
  %dim_0 = memref.dim %1, %c1 : memref<?x?x?xf32, "gpu">
  %dim_1 = memref.dim %1, %c0 : memref<?x?x?xf32, "gpu">
  %alloc = memref.alloc() : memref<f32, "gpu">
  %2 = arith.muli %dim, %dim_0 : index
  %reinterpret_cast = memref.reinterpret_cast %1 to offset: [0], sizes: [%dim_1, %dim_0, %dim], strides: [%2, %dim, 1] {kDiscSymbolicDimAttr = [@S0, @S1, @S2]} : memref<?x?x?xf32, "gpu"> to memref<?x?x?xf32, "gpu">
  %alloc_2 = memref.alloc(%dim_1, %dim_0, %dim) {kDiscSymbolicDimAttr = [@S0, @S1, @S2]} : memref<?x?x?xf32, "gpu">
  %3 = arith.index_cast %dim_1 : index to i32
  %4 = arith.index_cast %dim_0 : index to i32
  %5 = arith.index_cast %dim : index to i32
  %6 = arith.muli %4, %5 : i32
  %alloca = memref.alloca() : memref<2xi32, "cpu">
  memref.store %3, %alloca[%c0] : memref<2xi32, "cpu">
  memref.store %6, %alloca[%c1] : memref<2xi32, "cpu">
  %7 = arith.index_cast %6 : i32 to index
  %alloc_3 = memref.alloc(%dim_1, %7) {kDiscSymbolicDimAttr = [@S0, @S3]} : memref<?x?xf32, "gpu">
  %alloc_4 = memref.alloc(%7) {kDiscSymbolicDimAttr = [@S3]} : memref<?xf32, "gpu">
  %8 = arith.cmpi slt, %dim_1, %7 : index
  scf.if %8 {
    "lmhlo.fusion"() ({
      "lmhlo.constant"(%alloc) {disc.device = "gpu", value = dense<0xFF800000> : tensor<f32>} : (memref<f32, "gpu">) -> ()
      "lmhlo.abs"(%reinterpret_cast, %alloc_2) {disc.device = "gpu"} : (memref<?x?x?xf32, "gpu">, memref<?x?x?xf32, "gpu">) -> ()
      "lmhlo.dynamic_reshape"(%alloc_2, %alloca, %alloc_3) {disc.device = "gpu"} : (memref<?x?x?xf32, "gpu">, memref<2xi32, "cpu">, memref<?x?xf32, "gpu">) -> ()
      scf.parallel (%arg1) = (%c0) to (%7) step (%c1) {
        %14 = "disc_shape.delinearize"(%arg1, %7) : (index, index) -> index
        %15 = memref.load %alloc[] : memref<f32, "gpu">
        memref.store %15, %alloc_4[%14] : memref<?xf32, "gpu">
        scf.yield
      }
      %11 = arith.ceildivui %7, %c512 : index
      %12 = arith.ceildivui %dim_1, %c32 : index
      %13 = arith.muli %11, %12 : index
      scf.parallel (%arg1, %arg2) = (%c0, %c0) to (%13, %c512) step (%c1, %c1) {
        %14 = memref.load %alloc[] : memref<f32, "gpu">
        %15 = arith.divui %arg1, %11 : index
        %16 = arith.remui %arg1, %11 : index
        %17 = arith.muli %16, %c512 : index
        %18 = arith.addi %17, %arg2 : index
        %19 = arith.cmpi ult, %18, %7 : index
        %20 = scf.if %19 -> (f32) {
          %21 = scf.for %arg3 = %c0 to %c32 step %c1 iter_args(%arg4 = %14) -> (f32) {
            %22 = arith.muli %15, %c32 : index
            %23 = arith.addi %22, %arg3 : index
            %24 = arith.cmpi slt, %23, %dim_1 : index
            %25 = scf.if %24 -> (f32) {
              %26 = memref.load %alloc_3[%23, %18] : memref<?x?xf32, "gpu">
              %27 = arith.cmpf oge, %arg4, %26 : f32
              %28 = arith.select %27, %arg4, %26 : f32
              scf.yield %28 : f32
            } else {
              scf.yield %arg4 : f32
            }
            scf.yield %25 : f32
          }
          scf.yield %21 : f32
        } else {
          scf.yield %14 : f32
        }
        scf.if %19 {
          %21 = memref.generic_atomic_rmw %alloc_4[%18] : memref<?xf32, "gpu"> {
          ^bb0(%arg3: f32):
            %22 = arith.cmpf ogt, %arg3, %20 : f32
            %23 = arith.select %22, %arg3, %20 : f32
            memref.atomic_yield %23 : f32
          }
        }
        scf.yield
      }
      "lmhlo.terminator"() : () -> ()
    }) {disc.device = "gpu", disc.fusion.name = "main_kColReduction_reduce__4_1_0", disc.fusion.tag = "flat", disc.fusion_type = "kColReduction", disc_col_reduction_schedule_hint = 7 : i32, disc_thread_per_block_hint = 512 : i32} : () -> ()
  } else {
    "lmhlo.fusion"() ({
      "lmhlo.constant"(%alloc) {disc.device = "gpu", value = dense<0xFF800000> : tensor<f32>} : (memref<f32, "gpu">) -> ()
      "lmhlo.abs"(%reinterpret_cast, %alloc_2) {disc.device = "gpu"} : (memref<?x?x?xf32, "gpu">, memref<?x?x?xf32, "gpu">) -> ()
      "lmhlo.dynamic_reshape"(%alloc_2, %alloca, %alloc_3) {disc.device = "gpu"} : (memref<?x?x?xf32, "gpu">, memref<2xi32, "cpu">, memref<?x?xf32, "gpu">) -> ()
      scf.parallel (%arg1) = (%c0) to (%7) step (%c1) {
        %14 = "disc_shape.delinearize"(%arg1, %7) : (index, index) -> index
        %15 = memref.load %alloc[] : memref<f32, "gpu">
        memref.store %15, %alloc_4[%14] : memref<?xf32, "gpu">
        scf.yield
      }
      %11 = arith.ceildivui %7, %c32 : index
      %12 = arith.ceildivui %dim_1, %c512 : index
      %13 = arith.muli %11, %12 : index
      scf.parallel (%arg1, %arg2) = (%c0, %c0) to (%13, %c256) step (%c1, %c1) {
        %14 = memref.load %alloc[] : memref<f32, "gpu">
        %15 = arith.divui %arg1, %11 : index
        %16 = arith.remui %arg1, %11 : index
        %17 = arith.divui %arg2, %c32 : index
        %18 = arith.remui %arg2, %c32 : index
        %19 = arith.muli %18, %c8 : index
        %20 = arith.addi %17, %19 : index
        %21 = arith.muli %16, %c32 : index
        %22 = arith.addi %18, %21 : index
        %alloc_8 = memref.alloc() : memref<256xf32, #gpu.address_space<workgroup>>
        %23 = arith.cmpi ult, %22, %7 : index
        %24 = scf.if %23 -> (f32) {
          %29 = scf.for %arg3 = %c0 to %c64 step %c1 iter_args(%arg4 = %14) -> (f32) {
            %30 = arith.muli %15, %c8 : index
            %31 = arith.addi %17, %30 : index
            %32 = arith.muli %31, %c64 : index
            %33 = arith.addi %arg3, %32 : index
            %34 = arith.cmpi slt, %33, %dim_1 : index
            %35 = scf.if %34 -> (f32) {
              %36 = memref.load %alloc_3[%33, %22] : memref<?x?xf32, "gpu">
              %37 = arith.maxf %arg4, %36 : f32
              scf.yield %37 : f32
            } else {
              scf.yield %arg4 : f32
            }
            scf.yield %35 : f32
          }
          scf.yield %29 : f32
        } else {
          scf.yield %14 : f32
        }
        memref.store %24, %alloc_8[%20] : memref<256xf32, #gpu.address_space<workgroup>>
        gpu.barrier
        %25 = arith.cmpi slt, %17, %c4 : index
        scf.if %25 {
          %29 = arith.addi %20, %c4 : index
          %30 = memref.load %alloc_8[%20] : memref<256xf32, #gpu.address_space<workgroup>>
          %31 = memref.load %alloc_8[%29] : memref<256xf32, #gpu.address_space<workgroup>>
          %32 = arith.maxf %30, %31 : f32
          memref.store %32, %alloc_8[%20] : memref<256xf32, #gpu.address_space<workgroup>>
        }
        gpu.barrier
        %26 = arith.cmpi slt, %17, %c2 : index
        scf.if %26 {
          %29 = arith.addi %20, %c2 : index
          %30 = memref.load %alloc_8[%20] : memref<256xf32, #gpu.address_space<workgroup>>
          %31 = memref.load %alloc_8[%29] : memref<256xf32, #gpu.address_space<workgroup>>
          %32 = arith.maxf %30, %31 : f32
          memref.store %32, %alloc_8[%20] : memref<256xf32, #gpu.address_space<workgroup>>
        }
        gpu.barrier
        %27 = arith.cmpi eq, %17, %c0 : index
        %28 = arith.andi %27, %23 : i1
        scf.if %28 {
          %29 = arith.addi %20, %c1 : index
          %30 = memref.load %alloc_8[%20] : memref<256xf32, #gpu.address_space<workgroup>>
          %31 = memref.load %alloc_8[%29] : memref<256xf32, #gpu.address_space<workgroup>>
          %32 = arith.maxf %30, %31 : f32
          %33 = memref.generic_atomic_rmw %alloc_4[%22] : memref<?xf32, "gpu"> {
          ^bb0(%arg3: f32):
            %34 = arith.cmpf ogt, %arg3, %32 : f32
            %35 = arith.select %34, %arg3, %32 : f32
            memref.atomic_yield %35 : f32
          }
        }
        scf.yield
      }
      "lmhlo.terminator"() : () -> ()
    }) {disc.device = "gpu", disc.fusion.name = "main_kColReduction_reduce__4_1_0", disc.fusion.tag = "thin", disc.fusion_type = "kColReduction", disc_col_reduction_schedule_hint = 8 : i32, disc_thread_per_block_hint = 256 : i32} : () -> ()
  }
  memref.dealloc %alloc_3 : memref<?x?xf32, "gpu">
  memref.dealloc %alloc_2 : memref<?x?x?xf32, "gpu">
  memref.dealloc %alloc : memref<f32, "gpu">
  %alloca_5 = memref.alloca() : memref<2xi32, "cpu">
  memref.store %4, %alloca_5[%c0] : memref<2xi32, "cpu">
  memref.store %5, %alloca_5[%c1] : memref<2xi32, "cpu">
  %9 = llvm.inttoptr %0 : i32 to !llvm.ptr<i8>
  %alloca_6 = memref.alloca() : memref<2xindex, "cpu">
  memref.store %dim_0, %alloca_6[%c0] : memref<2xindex, "cpu">
  memref.store %dim, %alloca_6[%c1] : memref<2xindex, "cpu">
  %10 = "disc_ral.dispatch"(%arg0, %9, %alloc_4, %alloca_6) {backend_config = "", call_target_name = "inc_ref", device = "gpu", has_side_effect = false} : (!disc_ral.context, !llvm.ptr<i8>, memref<?xf32, "gpu">, memref<2xindex, "cpu">) -> memref<?x?xf32, "gpu">
  %reinterpret_cast_7 = memref.reinterpret_cast %10 to offset: [0], sizes: [%dim_0, %dim], strides: [%dim, 1] {kDiscSymbolicDimAttr = [@S1, @S2]} : memref<?x?xf32, "gpu"> to memref<?x?xf32, "gpu">
  memref.dealloc %alloc_4 : memref<?xf32, "gpu">
  "disc_ral.dispatch"(%arg0, %c0, %reinterpret_cast_7) {backend_config = "", call_target_name = "ral_send_output", device = "cpu", has_side_effect = false} : (!disc_ral.context, index, memref<?x?xf32, "gpu">) -> ()
  return
}

// -----// IR Dump After InputInlineFusionPass (disc-input-inline-fusion) //----- //
func.func @main(%arg0: !disc_ral.context) attributes {tf.entry_function = {input_placements = "gpu", inputs = "input0", output_placements = "gpu", outputs = "output0"}} {
  %cst = arith.constant 0xFF800000 : f32
  %c4 = arith.constant 4 : index
  %c256 = arith.constant 256 : index
  %c64 = arith.constant 64 : index
  %c8 = arith.constant 8 : index
  %c32 = arith.constant 32 : index
  %c512 = arith.constant 512 : index
  %0 = llvm.mlir.constant(0 : i32) : i32
  %c0 = arith.constant 0 : index
  %c1 = arith.constant 1 : index
  %c2 = arith.constant 2 : index
  %1 = "disc_ral.dispatch"(%arg0, %c0) {backend_config = "", call_target_name = "ral_recv_input", device = "cpu", has_side_effect = false} : (!disc_ral.context, index) -> memref<?x?x?xf32, "gpu">
  %dim = memref.dim %1, %c2 : memref<?x?x?xf32, "gpu">
  %dim_0 = memref.dim %1, %c1 : memref<?x?x?xf32, "gpu">
  %dim_1 = memref.dim %1, %c0 : memref<?x?x?xf32, "gpu">
  %alloc = memref.alloc() : memref<f32, "gpu">
  %2 = arith.muli %dim, %dim_0 : index
  %reinterpret_cast = memref.reinterpret_cast %1 to offset: [0], sizes: [%dim_1, %dim_0, %dim], strides: [%2, %dim, 1] {kDiscSymbolicDimAttr = [@S0, @S1, @S2]} : memref<?x?x?xf32, "gpu"> to memref<?x?x?xf32, "gpu">
  %alloc_2 = memref.alloc(%dim_1, %dim_0, %dim) {kDiscSymbolicDimAttr = [@S0, @S1, @S2]} : memref<?x?x?xf32, "gpu">
  %3 = arith.index_cast %dim_1 : index to i32
  %4 = arith.index_cast %dim_0 : index to i32
  %5 = arith.index_cast %dim : index to i32
  %6 = arith.muli %4, %5 : i32
  %alloca = memref.alloca() : memref<2xi32, "cpu">
  memref.store %3, %alloca[%c0] : memref<2xi32, "cpu">
  memref.store %6, %alloca[%c1] : memref<2xi32, "cpu">
  %7 = arith.index_cast %6 : i32 to index
  %alloc_3 = memref.alloc(%dim_1, %7) {kDiscSymbolicDimAttr = [@S0, @S3]} : memref<?x?xf32, "gpu">
  %alloc_4 = memref.alloc(%7) {kDiscSymbolicDimAttr = [@S3]} : memref<?xf32, "gpu">
  %8 = arith.cmpi slt, %dim_1, %7 : index
  scf.if %8 {
    "lmhlo.fusion"() ({
      scf.parallel (%arg1) = (%c0) to (%7) step (%c1) {
        %14 = "disc_shape.delinearize"(%arg1, %7) : (index, index) -> index
        memref.store %cst, %alloc_4[%14] : memref<?xf32, "gpu">
        scf.yield
      }
      %11 = arith.ceildivui %7, %c512 : index
      %12 = arith.ceildivui %dim_1, %c32 : index
      %13 = arith.muli %11, %12 : index
      scf.parallel (%arg1, %arg2) = (%c0, %c0) to (%13, %c512) step (%c1, %c1) {
        %14 = arith.divui %arg1, %11 : index
        %15 = arith.remui %arg1, %11 : index
        %16 = arith.muli %15, %c512 : index
        %17 = arith.addi %16, %arg2 : index
        %18 = arith.cmpi ult, %17, %7 : index
        %19 = scf.if %18 -> (f32) {
          %20 = scf.for %arg3 = %c0 to %c32 step %c1 iter_args(%arg4 = %cst) -> (f32) {
            %21 = arith.muli %14, %c32 : index
            %22 = arith.addi %21, %arg3 : index
            %23 = arith.cmpi slt, %22, %dim_1 : index
            %24 = scf.if %23 -> (f32) {
              %25 = "disc_shape.linearize"(%22, %17, %dim_1, %7) {operand_segment_sizes = array<i32: 2, 2>} : (index, index, index, index) -> index
              %26:3 = "disc_shape.delinearize"(%25, %dim_1, %dim_0, %dim) : (index, index, index, index) -> (index, index, index)
              %27 = memref.load %reinterpret_cast[%26#0, %26#1, %26#2] : memref<?x?x?xf32, "gpu">
              %28 = math.absf %27 : f32
              %29 = arith.cmpf oge, %arg4, %28 : f32
              %30 = arith.select %29, %arg4, %28 : f32
              scf.yield %30 : f32
            } else {
              scf.yield %arg4 : f32
            }
            scf.yield %24 : f32
          }
          scf.yield %20 : f32
        } else {
          scf.yield %cst : f32
        }
        scf.if %18 {
          %20 = memref.generic_atomic_rmw %alloc_4[%17] : memref<?xf32, "gpu"> {
          ^bb0(%arg3: f32):
            %21 = arith.cmpf ogt, %arg3, %19 : f32
            %22 = arith.select %21, %arg3, %19 : f32
            memref.atomic_yield %22 : f32
          }
        }
        scf.yield
      }
      "lmhlo.terminator"() : () -> ()
    }) {disc.device = "gpu", disc.fusion.name = "main_kColReduction_reduce__4_1_0", disc.fusion.tag = "flat", disc.fusion_type = "kColReduction", disc_col_reduction_schedule_hint = 7 : i32, disc_thread_per_block_hint = 512 : i32} : () -> ()
  } else {
    "lmhlo.fusion"() ({
      scf.parallel (%arg1) = (%c0) to (%7) step (%c1) {
        %14 = "disc_shape.delinearize"(%arg1, %7) : (index, index) -> index
        memref.store %cst, %alloc_4[%14] : memref<?xf32, "gpu">
        scf.yield
      }
      %11 = arith.ceildivui %7, %c32 : index
      %12 = arith.ceildivui %dim_1, %c512 : index
      %13 = arith.muli %11, %12 : index
      scf.parallel (%arg1, %arg2) = (%c0, %c0) to (%13, %c256) step (%c1, %c1) {
        %14 = arith.divui %arg1, %11 : index
        %15 = arith.remui %arg1, %11 : index
        %16 = arith.divui %arg2, %c32 : index
        %17 = arith.remui %arg2, %c32 : index
        %18 = arith.muli %17, %c8 : index
        %19 = arith.addi %16, %18 : index
        %20 = arith.muli %15, %c32 : index
        %21 = arith.addi %17, %20 : index
        %alloc_8 = memref.alloc() : memref<256xf32, #gpu.address_space<workgroup>>
        %22 = arith.cmpi ult, %21, %7 : index
        %23 = scf.if %22 -> (f32) {
          %28 = scf.for %arg3 = %c0 to %c64 step %c1 iter_args(%arg4 = %cst) -> (f32) {
            %29 = arith.muli %14, %c8 : index
            %30 = arith.addi %16, %29 : index
            %31 = arith.muli %30, %c64 : index
            %32 = arith.addi %arg3, %31 : index
            %33 = arith.cmpi slt, %32, %dim_1 : index
            %34 = scf.if %33 -> (f32) {
              %35 = "disc_shape.linearize"(%32, %21, %dim_1, %7) {operand_segment_sizes = array<i32: 2, 2>} : (index, index, index, index) -> index
              %36:3 = "disc_shape.delinearize"(%35, %dim_1, %dim_0, %dim) : (index, index, index, index) -> (index, index, index)
              %37 = memref.load %reinterpret_cast[%36#0, %36#1, %36#2] : memref<?x?x?xf32, "gpu">
              %38 = math.absf %37 : f32
              %39 = arith.maxf %arg4, %38 : f32
              scf.yield %39 : f32
            } else {
              scf.yield %arg4 : f32
            }
            scf.yield %34 : f32
          }
          scf.yield %28 : f32
        } else {
          scf.yield %cst : f32
        }
        memref.store %23, %alloc_8[%19] : memref<256xf32, #gpu.address_space<workgroup>>
        gpu.barrier
        %24 = arith.cmpi slt, %16, %c4 : index
        scf.if %24 {
          %28 = arith.addi %19, %c4 : index
          %29 = memref.load %alloc_8[%19] : memref<256xf32, #gpu.address_space<workgroup>>
          %30 = memref.load %alloc_8[%28] : memref<256xf32, #gpu.address_space<workgroup>>
          %31 = arith.maxf %29, %30 : f32
          memref.store %31, %alloc_8[%19] : memref<256xf32, #gpu.address_space<workgroup>>
        }
        gpu.barrier
        %25 = arith.cmpi slt, %16, %c2 : index
        scf.if %25 {
          %28 = arith.addi %19, %c2 : index
          %29 = memref.load %alloc_8[%19] : memref<256xf32, #gpu.address_space<workgroup>>
          %30 = memref.load %alloc_8[%28] : memref<256xf32, #gpu.address_space<workgroup>>
          %31 = arith.maxf %29, %30 : f32
          memref.store %31, %alloc_8[%19] : memref<256xf32, #gpu.address_space<workgroup>>
        }
        gpu.barrier
        %26 = arith.cmpi eq, %16, %c0 : index
        %27 = arith.andi %26, %22 : i1
        scf.if %27 {
          %28 = arith.addi %19, %c1 : index
          %29 = memref.load %alloc_8[%19] : memref<256xf32, #gpu.address_space<workgroup>>
          %30 = memref.load %alloc_8[%28] : memref<256xf32, #gpu.address_space<workgroup>>
          %31 = arith.maxf %29, %30 : f32
          %32 = memref.generic_atomic_rmw %alloc_4[%21] : memref<?xf32, "gpu"> {
          ^bb0(%arg3: f32):
            %33 = arith.cmpf ogt, %arg3, %31 : f32
            %34 = arith.select %33, %arg3, %31 : f32
            memref.atomic_yield %34 : f32
          }
        }
        scf.yield
      }
      "lmhlo.terminator"() : () -> ()
    }) {disc.device = "gpu", disc.fusion.name = "main_kColReduction_reduce__4_1_0", disc.fusion.tag = "thin", disc.fusion_type = "kColReduction", disc_col_reduction_schedule_hint = 8 : i32, disc_thread_per_block_hint = 256 : i32} : () -> ()
  }
  memref.dealloc %alloc_3 : memref<?x?xf32, "gpu">
  memref.dealloc %alloc_2 : memref<?x?x?xf32, "gpu">
  memref.dealloc %alloc : memref<f32, "gpu">
  %alloca_5 = memref.alloca() : memref<2xi32, "cpu">
  memref.store %4, %alloca_5[%c0] : memref<2xi32, "cpu">
  memref.store %5, %alloca_5[%c1] : memref<2xi32, "cpu">
  %9 = llvm.inttoptr %0 : i32 to !llvm.ptr<i8>
  %alloca_6 = memref.alloca() : memref<2xindex, "cpu">
  memref.store %dim_0, %alloca_6[%c0] : memref<2xindex, "cpu">
  memref.store %dim, %alloca_6[%c1] : memref<2xindex, "cpu">
  %10 = "disc_ral.dispatch"(%arg0, %9, %alloc_4, %alloca_6) {backend_config = "", call_target_name = "inc_ref", device = "gpu", has_side_effect = false} : (!disc_ral.context, !llvm.ptr<i8>, memref<?xf32, "gpu">, memref<2xindex, "cpu">) -> memref<?x?xf32, "gpu">
  %reinterpret_cast_7 = memref.reinterpret_cast %10 to offset: [0], sizes: [%dim_0, %dim], strides: [%dim, 1] {kDiscSymbolicDimAttr = [@S1, @S2]} : memref<?x?xf32, "gpu"> to memref<?x?xf32, "gpu">
  memref.dealloc %alloc_4 : memref<?xf32, "gpu">
  "disc_ral.dispatch"(%arg0, %c0, %reinterpret_cast_7) {backend_config = "", call_target_name = "ral_send_output", device = "cpu", has_side_effect = false} : (!disc_ral.context, index, memref<?x?xf32, "gpu">) -> ()
  return
}

// -----// IR Dump After ArithExpandOps (arith-expand) //----- //
func.func @main(%arg0: !disc_ral.context) attributes {tf.entry_function = {input_placements = "gpu", inputs = "input0", output_placements = "gpu", outputs = "output0"}} {
  %cst = arith.constant 0xFF800000 : f32
  %c4 = arith.constant 4 : index
  %c256 = arith.constant 256 : index
  %c64 = arith.constant 64 : index
  %c8 = arith.constant 8 : index
  %c32 = arith.constant 32 : index
  %c512 = arith.constant 512 : index
  %0 = llvm.mlir.constant(0 : i32) : i32
  %c0 = arith.constant 0 : index
  %c1 = arith.constant 1 : index
  %c2 = arith.constant 2 : index
  %1 = "disc_ral.dispatch"(%arg0, %c0) {backend_config = "", call_target_name = "ral_recv_input", device = "cpu", has_side_effect = false} : (!disc_ral.context, index) -> memref<?x?x?xf32, "gpu">
  %dim = memref.dim %1, %c2 : memref<?x?x?xf32, "gpu">
  %dim_0 = memref.dim %1, %c1 : memref<?x?x?xf32, "gpu">
  %dim_1 = memref.dim %1, %c0 : memref<?x?x?xf32, "gpu">
  %alloc = memref.alloc() : memref<f32, "gpu">
  %2 = arith.muli %dim, %dim_0 : index
  %reinterpret_cast = memref.reinterpret_cast %1 to offset: [0], sizes: [%dim_1, %dim_0, %dim], strides: [%2, %dim, 1] {kDiscSymbolicDimAttr = [@S0, @S1, @S2]} : memref<?x?x?xf32, "gpu"> to memref<?x?x?xf32, "gpu">
  %alloc_2 = memref.alloc(%dim_1, %dim_0, %dim) {kDiscSymbolicDimAttr = [@S0, @S1, @S2]} : memref<?x?x?xf32, "gpu">
  %3 = arith.index_cast %dim_1 : index to i32
  %4 = arith.index_cast %dim_0 : index to i32
  %5 = arith.index_cast %dim : index to i32
  %6 = arith.muli %4, %5 : i32
  %alloca = memref.alloca() : memref<2xi32, "cpu">
  memref.store %3, %alloca[%c0] : memref<2xi32, "cpu">
  memref.store %6, %alloca[%c1] : memref<2xi32, "cpu">
  %7 = arith.index_cast %6 : i32 to index
  %alloc_3 = memref.alloc(%dim_1, %7) {kDiscSymbolicDimAttr = [@S0, @S3]} : memref<?x?xf32, "gpu">
  %alloc_4 = memref.alloc(%7) {kDiscSymbolicDimAttr = [@S3]} : memref<?xf32, "gpu">
  %8 = arith.cmpi slt, %dim_1, %7 : index
  scf.if %8 {
    "lmhlo.fusion"() ({
      scf.parallel (%arg1) = (%c0) to (%7) step (%c1) {
        %22 = "disc_shape.delinearize"(%arg1, %7) : (index, index) -> index
        memref.store %cst, %alloc_4[%22] : memref<?xf32, "gpu">
        scf.yield
      }
      %c0_8 = arith.constant 0 : index
      %11 = arith.cmpi eq, %7, %c0_8 : index
      %c1_9 = arith.constant 1 : index
      %12 = arith.subi %7, %c1_9 : index
      %13 = arith.divui %12, %c512 : index
      %14 = arith.addi %13, %c1_9 : index
      %15 = arith.select %11, %c0_8, %14 : index
      %c0_10 = arith.constant 0 : index
      %16 = arith.cmpi eq, %dim_1, %c0_10 : index
      %c1_11 = arith.constant 1 : index
      %17 = arith.subi %dim_1, %c1_11 : index
      %18 = arith.divui %17, %c32 : index
      %19 = arith.addi %18, %c1_11 : index
      %20 = arith.select %16, %c0_10, %19 : index
      %21 = arith.muli %15, %20 : index
      scf.parallel (%arg1, %arg2) = (%c0, %c0) to (%21, %c512) step (%c1, %c1) {
        %22 = arith.divui %arg1, %15 : index
        %23 = arith.remui %arg1, %15 : index
        %24 = arith.muli %23, %c512 : index
        %25 = arith.addi %24, %arg2 : index
        %26 = arith.cmpi ult, %25, %7 : index
        %27 = scf.if %26 -> (f32) {
          %28 = scf.for %arg3 = %c0 to %c32 step %c1 iter_args(%arg4 = %cst) -> (f32) {
            %29 = arith.muli %22, %c32 : index
            %30 = arith.addi %29, %arg3 : index
            %31 = arith.cmpi slt, %30, %dim_1 : index
            %32 = scf.if %31 -> (f32) {
              %33 = "disc_shape.linearize"(%30, %25, %dim_1, %7) {operand_segment_sizes = array<i32: 2, 2>} : (index, index, index, index) -> index
              %34:3 = "disc_shape.delinearize"(%33, %dim_1, %dim_0, %dim) : (index, index, index, index) -> (index, index, index)
              %35 = memref.load %reinterpret_cast[%34#0, %34#1, %34#2] : memref<?x?x?xf32, "gpu">
              %36 = math.absf %35 : f32
              %37 = arith.cmpf oge, %arg4, %36 : f32
              %38 = arith.select %37, %arg4, %36 : f32
              scf.yield %38 : f32
            } else {
              scf.yield %arg4 : f32
            }
            scf.yield %32 : f32
          }
          scf.yield %28 : f32
        } else {
          scf.yield %cst : f32
        }
        scf.if %26 {
          %28 = memref.generic_atomic_rmw %alloc_4[%25] : memref<?xf32, "gpu"> {
          ^bb0(%arg3: f32):
            %29 = arith.cmpf ogt, %arg3, %27 : f32
            %30 = arith.select %29, %arg3, %27 : f32
            memref.atomic_yield %30 : f32
          }
        }
        scf.yield
      }
      "lmhlo.terminator"() : () -> ()
    }) {disc.device = "gpu", disc.fusion.name = "main_kColReduction_reduce__4_1_0", disc.fusion.tag = "flat", disc.fusion_type = "kColReduction", disc_col_reduction_schedule_hint = 7 : i32, disc_thread_per_block_hint = 512 : i32} : () -> ()
  } else {
    "lmhlo.fusion"() ({
      scf.parallel (%arg1) = (%c0) to (%7) step (%c1) {
        %22 = "disc_shape.delinearize"(%arg1, %7) : (index, index) -> index
        memref.store %cst, %alloc_4[%22] : memref<?xf32, "gpu">
        scf.yield
      }
      %c0_8 = arith.constant 0 : index
      %11 = arith.cmpi eq, %7, %c0_8 : index
      %c1_9 = arith.constant 1 : index
      %12 = arith.subi %7, %c1_9 : index
      %13 = arith.divui %12, %c32 : index
      %14 = arith.addi %13, %c1_9 : index
      %15 = arith.select %11, %c0_8, %14 : index
      %c0_10 = arith.constant 0 : index
      %16 = arith.cmpi eq, %dim_1, %c0_10 : index
      %c1_11 = arith.constant 1 : index
      %17 = arith.subi %dim_1, %c1_11 : index
      %18 = arith.divui %17, %c512 : index
      %19 = arith.addi %18, %c1_11 : index
      %20 = arith.select %16, %c0_10, %19 : index
      %21 = arith.muli %15, %20 : index
      scf.parallel (%arg1, %arg2) = (%c0, %c0) to (%21, %c256) step (%c1, %c1) {
        %22 = arith.divui %arg1, %15 : index
        %23 = arith.remui %arg1, %15 : index
        %24 = arith.divui %arg2, %c32 : index
        %25 = arith.remui %arg2, %c32 : index
        %26 = arith.muli %25, %c8 : index
        %27 = arith.addi %24, %26 : index
        %28 = arith.muli %23, %c32 : index
        %29 = arith.addi %25, %28 : index
        %alloc_12 = memref.alloc() : memref<256xf32, #gpu.address_space<workgroup>>
        %30 = arith.cmpi ult, %29, %7 : index
        %31 = scf.if %30 -> (f32) {
          %36 = scf.for %arg3 = %c0 to %c64 step %c1 iter_args(%arg4 = %cst) -> (f32) {
            %37 = arith.muli %22, %c8 : index
            %38 = arith.addi %24, %37 : index
            %39 = arith.muli %38, %c64 : index
            %40 = arith.addi %arg3, %39 : index
            %41 = arith.cmpi slt, %40, %dim_1 : index
            %42 = scf.if %41 -> (f32) {
              %43 = "disc_shape.linearize"(%40, %29, %dim_1, %7) {operand_segment_sizes = array<i32: 2, 2>} : (index, index, index, index) -> index
              %44:3 = "disc_shape.delinearize"(%43, %dim_1, %dim_0, %dim) : (index, index, index, index) -> (index, index, index)
              %45 = memref.load %reinterpret_cast[%44#0, %44#1, %44#2] : memref<?x?x?xf32, "gpu">
              %46 = math.absf %45 : f32
              %47 = arith.cmpf ugt, %arg4, %46 : f32
              %48 = arith.select %47, %arg4, %46 : f32
              %49 = arith.cmpf uno, %46, %46 : f32
              %50 = arith.select %49, %46, %48 : f32
              scf.yield %50 : f32
            } else {
              scf.yield %arg4 : f32
            }
            scf.yield %42 : f32
          }
          scf.yield %36 : f32
        } else {
          scf.yield %cst : f32
        }
        memref.store %31, %alloc_12[%27] : memref<256xf32, #gpu.address_space<workgroup>>
        gpu.barrier
        %32 = arith.cmpi slt, %24, %c4 : index
        scf.if %32 {
          %36 = arith.addi %27, %c4 : index
          %37 = memref.load %alloc_12[%27] : memref<256xf32, #gpu.address_space<workgroup>>
          %38 = memref.load %alloc_12[%36] : memref<256xf32, #gpu.address_space<workgroup>>
          %39 = arith.cmpf ugt, %37, %38 : f32
          %40 = arith.select %39, %37, %38 : f32
          %41 = arith.cmpf uno, %38, %38 : f32
          %42 = arith.select %41, %38, %40 : f32
          memref.store %42, %alloc_12[%27] : memref<256xf32, #gpu.address_space<workgroup>>
        }
        gpu.barrier
        %33 = arith.cmpi slt, %24, %c2 : index
        scf.if %33 {
          %36 = arith.addi %27, %c2 : index
          %37 = memref.load %alloc_12[%27] : memref<256xf32, #gpu.address_space<workgroup>>
          %38 = memref.load %alloc_12[%36] : memref<256xf32, #gpu.address_space<workgroup>>
          %39 = arith.cmpf ugt, %37, %38 : f32
          %40 = arith.select %39, %37, %38 : f32
          %41 = arith.cmpf uno, %38, %38 : f32
          %42 = arith.select %41, %38, %40 : f32
          memref.store %42, %alloc_12[%27] : memref<256xf32, #gpu.address_space<workgroup>>
        }
        gpu.barrier
        %34 = arith.cmpi eq, %24, %c0 : index
        %35 = arith.andi %34, %30 : i1
        scf.if %35 {
          %36 = arith.addi %27, %c1 : index
          %37 = memref.load %alloc_12[%27] : memref<256xf32, #gpu.address_space<workgroup>>
          %38 = memref.load %alloc_12[%36] : memref<256xf32, #gpu.address_space<workgroup>>
          %39 = arith.cmpf ugt, %37, %38 : f32
          %40 = arith.select %39, %37, %38 : f32
          %41 = arith.cmpf uno, %38, %38 : f32
          %42 = arith.select %41, %38, %40 : f32
          %43 = memref.generic_atomic_rmw %alloc_4[%29] : memref<?xf32, "gpu"> {
          ^bb0(%arg3: f32):
            %44 = arith.cmpf ogt, %arg3, %42 : f32
            %45 = arith.select %44, %arg3, %42 : f32
            memref.atomic_yield %45 : f32
          }
        }
        scf.yield
      }
      "lmhlo.terminator"() : () -> ()
    }) {disc.device = "gpu", disc.fusion.name = "main_kColReduction_reduce__4_1_0", disc.fusion.tag = "thin", disc.fusion_type = "kColReduction", disc_col_reduction_schedule_hint = 8 : i32, disc_thread_per_block_hint = 256 : i32} : () -> ()
  }
  memref.dealloc %alloc_3 : memref<?x?xf32, "gpu">
  memref.dealloc %alloc_2 : memref<?x?x?xf32, "gpu">
  memref.dealloc %alloc : memref<f32, "gpu">
  %alloca_5 = memref.alloca() : memref<2xi32, "cpu">
  memref.store %4, %alloca_5[%c0] : memref<2xi32, "cpu">
  memref.store %5, %alloca_5[%c1] : memref<2xi32, "cpu">
  %9 = llvm.inttoptr %0 : i32 to !llvm.ptr<i8>
  %alloca_6 = memref.alloca() : memref<2xindex, "cpu">
  memref.store %dim_0, %alloca_6[%c0] : memref<2xindex, "cpu">
  memref.store %dim, %alloca_6[%c1] : memref<2xindex, "cpu">
  %10 = "disc_ral.dispatch"(%arg0, %9, %alloc_4, %alloca_6) {backend_config = "", call_target_name = "inc_ref", device = "gpu", has_side_effect = false} : (!disc_ral.context, !llvm.ptr<i8>, memref<?xf32, "gpu">, memref<2xindex, "cpu">) -> memref<?x?xf32, "gpu">
  %reinterpret_cast_7 = memref.reinterpret_cast %10 to offset: [0], sizes: [%dim_0, %dim], strides: [%dim, 1] {kDiscSymbolicDimAttr = [@S1, @S2]} : memref<?x?xf32, "gpu"> to memref<?x?xf32, "gpu">
  memref.dealloc %alloc_4 : memref<?xf32, "gpu">
  "disc_ral.dispatch"(%arg0, %c0, %reinterpret_cast_7) {backend_config = "", call_target_name = "ral_send_output", device = "cpu", has_side_effect = false} : (!disc_ral.context, index, memref<?x?xf32, "gpu">) -> ()
  return
}

// -----// IR Dump After DiscBF16ExpansionPass (disc-bf16-expansion) //----- //
func.func @main(%arg0: !disc_ral.context) attributes {tf.entry_function = {input_placements = "gpu", inputs = "input0", output_placements = "gpu", outputs = "output0"}} {
  %cst = arith.constant 0xFF800000 : f32
  %c4 = arith.constant 4 : index
  %c256 = arith.constant 256 : index
  %c64 = arith.constant 64 : index
  %c8 = arith.constant 8 : index
  %c32 = arith.constant 32 : index
  %c512 = arith.constant 512 : index
  %0 = llvm.mlir.constant(0 : i32) : i32
  %c0 = arith.constant 0 : index
  %c1 = arith.constant 1 : index
  %c2 = arith.constant 2 : index
  %1 = "disc_ral.dispatch"(%arg0, %c0) {backend_config = "", call_target_name = "ral_recv_input", device = "cpu", has_side_effect = false} : (!disc_ral.context, index) -> memref<?x?x?xf32, "gpu">
  %dim = memref.dim %1, %c2 : memref<?x?x?xf32, "gpu">
  %dim_0 = memref.dim %1, %c1 : memref<?x?x?xf32, "gpu">
  %dim_1 = memref.dim %1, %c0 : memref<?x?x?xf32, "gpu">
  %alloc = memref.alloc() : memref<f32, "gpu">
  %2 = arith.muli %dim, %dim_0 : index
  %reinterpret_cast = memref.reinterpret_cast %1 to offset: [0], sizes: [%dim_1, %dim_0, %dim], strides: [%2, %dim, 1] {kDiscSymbolicDimAttr = [@S0, @S1, @S2]} : memref<?x?x?xf32, "gpu"> to memref<?x?x?xf32, "gpu">
  %alloc_2 = memref.alloc(%dim_1, %dim_0, %dim) {kDiscSymbolicDimAttr = [@S0, @S1, @S2]} : memref<?x?x?xf32, "gpu">
  %3 = arith.index_cast %dim_1 : index to i32
  %4 = arith.index_cast %dim_0 : index to i32
  %5 = arith.index_cast %dim : index to i32
  %6 = arith.muli %4, %5 : i32
  %alloca = memref.alloca() : memref<2xi32, "cpu">
  memref.store %3, %alloca[%c0] : memref<2xi32, "cpu">
  memref.store %6, %alloca[%c1] : memref<2xi32, "cpu">
  %7 = arith.index_cast %6 : i32 to index
  %alloc_3 = memref.alloc(%dim_1, %7) {kDiscSymbolicDimAttr = [@S0, @S3]} : memref<?x?xf32, "gpu">
  %alloc_4 = memref.alloc(%7) {kDiscSymbolicDimAttr = [@S3]} : memref<?xf32, "gpu">
  %8 = arith.cmpi slt, %dim_1, %7 : index
  scf.if %8 {
    "lmhlo.fusion"() ({
      scf.parallel (%arg1) = (%c0) to (%7) step (%c1) {
        %22 = "disc_shape.delinearize"(%arg1, %7) : (index, index) -> index
        memref.store %cst, %alloc_4[%22] : memref<?xf32, "gpu">
        scf.yield
      }
      %11 = arith.cmpi eq, %7, %c0 : index
      %12 = arith.subi %7, %c1 : index
      %13 = arith.divui %12, %c512 : index
      %14 = arith.addi %13, %c1 : index
      %15 = arith.select %11, %c0, %14 : index
      %16 = arith.cmpi eq, %dim_1, %c0 : index
      %17 = arith.subi %dim_1, %c1 : index
      %18 = arith.divui %17, %c32 : index
      %19 = arith.addi %18, %c1 : index
      %20 = arith.select %16, %c0, %19 : index
      %21 = arith.muli %15, %20 : index
      scf.parallel (%arg1, %arg2) = (%c0, %c0) to (%21, %c512) step (%c1, %c1) {
        %22 = arith.divui %arg1, %15 : index
        %23 = arith.remui %arg1, %15 : index
        %24 = arith.muli %23, %c512 : index
        %25 = arith.addi %24, %arg2 : index
        %26 = arith.cmpi ult, %25, %7 : index
        %27 = scf.if %26 -> (f32) {
          %28 = scf.for %arg3 = %c0 to %c32 step %c1 iter_args(%arg4 = %cst) -> (f32) {
            %29 = arith.muli %22, %c32 : index
            %30 = arith.addi %29, %arg3 : index
            %31 = arith.cmpi slt, %30, %dim_1 : index
            %32 = scf.if %31 -> (f32) {
              %33 = "disc_shape.linearize"(%30, %25, %dim_1, %7) {operand_segment_sizes = array<i32: 2, 2>} : (index, index, index, index) -> index
              %34:3 = "disc_shape.delinearize"(%33, %dim_1, %dim_0, %dim) : (index, index, index, index) -> (index, index, index)
              %35 = memref.load %reinterpret_cast[%34#0, %34#1, %34#2] : memref<?x?x?xf32, "gpu">
              %36 = math.absf %35 : f32
              %37 = arith.cmpf oge, %arg4, %36 : f32
              %38 = arith.select %37, %arg4, %36 : f32
              scf.yield %38 : f32
            } else {
              scf.yield %arg4 : f32
            }
            scf.yield %32 : f32
          }
          scf.yield %28 : f32
        } else {
          scf.yield %cst : f32
        }
        scf.if %26 {
          %28 = memref.generic_atomic_rmw %alloc_4[%25] : memref<?xf32, "gpu"> {
          ^bb0(%arg3: f32):
            %29 = arith.cmpf ogt, %arg3, %27 : f32
            %30 = arith.select %29, %arg3, %27 : f32
            memref.atomic_yield %30 : f32
          }
        }
        scf.yield
      }
      "lmhlo.terminator"() : () -> ()
    }) {disc.device = "gpu", disc.fusion.name = "main_kColReduction_reduce__4_1_0", disc.fusion.tag = "flat", disc.fusion_type = "kColReduction", disc_col_reduction_schedule_hint = 7 : i32, disc_thread_per_block_hint = 512 : i32} : () -> ()
  } else {
    "lmhlo.fusion"() ({
      scf.parallel (%arg1) = (%c0) to (%7) step (%c1) {
        %22 = "disc_shape.delinearize"(%arg1, %7) : (index, index) -> index
        memref.store %cst, %alloc_4[%22] : memref<?xf32, "gpu">
        scf.yield
      }
      %11 = arith.cmpi eq, %7, %c0 : index
      %12 = arith.subi %7, %c1 : index
      %13 = arith.divui %12, %c32 : index
      %14 = arith.addi %13, %c1 : index
      %15 = arith.select %11, %c0, %14 : index
      %16 = arith.cmpi eq, %dim_1, %c0 : index
      %17 = arith.subi %dim_1, %c1 : index
      %18 = arith.divui %17, %c512 : index
      %19 = arith.addi %18, %c1 : index
      %20 = arith.select %16, %c0, %19 : index
      %21 = arith.muli %15, %20 : index
      scf.parallel (%arg1, %arg2) = (%c0, %c0) to (%21, %c256) step (%c1, %c1) {
        %22 = arith.divui %arg1, %15 : index
        %23 = arith.remui %arg1, %15 : index
        %24 = arith.divui %arg2, %c32 : index
        %25 = arith.remui %arg2, %c32 : index
        %26 = arith.muli %25, %c8 : index
        %27 = arith.addi %24, %26 : index
        %28 = arith.muli %23, %c32 : index
        %29 = arith.addi %25, %28 : index
        %alloc_8 = memref.alloc() : memref<256xf32, #gpu.address_space<workgroup>>
        %30 = arith.cmpi ult, %29, %7 : index
        %31 = scf.if %30 -> (f32) {
          %36 = scf.for %arg3 = %c0 to %c64 step %c1 iter_args(%arg4 = %cst) -> (f32) {
            %37 = arith.muli %22, %c8 : index
            %38 = arith.addi %24, %37 : index
            %39 = arith.muli %38, %c64 : index
            %40 = arith.addi %arg3, %39 : index
            %41 = arith.cmpi slt, %40, %dim_1 : index
            %42 = scf.if %41 -> (f32) {
              %43 = "disc_shape.linearize"(%40, %29, %dim_1, %7) {operand_segment_sizes = array<i32: 2, 2>} : (index, index, index, index) -> index
              %44:3 = "disc_shape.delinearize"(%43, %dim_1, %dim_0, %dim) : (index, index, index, index) -> (index, index, index)
              %45 = memref.load %reinterpret_cast[%44#0, %44#1, %44#2] : memref<?x?x?xf32, "gpu">
              %46 = math.absf %45 : f32
              %47 = arith.cmpf ugt, %arg4, %46 : f32
              %48 = arith.select %47, %arg4, %46 : f32
              %49 = arith.cmpf uno, %46, %46 : f32
              %50 = arith.select %49, %46, %48 : f32
              scf.yield %50 : f32
            } else {
              scf.yield %arg4 : f32
            }
            scf.yield %42 : f32
          }
          scf.yield %36 : f32
        } else {
          scf.yield %cst : f32
        }
        memref.store %31, %alloc_8[%27] : memref<256xf32, #gpu.address_space<workgroup>>
        gpu.barrier
        %32 = arith.cmpi slt, %24, %c4 : index
        scf.if %32 {
          %36 = arith.addi %27, %c4 : index
          %37 = memref.load %alloc_8[%27] : memref<256xf32, #gpu.address_space<workgroup>>
          %38 = memref.load %alloc_8[%36] : memref<256xf32, #gpu.address_space<workgroup>>
          %39 = arith.cmpf ugt, %37, %38 : f32
          %40 = arith.select %39, %37, %38 : f32
          %41 = arith.cmpf uno, %38, %38 : f32
          %42 = arith.select %41, %38, %40 : f32
          memref.store %42, %alloc_8[%27] : memref<256xf32, #gpu.address_space<workgroup>>
        }
        gpu.barrier
        %33 = arith.cmpi slt, %24, %c2 : index
        scf.if %33 {
          %36 = arith.addi %27, %c2 : index
          %37 = memref.load %alloc_8[%27] : memref<256xf32, #gpu.address_space<workgroup>>
          %38 = memref.load %alloc_8[%36] : memref<256xf32, #gpu.address_space<workgroup>>
          %39 = arith.cmpf ugt, %37, %38 : f32
          %40 = arith.select %39, %37, %38 : f32
          %41 = arith.cmpf uno, %38, %38 : f32
          %42 = arith.select %41, %38, %40 : f32
          memref.store %42, %alloc_8[%27] : memref<256xf32, #gpu.address_space<workgroup>>
        }
        gpu.barrier
        %34 = arith.cmpi eq, %24, %c0 : index
        %35 = arith.andi %34, %30 : i1
        scf.if %35 {
          %36 = arith.addi %27, %c1 : index
          %37 = memref.load %alloc_8[%27] : memref<256xf32, #gpu.address_space<workgroup>>
          %38 = memref.load %alloc_8[%36] : memref<256xf32, #gpu.address_space<workgroup>>
          %39 = arith.cmpf ugt, %37, %38 : f32
          %40 = arith.select %39, %37, %38 : f32
          %41 = arith.cmpf uno, %38, %38 : f32
          %42 = arith.select %41, %38, %40 : f32
          %43 = memref.generic_atomic_rmw %alloc_4[%29] : memref<?xf32, "gpu"> {
          ^bb0(%arg3: f32):
            %44 = arith.cmpf ogt, %arg3, %42 : f32
            %45 = arith.select %44, %arg3, %42 : f32
            memref.atomic_yield %45 : f32
          }
        }
        scf.yield
      }
      "lmhlo.terminator"() : () -> ()
    }) {disc.device = "gpu", disc.fusion.name = "main_kColReduction_reduce__4_1_0", disc.fusion.tag = "thin", disc.fusion_type = "kColReduction", disc_col_reduction_schedule_hint = 8 : i32, disc_thread_per_block_hint = 256 : i32} : () -> ()
  }
  memref.dealloc %alloc_3 : memref<?x?xf32, "gpu">
  memref.dealloc %alloc_2 : memref<?x?x?xf32, "gpu">
  memref.dealloc %alloc : memref<f32, "gpu">
  %alloca_5 = memref.alloca() : memref<2xi32, "cpu">
  memref.store %4, %alloca_5[%c0] : memref<2xi32, "cpu">
  memref.store %5, %alloca_5[%c1] : memref<2xi32, "cpu">
  %9 = llvm.inttoptr %0 : i32 to !llvm.ptr<i8>
  %alloca_6 = memref.alloca() : memref<2xindex, "cpu">
  memref.store %dim_0, %alloca_6[%c0] : memref<2xindex, "cpu">
  memref.store %dim, %alloca_6[%c1] : memref<2xindex, "cpu">
  %10 = "disc_ral.dispatch"(%arg0, %9, %alloc_4, %alloca_6) {backend_config = "", call_target_name = "inc_ref", device = "gpu", has_side_effect = false} : (!disc_ral.context, !llvm.ptr<i8>, memref<?xf32, "gpu">, memref<2xindex, "cpu">) -> memref<?x?xf32, "gpu">
  %reinterpret_cast_7 = memref.reinterpret_cast %10 to offset: [0], sizes: [%dim_0, %dim], strides: [%dim, 1] {kDiscSymbolicDimAttr = [@S1, @S2]} : memref<?x?xf32, "gpu"> to memref<?x?xf32, "gpu">
  memref.dealloc %alloc_4 : memref<?xf32, "gpu">
  "disc_ral.dispatch"(%arg0, %c0, %reinterpret_cast_7) {backend_config = "", call_target_name = "ral_send_output", device = "cpu", has_side_effect = false} : (!disc_ral.context, index, memref<?x?xf32, "gpu">) -> ()
  return
}

// -----// IR Dump After DiscFlattenMemrefAccessPass (disc-flatten-memref-access) //----- //
func.func @main(%arg0: !disc_ral.context) attributes {tf.entry_function = {input_placements = "gpu", inputs = "input0", output_placements = "gpu", outputs = "output0"}} {
  %cst = arith.constant 0xFF800000 : f32
  %c4 = arith.constant 4 : index
  %c256 = arith.constant 256 : index
  %c64 = arith.constant 64 : index
  %c8 = arith.constant 8 : index
  %c32 = arith.constant 32 : index
  %c512 = arith.constant 512 : index
  %0 = llvm.mlir.constant(0 : i32) : i32
  %c0 = arith.constant 0 : index
  %c1 = arith.constant 1 : index
  %c2 = arith.constant 2 : index
  %1 = "disc_ral.dispatch"(%arg0, %c0) {backend_config = "", call_target_name = "ral_recv_input", device = "cpu", has_side_effect = false} : (!disc_ral.context, index) -> memref<?x?x?xf32, "gpu">
  %dim = memref.dim %1, %c2 : memref<?x?x?xf32, "gpu">
  %dim_0 = memref.dim %1, %c1 : memref<?x?x?xf32, "gpu">
  %dim_1 = memref.dim %1, %c0 : memref<?x?x?xf32, "gpu">
  %alloc = memref.alloc() : memref<f32, "gpu">
  %2 = arith.muli %dim, %dim_0 : index
  %reinterpret_cast = memref.reinterpret_cast %1 to offset: [0], sizes: [%dim_1, %dim_0, %dim], strides: [%2, %dim, 1] {kDiscSymbolicDimAttr = [@S0, @S1, @S2]} : memref<?x?x?xf32, "gpu"> to memref<?x?x?xf32, "gpu">
  %alloc_2 = memref.alloc(%dim_1, %dim_0, %dim) {kDiscSymbolicDimAttr = [@S0, @S1, @S2]} : memref<?x?x?xf32, "gpu">
  %3 = arith.index_cast %dim_1 : index to i32
  %4 = arith.index_cast %dim_0 : index to i32
  %5 = arith.index_cast %dim : index to i32
  %6 = arith.muli %4, %5 : i32
  %alloca = memref.alloca() : memref<2xi32, "cpu">
  memref.store %3, %alloca[%c0] : memref<2xi32, "cpu">
  memref.store %6, %alloca[%c1] : memref<2xi32, "cpu">
  %7 = arith.index_cast %6 : i32 to index
  %alloc_3 = memref.alloc(%dim_1, %7) {kDiscSymbolicDimAttr = [@S0, @S3]} : memref<?x?xf32, "gpu">
  %alloc_4 = memref.alloc(%7) {kDiscSymbolicDimAttr = [@S3]} : memref<?xf32, "gpu">
  %8 = arith.cmpi slt, %dim_1, %7 : index
  scf.if %8 {
    "lmhlo.fusion"() ({
      scf.parallel (%arg1) = (%c0) to (%7) step (%c1) {
        %22 = "disc_shape.delinearize"(%arg1, %7) : (index, index) -> index
        %c0_8 = arith.constant 0 : index
        %dim_9 = memref.dim %alloc_4, %c0_8 : memref<?xf32, "gpu">
        %23 = "disc_shape.linearize"(%22, %dim_9) {operand_segment_sizes = array<i32: 1, 1>} : (index, index) -> index
        %c1_10 = arith.constant 1 : index
        %c0_11 = arith.constant 0 : index
        %dim_12 = memref.dim %alloc_4, %c0_11 : memref<?xf32, "gpu">
        %24 = arith.muli %c1_10, %dim_12 : index
        %c1_13 = arith.constant 1 : index
        %c0_14 = arith.constant 0 : index
        %reinterpret_cast_15 = memref.reinterpret_cast %alloc_4 to offset: [%c0_14], sizes: [%24], strides: [%c1_13] : memref<?xf32, "gpu"> to memref<?xf32, "gpu">
        memref.store %cst, %reinterpret_cast_15[%23] : memref<?xf32, "gpu">
        scf.yield
      }
      %11 = arith.cmpi eq, %7, %c0 : index
      %12 = arith.subi %7, %c1 : index
      %13 = arith.divui %12, %c512 : index
      %14 = arith.addi %13, %c1 : index
      %15 = arith.select %11, %c0, %14 : index
      %16 = arith.cmpi eq, %dim_1, %c0 : index
      %17 = arith.subi %dim_1, %c1 : index
      %18 = arith.divui %17, %c32 : index
      %19 = arith.addi %18, %c1 : index
      %20 = arith.select %16, %c0, %19 : index
      %21 = arith.muli %15, %20 : index
      scf.parallel (%arg1, %arg2) = (%c0, %c0) to (%21, %c512) step (%c1, %c1) {
        %22 = arith.divui %arg1, %15 : index
        %23 = arith.remui %arg1, %15 : index
        %24 = arith.muli %23, %c512 : index
        %25 = arith.addi %24, %arg2 : index
        %26 = arith.cmpi ult, %25, %7 : index
        %27 = scf.if %26 -> (f32) {
          %28 = scf.for %arg3 = %c0 to %c32 step %c1 iter_args(%arg4 = %cst) -> (f32) {
            %29 = arith.muli %22, %c32 : index
            %30 = arith.addi %29, %arg3 : index
            %31 = arith.cmpi slt, %30, %dim_1 : index
            %32 = scf.if %31 -> (f32) {
              %33 = "disc_shape.linearize"(%30, %25, %dim_1, %7) {operand_segment_sizes = array<i32: 2, 2>} : (index, index, index, index) -> index
              %34:3 = "disc_shape.delinearize"(%33, %dim_1, %dim_0, %dim) : (index, index, index, index) -> (index, index, index)
              %c0_8 = arith.constant 0 : index
              %dim_9 = memref.dim %reinterpret_cast, %c0_8 : memref<?x?x?xf32, "gpu">
              %c1_10 = arith.constant 1 : index
              %dim_11 = memref.dim %reinterpret_cast, %c1_10 : memref<?x?x?xf32, "gpu">
              %c2_12 = arith.constant 2 : index
              %dim_13 = memref.dim %reinterpret_cast, %c2_12 : memref<?x?x?xf32, "gpu">
              %35 = "disc_shape.linearize"(%34#0, %34#1, %34#2, %dim_9, %dim_11, %dim_13) {operand_segment_sizes = array<i32: 3, 3>} : (index, index, index, index, index, index) -> index
              %c1_14 = arith.constant 1 : index
              %c0_15 = arith.constant 0 : index
              %dim_16 = memref.dim %reinterpret_cast, %c0_15 : memref<?x?x?xf32, "gpu">
              %36 = arith.muli %c1_14, %dim_16 : index
              %c1_17 = arith.constant 1 : index
              %dim_18 = memref.dim %reinterpret_cast, %c1_17 : memref<?x?x?xf32, "gpu">
              %37 = arith.muli %36, %dim_18 : index
              %c2_19 = arith.constant 2 : index
              %dim_20 = memref.dim %reinterpret_cast, %c2_19 : memref<?x?x?xf32, "gpu">
              %38 = arith.muli %37, %dim_20 : index
              %c1_21 = arith.constant 1 : index
              %c0_22 = arith.constant 0 : index
              %reinterpret_cast_23 = memref.reinterpret_cast %reinterpret_cast to offset: [%c0_22], sizes: [%38], strides: [%c1_21] : memref<?x?x?xf32, "gpu"> to memref<?xf32, "gpu">
              %39 = memref.load %reinterpret_cast_23[%35] : memref<?xf32, "gpu">
              %40 = math.absf %39 : f32
              %41 = arith.cmpf oge, %arg4, %40 : f32
              %42 = arith.select %41, %arg4, %40 : f32
              scf.yield %42 : f32
            } else {
              scf.yield %arg4 : f32
            }
            scf.yield %32 : f32
          }
          scf.yield %28 : f32
        } else {
          scf.yield %cst : f32
        }
        scf.if %26 {
          %28 = memref.generic_atomic_rmw %alloc_4[%25] : memref<?xf32, "gpu"> {
          ^bb0(%arg3: f32):
            %29 = arith.cmpf ogt, %arg3, %27 : f32
            %30 = arith.select %29, %arg3, %27 : f32
            memref.atomic_yield %30 : f32
          }
        }
        scf.yield
      }
      "lmhlo.terminator"() : () -> ()
    }) {disc.device = "gpu", disc.fusion.name = "main_kColReduction_reduce__4_1_0", disc.fusion.tag = "flat", disc.fusion_type = "kColReduction", disc_col_reduction_schedule_hint = 7 : i32, disc_thread_per_block_hint = 512 : i32} : () -> ()
  } else {
    "lmhlo.fusion"() ({
      scf.parallel (%arg1) = (%c0) to (%7) step (%c1) {
        %22 = "disc_shape.delinearize"(%arg1, %7) : (index, index) -> index
        %c0_8 = arith.constant 0 : index
        %dim_9 = memref.dim %alloc_4, %c0_8 : memref<?xf32, "gpu">
        %23 = "disc_shape.linearize"(%22, %dim_9) {operand_segment_sizes = array<i32: 1, 1>} : (index, index) -> index
        %c1_10 = arith.constant 1 : index
        %c0_11 = arith.constant 0 : index
        %dim_12 = memref.dim %alloc_4, %c0_11 : memref<?xf32, "gpu">
        %24 = arith.muli %c1_10, %dim_12 : index
        %c1_13 = arith.constant 1 : index
        %c0_14 = arith.constant 0 : index
        %reinterpret_cast_15 = memref.reinterpret_cast %alloc_4 to offset: [%c0_14], sizes: [%24], strides: [%c1_13] : memref<?xf32, "gpu"> to memref<?xf32, "gpu">
        memref.store %cst, %reinterpret_cast_15[%23] : memref<?xf32, "gpu">
        scf.yield
      }
      %11 = arith.cmpi eq, %7, %c0 : index
      %12 = arith.subi %7, %c1 : index
      %13 = arith.divui %12, %c32 : index
      %14 = arith.addi %13, %c1 : index
      %15 = arith.select %11, %c0, %14 : index
      %16 = arith.cmpi eq, %dim_1, %c0 : index
      %17 = arith.subi %dim_1, %c1 : index
      %18 = arith.divui %17, %c512 : index
      %19 = arith.addi %18, %c1 : index
      %20 = arith.select %16, %c0, %19 : index
      %21 = arith.muli %15, %20 : index
      scf.parallel (%arg1, %arg2) = (%c0, %c0) to (%21, %c256) step (%c1, %c1) {
        %22 = arith.divui %arg1, %15 : index
        %23 = arith.remui %arg1, %15 : index
        %24 = arith.divui %arg2, %c32 : index
        %25 = arith.remui %arg2, %c32 : index
        %26 = arith.muli %25, %c8 : index
        %27 = arith.addi %24, %26 : index
        %28 = arith.muli %23, %c32 : index
        %29 = arith.addi %25, %28 : index
        %alloc_8 = memref.alloc() : memref<256xf32, #gpu.address_space<workgroup>>
        %30 = arith.cmpi ult, %29, %7 : index
        %31 = scf.if %30 -> (f32) {
          %37 = scf.for %arg3 = %c0 to %c64 step %c1 iter_args(%arg4 = %cst) -> (f32) {
            %38 = arith.muli %22, %c8 : index
            %39 = arith.addi %24, %38 : index
            %40 = arith.muli %39, %c64 : index
            %41 = arith.addi %arg3, %40 : index
            %42 = arith.cmpi slt, %41, %dim_1 : index
            %43 = scf.if %42 -> (f32) {
              %44 = "disc_shape.linearize"(%41, %29, %dim_1, %7) {operand_segment_sizes = array<i32: 2, 2>} : (index, index, index, index) -> index
              %45:3 = "disc_shape.delinearize"(%44, %dim_1, %dim_0, %dim) : (index, index, index, index) -> (index, index, index)
              %c0_11 = arith.constant 0 : index
              %dim_12 = memref.dim %reinterpret_cast, %c0_11 : memref<?x?x?xf32, "gpu">
              %c1_13 = arith.constant 1 : index
              %dim_14 = memref.dim %reinterpret_cast, %c1_13 : memref<?x?x?xf32, "gpu">
              %c2_15 = arith.constant 2 : index
              %dim_16 = memref.dim %reinterpret_cast, %c2_15 : memref<?x?x?xf32, "gpu">
              %46 = "disc_shape.linearize"(%45#0, %45#1, %45#2, %dim_12, %dim_14, %dim_16) {operand_segment_sizes = array<i32: 3, 3>} : (index, index, index, index, index, index) -> index
              %c1_17 = arith.constant 1 : index
              %c0_18 = arith.constant 0 : index
              %dim_19 = memref.dim %reinterpret_cast, %c0_18 : memref<?x?x?xf32, "gpu">
              %47 = arith.muli %c1_17, %dim_19 : index
              %c1_20 = arith.constant 1 : index
              %dim_21 = memref.dim %reinterpret_cast, %c1_20 : memref<?x?x?xf32, "gpu">
              %48 = arith.muli %47, %dim_21 : index
              %c2_22 = arith.constant 2 : index
              %dim_23 = memref.dim %reinterpret_cast, %c2_22 : memref<?x?x?xf32, "gpu">
              %49 = arith.muli %48, %dim_23 : index
              %c1_24 = arith.constant 1 : index
              %c0_25 = arith.constant 0 : index
              %reinterpret_cast_26 = memref.reinterpret_cast %reinterpret_cast to offset: [%c0_25], sizes: [%49], strides: [%c1_24] : memref<?x?x?xf32, "gpu"> to memref<?xf32, "gpu">
              %50 = memref.load %reinterpret_cast_26[%46] : memref<?xf32, "gpu">
              %51 = math.absf %50 : f32
              %52 = arith.cmpf ugt, %arg4, %51 : f32
              %53 = arith.select %52, %arg4, %51 : f32
              %54 = arith.cmpf uno, %51, %51 : f32
              %55 = arith.select %54, %51, %53 : f32
              scf.yield %55 : f32
            } else {
              scf.yield %arg4 : f32
            }
            scf.yield %43 : f32
          }
          scf.yield %37 : f32
        } else {
          scf.yield %cst : f32
        }
        %c256_9 = arith.constant 256 : index
        %32 = "disc_shape.linearize"(%27, %c256_9) {operand_segment_sizes = array<i32: 1, 1>} : (index, index) -> index
        %reinterpret_cast_10 = memref.reinterpret_cast %alloc_8 to offset: [0], sizes: [256], strides: [1] : memref<256xf32, #gpu.address_space<workgroup>> to memref<256xf32, #gpu.address_space<workgroup>>
        memref.store %31, %reinterpret_cast_10[%32] : memref<256xf32, #gpu.address_space<workgroup>>
        gpu.barrier
        %33 = arith.cmpi slt, %24, %c4 : index
        scf.if %33 {
          %37 = arith.addi %27, %c4 : index
          %c256_11 = arith.constant 256 : index
          %38 = "disc_shape.linearize"(%27, %c256_11) {operand_segment_sizes = array<i32: 1, 1>} : (index, index) -> index
          %reinterpret_cast_12 = memref.reinterpret_cast %alloc_8 to offset: [0], sizes: [256], strides: [1] : memref<256xf32, #gpu.address_space<workgroup>> to memref<256xf32, #gpu.address_space<workgroup>>
          %39 = memref.load %reinterpret_cast_12[%38] : memref<256xf32, #gpu.address_space<workgroup>>
          %c256_13 = arith.constant 256 : index
          %40 = "disc_shape.linearize"(%37, %c256_13) {operand_segment_sizes = array<i32: 1, 1>} : (index, index) -> index
          %reinterpret_cast_14 = memref.reinterpret_cast %alloc_8 to offset: [0], sizes: [256], strides: [1] : memref<256xf32, #gpu.address_space<workgroup>> to memref<256xf32, #gpu.address_space<workgroup>>
          %41 = memref.load %reinterpret_cast_14[%40] : memref<256xf32, #gpu.address_space<workgroup>>
          %42 = arith.cmpf ugt, %39, %41 : f32
          %43 = arith.select %42, %39, %41 : f32
          %44 = arith.cmpf uno, %41, %41 : f32
          %45 = arith.select %44, %41, %43 : f32
          %c256_15 = arith.constant 256 : index
          %46 = "disc_shape.linearize"(%27, %c256_15) {operand_segment_sizes = array<i32: 1, 1>} : (index, index) -> index
          %reinterpret_cast_16 = memref.reinterpret_cast %alloc_8 to offset: [0], sizes: [256], strides: [1] : memref<256xf32, #gpu.address_space<workgroup>> to memref<256xf32, #gpu.address_space<workgroup>>
          memref.store %45, %reinterpret_cast_16[%46] : memref<256xf32, #gpu.address_space<workgroup>>
        }
        gpu.barrier
        %34 = arith.cmpi slt, %24, %c2 : index
        scf.if %34 {
          %37 = arith.addi %27, %c2 : index
          %c256_11 = arith.constant 256 : index
          %38 = "disc_shape.linearize"(%27, %c256_11) {operand_segment_sizes = array<i32: 1, 1>} : (index, index) -> index
          %reinterpret_cast_12 = memref.reinterpret_cast %alloc_8 to offset: [0], sizes: [256], strides: [1] : memref<256xf32, #gpu.address_space<workgroup>> to memref<256xf32, #gpu.address_space<workgroup>>
          %39 = memref.load %reinterpret_cast_12[%38] : memref<256xf32, #gpu.address_space<workgroup>>
          %c256_13 = arith.constant 256 : index
          %40 = "disc_shape.linearize"(%37, %c256_13) {operand_segment_sizes = array<i32: 1, 1>} : (index, index) -> index
          %reinterpret_cast_14 = memref.reinterpret_cast %alloc_8 to offset: [0], sizes: [256], strides: [1] : memref<256xf32, #gpu.address_space<workgroup>> to memref<256xf32, #gpu.address_space<workgroup>>
          %41 = memref.load %reinterpret_cast_14[%40] : memref<256xf32, #gpu.address_space<workgroup>>
          %42 = arith.cmpf ugt, %39, %41 : f32
          %43 = arith.select %42, %39, %41 : f32
          %44 = arith.cmpf uno, %41, %41 : f32
          %45 = arith.select %44, %41, %43 : f32
          %c256_15 = arith.constant 256 : index
          %46 = "disc_shape.linearize"(%27, %c256_15) {operand_segment_sizes = array<i32: 1, 1>} : (index, index) -> index
          %reinterpret_cast_16 = memref.reinterpret_cast %alloc_8 to offset: [0], sizes: [256], strides: [1] : memref<256xf32, #gpu.address_space<workgroup>> to memref<256xf32, #gpu.address_space<workgroup>>
          memref.store %45, %reinterpret_cast_16[%46] : memref<256xf32, #gpu.address_space<workgroup>>
        }
        gpu.barrier
        %35 = arith.cmpi eq, %24, %c0 : index
        %36 = arith.andi %35, %30 : i1
        scf.if %36 {
          %37 = arith.addi %27, %c1 : index
          %c256_11 = arith.constant 256 : index
          %38 = "disc_shape.linearize"(%27, %c256_11) {operand_segment_sizes = array<i32: 1, 1>} : (index, index) -> index
          %reinterpret_cast_12 = memref.reinterpret_cast %alloc_8 to offset: [0], sizes: [256], strides: [1] : memref<256xf32, #gpu.address_space<workgroup>> to memref<256xf32, #gpu.address_space<workgroup>>
          %39 = memref.load %reinterpret_cast_12[%38] : memref<256xf32, #gpu.address_space<workgroup>>
          %c256_13 = arith.constant 256 : index
          %40 = "disc_shape.linearize"(%37, %c256_13) {operand_segment_sizes = array<i32: 1, 1>} : (index, index) -> index
          %reinterpret_cast_14 = memref.reinterpret_cast %alloc_8 to offset: [0], sizes: [256], strides: [1] : memref<256xf32, #gpu.address_space<workgroup>> to memref<256xf32, #gpu.address_space<workgroup>>
          %41 = memref.load %reinterpret_cast_14[%40] : memref<256xf32, #gpu.address_space<workgroup>>
          %42 = arith.cmpf ugt, %39, %41 : f32
          %43 = arith.select %42, %39, %41 : f32
          %44 = arith.cmpf uno, %41, %41 : f32
          %45 = arith.select %44, %41, %43 : f32
          %46 = memref.generic_atomic_rmw %alloc_4[%29] : memref<?xf32, "gpu"> {
          ^bb0(%arg3: f32):
            %47 = arith.cmpf ogt, %arg3, %45 : f32
            %48 = arith.select %47, %arg3, %45 : f32
            memref.atomic_yield %48 : f32
          }
        }
        scf.yield
      }
      "lmhlo.terminator"() : () -> ()
    }) {disc.device = "gpu", disc.fusion.name = "main_kColReduction_reduce__4_1_0", disc.fusion.tag = "thin", disc.fusion_type = "kColReduction", disc_col_reduction_schedule_hint = 8 : i32, disc_thread_per_block_hint = 256 : i32} : () -> ()
  }
  memref.dealloc %alloc_3 : memref<?x?xf32, "gpu">
  memref.dealloc %alloc_2 : memref<?x?x?xf32, "gpu">
  memref.dealloc %alloc : memref<f32, "gpu">
  %alloca_5 = memref.alloca() : memref<2xi32, "cpu">
  memref.store %4, %alloca_5[%c0] : memref<2xi32, "cpu">
  memref.store %5, %alloca_5[%c1] : memref<2xi32, "cpu">
  %9 = llvm.inttoptr %0 : i32 to !llvm.ptr<i8>
  %alloca_6 = memref.alloca() : memref<2xindex, "cpu">
  memref.store %dim_0, %alloca_6[%c0] : memref<2xindex, "cpu">
  memref.store %dim, %alloca_6[%c1] : memref<2xindex, "cpu">
  %10 = "disc_ral.dispatch"(%arg0, %9, %alloc_4, %alloca_6) {backend_config = "", call_target_name = "inc_ref", device = "gpu", has_side_effect = false} : (!disc_ral.context, !llvm.ptr<i8>, memref<?xf32, "gpu">, memref<2xindex, "cpu">) -> memref<?x?xf32, "gpu">
  %reinterpret_cast_7 = memref.reinterpret_cast %10 to offset: [0], sizes: [%dim_0, %dim], strides: [%dim, 1] {kDiscSymbolicDimAttr = [@S1, @S2]} : memref<?x?xf32, "gpu"> to memref<?x?xf32, "gpu">
  memref.dealloc %alloc_4 : memref<?xf32, "gpu">
  "disc_ral.dispatch"(%arg0, %c0, %reinterpret_cast_7) {backend_config = "", call_target_name = "ral_send_output", device = "cpu", has_side_effect = false} : (!disc_ral.context, index, memref<?x?xf32, "gpu">) -> ()
  return
}

// -----// IR Dump After Canonicalizer (disc-canonicalize) //----- //
func.func @main(%arg0: !disc_ral.context) attributes {tf.entry_function = {input_placements = "gpu", inputs = "input0", output_placements = "gpu", outputs = "output0"}} {
  %cst = arith.constant 0xFF800000 : f32
  %c4 = arith.constant 4 : index
  %c256 = arith.constant 256 : index
  %c64 = arith.constant 64 : index
  %c8 = arith.constant 8 : index
  %c32 = arith.constant 32 : index
  %c512 = arith.constant 512 : index
  %0 = llvm.mlir.constant(0 : i32) : i32
  %c0 = arith.constant 0 : index
  %c1 = arith.constant 1 : index
  %c2 = arith.constant 2 : index
  %1 = "disc_ral.dispatch"(%arg0, %c0) {backend_config = "", call_target_name = "ral_recv_input", device = "cpu", has_side_effect = false} : (!disc_ral.context, index) -> memref<?x?x?xf32, "gpu">
  %dim = memref.dim %1, %c2 : memref<?x?x?xf32, "gpu">
  %dim_0 = memref.dim %1, %c1 : memref<?x?x?xf32, "gpu">
  %dim_1 = memref.dim %1, %c0 : memref<?x?x?xf32, "gpu">
  %2 = arith.index_cast %dim_0 : index to i32
  %3 = arith.index_cast %dim : index to i32
  %4 = arith.muli %2, %3 : i32
  %5 = arith.index_cast %4 : i32 to index
  %alloc = memref.alloc(%5) {kDiscSymbolicDimAttr = [@S3]} : memref<?xf32, "gpu">
  %6 = arith.cmpi slt, %dim_1, %5 : index
  scf.if %6 {
    "lmhlo.fusion"() ({
      scf.parallel (%arg1) = (%c0) to (%5) step (%c1) {
        %reinterpret_cast_2 = memref.reinterpret_cast %alloc to offset: [%c0], sizes: [%5], strides: [%c1] : memref<?xf32, "gpu"> to memref<?xf32, "gpu">
        memref.store %cst, %reinterpret_cast_2[%arg1] : memref<?xf32, "gpu">
        scf.yield
      }
      %9 = arith.cmpi eq, %5, %c0 : index
      %10 = arith.subi %5, %c1 : index
      %11 = arith.divui %10, %c512 : index
      %12 = arith.addi %11, %c1 : index
      %13 = arith.select %9, %c0, %12 : index
      %14 = arith.cmpi eq, %dim_1, %c0 : index
      %15 = arith.subi %dim_1, %c1 : index
      %16 = arith.divui %15, %c32 : index
      %17 = arith.addi %16, %c1 : index
      %18 = arith.select %14, %c0, %17 : index
      %19 = arith.muli %13, %18 : index
      scf.parallel (%arg1, %arg2) = (%c0, %c0) to (%19, %c512) step (%c1, %c1) {
        %20 = arith.divui %arg1, %13 : index
        %21 = arith.remui %arg1, %13 : index
        %22 = arith.muli %21, %c512 : index
        %23 = arith.addi %22, %arg2 : index
        %24 = arith.cmpi ult, %23, %5 : index
        scf.if %24 {
          %25 = scf.for %arg3 = %c0 to %c32 step %c1 iter_args(%arg4 = %cst) -> (f32) {
            %27 = arith.muli %20, %c32 : index
            %28 = arith.addi %27, %arg3 : index
            %29 = arith.cmpi slt, %28, %dim_1 : index
            %30 = scf.if %29 -> (f32) {
              %31 = "disc_shape.linearize"(%28, %23, %dim_1, %5) {operand_segment_sizes = array<i32: 2, 2>} : (index, index, index, index) -> index
              %32 = arith.muli %dim_1, %dim_0 : index
              %33 = arith.muli %32, %dim : index
              %reinterpret_cast_2 = memref.reinterpret_cast %1 to offset: [%c0], sizes: [%33], strides: [%c1] : memref<?x?x?xf32, "gpu"> to memref<?xf32, "gpu">
              %34 = memref.load %reinterpret_cast_2[%31] : memref<?xf32, "gpu">
              %35 = math.absf %34 : f32
              %36 = arith.cmpf oge, %arg4, %35 : f32
              %37 = arith.select %36, %arg4, %35 : f32
              scf.yield %37 : f32
            } else {
              scf.yield %arg4 : f32
            }
            scf.yield %30 : f32
          }
          %26 = memref.generic_atomic_rmw %alloc[%23] : memref<?xf32, "gpu"> {
          ^bb0(%arg3: f32):
            %27 = arith.cmpf ogt, %arg3, %25 : f32
            %28 = arith.select %27, %arg3, %25 : f32
            memref.atomic_yield %28 : f32
          }
        }
        scf.yield
      }
      "lmhlo.terminator"() : () -> ()
    }) {disc.device = "gpu", disc.fusion.name = "main_kColReduction_reduce__4_1_0", disc.fusion.tag = "flat", disc.fusion_type = "kColReduction", disc_col_reduction_schedule_hint = 7 : i32, disc_thread_per_block_hint = 512 : i32} : () -> ()
  } else {
    "lmhlo.fusion"() ({
      scf.parallel (%arg1) = (%c0) to (%5) step (%c1) {
        %reinterpret_cast_2 = memref.reinterpret_cast %alloc to offset: [%c0], sizes: [%5], strides: [%c1] : memref<?xf32, "gpu"> to memref<?xf32, "gpu">
        memref.store %cst, %reinterpret_cast_2[%arg1] : memref<?xf32, "gpu">
        scf.yield
      }
      %9 = arith.cmpi eq, %5, %c0 : index
      %10 = arith.subi %5, %c1 : index
      %11 = arith.divui %10, %c32 : index
      %12 = arith.addi %11, %c1 : index
      %13 = arith.select %9, %c0, %12 : index
      %14 = arith.cmpi eq, %dim_1, %c0 : index
      %15 = arith.subi %dim_1, %c1 : index
      %16 = arith.divui %15, %c512 : index
      %17 = arith.addi %16, %c1 : index
      %18 = arith.select %14, %c0, %17 : index
      %19 = arith.muli %13, %18 : index
      scf.parallel (%arg1, %arg2) = (%c0, %c0) to (%19, %c256) step (%c1, %c1) {
        %20 = arith.divui %arg1, %13 : index
        %21 = arith.remui %arg1, %13 : index
        %22 = arith.divui %arg2, %c32 : index
        %23 = arith.remui %arg2, %c32 : index
        %24 = arith.muli %23, %c8 : index
        %25 = arith.addi %22, %24 : index
        %26 = arith.muli %21, %c32 : index
        %27 = arith.addi %23, %26 : index
        %alloc_2 = memref.alloc() : memref<256xf32, #gpu.address_space<workgroup>>
        %28 = arith.cmpi ult, %27, %5 : index
        %29 = scf.if %28 -> (f32) {
          %35 = scf.for %arg3 = %c0 to %c64 step %c1 iter_args(%arg4 = %cst) -> (f32) {
            %36 = arith.muli %20, %c8 : index
            %37 = arith.addi %22, %36 : index
            %38 = arith.muli %37, %c64 : index
            %39 = arith.addi %arg3, %38 : index
            %40 = arith.cmpi slt, %39, %dim_1 : index
            %41 = scf.if %40 -> (f32) {
              %42 = "disc_shape.linearize"(%39, %27, %dim_1, %5) {operand_segment_sizes = array<i32: 2, 2>} : (index, index, index, index) -> index
              %43 = arith.muli %dim_1, %dim_0 : index
              %44 = arith.muli %43, %dim : index
              %reinterpret_cast_4 = memref.reinterpret_cast %1 to offset: [%c0], sizes: [%44], strides: [%c1] : memref<?x?x?xf32, "gpu"> to memref<?xf32, "gpu">
              %45 = memref.load %reinterpret_cast_4[%42] : memref<?xf32, "gpu">
              %46 = math.absf %45 : f32
              %47 = arith.cmpf ugt, %arg4, %46 : f32
              %48 = arith.select %47, %arg4, %46 : f32
              %49 = arith.cmpf uno, %46, %46 : f32
              %50 = arith.select %49, %46, %48 : f32
              scf.yield %50 : f32
            } else {
              scf.yield %arg4 : f32
            }
            scf.yield %41 : f32
          }
          scf.yield %35 : f32
        } else {
          scf.yield %cst : f32
        }
        %30 = "disc_shape.linearize"(%25, %c256) {operand_segment_sizes = array<i32: 1, 1>} : (index, index) -> index
        %reinterpret_cast_3 = memref.reinterpret_cast %alloc_2 to offset: [0], sizes: [256], strides: [1] : memref<256xf32, #gpu.address_space<workgroup>> to memref<256xf32, #gpu.address_space<workgroup>>
        memref.store %29, %reinterpret_cast_3[%30] : memref<256xf32, #gpu.address_space<workgroup>>
        gpu.barrier
        %31 = arith.cmpi slt, %22, %c4 : index
        scf.if %31 {
          %35 = arith.addi %25, %c4 : index
          %36 = "disc_shape.linearize"(%25, %c256) {operand_segment_sizes = array<i32: 1, 1>} : (index, index) -> index
          %reinterpret_cast_4 = memref.reinterpret_cast %alloc_2 to offset: [0], sizes: [256], strides: [1] : memref<256xf32, #gpu.address_space<workgroup>> to memref<256xf32, #gpu.address_space<workgroup>>
          %37 = memref.load %reinterpret_cast_4[%36] : memref<256xf32, #gpu.address_space<workgroup>>
          %38 = "disc_shape.linearize"(%35, %c256) {operand_segment_sizes = array<i32: 1, 1>} : (index, index) -> index
          %reinterpret_cast_5 = memref.reinterpret_cast %alloc_2 to offset: [0], sizes: [256], strides: [1] : memref<256xf32, #gpu.address_space<workgroup>> to memref<256xf32, #gpu.address_space<workgroup>>
          %39 = memref.load %reinterpret_cast_5[%38] : memref<256xf32, #gpu.address_space<workgroup>>
          %40 = arith.cmpf ugt, %37, %39 : f32
          %41 = arith.select %40, %37, %39 : f32
          %42 = arith.cmpf uno, %39, %39 : f32
          %43 = arith.select %42, %39, %41 : f32
          %44 = "disc_shape.linearize"(%25, %c256) {operand_segment_sizes = array<i32: 1, 1>} : (index, index) -> index
          %reinterpret_cast_6 = memref.reinterpret_cast %alloc_2 to offset: [0], sizes: [256], strides: [1] : memref<256xf32, #gpu.address_space<workgroup>> to memref<256xf32, #gpu.address_space<workgroup>>
          memref.store %43, %reinterpret_cast_6[%44] : memref<256xf32, #gpu.address_space<workgroup>>
        }
        gpu.barrier
        %32 = arith.cmpi slt, %22, %c2 : index
        scf.if %32 {
          %35 = arith.addi %25, %c2 : index
          %36 = "disc_shape.linearize"(%25, %c256) {operand_segment_sizes = array<i32: 1, 1>} : (index, index) -> index
          %reinterpret_cast_4 = memref.reinterpret_cast %alloc_2 to offset: [0], sizes: [256], strides: [1] : memref<256xf32, #gpu.address_space<workgroup>> to memref<256xf32, #gpu.address_space<workgroup>>
          %37 = memref.load %reinterpret_cast_4[%36] : memref<256xf32, #gpu.address_space<workgroup>>
          %38 = "disc_shape.linearize"(%35, %c256) {operand_segment_sizes = array<i32: 1, 1>} : (index, index) -> index
          %reinterpret_cast_5 = memref.reinterpret_cast %alloc_2 to offset: [0], sizes: [256], strides: [1] : memref<256xf32, #gpu.address_space<workgroup>> to memref<256xf32, #gpu.address_space<workgroup>>
          %39 = memref.load %reinterpret_cast_5[%38] : memref<256xf32, #gpu.address_space<workgroup>>
          %40 = arith.cmpf ugt, %37, %39 : f32
          %41 = arith.select %40, %37, %39 : f32
          %42 = arith.cmpf uno, %39, %39 : f32
          %43 = arith.select %42, %39, %41 : f32
          %44 = "disc_shape.linearize"(%25, %c256) {operand_segment_sizes = array<i32: 1, 1>} : (index, index) -> index
          %reinterpret_cast_6 = memref.reinterpret_cast %alloc_2 to offset: [0], sizes: [256], strides: [1] : memref<256xf32, #gpu.address_space<workgroup>> to memref<256xf32, #gpu.address_space<workgroup>>
          memref.store %43, %reinterpret_cast_6[%44] : memref<256xf32, #gpu.address_space<workgroup>>
        }
        gpu.barrier
        %33 = arith.cmpi eq, %22, %c0 : index
        %34 = arith.andi %33, %28 : i1
        scf.if %34 {
          %35 = arith.addi %25, %c1 : index
          %36 = "disc_shape.linearize"(%25, %c256) {operand_segment_sizes = array<i32: 1, 1>} : (index, index) -> index
          %reinterpret_cast_4 = memref.reinterpret_cast %alloc_2 to offset: [0], sizes: [256], strides: [1] : memref<256xf32, #gpu.address_space<workgroup>> to memref<256xf32, #gpu.address_space<workgroup>>
          %37 = memref.load %reinterpret_cast_4[%36] : memref<256xf32, #gpu.address_space<workgroup>>
          %38 = "disc_shape.linearize"(%35, %c256) {operand_segment_sizes = array<i32: 1, 1>} : (index, index) -> index
          %reinterpret_cast_5 = memref.reinterpret_cast %alloc_2 to offset: [0], sizes: [256], strides: [1] : memref<256xf32, #gpu.address_space<workgroup>> to memref<256xf32, #gpu.address_space<workgroup>>
          %39 = memref.load %reinterpret_cast_5[%38] : memref<256xf32, #gpu.address_space<workgroup>>
          %40 = arith.cmpf ugt, %37, %39 : f32
          %41 = arith.select %40, %37, %39 : f32
          %42 = arith.cmpf uno, %39, %39 : f32
          %43 = arith.select %42, %39, %41 : f32
          %44 = memref.generic_atomic_rmw %alloc[%27] : memref<?xf32, "gpu"> {
          ^bb0(%arg3: f32):
            %45 = arith.cmpf ogt, %arg3, %43 : f32
            %46 = arith.select %45, %arg3, %43 : f32
            memref.atomic_yield %46 : f32
          }
        }
        scf.yield
      }
      "lmhlo.terminator"() : () -> ()
    }) {disc.device = "gpu", disc.fusion.name = "main_kColReduction_reduce__4_1_0", disc.fusion.tag = "thin", disc.fusion_type = "kColReduction", disc_col_reduction_schedule_hint = 8 : i32, disc_thread_per_block_hint = 256 : i32} : () -> ()
  }
  %7 = llvm.inttoptr %0 : i32 to !llvm.ptr<i8>
  %alloca = memref.alloca() : memref<2xindex, "cpu">
  memref.store %dim_0, %alloca[%c0] : memref<2xindex, "cpu">
  memref.store %dim, %alloca[%c1] : memref<2xindex, "cpu">
  %8 = "disc_ral.dispatch"(%arg0, %7, %alloc, %alloca) {backend_config = "", call_target_name = "inc_ref", device = "gpu", has_side_effect = false} : (!disc_ral.context, !llvm.ptr<i8>, memref<?xf32, "gpu">, memref<2xindex, "cpu">) -> memref<?x?xf32, "gpu">
  %reinterpret_cast = memref.reinterpret_cast %8 to offset: [0], sizes: [%dim_0, %dim], strides: [%dim, 1] {kDiscSymbolicDimAttr = [@S1, @S2]} : memref<?x?xf32, "gpu"> to memref<?x?xf32, "gpu">
  memref.dealloc %alloc : memref<?xf32, "gpu">
  "disc_ral.dispatch"(%arg0, %c0, %reinterpret_cast) {backend_config = "", call_target_name = "ral_send_output", device = "cpu", has_side_effect = false} : (!disc_ral.context, index, memref<?x?xf32, "gpu">) -> ()
  return
}

// -----// IR Dump After CSE (cse) //----- //
func.func @main(%arg0: !disc_ral.context) attributes {tf.entry_function = {input_placements = "gpu", inputs = "input0", output_placements = "gpu", outputs = "output0"}} {
  %cst = arith.constant 0xFF800000 : f32
  %c4 = arith.constant 4 : index
  %c256 = arith.constant 256 : index
  %c64 = arith.constant 64 : index
  %c8 = arith.constant 8 : index
  %c32 = arith.constant 32 : index
  %c512 = arith.constant 512 : index
  %0 = llvm.mlir.constant(0 : i32) : i32
  %c0 = arith.constant 0 : index
  %c1 = arith.constant 1 : index
  %c2 = arith.constant 2 : index
  %1 = "disc_ral.dispatch"(%arg0, %c0) {backend_config = "", call_target_name = "ral_recv_input", device = "cpu", has_side_effect = false} : (!disc_ral.context, index) -> memref<?x?x?xf32, "gpu">
  %dim = memref.dim %1, %c2 : memref<?x?x?xf32, "gpu">
  %dim_0 = memref.dim %1, %c1 : memref<?x?x?xf32, "gpu">
  %dim_1 = memref.dim %1, %c0 : memref<?x?x?xf32, "gpu">
  %2 = arith.index_cast %dim_0 : index to i32
  %3 = arith.index_cast %dim : index to i32
  %4 = arith.muli %2, %3 : i32
  %5 = arith.index_cast %4 : i32 to index
  %alloc = memref.alloc(%5) {kDiscSymbolicDimAttr = [@S3]} : memref<?xf32, "gpu">
  %6 = arith.cmpi slt, %dim_1, %5 : index
  scf.if %6 {
    "lmhlo.fusion"() ({
      scf.parallel (%arg1) = (%c0) to (%5) step (%c1) {
        %reinterpret_cast_2 = memref.reinterpret_cast %alloc to offset: [%c0], sizes: [%5], strides: [%c1] : memref<?xf32, "gpu"> to memref<?xf32, "gpu">
        memref.store %cst, %reinterpret_cast_2[%arg1] : memref<?xf32, "gpu">
        scf.yield
      }
      %9 = arith.cmpi eq, %5, %c0 : index
      %10 = arith.subi %5, %c1 : index
      %11 = arith.divui %10, %c512 : index
      %12 = arith.addi %11, %c1 : index
      %13 = arith.select %9, %c0, %12 : index
      %14 = arith.cmpi eq, %dim_1, %c0 : index
      %15 = arith.subi %dim_1, %c1 : index
      %16 = arith.divui %15, %c32 : index
      %17 = arith.addi %16, %c1 : index
      %18 = arith.select %14, %c0, %17 : index
      %19 = arith.muli %13, %18 : index
      scf.parallel (%arg1, %arg2) = (%c0, %c0) to (%19, %c512) step (%c1, %c1) {
        %20 = arith.divui %arg1, %13 : index
        %21 = arith.remui %arg1, %13 : index
        %22 = arith.muli %21, %c512 : index
        %23 = arith.addi %22, %arg2 : index
        %24 = arith.cmpi ult, %23, %5 : index
        scf.if %24 {
          %25 = scf.for %arg3 = %c0 to %c32 step %c1 iter_args(%arg4 = %cst) -> (f32) {
            %27 = arith.muli %20, %c32 : index
            %28 = arith.addi %27, %arg3 : index
            %29 = arith.cmpi slt, %28, %dim_1 : index
            %30 = scf.if %29 -> (f32) {
              %31 = "disc_shape.linearize"(%28, %23, %dim_1, %5) {operand_segment_sizes = array<i32: 2, 2>} : (index, index, index, index) -> index
              %32 = arith.muli %dim_1, %dim_0 : index
              %33 = arith.muli %32, %dim : index
              %reinterpret_cast_2 = memref.reinterpret_cast %1 to offset: [%c0], sizes: [%33], strides: [%c1] : memref<?x?x?xf32, "gpu"> to memref<?xf32, "gpu">
              %34 = memref.load %reinterpret_cast_2[%31] : memref<?xf32, "gpu">
              %35 = math.absf %34 : f32
              %36 = arith.cmpf oge, %arg4, %35 : f32
              %37 = arith.select %36, %arg4, %35 : f32
              scf.yield %37 : f32
            } else {
              scf.yield %arg4 : f32
            }
            scf.yield %30 : f32
          }
          %26 = memref.generic_atomic_rmw %alloc[%23] : memref<?xf32, "gpu"> {
          ^bb0(%arg3: f32):
            %27 = arith.cmpf ogt, %arg3, %25 : f32
            %28 = arith.select %27, %arg3, %25 : f32
            memref.atomic_yield %28 : f32
          }
        }
        scf.yield
      }
      "lmhlo.terminator"() : () -> ()
    }) {disc.device = "gpu", disc.fusion.name = "main_kColReduction_reduce__4_1_0", disc.fusion.tag = "flat", disc.fusion_type = "kColReduction", disc_col_reduction_schedule_hint = 7 : i32, disc_thread_per_block_hint = 512 : i32} : () -> ()
  } else {
    "lmhlo.fusion"() ({
      scf.parallel (%arg1) = (%c0) to (%5) step (%c1) {
        %reinterpret_cast_2 = memref.reinterpret_cast %alloc to offset: [%c0], sizes: [%5], strides: [%c1] : memref<?xf32, "gpu"> to memref<?xf32, "gpu">
        memref.store %cst, %reinterpret_cast_2[%arg1] : memref<?xf32, "gpu">
        scf.yield
      }
      %9 = arith.cmpi eq, %5, %c0 : index
      %10 = arith.subi %5, %c1 : index
      %11 = arith.divui %10, %c32 : index
      %12 = arith.addi %11, %c1 : index
      %13 = arith.select %9, %c0, %12 : index
      %14 = arith.cmpi eq, %dim_1, %c0 : index
      %15 = arith.subi %dim_1, %c1 : index
      %16 = arith.divui %15, %c512 : index
      %17 = arith.addi %16, %c1 : index
      %18 = arith.select %14, %c0, %17 : index
      %19 = arith.muli %13, %18 : index
      scf.parallel (%arg1, %arg2) = (%c0, %c0) to (%19, %c256) step (%c1, %c1) {
        %20 = arith.divui %arg1, %13 : index
        %21 = arith.remui %arg1, %13 : index
        %22 = arith.divui %arg2, %c32 : index
        %23 = arith.remui %arg2, %c32 : index
        %24 = arith.muli %23, %c8 : index
        %25 = arith.addi %22, %24 : index
        %26 = arith.muli %21, %c32 : index
        %27 = arith.addi %23, %26 : index
        %alloc_2 = memref.alloc() : memref<256xf32, #gpu.address_space<workgroup>>
        %28 = arith.cmpi ult, %27, %5 : index
        %29 = scf.if %28 -> (f32) {
          %35 = scf.for %arg3 = %c0 to %c64 step %c1 iter_args(%arg4 = %cst) -> (f32) {
            %36 = arith.muli %20, %c8 : index
            %37 = arith.addi %22, %36 : index
            %38 = arith.muli %37, %c64 : index
            %39 = arith.addi %arg3, %38 : index
            %40 = arith.cmpi slt, %39, %dim_1 : index
            %41 = scf.if %40 -> (f32) {
              %42 = "disc_shape.linearize"(%39, %27, %dim_1, %5) {operand_segment_sizes = array<i32: 2, 2>} : (index, index, index, index) -> index
              %43 = arith.muli %dim_1, %dim_0 : index
              %44 = arith.muli %43, %dim : index
              %reinterpret_cast_4 = memref.reinterpret_cast %1 to offset: [%c0], sizes: [%44], strides: [%c1] : memref<?x?x?xf32, "gpu"> to memref<?xf32, "gpu">
              %45 = memref.load %reinterpret_cast_4[%42] : memref<?xf32, "gpu">
              %46 = math.absf %45 : f32
              %47 = arith.cmpf ugt, %arg4, %46 : f32
              %48 = arith.select %47, %arg4, %46 : f32
              %49 = arith.cmpf uno, %46, %46 : f32
              %50 = arith.select %49, %46, %48 : f32
              scf.yield %50 : f32
            } else {
              scf.yield %arg4 : f32
            }
            scf.yield %41 : f32
          }
          scf.yield %35 : f32
        } else {
          scf.yield %cst : f32
        }
        %30 = "disc_shape.linearize"(%25, %c256) {operand_segment_sizes = array<i32: 1, 1>} : (index, index) -> index
        %reinterpret_cast_3 = memref.reinterpret_cast %alloc_2 to offset: [0], sizes: [256], strides: [1] : memref<256xf32, #gpu.address_space<workgroup>> to memref<256xf32, #gpu.address_space<workgroup>>
        memref.store %29, %reinterpret_cast_3[%30] : memref<256xf32, #gpu.address_space<workgroup>>
        gpu.barrier
        %31 = arith.cmpi slt, %22, %c4 : index
        scf.if %31 {
          %35 = arith.addi %25, %c4 : index
          %36 = memref.load %reinterpret_cast_3[%30] : memref<256xf32, #gpu.address_space<workgroup>>
          %37 = "disc_shape.linearize"(%35, %c256) {operand_segment_sizes = array<i32: 1, 1>} : (index, index) -> index
          %38 = memref.load %reinterpret_cast_3[%37] : memref<256xf32, #gpu.address_space<workgroup>>
          %39 = arith.cmpf ugt, %36, %38 : f32
          %40 = arith.select %39, %36, %38 : f32
          %41 = arith.cmpf uno, %38, %38 : f32
          %42 = arith.select %41, %38, %40 : f32
          memref.store %42, %reinterpret_cast_3[%30] : memref<256xf32, #gpu.address_space<workgroup>>
        }
        gpu.barrier
        %32 = arith.cmpi slt, %22, %c2 : index
        scf.if %32 {
          %35 = arith.addi %25, %c2 : index
          %36 = memref.load %reinterpret_cast_3[%30] : memref<256xf32, #gpu.address_space<workgroup>>
          %37 = "disc_shape.linearize"(%35, %c256) {operand_segment_sizes = array<i32: 1, 1>} : (index, index) -> index
          %38 = memref.load %reinterpret_cast_3[%37] : memref<256xf32, #gpu.address_space<workgroup>>
          %39 = arith.cmpf ugt, %36, %38 : f32
          %40 = arith.select %39, %36, %38 : f32
          %41 = arith.cmpf uno, %38, %38 : f32
          %42 = arith.select %41, %38, %40 : f32
          memref.store %42, %reinterpret_cast_3[%30] : memref<256xf32, #gpu.address_space<workgroup>>
        }
        gpu.barrier
        %33 = arith.cmpi eq, %22, %c0 : index
        %34 = arith.andi %33, %28 : i1
        scf.if %34 {
          %35 = arith.addi %25, %c1 : index
          %36 = memref.load %reinterpret_cast_3[%30] : memref<256xf32, #gpu.address_space<workgroup>>
          %37 = "disc_shape.linearize"(%35, %c256) {operand_segment_sizes = array<i32: 1, 1>} : (index, index) -> index
          %38 = memref.load %reinterpret_cast_3[%37] : memref<256xf32, #gpu.address_space<workgroup>>
          %39 = arith.cmpf ugt, %36, %38 : f32
          %40 = arith.select %39, %36, %38 : f32
          %41 = arith.cmpf uno, %38, %38 : f32
          %42 = arith.select %41, %38, %40 : f32
          %43 = memref.generic_atomic_rmw %alloc[%27] : memref<?xf32, "gpu"> {
          ^bb0(%arg3: f32):
            %44 = arith.cmpf ogt, %arg3, %42 : f32
            %45 = arith.select %44, %arg3, %42 : f32
            memref.atomic_yield %45 : f32
          }
        }
        scf.yield
      }
      "lmhlo.terminator"() : () -> ()
    }) {disc.device = "gpu", disc.fusion.name = "main_kColReduction_reduce__4_1_0", disc.fusion.tag = "thin", disc.fusion_type = "kColReduction", disc_col_reduction_schedule_hint = 8 : i32, disc_thread_per_block_hint = 256 : i32} : () -> ()
  }
  %7 = llvm.inttoptr %0 : i32 to !llvm.ptr<i8>
  %alloca = memref.alloca() : memref<2xindex, "cpu">
  memref.store %dim_0, %alloca[%c0] : memref<2xindex, "cpu">
  memref.store %dim, %alloca[%c1] : memref<2xindex, "cpu">
  %8 = "disc_ral.dispatch"(%arg0, %7, %alloc, %alloca) {backend_config = "", call_target_name = "inc_ref", device = "gpu", has_side_effect = false} : (!disc_ral.context, !llvm.ptr<i8>, memref<?xf32, "gpu">, memref<2xindex, "cpu">) -> memref<?x?xf32, "gpu">
  %reinterpret_cast = memref.reinterpret_cast %8 to offset: [0], sizes: [%dim_0, %dim], strides: [%dim, 1] {kDiscSymbolicDimAttr = [@S1, @S2]} : memref<?x?xf32, "gpu"> to memref<?x?xf32, "gpu">
  memref.dealloc %alloc : memref<?xf32, "gpu">
  "disc_ral.dispatch"(%arg0, %c0, %reinterpret_cast) {backend_config = "", call_target_name = "ral_send_output", device = "cpu", has_side_effect = false} : (!disc_ral.context, index, memref<?x?xf32, "gpu">) -> ()
  return
}

// -----// IR Dump After ConvertShapeToStandardPass (disc-convert-shape-to-std) //----- //
func.func @main(%arg0: !disc_ral.context) attributes {tf.entry_function = {input_placements = "gpu", inputs = "input0", output_placements = "gpu", outputs = "output0"}} {
  %cst = arith.constant 0xFF800000 : f32
  %c4 = arith.constant 4 : index
  %c256 = arith.constant 256 : index
  %c64 = arith.constant 64 : index
  %c8 = arith.constant 8 : index
  %c32 = arith.constant 32 : index
  %c512 = arith.constant 512 : index
  %0 = llvm.mlir.constant(0 : i32) : i32
  %c0 = arith.constant 0 : index
  %c1 = arith.constant 1 : index
  %c2 = arith.constant 2 : index
  %1 = "disc_ral.dispatch"(%arg0, %c0) {backend_config = "", call_target_name = "ral_recv_input", device = "cpu", has_side_effect = false} : (!disc_ral.context, index) -> memref<?x?x?xf32, "gpu">
  %dim = memref.dim %1, %c2 : memref<?x?x?xf32, "gpu">
  %dim_0 = memref.dim %1, %c1 : memref<?x?x?xf32, "gpu">
  %dim_1 = memref.dim %1, %c0 : memref<?x?x?xf32, "gpu">
  %2 = arith.index_cast %dim_0 : index to i32
  %3 = arith.index_cast %dim : index to i32
  %4 = arith.muli %2, %3 : i32
  %5 = arith.index_cast %4 : i32 to index
  %alloc = memref.alloc(%5) {kDiscSymbolicDimAttr = [@S3]} : memref<?xf32, "gpu">
  %6 = arith.cmpi slt, %dim_1, %5 : index
  scf.if %6 {
    "lmhlo.fusion"() ({
      scf.parallel (%arg1) = (%c0) to (%5) step (%c1) {
        %reinterpret_cast_2 = memref.reinterpret_cast %alloc to offset: [%c0], sizes: [%5], strides: [%c1] : memref<?xf32, "gpu"> to memref<?xf32, "gpu">
        memref.store %cst, %reinterpret_cast_2[%arg1] : memref<?xf32, "gpu">
        scf.yield
      }
      %9 = arith.cmpi eq, %5, %c0 : index
      %10 = arith.subi %5, %c1 : index
      %11 = arith.divui %10, %c512 : index
      %12 = arith.addi %11, %c1 : index
      %13 = arith.select %9, %c0, %12 : index
      %14 = arith.cmpi eq, %dim_1, %c0 : index
      %15 = arith.subi %dim_1, %c1 : index
      %16 = arith.divui %15, %c32 : index
      %17 = arith.addi %16, %c1 : index
      %18 = arith.select %14, %c0, %17 : index
      %19 = arith.muli %13, %18 : index
      scf.parallel (%arg1, %arg2) = (%c0, %c0) to (%19, %c512) step (%c1, %c1) {
        %20 = arith.divui %arg1, %13 : index
        %21 = arith.remui %arg1, %13 : index
        %22 = arith.muli %21, %c512 : index
        %23 = arith.addi %22, %arg2 : index
        %24 = arith.cmpi ult, %23, %5 : index
        scf.if %24 {
          %25 = scf.for %arg3 = %c0 to %c32 step %c1 iter_args(%arg4 = %cst) -> (f32) {
            %27 = arith.muli %20, %c32 : index
            %28 = arith.addi %27, %arg3 : index
            %29 = arith.cmpi slt, %28, %dim_1 : index
            %30 = scf.if %29 -> (f32) {
              %c0_2 = arith.constant 0 : index
              %31 = arith.muli %28, %5 : index
              %32 = arith.addi %31, %23 : index
              %33 = arith.muli %dim_1, %dim_0 : index
              %34 = arith.muli %33, %dim : index
              %reinterpret_cast_3 = memref.reinterpret_cast %1 to offset: [%c0], sizes: [%34], strides: [%c1] : memref<?x?x?xf32, "gpu"> to memref<?xf32, "gpu">
              %35 = memref.load %reinterpret_cast_3[%32] : memref<?xf32, "gpu">
              %36 = math.absf %35 : f32
              %37 = arith.cmpf oge, %arg4, %36 : f32
              %38 = arith.select %37, %arg4, %36 : f32
              scf.yield %38 : f32
            } else {
              scf.yield %arg4 : f32
            }
            scf.yield %30 : f32
          }
          %26 = memref.generic_atomic_rmw %alloc[%23] : memref<?xf32, "gpu"> {
          ^bb0(%arg3: f32):
            %27 = arith.cmpf ogt, %arg3, %25 : f32
            %28 = arith.select %27, %arg3, %25 : f32
            memref.atomic_yield %28 : f32
          }
        }
        scf.yield
      }
      "lmhlo.terminator"() : () -> ()
    }) {disc.device = "gpu", disc.fusion.name = "main_kColReduction_reduce__4_1_0", disc.fusion.tag = "flat", disc.fusion_type = "kColReduction", disc_col_reduction_schedule_hint = 7 : i32, disc_thread_per_block_hint = 512 : i32} : () -> ()
  } else {
    "lmhlo.fusion"() ({
      scf.parallel (%arg1) = (%c0) to (%5) step (%c1) {
        %reinterpret_cast_2 = memref.reinterpret_cast %alloc to offset: [%c0], sizes: [%5], strides: [%c1] : memref<?xf32, "gpu"> to memref<?xf32, "gpu">
        memref.store %cst, %reinterpret_cast_2[%arg1] : memref<?xf32, "gpu">
        scf.yield
      }
      %9 = arith.cmpi eq, %5, %c0 : index
      %10 = arith.subi %5, %c1 : index
      %11 = arith.divui %10, %c32 : index
      %12 = arith.addi %11, %c1 : index
      %13 = arith.select %9, %c0, %12 : index
      %14 = arith.cmpi eq, %dim_1, %c0 : index
      %15 = arith.subi %dim_1, %c1 : index
      %16 = arith.divui %15, %c512 : index
      %17 = arith.addi %16, %c1 : index
      %18 = arith.select %14, %c0, %17 : index
      %19 = arith.muli %13, %18 : index
      scf.parallel (%arg1, %arg2) = (%c0, %c0) to (%19, %c256) step (%c1, %c1) {
        %20 = arith.divui %arg1, %13 : index
        %21 = arith.remui %arg1, %13 : index
        %22 = arith.divui %arg2, %c32 : index
        %23 = arith.remui %arg2, %c32 : index
        %24 = arith.muli %23, %c8 : index
        %25 = arith.addi %22, %24 : index
        %26 = arith.muli %21, %c32 : index
        %27 = arith.addi %23, %26 : index
        %alloc_2 = memref.alloc() : memref<256xf32, #gpu.address_space<workgroup>>
        %28 = arith.cmpi ult, %27, %5 : index
        %29 = scf.if %28 -> (f32) {
          %34 = scf.for %arg3 = %c0 to %c64 step %c1 iter_args(%arg4 = %cst) -> (f32) {
            %35 = arith.muli %20, %c8 : index
            %36 = arith.addi %22, %35 : index
            %37 = arith.muli %36, %c64 : index
            %38 = arith.addi %arg3, %37 : index
            %39 = arith.cmpi slt, %38, %dim_1 : index
            %40 = scf.if %39 -> (f32) {
              %c0_5 = arith.constant 0 : index
              %41 = arith.muli %38, %5 : index
              %42 = arith.addi %41, %27 : index
              %43 = arith.muli %dim_1, %dim_0 : index
              %44 = arith.muli %43, %dim : index
              %reinterpret_cast_6 = memref.reinterpret_cast %1 to offset: [%c0], sizes: [%44], strides: [%c1] : memref<?x?x?xf32, "gpu"> to memref<?xf32, "gpu">
              %45 = memref.load %reinterpret_cast_6[%42] : memref<?xf32, "gpu">
              %46 = math.absf %45 : f32
              %47 = arith.cmpf ugt, %arg4, %46 : f32
              %48 = arith.select %47, %arg4, %46 : f32
              %49 = arith.cmpf uno, %46, %46 : f32
              %50 = arith.select %49, %46, %48 : f32
              scf.yield %50 : f32
            } else {
              scf.yield %arg4 : f32
            }
            scf.yield %40 : f32
          }
          scf.yield %34 : f32
        } else {
          scf.yield %cst : f32
        }
        %c0_3 = arith.constant 0 : index
        %reinterpret_cast_4 = memref.reinterpret_cast %alloc_2 to offset: [0], sizes: [256], strides: [1] : memref<256xf32, #gpu.address_space<workgroup>> to memref<256xf32, #gpu.address_space<workgroup>>
        memref.store %29, %reinterpret_cast_4[%25] : memref<256xf32, #gpu.address_space<workgroup>>
        gpu.barrier
        %30 = arith.cmpi slt, %22, %c4 : index
        scf.if %30 {
          %34 = arith.addi %25, %c4 : index
          %35 = memref.load %reinterpret_cast_4[%25] : memref<256xf32, #gpu.address_space<workgroup>>
          %c0_5 = arith.constant 0 : index
          %36 = memref.load %reinterpret_cast_4[%34] : memref<256xf32, #gpu.address_space<workgroup>>
          %37 = arith.cmpf ugt, %35, %36 : f32
          %38 = arith.select %37, %35, %36 : f32
          %39 = arith.cmpf uno, %36, %36 : f32
          %40 = arith.select %39, %36, %38 : f32
          memref.store %40, %reinterpret_cast_4[%25] : memref<256xf32, #gpu.address_space<workgroup>>
        }
        gpu.barrier
        %31 = arith.cmpi slt, %22, %c2 : index
        scf.if %31 {
          %34 = arith.addi %25, %c2 : index
          %35 = memref.load %reinterpret_cast_4[%25] : memref<256xf32, #gpu.address_space<workgroup>>
          %c0_5 = arith.constant 0 : index
          %36 = memref.load %reinterpret_cast_4[%34] : memref<256xf32, #gpu.address_space<workgroup>>
          %37 = arith.cmpf ugt, %35, %36 : f32
          %38 = arith.select %37, %35, %36 : f32
          %39 = arith.cmpf uno, %36, %36 : f32
          %40 = arith.select %39, %36, %38 : f32
          memref.store %40, %reinterpret_cast_4[%25] : memref<256xf32, #gpu.address_space<workgroup>>
        }
        gpu.barrier
        %32 = arith.cmpi eq, %22, %c0 : index
        %33 = arith.andi %32, %28 : i1
        scf.if %33 {
          %34 = arith.addi %25, %c1 : index
          %35 = memref.load %reinterpret_cast_4[%25] : memref<256xf32, #gpu.address_space<workgroup>>
          %c0_5 = arith.constant 0 : index
          %36 = memref.load %reinterpret_cast_4[%34] : memref<256xf32, #gpu.address_space<workgroup>>
          %37 = arith.cmpf ugt, %35, %36 : f32
          %38 = arith.select %37, %35, %36 : f32
          %39 = arith.cmpf uno, %36, %36 : f32
          %40 = arith.select %39, %36, %38 : f32
          %41 = memref.generic_atomic_rmw %alloc[%27] : memref<?xf32, "gpu"> {
          ^bb0(%arg3: f32):
            %42 = arith.cmpf ogt, %arg3, %40 : f32
            %43 = arith.select %42, %arg3, %40 : f32
            memref.atomic_yield %43 : f32
          }
        }
        scf.yield
      }
      "lmhlo.terminator"() : () -> ()
    }) {disc.device = "gpu", disc.fusion.name = "main_kColReduction_reduce__4_1_0", disc.fusion.tag = "thin", disc.fusion_type = "kColReduction", disc_col_reduction_schedule_hint = 8 : i32, disc_thread_per_block_hint = 256 : i32} : () -> ()
  }
  %7 = llvm.inttoptr %0 : i32 to !llvm.ptr<i8>
  %alloca = memref.alloca() : memref<2xindex, "cpu">
  memref.store %dim_0, %alloca[%c0] : memref<2xindex, "cpu">
  memref.store %dim, %alloca[%c1] : memref<2xindex, "cpu">
  %8 = "disc_ral.dispatch"(%arg0, %7, %alloc, %alloca) {backend_config = "", call_target_name = "inc_ref", device = "gpu", has_side_effect = false} : (!disc_ral.context, !llvm.ptr<i8>, memref<?xf32, "gpu">, memref<2xindex, "cpu">) -> memref<?x?xf32, "gpu">
  %reinterpret_cast = memref.reinterpret_cast %8 to offset: [0], sizes: [%dim_0, %dim], strides: [%dim, 1] {kDiscSymbolicDimAttr = [@S1, @S2]} : memref<?x?xf32, "gpu"> to memref<?x?xf32, "gpu">
  memref.dealloc %alloc : memref<?xf32, "gpu">
  "disc_ral.dispatch"(%arg0, %c0, %reinterpret_cast) {backend_config = "", call_target_name = "ral_send_output", device = "cpu", has_side_effect = false} : (!disc_ral.context, index, memref<?x?xf32, "gpu">) -> ()
  return
}

// -----// IR Dump After Canonicalizer (disc-canonicalize) //----- //
func.func @main(%arg0: !disc_ral.context) attributes {tf.entry_function = {input_placements = "gpu", inputs = "input0", output_placements = "gpu", outputs = "output0"}} {
  %cst = arith.constant 0xFF800000 : f32
  %c4 = arith.constant 4 : index
  %c256 = arith.constant 256 : index
  %c64 = arith.constant 64 : index
  %c8 = arith.constant 8 : index
  %c32 = arith.constant 32 : index
  %c512 = arith.constant 512 : index
  %0 = llvm.mlir.constant(0 : i32) : i32
  %c0 = arith.constant 0 : index
  %c1 = arith.constant 1 : index
  %c2 = arith.constant 2 : index
  %1 = "disc_ral.dispatch"(%arg0, %c0) {backend_config = "", call_target_name = "ral_recv_input", device = "cpu", has_side_effect = false} : (!disc_ral.context, index) -> memref<?x?x?xf32, "gpu">
  %dim = memref.dim %1, %c2 : memref<?x?x?xf32, "gpu">
  %dim_0 = memref.dim %1, %c1 : memref<?x?x?xf32, "gpu">
  %dim_1 = memref.dim %1, %c0 : memref<?x?x?xf32, "gpu">
  %2 = arith.index_cast %dim_0 : index to i32
  %3 = arith.index_cast %dim : index to i32
  %4 = arith.muli %2, %3 : i32
  %5 = arith.index_cast %4 : i32 to index
  %alloc = memref.alloc(%5) {kDiscSymbolicDimAttr = [@S3]} : memref<?xf32, "gpu">
  %6 = arith.cmpi slt, %dim_1, %5 : index
  scf.if %6 {
    "lmhlo.fusion"() ({
      scf.parallel (%arg1) = (%c0) to (%5) step (%c1) {
        %reinterpret_cast_2 = memref.reinterpret_cast %alloc to offset: [%c0], sizes: [%5], strides: [%c1] : memref<?xf32, "gpu"> to memref<?xf32, "gpu">
        memref.store %cst, %reinterpret_cast_2[%arg1] : memref<?xf32, "gpu">
        scf.yield
      }
      %9 = arith.cmpi eq, %5, %c0 : index
      %10 = arith.subi %5, %c1 : index
      %11 = arith.divui %10, %c512 : index
      %12 = arith.addi %11, %c1 : index
      %13 = arith.select %9, %c0, %12 : index
      %14 = arith.cmpi eq, %dim_1, %c0 : index
      %15 = arith.subi %dim_1, %c1 : index
      %16 = arith.divui %15, %c32 : index
      %17 = arith.addi %16, %c1 : index
      %18 = arith.select %14, %c0, %17 : index
      %19 = arith.muli %13, %18 : index
      scf.parallel (%arg1, %arg2) = (%c0, %c0) to (%19, %c512) step (%c1, %c1) {
        %20 = arith.divui %arg1, %13 : index
        %21 = arith.remui %arg1, %13 : index
        %22 = arith.muli %21, %c512 : index
        %23 = arith.addi %22, %arg2 : index
        %24 = arith.cmpi ult, %23, %5 : index
        scf.if %24 {
          %25 = scf.for %arg3 = %c0 to %c32 step %c1 iter_args(%arg4 = %cst) -> (f32) {
            %27 = arith.muli %20, %c32 : index
            %28 = arith.addi %27, %arg3 : index
            %29 = arith.cmpi slt, %28, %dim_1 : index
            %30 = scf.if %29 -> (f32) {
              %31 = arith.muli %28, %5 : index
              %32 = arith.addi %31, %23 : index
              %33 = arith.muli %dim_1, %dim_0 : index
              %34 = arith.muli %33, %dim : index
              %reinterpret_cast_2 = memref.reinterpret_cast %1 to offset: [%c0], sizes: [%34], strides: [%c1] : memref<?x?x?xf32, "gpu"> to memref<?xf32, "gpu">
              %35 = memref.load %reinterpret_cast_2[%32] : memref<?xf32, "gpu">
              %36 = math.absf %35 : f32
              %37 = arith.cmpf oge, %arg4, %36 : f32
              %38 = arith.select %37, %arg4, %36 : f32
              scf.yield %38 : f32
            } else {
              scf.yield %arg4 : f32
            }
            scf.yield %30 : f32
          }
          %26 = memref.generic_atomic_rmw %alloc[%23] : memref<?xf32, "gpu"> {
          ^bb0(%arg3: f32):
            %27 = arith.cmpf ogt, %arg3, %25 : f32
            %28 = arith.select %27, %arg3, %25 : f32
            memref.atomic_yield %28 : f32
          }
        }
        scf.yield
      }
      "lmhlo.terminator"() : () -> ()
    }) {disc.device = "gpu", disc.fusion.name = "main_kColReduction_reduce__4_1_0", disc.fusion.tag = "flat", disc.fusion_type = "kColReduction", disc_col_reduction_schedule_hint = 7 : i32, disc_thread_per_block_hint = 512 : i32} : () -> ()
  } else {
    "lmhlo.fusion"() ({
      scf.parallel (%arg1) = (%c0) to (%5) step (%c1) {
        %reinterpret_cast_2 = memref.reinterpret_cast %alloc to offset: [%c0], sizes: [%5], strides: [%c1] : memref<?xf32, "gpu"> to memref<?xf32, "gpu">
        memref.store %cst, %reinterpret_cast_2[%arg1] : memref<?xf32, "gpu">
        scf.yield
      }
      %9 = arith.cmpi eq, %5, %c0 : index
      %10 = arith.subi %5, %c1 : index
      %11 = arith.divui %10, %c32 : index
      %12 = arith.addi %11, %c1 : index
      %13 = arith.select %9, %c0, %12 : index
      %14 = arith.cmpi eq, %dim_1, %c0 : index
      %15 = arith.subi %dim_1, %c1 : index
      %16 = arith.divui %15, %c512 : index
      %17 = arith.addi %16, %c1 : index
      %18 = arith.select %14, %c0, %17 : index
      %19 = arith.muli %13, %18 : index
      scf.parallel (%arg1, %arg2) = (%c0, %c0) to (%19, %c256) step (%c1, %c1) {
        %20 = arith.divui %arg1, %13 : index
        %21 = arith.remui %arg1, %13 : index
        %22 = arith.divui %arg2, %c32 : index
        %23 = arith.remui %arg2, %c32 : index
        %24 = arith.muli %23, %c8 : index
        %25 = arith.addi %22, %24 : index
        %26 = arith.muli %21, %c32 : index
        %27 = arith.addi %23, %26 : index
        %alloc_2 = memref.alloc() : memref<256xf32, #gpu.address_space<workgroup>>
        %28 = arith.cmpi ult, %27, %5 : index
        %29 = scf.if %28 -> (f32) {
          %34 = scf.for %arg3 = %c0 to %c64 step %c1 iter_args(%arg4 = %cst) -> (f32) {
            %35 = arith.muli %20, %c8 : index
            %36 = arith.addi %22, %35 : index
            %37 = arith.muli %36, %c64 : index
            %38 = arith.addi %arg3, %37 : index
            %39 = arith.cmpi slt, %38, %dim_1 : index
            %40 = scf.if %39 -> (f32) {
              %41 = arith.muli %38, %5 : index
              %42 = arith.addi %41, %27 : index
              %43 = arith.muli %dim_1, %dim_0 : index
              %44 = arith.muli %43, %dim : index
              %reinterpret_cast_4 = memref.reinterpret_cast %1 to offset: [%c0], sizes: [%44], strides: [%c1] : memref<?x?x?xf32, "gpu"> to memref<?xf32, "gpu">
              %45 = memref.load %reinterpret_cast_4[%42] : memref<?xf32, "gpu">
              %46 = math.absf %45 : f32
              %47 = arith.cmpf ugt, %arg4, %46 : f32
              %48 = arith.select %47, %arg4, %46 : f32
              %49 = arith.cmpf uno, %46, %46 : f32
              %50 = arith.select %49, %46, %48 : f32
              scf.yield %50 : f32
            } else {
              scf.yield %arg4 : f32
            }
            scf.yield %40 : f32
          }
          scf.yield %34 : f32
        } else {
          scf.yield %cst : f32
        }
        %reinterpret_cast_3 = memref.reinterpret_cast %alloc_2 to offset: [0], sizes: [256], strides: [1] : memref<256xf32, #gpu.address_space<workgroup>> to memref<256xf32, #gpu.address_space<workgroup>>
        memref.store %29, %reinterpret_cast_3[%25] : memref<256xf32, #gpu.address_space<workgroup>>
        gpu.barrier
        %30 = arith.cmpi slt, %22, %c4 : index
        scf.if %30 {
          %34 = arith.addi %25, %c4 : index
          %35 = memref.load %reinterpret_cast_3[%25] : memref<256xf32, #gpu.address_space<workgroup>>
          %36 = memref.load %reinterpret_cast_3[%34] : memref<256xf32, #gpu.address_space<workgroup>>
          %37 = arith.cmpf ugt, %35, %36 : f32
          %38 = arith.select %37, %35, %36 : f32
          %39 = arith.cmpf uno, %36, %36 : f32
          %40 = arith.select %39, %36, %38 : f32
          memref.store %40, %reinterpret_cast_3[%25] : memref<256xf32, #gpu.address_space<workgroup>>
        }
        gpu.barrier
        %31 = arith.cmpi slt, %22, %c2 : index
        scf.if %31 {
          %34 = arith.addi %25, %c2 : index
          %35 = memref.load %reinterpret_cast_3[%25] : memref<256xf32, #gpu.address_space<workgroup>>
          %36 = memref.load %reinterpret_cast_3[%34] : memref<256xf32, #gpu.address_space<workgroup>>
          %37 = arith.cmpf ugt, %35, %36 : f32
          %38 = arith.select %37, %35, %36 : f32
          %39 = arith.cmpf uno, %36, %36 : f32
          %40 = arith.select %39, %36, %38 : f32
          memref.store %40, %reinterpret_cast_3[%25] : memref<256xf32, #gpu.address_space<workgroup>>
        }
        gpu.barrier
        %32 = arith.cmpi eq, %22, %c0 : index
        %33 = arith.andi %32, %28 : i1
        scf.if %33 {
          %34 = arith.addi %25, %c1 : index
          %35 = memref.load %reinterpret_cast_3[%25] : memref<256xf32, #gpu.address_space<workgroup>>
          %36 = memref.load %reinterpret_cast_3[%34] : memref<256xf32, #gpu.address_space<workgroup>>
          %37 = arith.cmpf ugt, %35, %36 : f32
          %38 = arith.select %37, %35, %36 : f32
          %39 = arith.cmpf uno, %36, %36 : f32
          %40 = arith.select %39, %36, %38 : f32
          %41 = memref.generic_atomic_rmw %alloc[%27] : memref<?xf32, "gpu"> {
          ^bb0(%arg3: f32):
            %42 = arith.cmpf ogt, %arg3, %40 : f32
            %43 = arith.select %42, %arg3, %40 : f32
            memref.atomic_yield %43 : f32
          }
        }
        scf.yield
      }
      "lmhlo.terminator"() : () -> ()
    }) {disc.device = "gpu", disc.fusion.name = "main_kColReduction_reduce__4_1_0", disc.fusion.tag = "thin", disc.fusion_type = "kColReduction", disc_col_reduction_schedule_hint = 8 : i32, disc_thread_per_block_hint = 256 : i32} : () -> ()
  }
  %7 = llvm.inttoptr %0 : i32 to !llvm.ptr<i8>
  %alloca = memref.alloca() : memref<2xindex, "cpu">
  memref.store %dim_0, %alloca[%c0] : memref<2xindex, "cpu">
  memref.store %dim, %alloca[%c1] : memref<2xindex, "cpu">
  %8 = "disc_ral.dispatch"(%arg0, %7, %alloc, %alloca) {backend_config = "", call_target_name = "inc_ref", device = "gpu", has_side_effect = false} : (!disc_ral.context, !llvm.ptr<i8>, memref<?xf32, "gpu">, memref<2xindex, "cpu">) -> memref<?x?xf32, "gpu">
  %reinterpret_cast = memref.reinterpret_cast %8 to offset: [0], sizes: [%dim_0, %dim], strides: [%dim, 1] {kDiscSymbolicDimAttr = [@S1, @S2]} : memref<?x?xf32, "gpu"> to memref<?x?xf32, "gpu">
  memref.dealloc %alloc : memref<?xf32, "gpu">
  "disc_ral.dispatch"(%arg0, %c0, %reinterpret_cast) {backend_config = "", call_target_name = "ral_send_output", device = "cpu", has_side_effect = false} : (!disc_ral.context, index, memref<?x?xf32, "gpu">) -> ()
  return
}

// -----// IR Dump After ParallelLoopCollapsing (disc-parallel-loop-collapsing) //----- //
func.func @main(%arg0: !disc_ral.context) attributes {tf.entry_function = {input_placements = "gpu", inputs = "input0", output_placements = "gpu", outputs = "output0"}} {
  %cst = arith.constant 0xFF800000 : f32
  %c4 = arith.constant 4 : index
  %c256 = arith.constant 256 : index
  %c64 = arith.constant 64 : index
  %c8 = arith.constant 8 : index
  %c32 = arith.constant 32 : index
  %c512 = arith.constant 512 : index
  %0 = llvm.mlir.constant(0 : i32) : i32
  %c0 = arith.constant 0 : index
  %c1 = arith.constant 1 : index
  %c2 = arith.constant 2 : index
  %1 = "disc_ral.dispatch"(%arg0, %c0) {backend_config = "", call_target_name = "ral_recv_input", device = "cpu", has_side_effect = false} : (!disc_ral.context, index) -> memref<?x?x?xf32, "gpu">
  %dim = memref.dim %1, %c2 : memref<?x?x?xf32, "gpu">
  %dim_0 = memref.dim %1, %c1 : memref<?x?x?xf32, "gpu">
  %dim_1 = memref.dim %1, %c0 : memref<?x?x?xf32, "gpu">
  %2 = arith.index_cast %dim_0 : index to i32
  %3 = arith.index_cast %dim : index to i32
  %4 = arith.muli %2, %3 : i32
  %5 = arith.index_cast %4 : i32 to index
  %alloc = memref.alloc(%5) {kDiscSymbolicDimAttr = [@S3]} : memref<?xf32, "gpu">
  %6 = arith.cmpi slt, %dim_1, %5 : index
  scf.if %6 {
    "lmhlo.fusion"() ({
      scf.parallel (%arg1) = (%c0) to (%5) step (%c1) {
        %reinterpret_cast_5 = memref.reinterpret_cast %alloc to offset: [%c0], sizes: [%5], strides: [%c1] : memref<?xf32, "gpu"> to memref<?xf32, "gpu">
        memref.store %cst, %reinterpret_cast_5[%arg1] : memref<?xf32, "gpu">
        scf.yield
      }
      %9 = arith.cmpi eq, %5, %c0 : index
      %10 = arith.subi %5, %c1 : index
      %11 = arith.divui %10, %c512 : index
      %12 = arith.addi %11, %c1 : index
      %13 = arith.select %9, %c0, %12 : index
      %14 = arith.cmpi eq, %dim_1, %c0 : index
      %15 = arith.subi %dim_1, %c1 : index
      %16 = arith.divui %15, %c32 : index
      %17 = arith.addi %16, %c1 : index
      %18 = arith.select %14, %c0, %17 : index
      %19 = arith.muli %13, %18 : index
      %c0_2 = arith.constant 0 : index
      %c1_3 = arith.constant 1 : index
      %c1_4 = arith.constant 1 : index
      %20 = arith.muli %c1_4, %19 : index
      %21 = arith.muli %20, %c512 : index
      scf.parallel (%arg1) = (%c0_2) to (%21) step (%c1_3) {
        %22 = arith.remsi %arg1, %c512 : index
        %23 = arith.divsi %arg1, %c512 : index
        %24 = arith.divui %23, %13 : index
        %25 = arith.remui %23, %13 : index
        %26 = arith.muli %25, %c512 : index
        %27 = arith.addi %26, %22 : index
        %28 = arith.cmpi ult, %27, %5 : index
        scf.if %28 {
          %29 = scf.for %arg2 = %c0 to %c32 step %c1 iter_args(%arg3 = %cst) -> (f32) {
            %31 = arith.muli %24, %c32 : index
            %32 = arith.addi %31, %arg2 : index
            %33 = arith.cmpi slt, %32, %dim_1 : index
            %34 = scf.if %33 -> (f32) {
              %35 = arith.muli %32, %5 : index
              %36 = arith.addi %35, %27 : index
              %37 = arith.muli %dim_1, %dim_0 : index
              %38 = arith.muli %37, %dim : index
              %reinterpret_cast_5 = memref.reinterpret_cast %1 to offset: [%c0], sizes: [%38], strides: [%c1] : memref<?x?x?xf32, "gpu"> to memref<?xf32, "gpu">
              %39 = memref.load %reinterpret_cast_5[%36] : memref<?xf32, "gpu">
              %40 = math.absf %39 : f32
              %41 = arith.cmpf oge, %arg3, %40 : f32
              %42 = arith.select %41, %arg3, %40 : f32
              scf.yield %42 : f32
            } else {
              scf.yield %arg3 : f32
            }
            scf.yield %34 : f32
          }
          %30 = memref.generic_atomic_rmw %alloc[%27] : memref<?xf32, "gpu"> {
          ^bb0(%arg2: f32):
            %31 = arith.cmpf ogt, %arg2, %29 : f32
            %32 = arith.select %31, %arg2, %29 : f32
            memref.atomic_yield %32 : f32
          }
        }
        scf.yield
      }
      "lmhlo.terminator"() : () -> ()
    }) {disc.device = "gpu", disc.fusion.name = "main_kColReduction_reduce__4_1_0", disc.fusion.tag = "flat", disc.fusion_type = "kColReduction", disc_col_reduction_schedule_hint = 7 : i32, disc_thread_per_block_hint = 512 : i32} : () -> ()
  } else {
    "lmhlo.fusion"() ({
      scf.parallel (%arg1) = (%c0) to (%5) step (%c1) {
        %reinterpret_cast_5 = memref.reinterpret_cast %alloc to offset: [%c0], sizes: [%5], strides: [%c1] : memref<?xf32, "gpu"> to memref<?xf32, "gpu">
        memref.store %cst, %reinterpret_cast_5[%arg1] : memref<?xf32, "gpu">
        scf.yield
      }
      %9 = arith.cmpi eq, %5, %c0 : index
      %10 = arith.subi %5, %c1 : index
      %11 = arith.divui %10, %c32 : index
      %12 = arith.addi %11, %c1 : index
      %13 = arith.select %9, %c0, %12 : index
      %14 = arith.cmpi eq, %dim_1, %c0 : index
      %15 = arith.subi %dim_1, %c1 : index
      %16 = arith.divui %15, %c512 : index
      %17 = arith.addi %16, %c1 : index
      %18 = arith.select %14, %c0, %17 : index
      %19 = arith.muli %13, %18 : index
      %c0_2 = arith.constant 0 : index
      %c1_3 = arith.constant 1 : index
      %c1_4 = arith.constant 1 : index
      %20 = arith.muli %c1_4, %19 : index
      %21 = arith.muli %20, %c256 : index
      scf.parallel (%arg1) = (%c0_2) to (%21) step (%c1_3) {
        %22 = arith.remsi %arg1, %c256 : index
        %23 = arith.divsi %arg1, %c256 : index
        %24 = arith.divui %23, %13 : index
        %25 = arith.remui %23, %13 : index
        %26 = arith.divui %22, %c32 : index
        %27 = arith.remui %22, %c32 : index
        %28 = arith.muli %27, %c8 : index
        %29 = arith.addi %26, %28 : index
        %30 = arith.muli %25, %c32 : index
        %31 = arith.addi %27, %30 : index
        %alloc_5 = memref.alloc() : memref<256xf32, #gpu.address_space<workgroup>>
        %32 = arith.cmpi ult, %31, %5 : index
        %33 = scf.if %32 -> (f32) {
          %38 = scf.for %arg2 = %c0 to %c64 step %c1 iter_args(%arg3 = %cst) -> (f32) {
            %39 = arith.muli %24, %c8 : index
            %40 = arith.addi %26, %39 : index
            %41 = arith.muli %40, %c64 : index
            %42 = arith.addi %arg2, %41 : index
            %43 = arith.cmpi slt, %42, %dim_1 : index
            %44 = scf.if %43 -> (f32) {
              %45 = arith.muli %42, %5 : index
              %46 = arith.addi %45, %31 : index
              %47 = arith.muli %dim_1, %dim_0 : index
              %48 = arith.muli %47, %dim : index
              %reinterpret_cast_7 = memref.reinterpret_cast %1 to offset: [%c0], sizes: [%48], strides: [%c1] : memref<?x?x?xf32, "gpu"> to memref<?xf32, "gpu">
              %49 = memref.load %reinterpret_cast_7[%46] : memref<?xf32, "gpu">
              %50 = math.absf %49 : f32
              %51 = arith.cmpf ugt, %arg3, %50 : f32
              %52 = arith.select %51, %arg3, %50 : f32
              %53 = arith.cmpf uno, %50, %50 : f32
              %54 = arith.select %53, %50, %52 : f32
              scf.yield %54 : f32
            } else {
              scf.yield %arg3 : f32
            }
            scf.yield %44 : f32
          }
          scf.yield %38 : f32
        } else {
          scf.yield %cst : f32
        }
        %reinterpret_cast_6 = memref.reinterpret_cast %alloc_5 to offset: [0], sizes: [256], strides: [1] : memref<256xf32, #gpu.address_space<workgroup>> to memref<256xf32, #gpu.address_space<workgroup>>
        memref.store %33, %reinterpret_cast_6[%29] : memref<256xf32, #gpu.address_space<workgroup>>
        gpu.barrier
        %34 = arith.cmpi slt, %26, %c4 : index
        scf.if %34 {
          %38 = arith.addi %29, %c4 : index
          %39 = memref.load %reinterpret_cast_6[%29] : memref<256xf32, #gpu.address_space<workgroup>>
          %40 = memref.load %reinterpret_cast_6[%38] : memref<256xf32, #gpu.address_space<workgroup>>
          %41 = arith.cmpf ugt, %39, %40 : f32
          %42 = arith.select %41, %39, %40 : f32
          %43 = arith.cmpf uno, %40, %40 : f32
          %44 = arith.select %43, %40, %42 : f32
          memref.store %44, %reinterpret_cast_6[%29] : memref<256xf32, #gpu.address_space<workgroup>>
        }
        gpu.barrier
        %35 = arith.cmpi slt, %26, %c2 : index
        scf.if %35 {
          %38 = arith.addi %29, %c2 : index
          %39 = memref.load %reinterpret_cast_6[%29] : memref<256xf32, #gpu.address_space<workgroup>>
          %40 = memref.load %reinterpret_cast_6[%38] : memref<256xf32, #gpu.address_space<workgroup>>
          %41 = arith.cmpf ugt, %39, %40 : f32
          %42 = arith.select %41, %39, %40 : f32
          %43 = arith.cmpf uno, %40, %40 : f32
          %44 = arith.select %43, %40, %42 : f32
          memref.store %44, %reinterpret_cast_6[%29] : memref<256xf32, #gpu.address_space<workgroup>>
        }
        gpu.barrier
        %36 = arith.cmpi eq, %26, %c0 : index
        %37 = arith.andi %36, %32 : i1
        scf.if %37 {
          %38 = arith.addi %29, %c1 : index
          %39 = memref.load %reinterpret_cast_6[%29] : memref<256xf32, #gpu.address_space<workgroup>>
          %40 = memref.load %reinterpret_cast_6[%38] : memref<256xf32, #gpu.address_space<workgroup>>
          %41 = arith.cmpf ugt, %39, %40 : f32
          %42 = arith.select %41, %39, %40 : f32
          %43 = arith.cmpf uno, %40, %40 : f32
          %44 = arith.select %43, %40, %42 : f32
          %45 = memref.generic_atomic_rmw %alloc[%31] : memref<?xf32, "gpu"> {
          ^bb0(%arg2: f32):
            %46 = arith.cmpf ogt, %arg2, %44 : f32
            %47 = arith.select %46, %arg2, %44 : f32
            memref.atomic_yield %47 : f32
          }
        }
        scf.yield
      }
      "lmhlo.terminator"() : () -> ()
    }) {disc.device = "gpu", disc.fusion.name = "main_kColReduction_reduce__4_1_0", disc.fusion.tag = "thin", disc.fusion_type = "kColReduction", disc_col_reduction_schedule_hint = 8 : i32, disc_thread_per_block_hint = 256 : i32} : () -> ()
  }
  %7 = llvm.inttoptr %0 : i32 to !llvm.ptr<i8>
  %alloca = memref.alloca() : memref<2xindex, "cpu">
  memref.store %dim_0, %alloca[%c0] : memref<2xindex, "cpu">
  memref.store %dim, %alloca[%c1] : memref<2xindex, "cpu">
  %8 = "disc_ral.dispatch"(%arg0, %7, %alloc, %alloca) {backend_config = "", call_target_name = "inc_ref", device = "gpu", has_side_effect = false} : (!disc_ral.context, !llvm.ptr<i8>, memref<?xf32, "gpu">, memref<2xindex, "cpu">) -> memref<?x?xf32, "gpu">
  %reinterpret_cast = memref.reinterpret_cast %8 to offset: [0], sizes: [%dim_0, %dim], strides: [%dim, 1] {kDiscSymbolicDimAttr = [@S1, @S2]} : memref<?x?xf32, "gpu"> to memref<?x?xf32, "gpu">
  memref.dealloc %alloc : memref<?xf32, "gpu">
  "disc_ral.dispatch"(%arg0, %c0, %reinterpret_cast) {backend_config = "", call_target_name = "ral_send_output", device = "cpu", has_side_effect = false} : (!disc_ral.context, index, memref<?x?xf32, "gpu">) -> ()
  return
}

// -----// IR Dump After SCFParallelLoopTiling (disc-parallel-loop-tiling) //----- //
func.func @main(%arg0: !disc_ral.context) attributes {tf.entry_function = {input_placements = "gpu", inputs = "input0", output_placements = "gpu", outputs = "output0"}} {
  %cst = arith.constant 0xFF800000 : f32
  %c4 = arith.constant 4 : index
  %c256 = arith.constant 256 : index
  %c64 = arith.constant 64 : index
  %c8 = arith.constant 8 : index
  %c32 = arith.constant 32 : index
  %c512 = arith.constant 512 : index
  %0 = llvm.mlir.constant(0 : i32) : i32
  %c0 = arith.constant 0 : index
  %c1 = arith.constant 1 : index
  %c2 = arith.constant 2 : index
  %1 = "disc_ral.dispatch"(%arg0, %c0) {backend_config = "", call_target_name = "ral_recv_input", device = "cpu", has_side_effect = false} : (!disc_ral.context, index) -> memref<?x?x?xf32, "gpu">
  %dim = memref.dim %1, %c2 : memref<?x?x?xf32, "gpu">
  %dim_0 = memref.dim %1, %c1 : memref<?x?x?xf32, "gpu">
  %dim_1 = memref.dim %1, %c0 : memref<?x?x?xf32, "gpu">
  %2 = arith.index_cast %dim_0 : index to i32
  %3 = arith.index_cast %dim : index to i32
  %4 = arith.muli %2, %3 : i32
  %5 = arith.index_cast %4 : i32 to index
  %alloc = memref.alloc(%5) {kDiscSymbolicDimAttr = [@S3]} : memref<?xf32, "gpu">
  %6 = arith.cmpi slt, %dim_1, %5 : index
  scf.if %6 {
    "lmhlo.fusion"() ({
      %c0_2 = arith.constant 0 : index
      %c512_3 = arith.constant 512 : index
      %9 = arith.muli %c1, %c512_3 : index
      scf.parallel (%arg1) = (%c0) to (%5) step (%9) {
        scf.parallel (%arg2) = (%c0_2) to (%9) step (%c1) {
          %24 = arith.addi %arg2, %arg1 : index
          %true = arith.constant true
          %25 = arith.muli %arg2, %c1 : index
          %26 = arith.addi %25, %arg1 : index
          %27 = arith.cmpi ult, %26, %5 : index
          %28 = arith.andi %true, %27 : i1
          scf.if %28 {
            %reinterpret_cast_9 = memref.reinterpret_cast %alloc to offset: [%c0], sizes: [%5], strides: [%c1] : memref<?xf32, "gpu"> to memref<?xf32, "gpu">
            memref.store %cst, %reinterpret_cast_9[%24] : memref<?xf32, "gpu">
          }
          scf.yield
        }
        scf.yield
      }
      %10 = arith.cmpi eq, %5, %c0 : index
      %11 = arith.subi %5, %c1 : index
      %12 = arith.divui %11, %c512 : index
      %13 = arith.addi %12, %c1 : index
      %14 = arith.select %10, %c0, %13 : index
      %15 = arith.cmpi eq, %dim_1, %c0 : index
      %16 = arith.subi %dim_1, %c1 : index
      %17 = arith.divui %16, %c32 : index
      %18 = arith.addi %17, %c1 : index
      %19 = arith.select %15, %c0, %18 : index
      %20 = arith.muli %14, %19 : index
      %c0_4 = arith.constant 0 : index
      %c1_5 = arith.constant 1 : index
      %c1_6 = arith.constant 1 : index
      %21 = arith.muli %c1_6, %20 : index
      %22 = arith.muli %21, %c512 : index
      %c0_7 = arith.constant 0 : index
      %c512_8 = arith.constant 512 : index
      %23 = arith.muli %c1_5, %c512_8 : index
      scf.parallel (%arg1) = (%c0_4) to (%22) step (%23) {
        scf.parallel (%arg2) = (%c0_7) to (%23) step (%c1_5) {
          %24 = arith.addi %arg2, %arg1 : index
          %true = arith.constant true
          %25 = arith.muli %arg2, %c1_5 : index
          %26 = arith.addi %25, %arg1 : index
          %27 = arith.cmpi ult, %26, %22 : index
          %28 = arith.andi %true, %27 : i1
          scf.if %28 {
            %29 = arith.remsi %24, %c512 : index
            %30 = arith.divsi %24, %c512 : index
            %31 = arith.divui %30, %14 : index
            %32 = arith.remui %30, %14 : index
            %33 = arith.muli %32, %c512 : index
            %34 = arith.addi %33, %29 : index
            %35 = arith.cmpi ult, %34, %5 : index
            scf.if %35 {
              %36 = scf.for %arg3 = %c0 to %c32 step %c1 iter_args(%arg4 = %cst) -> (f32) {
                %38 = arith.muli %31, %c32 : index
                %39 = arith.addi %38, %arg3 : index
                %40 = arith.cmpi slt, %39, %dim_1 : index
                %41 = scf.if %40 -> (f32) {
                  %42 = arith.muli %39, %5 : index
                  %43 = arith.addi %42, %34 : index
                  %44 = arith.muli %dim_1, %dim_0 : index
                  %45 = arith.muli %44, %dim : index
                  %reinterpret_cast_9 = memref.reinterpret_cast %1 to offset: [%c0], sizes: [%45], strides: [%c1] : memref<?x?x?xf32, "gpu"> to memref<?xf32, "gpu">
                  %46 = memref.load %reinterpret_cast_9[%43] : memref<?xf32, "gpu">
                  %47 = math.absf %46 : f32
                  %48 = arith.cmpf oge, %arg4, %47 : f32
                  %49 = arith.select %48, %arg4, %47 : f32
                  scf.yield %49 : f32
                } else {
                  scf.yield %arg4 : f32
                }
                scf.yield %41 : f32
              }
              %37 = memref.generic_atomic_rmw %alloc[%34] : memref<?xf32, "gpu"> {
              ^bb0(%arg3: f32):
                %38 = arith.cmpf ogt, %arg3, %36 : f32
                %39 = arith.select %38, %arg3, %36 : f32
                memref.atomic_yield %39 : f32
              }
            }
          }
          scf.yield
        }
        scf.yield
      }
      "lmhlo.terminator"() : () -> ()
    }) {disc.device = "gpu", disc.fusion.name = "main_kColReduction_reduce__4_1_0", disc.fusion.tag = "flat", disc.fusion_type = "kColReduction", disc_col_reduction_schedule_hint = 7 : i32, disc_thread_per_block_hint = 512 : i32} : () -> ()
  } else {
    "lmhlo.fusion"() ({
      %c0_2 = arith.constant 0 : index
      %c256_3 = arith.constant 256 : index
      %9 = arith.muli %c1, %c256_3 : index
      scf.parallel (%arg1) = (%c0) to (%5) step (%9) {
        scf.parallel (%arg2) = (%c0_2) to (%9) step (%c1) {
          %24 = arith.addi %arg2, %arg1 : index
          %true = arith.constant true
          %25 = arith.muli %arg2, %c1 : index
          %26 = arith.addi %25, %arg1 : index
          %27 = arith.cmpi ult, %26, %5 : index
          %28 = arith.andi %true, %27 : i1
          scf.if %28 {
            %reinterpret_cast_9 = memref.reinterpret_cast %alloc to offset: [%c0], sizes: [%5], strides: [%c1] : memref<?xf32, "gpu"> to memref<?xf32, "gpu">
            memref.store %cst, %reinterpret_cast_9[%24] : memref<?xf32, "gpu">
          }
          scf.yield
        }
        scf.yield
      }
      %10 = arith.cmpi eq, %5, %c0 : index
      %11 = arith.subi %5, %c1 : index
      %12 = arith.divui %11, %c32 : index
      %13 = arith.addi %12, %c1 : index
      %14 = arith.select %10, %c0, %13 : index
      %15 = arith.cmpi eq, %dim_1, %c0 : index
      %16 = arith.subi %dim_1, %c1 : index
      %17 = arith.divui %16, %c512 : index
      %18 = arith.addi %17, %c1 : index
      %19 = arith.select %15, %c0, %18 : index
      %20 = arith.muli %14, %19 : index
      %c0_4 = arith.constant 0 : index
      %c1_5 = arith.constant 1 : index
      %c1_6 = arith.constant 1 : index
      %21 = arith.muli %c1_6, %20 : index
      %22 = arith.muli %21, %c256 : index
      %c0_7 = arith.constant 0 : index
      %c256_8 = arith.constant 256 : index
      %23 = arith.muli %c1_5, %c256_8 : index
      scf.parallel (%arg1) = (%c0_4) to (%22) step (%23) {
        scf.parallel (%arg2) = (%c0_7) to (%23) step (%c1_5) {
          %24 = arith.addi %arg2, %arg1 : index
          %true = arith.constant true
          %25 = arith.muli %arg2, %c1_5 : index
          %26 = arith.addi %25, %arg1 : index
          %27 = arith.cmpi ult, %26, %22 : index
          %28 = arith.andi %true, %27 : i1
          scf.if %28 {
            %29 = arith.remsi %24, %c256 : index
            %30 = arith.divsi %24, %c256 : index
            %31 = arith.divui %30, %14 : index
            %32 = arith.remui %30, %14 : index
            %33 = arith.divui %29, %c32 : index
            %34 = arith.remui %29, %c32 : index
            %35 = arith.muli %34, %c8 : index
            %36 = arith.addi %33, %35 : index
            %37 = arith.muli %32, %c32 : index
            %38 = arith.addi %34, %37 : index
            %alloc_9 = memref.alloc() : memref<256xf32, #gpu.address_space<workgroup>>
            %39 = arith.cmpi ult, %38, %5 : index
            %40 = scf.if %39 -> (f32) {
              %45 = scf.for %arg3 = %c0 to %c64 step %c1 iter_args(%arg4 = %cst) -> (f32) {
                %46 = arith.muli %31, %c8 : index
                %47 = arith.addi %33, %46 : index
                %48 = arith.muli %47, %c64 : index
                %49 = arith.addi %arg3, %48 : index
                %50 = arith.cmpi slt, %49, %dim_1 : index
                %51 = scf.if %50 -> (f32) {
                  %52 = arith.muli %49, %5 : index
                  %53 = arith.addi %52, %38 : index
                  %54 = arith.muli %dim_1, %dim_0 : index
                  %55 = arith.muli %54, %dim : index
                  %reinterpret_cast_11 = memref.reinterpret_cast %1 to offset: [%c0], sizes: [%55], strides: [%c1] : memref<?x?x?xf32, "gpu"> to memref<?xf32, "gpu">
                  %56 = memref.load %reinterpret_cast_11[%53] : memref<?xf32, "gpu">
                  %57 = math.absf %56 : f32
                  %58 = arith.cmpf ugt, %arg4, %57 : f32
                  %59 = arith.select %58, %arg4, %57 : f32
                  %60 = arith.cmpf uno, %57, %57 : f32
                  %61 = arith.select %60, %57, %59 : f32
                  scf.yield %61 : f32
                } else {
                  scf.yield %arg4 : f32
                }
                scf.yield %51 : f32
              }
              scf.yield %45 : f32
            } else {
              scf.yield %cst : f32
            }
            %reinterpret_cast_10 = memref.reinterpret_cast %alloc_9 to offset: [0], sizes: [256], strides: [1] : memref<256xf32, #gpu.address_space<workgroup>> to memref<256xf32, #gpu.address_space<workgroup>>
            memref.store %40, %reinterpret_cast_10[%36] : memref<256xf32, #gpu.address_space<workgroup>>
            gpu.barrier
            %41 = arith.cmpi slt, %33, %c4 : index
            scf.if %41 {
              %45 = arith.addi %36, %c4 : index
              %46 = memref.load %reinterpret_cast_10[%36] : memref<256xf32, #gpu.address_space<workgroup>>
              %47 = memref.load %reinterpret_cast_10[%45] : memref<256xf32, #gpu.address_space<workgroup>>
              %48 = arith.cmpf ugt, %46, %47 : f32
              %49 = arith.select %48, %46, %47 : f32
              %50 = arith.cmpf uno, %47, %47 : f32
              %51 = arith.select %50, %47, %49 : f32
              memref.store %51, %reinterpret_cast_10[%36] : memref<256xf32, #gpu.address_space<workgroup>>
            }
            gpu.barrier
            %42 = arith.cmpi slt, %33, %c2 : index
            scf.if %42 {
              %45 = arith.addi %36, %c2 : index
              %46 = memref.load %reinterpret_cast_10[%36] : memref<256xf32, #gpu.address_space<workgroup>>
              %47 = memref.load %reinterpret_cast_10[%45] : memref<256xf32, #gpu.address_space<workgroup>>
              %48 = arith.cmpf ugt, %46, %47 : f32
              %49 = arith.select %48, %46, %47 : f32
              %50 = arith.cmpf uno, %47, %47 : f32
              %51 = arith.select %50, %47, %49 : f32
              memref.store %51, %reinterpret_cast_10[%36] : memref<256xf32, #gpu.address_space<workgroup>>
            }
            gpu.barrier
            %43 = arith.cmpi eq, %33, %c0 : index
            %44 = arith.andi %43, %39 : i1
            scf.if %44 {
              %45 = arith.addi %36, %c1 : index
              %46 = memref.load %reinterpret_cast_10[%36] : memref<256xf32, #gpu.address_space<workgroup>>
              %47 = memref.load %reinterpret_cast_10[%45] : memref<256xf32, #gpu.address_space<workgroup>>
              %48 = arith.cmpf ugt, %46, %47 : f32
              %49 = arith.select %48, %46, %47 : f32
              %50 = arith.cmpf uno, %47, %47 : f32
              %51 = arith.select %50, %47, %49 : f32
              %52 = memref.generic_atomic_rmw %alloc[%38] : memref<?xf32, "gpu"> {
              ^bb0(%arg3: f32):
                %53 = arith.cmpf ogt, %arg3, %51 : f32
                %54 = arith.select %53, %arg3, %51 : f32
                memref.atomic_yield %54 : f32
              }
            }
          }
          scf.yield
        }
        scf.yield
      }
      "lmhlo.terminator"() : () -> ()
    }) {disc.device = "gpu", disc.fusion.name = "main_kColReduction_reduce__4_1_0", disc.fusion.tag = "thin", disc.fusion_type = "kColReduction", disc_col_reduction_schedule_hint = 8 : i32, disc_thread_per_block_hint = 256 : i32} : () -> ()
  }
  %7 = llvm.inttoptr %0 : i32 to !llvm.ptr<i8>
  %alloca = memref.alloca() : memref<2xindex, "cpu">
  memref.store %dim_0, %alloca[%c0] : memref<2xindex, "cpu">
  memref.store %dim, %alloca[%c1] : memref<2xindex, "cpu">
  %8 = "disc_ral.dispatch"(%arg0, %7, %alloc, %alloca) {backend_config = "", call_target_name = "inc_ref", device = "gpu", has_side_effect = false} : (!disc_ral.context, !llvm.ptr<i8>, memref<?xf32, "gpu">, memref<2xindex, "cpu">) -> memref<?x?xf32, "gpu">
  %reinterpret_cast = memref.reinterpret_cast %8 to offset: [0], sizes: [%dim_0, %dim], strides: [%dim, 1] {kDiscSymbolicDimAttr = [@S1, @S2]} : memref<?x?xf32, "gpu"> to memref<?x?xf32, "gpu">
  memref.dealloc %alloc : memref<?xf32, "gpu">
  "disc_ral.dispatch"(%arg0, %c0, %reinterpret_cast) {backend_config = "", call_target_name = "ral_send_output", device = "cpu", has_side_effect = false} : (!disc_ral.context, index, memref<?x?xf32, "gpu">) -> ()
  return
}

// -----// IR Dump After GpuMapParallelLoopsPass (gpu-map-parallel-loops) //----- //
func.func @main(%arg0: !disc_ral.context) attributes {tf.entry_function = {input_placements = "gpu", inputs = "input0", output_placements = "gpu", outputs = "output0"}} {
  %cst = arith.constant 0xFF800000 : f32
  %c4 = arith.constant 4 : index
  %c256 = arith.constant 256 : index
  %c64 = arith.constant 64 : index
  %c8 = arith.constant 8 : index
  %c32 = arith.constant 32 : index
  %c512 = arith.constant 512 : index
  %0 = llvm.mlir.constant(0 : i32) : i32
  %c0 = arith.constant 0 : index
  %c1 = arith.constant 1 : index
  %c2 = arith.constant 2 : index
  %1 = "disc_ral.dispatch"(%arg0, %c0) {backend_config = "", call_target_name = "ral_recv_input", device = "cpu", has_side_effect = false} : (!disc_ral.context, index) -> memref<?x?x?xf32, "gpu">
  %dim = memref.dim %1, %c2 : memref<?x?x?xf32, "gpu">
  %dim_0 = memref.dim %1, %c1 : memref<?x?x?xf32, "gpu">
  %dim_1 = memref.dim %1, %c0 : memref<?x?x?xf32, "gpu">
  %2 = arith.index_cast %dim_0 : index to i32
  %3 = arith.index_cast %dim : index to i32
  %4 = arith.muli %2, %3 : i32
  %5 = arith.index_cast %4 : i32 to index
  %alloc = memref.alloc(%5) {kDiscSymbolicDimAttr = [@S3]} : memref<?xf32, "gpu">
  %6 = arith.cmpi slt, %dim_1, %5 : index
  scf.if %6 {
    "lmhlo.fusion"() ({
      %c0_2 = arith.constant 0 : index
      %c512_3 = arith.constant 512 : index
      %9 = arith.muli %c1, %c512_3 : index
      scf.parallel (%arg1) = (%c0) to (%5) step (%9) {
        scf.parallel (%arg2) = (%c0_2) to (%9) step (%c1) {
          %24 = arith.addi %arg2, %arg1 : index
          %true = arith.constant true
          %25 = arith.muli %arg2, %c1 : index
          %26 = arith.addi %25, %arg1 : index
          %27 = arith.cmpi ult, %26, %5 : index
          %28 = arith.andi %true, %27 : i1
          scf.if %28 {
            %reinterpret_cast_9 = memref.reinterpret_cast %alloc to offset: [%c0], sizes: [%5], strides: [%c1] : memref<?xf32, "gpu"> to memref<?xf32, "gpu">
            memref.store %cst, %reinterpret_cast_9[%24] : memref<?xf32, "gpu">
          }
          scf.yield
        } {mapping = [#gpu.loop_dim_map<processor = thread_x, map = (d0) -> (d0), bound = (d0) -> (d0)>]}
        scf.yield
      } {mapping = [#gpu.loop_dim_map<processor = block_x, map = (d0) -> (d0), bound = (d0) -> (d0)>]}
      %10 = arith.cmpi eq, %5, %c0 : index
      %11 = arith.subi %5, %c1 : index
      %12 = arith.divui %11, %c512 : index
      %13 = arith.addi %12, %c1 : index
      %14 = arith.select %10, %c0, %13 : index
      %15 = arith.cmpi eq, %dim_1, %c0 : index
      %16 = arith.subi %dim_1, %c1 : index
      %17 = arith.divui %16, %c32 : index
      %18 = arith.addi %17, %c1 : index
      %19 = arith.select %15, %c0, %18 : index
      %20 = arith.muli %14, %19 : index
      %c0_4 = arith.constant 0 : index
      %c1_5 = arith.constant 1 : index
      %c1_6 = arith.constant 1 : index
      %21 = arith.muli %c1_6, %20 : index
      %22 = arith.muli %21, %c512 : index
      %c0_7 = arith.constant 0 : index
      %c512_8 = arith.constant 512 : index
      %23 = arith.muli %c1_5, %c512_8 : index
      scf.parallel (%arg1) = (%c0_4) to (%22) step (%23) {
        scf.parallel (%arg2) = (%c0_7) to (%23) step (%c1_5) {
          %24 = arith.addi %arg2, %arg1 : index
          %true = arith.constant true
          %25 = arith.muli %arg2, %c1_5 : index
          %26 = arith.addi %25, %arg1 : index
          %27 = arith.cmpi ult, %26, %22 : index
          %28 = arith.andi %true, %27 : i1
          scf.if %28 {
            %29 = arith.remsi %24, %c512 : index
            %30 = arith.divsi %24, %c512 : index
            %31 = arith.divui %30, %14 : index
            %32 = arith.remui %30, %14 : index
            %33 = arith.muli %32, %c512 : index
            %34 = arith.addi %33, %29 : index
            %35 = arith.cmpi ult, %34, %5 : index
            scf.if %35 {
              %36 = scf.for %arg3 = %c0 to %c32 step %c1 iter_args(%arg4 = %cst) -> (f32) {
                %38 = arith.muli %31, %c32 : index
                %39 = arith.addi %38, %arg3 : index
                %40 = arith.cmpi slt, %39, %dim_1 : index
                %41 = scf.if %40 -> (f32) {
                  %42 = arith.muli %39, %5 : index
                  %43 = arith.addi %42, %34 : index
                  %44 = arith.muli %dim_1, %dim_0 : index
                  %45 = arith.muli %44, %dim : index
                  %reinterpret_cast_9 = memref.reinterpret_cast %1 to offset: [%c0], sizes: [%45], strides: [%c1] : memref<?x?x?xf32, "gpu"> to memref<?xf32, "gpu">
                  %46 = memref.load %reinterpret_cast_9[%43] : memref<?xf32, "gpu">
                  %47 = math.absf %46 : f32
                  %48 = arith.cmpf oge, %arg4, %47 : f32
                  %49 = arith.select %48, %arg4, %47 : f32
                  scf.yield %49 : f32
                } else {
                  scf.yield %arg4 : f32
                }
                scf.yield %41 : f32
              }
              %37 = memref.generic_atomic_rmw %alloc[%34] : memref<?xf32, "gpu"> {
              ^bb0(%arg3: f32):
                %38 = arith.cmpf ogt, %arg3, %36 : f32
                %39 = arith.select %38, %arg3, %36 : f32
                memref.atomic_yield %39 : f32
              }
            }
          }
          scf.yield
        } {mapping = [#gpu.loop_dim_map<processor = thread_x, map = (d0) -> (d0), bound = (d0) -> (d0)>]}
        scf.yield
      } {mapping = [#gpu.loop_dim_map<processor = block_x, map = (d0) -> (d0), bound = (d0) -> (d0)>]}
      "lmhlo.terminator"() : () -> ()
    }) {disc.device = "gpu", disc.fusion.name = "main_kColReduction_reduce__4_1_0", disc.fusion.tag = "flat", disc.fusion_type = "kColReduction", disc_col_reduction_schedule_hint = 7 : i32, disc_thread_per_block_hint = 512 : i32} : () -> ()
  } else {
    "lmhlo.fusion"() ({
      %c0_2 = arith.constant 0 : index
      %c256_3 = arith.constant 256 : index
      %9 = arith.muli %c1, %c256_3 : index
      scf.parallel (%arg1) = (%c0) to (%5) step (%9) {
        scf.parallel (%arg2) = (%c0_2) to (%9) step (%c1) {
          %24 = arith.addi %arg2, %arg1 : index
          %true = arith.constant true
          %25 = arith.muli %arg2, %c1 : index
          %26 = arith.addi %25, %arg1 : index
          %27 = arith.cmpi ult, %26, %5 : index
          %28 = arith.andi %true, %27 : i1
          scf.if %28 {
            %reinterpret_cast_9 = memref.reinterpret_cast %alloc to offset: [%c0], sizes: [%5], strides: [%c1] : memref<?xf32, "gpu"> to memref<?xf32, "gpu">
            memref.store %cst, %reinterpret_cast_9[%24] : memref<?xf32, "gpu">
          }
          scf.yield
        } {mapping = [#gpu.loop_dim_map<processor = thread_x, map = (d0) -> (d0), bound = (d0) -> (d0)>]}
        scf.yield
      } {mapping = [#gpu.loop_dim_map<processor = block_x, map = (d0) -> (d0), bound = (d0) -> (d0)>]}
      %10 = arith.cmpi eq, %5, %c0 : index
      %11 = arith.subi %5, %c1 : index
      %12 = arith.divui %11, %c32 : index
      %13 = arith.addi %12, %c1 : index
      %14 = arith.select %10, %c0, %13 : index
      %15 = arith.cmpi eq, %dim_1, %c0 : index
      %16 = arith.subi %dim_1, %c1 : index
      %17 = arith.divui %16, %c512 : index
      %18 = arith.addi %17, %c1 : index
      %19 = arith.select %15, %c0, %18 : index
      %20 = arith.muli %14, %19 : index
      %c0_4 = arith.constant 0 : index
      %c1_5 = arith.constant 1 : index
      %c1_6 = arith.constant 1 : index
      %21 = arith.muli %c1_6, %20 : index
      %22 = arith.muli %21, %c256 : index
      %c0_7 = arith.constant 0 : index
      %c256_8 = arith.constant 256 : index
      %23 = arith.muli %c1_5, %c256_8 : index
      scf.parallel (%arg1) = (%c0_4) to (%22) step (%23) {
        scf.parallel (%arg2) = (%c0_7) to (%23) step (%c1_5) {
          %24 = arith.addi %arg2, %arg1 : index
          %true = arith.constant true
          %25 = arith.muli %arg2, %c1_5 : index
          %26 = arith.addi %25, %arg1 : index
          %27 = arith.cmpi ult, %26, %22 : index
          %28 = arith.andi %true, %27 : i1
          scf.if %28 {
            %29 = arith.remsi %24, %c256 : index
            %30 = arith.divsi %24, %c256 : index
            %31 = arith.divui %30, %14 : index
            %32 = arith.remui %30, %14 : index
            %33 = arith.divui %29, %c32 : index
            %34 = arith.remui %29, %c32 : index
            %35 = arith.muli %34, %c8 : index
            %36 = arith.addi %33, %35 : index
            %37 = arith.muli %32, %c32 : index
            %38 = arith.addi %34, %37 : index
            %alloc_9 = memref.alloc() : memref<256xf32, #gpu.address_space<workgroup>>
            %39 = arith.cmpi ult, %38, %5 : index
            %40 = scf.if %39 -> (f32) {
              %45 = scf.for %arg3 = %c0 to %c64 step %c1 iter_args(%arg4 = %cst) -> (f32) {
                %46 = arith.muli %31, %c8 : index
                %47 = arith.addi %33, %46 : index
                %48 = arith.muli %47, %c64 : index
                %49 = arith.addi %arg3, %48 : index
                %50 = arith.cmpi slt, %49, %dim_1 : index
                %51 = scf.if %50 -> (f32) {
                  %52 = arith.muli %49, %5 : index
                  %53 = arith.addi %52, %38 : index
                  %54 = arith.muli %dim_1, %dim_0 : index
                  %55 = arith.muli %54, %dim : index
                  %reinterpret_cast_11 = memref.reinterpret_cast %1 to offset: [%c0], sizes: [%55], strides: [%c1] : memref<?x?x?xf32, "gpu"> to memref<?xf32, "gpu">
                  %56 = memref.load %reinterpret_cast_11[%53] : memref<?xf32, "gpu">
                  %57 = math.absf %56 : f32
                  %58 = arith.cmpf ugt, %arg4, %57 : f32
                  %59 = arith.select %58, %arg4, %57 : f32
                  %60 = arith.cmpf uno, %57, %57 : f32
                  %61 = arith.select %60, %57, %59 : f32
                  scf.yield %61 : f32
                } else {
                  scf.yield %arg4 : f32
                }
                scf.yield %51 : f32
              }
              scf.yield %45 : f32
            } else {
              scf.yield %cst : f32
            }
            %reinterpret_cast_10 = memref.reinterpret_cast %alloc_9 to offset: [0], sizes: [256], strides: [1] : memref<256xf32, #gpu.address_space<workgroup>> to memref<256xf32, #gpu.address_space<workgroup>>
            memref.store %40, %reinterpret_cast_10[%36] : memref<256xf32, #gpu.address_space<workgroup>>
            gpu.barrier
            %41 = arith.cmpi slt, %33, %c4 : index
            scf.if %41 {
              %45 = arith.addi %36, %c4 : index
              %46 = memref.load %reinterpret_cast_10[%36] : memref<256xf32, #gpu.address_space<workgroup>>
              %47 = memref.load %reinterpret_cast_10[%45] : memref<256xf32, #gpu.address_space<workgroup>>
              %48 = arith.cmpf ugt, %46, %47 : f32
              %49 = arith.select %48, %46, %47 : f32
              %50 = arith.cmpf uno, %47, %47 : f32
              %51 = arith.select %50, %47, %49 : f32
              memref.store %51, %reinterpret_cast_10[%36] : memref<256xf32, #gpu.address_space<workgroup>>
            }
            gpu.barrier
            %42 = arith.cmpi slt, %33, %c2 : index
            scf.if %42 {
              %45 = arith.addi %36, %c2 : index
              %46 = memref.load %reinterpret_cast_10[%36] : memref<256xf32, #gpu.address_space<workgroup>>
              %47 = memref.load %reinterpret_cast_10[%45] : memref<256xf32, #gpu.address_space<workgroup>>
              %48 = arith.cmpf ugt, %46, %47 : f32
              %49 = arith.select %48, %46, %47 : f32
              %50 = arith.cmpf uno, %47, %47 : f32
              %51 = arith.select %50, %47, %49 : f32
              memref.store %51, %reinterpret_cast_10[%36] : memref<256xf32, #gpu.address_space<workgroup>>
            }
            gpu.barrier
            %43 = arith.cmpi eq, %33, %c0 : index
            %44 = arith.andi %43, %39 : i1
            scf.if %44 {
              %45 = arith.addi %36, %c1 : index
              %46 = memref.load %reinterpret_cast_10[%36] : memref<256xf32, #gpu.address_space<workgroup>>
              %47 = memref.load %reinterpret_cast_10[%45] : memref<256xf32, #gpu.address_space<workgroup>>
              %48 = arith.cmpf ugt, %46, %47 : f32
              %49 = arith.select %48, %46, %47 : f32
              %50 = arith.cmpf uno, %47, %47 : f32
              %51 = arith.select %50, %47, %49 : f32
              %52 = memref.generic_atomic_rmw %alloc[%38] : memref<?xf32, "gpu"> {
              ^bb0(%arg3: f32):
                %53 = arith.cmpf ogt, %arg3, %51 : f32
                %54 = arith.select %53, %arg3, %51 : f32
                memref.atomic_yield %54 : f32
              }
            }
          }
          scf.yield
        } {mapping = [#gpu.loop_dim_map<processor = thread_x, map = (d0) -> (d0), bound = (d0) -> (d0)>]}
        scf.yield
      } {mapping = [#gpu.loop_dim_map<processor = block_x, map = (d0) -> (d0), bound = (d0) -> (d0)>]}
      "lmhlo.terminator"() : () -> ()
    }) {disc.device = "gpu", disc.fusion.name = "main_kColReduction_reduce__4_1_0", disc.fusion.tag = "thin", disc.fusion_type = "kColReduction", disc_col_reduction_schedule_hint = 8 : i32, disc_thread_per_block_hint = 256 : i32} : () -> ()
  }
  %7 = llvm.inttoptr %0 : i32 to !llvm.ptr<i8>
  %alloca = memref.alloca() : memref<2xindex, "cpu">
  memref.store %dim_0, %alloca[%c0] : memref<2xindex, "cpu">
  memref.store %dim, %alloca[%c1] : memref<2xindex, "cpu">
  %8 = "disc_ral.dispatch"(%arg0, %7, %alloc, %alloca) {backend_config = "", call_target_name = "inc_ref", device = "gpu", has_side_effect = false} : (!disc_ral.context, !llvm.ptr<i8>, memref<?xf32, "gpu">, memref<2xindex, "cpu">) -> memref<?x?xf32, "gpu">
  %reinterpret_cast = memref.reinterpret_cast %8 to offset: [0], sizes: [%dim_0, %dim], strides: [%dim, 1] {kDiscSymbolicDimAttr = [@S1, @S2]} : memref<?x?xf32, "gpu"> to memref<?x?xf32, "gpu">
  memref.dealloc %alloc : memref<?xf32, "gpu">
  "disc_ral.dispatch"(%arg0, %c0, %reinterpret_cast) {backend_config = "", call_target_name = "ral_send_output", device = "cpu", has_side_effect = false} : (!disc_ral.context, index, memref<?x?xf32, "gpu">) -> ()
  return
}

// -----// IR Dump After ConvertParallelLoopToGpu (convert-parallel-loops-to-gpu) //----- //
func.func @main(%arg0: !disc_ral.context) attributes {tf.entry_function = {input_placements = "gpu", inputs = "input0", output_placements = "gpu", outputs = "output0"}} {
  %cst = arith.constant 0xFF800000 : f32
  %c4 = arith.constant 4 : index
  %c256 = arith.constant 256 : index
  %c64 = arith.constant 64 : index
  %c8 = arith.constant 8 : index
  %c32 = arith.constant 32 : index
  %c512 = arith.constant 512 : index
  %0 = llvm.mlir.constant(0 : i32) : i32
  %c0 = arith.constant 0 : index
  %c1 = arith.constant 1 : index
  %c2 = arith.constant 2 : index
  %1 = "disc_ral.dispatch"(%arg0, %c0) {backend_config = "", call_target_name = "ral_recv_input", device = "cpu", has_side_effect = false} : (!disc_ral.context, index) -> memref<?x?x?xf32, "gpu">
  %dim = memref.dim %1, %c2 : memref<?x?x?xf32, "gpu">
  %dim_0 = memref.dim %1, %c1 : memref<?x?x?xf32, "gpu">
  %dim_1 = memref.dim %1, %c0 : memref<?x?x?xf32, "gpu">
  %2 = arith.index_cast %dim_0 : index to i32
  %3 = arith.index_cast %dim : index to i32
  %4 = arith.muli %2, %3 : i32
  %5 = arith.index_cast %4 : i32 to index
  %alloc = memref.alloc(%5) {kDiscSymbolicDimAttr = [@S3]} : memref<?xf32, "gpu">
  %6 = arith.cmpi slt, %dim_1, %5 : index
  scf.if %6 {
    "lmhlo.fusion"() ({
      %c0_2 = arith.constant 0 : index
      %c512_3 = arith.constant 512 : index
      %9 = arith.muli %c1, %c512_3 : index
      %c1_4 = arith.constant 1 : index
      %10 = affine.apply affine_map<(d0)[s0, s1] -> ((d0 - s0) ceildiv s1)>(%5)[%c0, %9]
      %11 = affine.apply affine_map<(d0)[s0, s1] -> ((d0 - s0) ceildiv s1)>(%9)[%c0_2, %c1]
      gpu.launch blocks(%arg1, %arg2, %arg3) in (%arg7 = %10, %arg8 = %c1_4, %arg9 = %c1_4) threads(%arg4, %arg5, %arg6) in (%arg10 = %11, %arg11 = %c1_4, %arg12 = %c1_4) {
        %28 = affine.apply affine_map<(d0)[s0, s1] -> (d0 * s0 + s1)>(%arg1)[%9, %c0]
        %29 = affine.apply affine_map<(d0)[s0, s1] -> (d0 * s0 + s1)>(%arg4)[%c1, %c0_2]
        %30 = arith.addi %29, %28 : index
        %true = arith.constant true
        %31 = arith.muli %29, %c1 : index
        %32 = arith.addi %31, %28 : index
        %33 = arith.cmpi ult, %32, %5 : index
        %34 = arith.andi %true, %33 : i1
        scf.if %34 {
          %reinterpret_cast_11 = memref.reinterpret_cast %alloc to offset: [%c0], sizes: [%5], strides: [%c1] : memref<?xf32, "gpu"> to memref<?xf32, "gpu">
          memref.store %cst, %reinterpret_cast_11[%30] : memref<?xf32, "gpu">
        }
        gpu.terminator
      } {SCFToGPU_visited}
      %12 = arith.cmpi eq, %5, %c0 : index
      %13 = arith.subi %5, %c1 : index
      %14 = arith.divui %13, %c512 : index
      %15 = arith.addi %14, %c1 : index
      %16 = arith.select %12, %c0, %15 : index
      %17 = arith.cmpi eq, %dim_1, %c0 : index
      %18 = arith.subi %dim_1, %c1 : index
      %19 = arith.divui %18, %c32 : index
      %20 = arith.addi %19, %c1 : index
      %21 = arith.select %17, %c0, %20 : index
      %22 = arith.muli %16, %21 : index
      %c0_5 = arith.constant 0 : index
      %c1_6 = arith.constant 1 : index
      %c1_7 = arith.constant 1 : index
      %23 = arith.muli %c1_7, %22 : index
      %24 = arith.muli %23, %c512 : index
      %c0_8 = arith.constant 0 : index
      %c512_9 = arith.constant 512 : index
      %25 = arith.muli %c1_6, %c512_9 : index
      %c1_10 = arith.constant 1 : index
      %26 = affine.apply affine_map<(d0)[s0, s1] -> ((d0 - s0) ceildiv s1)>(%24)[%c0_5, %25]
      %27 = affine.apply affine_map<(d0)[s0, s1] -> ((d0 - s0) ceildiv s1)>(%25)[%c0_8, %c1_6]
      gpu.launch blocks(%arg1, %arg2, %arg3) in (%arg7 = %26, %arg8 = %c1_10, %arg9 = %c1_10) threads(%arg4, %arg5, %arg6) in (%arg10 = %27, %arg11 = %c1_10, %arg12 = %c1_10) {
        %28 = affine.apply affine_map<(d0)[s0, s1] -> (d0 * s0 + s1)>(%arg1)[%25, %c0_5]
        %29 = affine.apply affine_map<(d0)[s0, s1] -> (d0 * s0 + s1)>(%arg4)[%c1_6, %c0_8]
        %30 = arith.addi %29, %28 : index
        %true = arith.constant true
        %31 = arith.muli %29, %c1_6 : index
        %32 = arith.addi %31, %28 : index
        %33 = arith.cmpi ult, %32, %24 : index
        %34 = arith.andi %true, %33 : i1
        scf.if %34 {
          %35 = arith.remsi %30, %c512 : index
          %36 = arith.divsi %30, %c512 : index
          %37 = arith.divui %36, %16 : index
          %38 = arith.remui %36, %16 : index
          %39 = arith.muli %38, %c512 : index
          %40 = arith.addi %39, %35 : index
          %41 = arith.cmpi ult, %40, %5 : index
          scf.if %41 {
            %42 = scf.for %arg13 = %c0 to %c32 step %c1 iter_args(%arg14 = %cst) -> (f32) {
              %44 = arith.muli %37, %c32 : index
              %45 = arith.addi %44, %arg13 : index
              %46 = arith.cmpi slt, %45, %dim_1 : index
              %47 = scf.if %46 -> (f32) {
                %48 = arith.muli %45, %5 : index
                %49 = arith.addi %48, %40 : index
                %50 = arith.muli %dim_1, %dim_0 : index
                %51 = arith.muli %50, %dim : index
                %reinterpret_cast_11 = memref.reinterpret_cast %1 to offset: [%c0], sizes: [%51], strides: [%c1] : memref<?x?x?xf32, "gpu"> to memref<?xf32, "gpu">
                %52 = memref.load %reinterpret_cast_11[%49] : memref<?xf32, "gpu">
                %53 = math.absf %52 : f32
                %54 = arith.cmpf oge, %arg14, %53 : f32
                %55 = arith.select %54, %arg14, %53 : f32
                scf.yield %55 : f32
              } else {
                scf.yield %arg14 : f32
              }
              scf.yield %47 : f32
            }
            %43 = memref.generic_atomic_rmw %alloc[%40] : memref<?xf32, "gpu"> {
            ^bb0(%arg13: f32):
              %44 = arith.cmpf ogt, %arg13, %42 : f32
              %45 = arith.select %44, %arg13, %42 : f32
              memref.atomic_yield %45 : f32
            }
          }
        }
        gpu.terminator
      } {SCFToGPU_visited}
      "lmhlo.terminator"() : () -> ()
    }) {disc.device = "gpu", disc.fusion.name = "main_kColReduction_reduce__4_1_0", disc.fusion.tag = "flat", disc.fusion_type = "kColReduction", disc_col_reduction_schedule_hint = 7 : i32, disc_thread_per_block_hint = 512 : i32} : () -> ()
  } else {
    "lmhlo.fusion"() ({
      %c0_2 = arith.constant 0 : index
      %c256_3 = arith.constant 256 : index
      %9 = arith.muli %c1, %c256_3 : index
      %c1_4 = arith.constant 1 : index
      %10 = affine.apply affine_map<(d0)[s0, s1] -> ((d0 - s0) ceildiv s1)>(%5)[%c0, %9]
      %11 = affine.apply affine_map<(d0)[s0, s1] -> ((d0 - s0) ceildiv s1)>(%9)[%c0_2, %c1]
      gpu.launch blocks(%arg1, %arg2, %arg3) in (%arg7 = %10, %arg8 = %c1_4, %arg9 = %c1_4) threads(%arg4, %arg5, %arg6) in (%arg10 = %11, %arg11 = %c1_4, %arg12 = %c1_4) {
        %28 = affine.apply affine_map<(d0)[s0, s1] -> (d0 * s0 + s1)>(%arg1)[%9, %c0]
        %29 = affine.apply affine_map<(d0)[s0, s1] -> (d0 * s0 + s1)>(%arg4)[%c1, %c0_2]
        %30 = arith.addi %29, %28 : index
        %true = arith.constant true
        %31 = arith.muli %29, %c1 : index
        %32 = arith.addi %31, %28 : index
        %33 = arith.cmpi ult, %32, %5 : index
        %34 = arith.andi %true, %33 : i1
        scf.if %34 {
          %reinterpret_cast_11 = memref.reinterpret_cast %alloc to offset: [%c0], sizes: [%5], strides: [%c1] : memref<?xf32, "gpu"> to memref<?xf32, "gpu">
          memref.store %cst, %reinterpret_cast_11[%30] : memref<?xf32, "gpu">
        }
        gpu.terminator
      } {SCFToGPU_visited}
      %12 = arith.cmpi eq, %5, %c0 : index
      %13 = arith.subi %5, %c1 : index
      %14 = arith.divui %13, %c32 : index
      %15 = arith.addi %14, %c1 : index
      %16 = arith.select %12, %c0, %15 : index
      %17 = arith.cmpi eq, %dim_1, %c0 : index
      %18 = arith.subi %dim_1, %c1 : index
      %19 = arith.divui %18, %c512 : index
      %20 = arith.addi %19, %c1 : index
      %21 = arith.select %17, %c0, %20 : index
      %22 = arith.muli %16, %21 : index
      %c0_5 = arith.constant 0 : index
      %c1_6 = arith.constant 1 : index
      %c1_7 = arith.constant 1 : index
      %23 = arith.muli %c1_7, %22 : index
      %24 = arith.muli %23, %c256 : index
      %c0_8 = arith.constant 0 : index
      %c256_9 = arith.constant 256 : index
      %25 = arith.muli %c1_6, %c256_9 : index
      %c1_10 = arith.constant 1 : index
      %26 = affine.apply affine_map<(d0)[s0, s1] -> ((d0 - s0) ceildiv s1)>(%24)[%c0_5, %25]
      %27 = affine.apply affine_map<(d0)[s0, s1] -> ((d0 - s0) ceildiv s1)>(%25)[%c0_8, %c1_6]
      gpu.launch blocks(%arg1, %arg2, %arg3) in (%arg7 = %26, %arg8 = %c1_10, %arg9 = %c1_10) threads(%arg4, %arg5, %arg6) in (%arg10 = %27, %arg11 = %c1_10, %arg12 = %c1_10) {
        %28 = affine.apply affine_map<(d0)[s0, s1] -> (d0 * s0 + s1)>(%arg1)[%25, %c0_5]
        %29 = affine.apply affine_map<(d0)[s0, s1] -> (d0 * s0 + s1)>(%arg4)[%c1_6, %c0_8]
        %30 = arith.addi %29, %28 : index
        %true = arith.constant true
        %31 = arith.muli %29, %c1_6 : index
        %32 = arith.addi %31, %28 : index
        %33 = arith.cmpi ult, %32, %24 : index
        %34 = arith.andi %true, %33 : i1
        scf.if %34 {
          %35 = arith.remsi %30, %c256 : index
          %36 = arith.divsi %30, %c256 : index
          %37 = arith.divui %36, %16 : index
          %38 = arith.remui %36, %16 : index
          %39 = arith.divui %35, %c32 : index
          %40 = arith.remui %35, %c32 : index
          %41 = arith.muli %40, %c8 : index
          %42 = arith.addi %39, %41 : index
          %43 = arith.muli %38, %c32 : index
          %44 = arith.addi %40, %43 : index
          %alloc_11 = memref.alloc() : memref<256xf32, #gpu.address_space<workgroup>>
          %45 = arith.cmpi ult, %44, %5 : index
          %46 = scf.if %45 -> (f32) {
            %51 = scf.for %arg13 = %c0 to %c64 step %c1 iter_args(%arg14 = %cst) -> (f32) {
              %52 = arith.muli %37, %c8 : index
              %53 = arith.addi %39, %52 : index
              %54 = arith.muli %53, %c64 : index
              %55 = arith.addi %arg13, %54 : index
              %56 = arith.cmpi slt, %55, %dim_1 : index
              %57 = scf.if %56 -> (f32) {
                %58 = arith.muli %55, %5 : index
                %59 = arith.addi %58, %44 : index
                %60 = arith.muli %dim_1, %dim_0 : index
                %61 = arith.muli %60, %dim : index
                %reinterpret_cast_13 = memref.reinterpret_cast %1 to offset: [%c0], sizes: [%61], strides: [%c1] : memref<?x?x?xf32, "gpu"> to memref<?xf32, "gpu">
                %62 = memref.load %reinterpret_cast_13[%59] : memref<?xf32, "gpu">
                %63 = math.absf %62 : f32
                %64 = arith.cmpf ugt, %arg14, %63 : f32
                %65 = arith.select %64, %arg14, %63 : f32
                %66 = arith.cmpf uno, %63, %63 : f32
                %67 = arith.select %66, %63, %65 : f32
                scf.yield %67 : f32
              } else {
                scf.yield %arg14 : f32
              }
              scf.yield %57 : f32
            }
            scf.yield %51 : f32
          } else {
            scf.yield %cst : f32
          }
          %reinterpret_cast_12 = memref.reinterpret_cast %alloc_11 to offset: [0], sizes: [256], strides: [1] : memref<256xf32, #gpu.address_space<workgroup>> to memref<256xf32, #gpu.address_space<workgroup>>
          memref.store %46, %reinterpret_cast_12[%42] : memref<256xf32, #gpu.address_space<workgroup>>
          gpu.barrier
          %47 = arith.cmpi slt, %39, %c4 : index
          scf.if %47 {
            %51 = arith.addi %42, %c4 : index
            %52 = memref.load %reinterpret_cast_12[%42] : memref<256xf32, #gpu.address_space<workgroup>>
            %53 = memref.load %reinterpret_cast_12[%51] : memref<256xf32, #gpu.address_space<workgroup>>
            %54 = arith.cmpf ugt, %52, %53 : f32
            %55 = arith.select %54, %52, %53 : f32
            %56 = arith.cmpf uno, %53, %53 : f32
            %57 = arith.select %56, %53, %55 : f32
            memref.store %57, %reinterpret_cast_12[%42] : memref<256xf32, #gpu.address_space<workgroup>>
          }
          gpu.barrier
          %48 = arith.cmpi slt, %39, %c2 : index
          scf.if %48 {
            %51 = arith.addi %42, %c2 : index
            %52 = memref.load %reinterpret_cast_12[%42] : memref<256xf32, #gpu.address_space<workgroup>>
            %53 = memref.load %reinterpret_cast_12[%51] : memref<256xf32, #gpu.address_space<workgroup>>
            %54 = arith.cmpf ugt, %52, %53 : f32
            %55 = arith.select %54, %52, %53 : f32
            %56 = arith.cmpf uno, %53, %53 : f32
            %57 = arith.select %56, %53, %55 : f32
            memref.store %57, %reinterpret_cast_12[%42] : memref<256xf32, #gpu.address_space<workgroup>>
          }
          gpu.barrier
          %49 = arith.cmpi eq, %39, %c0 : index
          %50 = arith.andi %49, %45 : i1
          scf.if %50 {
            %51 = arith.addi %42, %c1 : index
            %52 = memref.load %reinterpret_cast_12[%42] : memref<256xf32, #gpu.address_space<workgroup>>
            %53 = memref.load %reinterpret_cast_12[%51] : memref<256xf32, #gpu.address_space<workgroup>>
            %54 = arith.cmpf ugt, %52, %53 : f32
            %55 = arith.select %54, %52, %53 : f32
            %56 = arith.cmpf uno, %53, %53 : f32
            %57 = arith.select %56, %53, %55 : f32
            %58 = memref.generic_atomic_rmw %alloc[%44] : memref<?xf32, "gpu"> {
            ^bb0(%arg13: f32):
              %59 = arith.cmpf ogt, %arg13, %57 : f32
              %60 = arith.select %59, %arg13, %57 : f32
              memref.atomic_yield %60 : f32
            }
          }
        }
        gpu.terminator
      } {SCFToGPU_visited}
      "lmhlo.terminator"() : () -> ()
    }) {disc.device = "gpu", disc.fusion.name = "main_kColReduction_reduce__4_1_0", disc.fusion.tag = "thin", disc.fusion_type = "kColReduction", disc_col_reduction_schedule_hint = 8 : i32, disc_thread_per_block_hint = 256 : i32} : () -> ()
  }
  %7 = llvm.inttoptr %0 : i32 to !llvm.ptr<i8>
  %alloca = memref.alloca() : memref<2xindex, "cpu">
  memref.store %dim_0, %alloca[%c0] : memref<2xindex, "cpu">
  memref.store %dim, %alloca[%c1] : memref<2xindex, "cpu">
  %8 = "disc_ral.dispatch"(%arg0, %7, %alloc, %alloca) {backend_config = "", call_target_name = "inc_ref", device = "gpu", has_side_effect = false} : (!disc_ral.context, !llvm.ptr<i8>, memref<?xf32, "gpu">, memref<2xindex, "cpu">) -> memref<?x?xf32, "gpu">
  %reinterpret_cast = memref.reinterpret_cast %8 to offset: [0], sizes: [%dim_0, %dim], strides: [%dim, 1] {kDiscSymbolicDimAttr = [@S1, @S2]} : memref<?x?xf32, "gpu"> to memref<?x?xf32, "gpu">
  memref.dealloc %alloc : memref<?xf32, "gpu">
  "disc_ral.dispatch"(%arg0, %c0, %reinterpret_cast) {backend_config = "", call_target_name = "ral_send_output", device = "cpu", has_side_effect = false} : (!disc_ral.context, index, memref<?x?xf32, "gpu">) -> ()
  return
}

// -----// IR Dump After GpuLaunchSinkIndexComputations (gpu-launch-sink-index-computations) //----- //
#map = affine_map<(d0)[s0, s1] -> ((d0 - s0) ceildiv s1)>
#map1 = affine_map<(d0)[s0, s1] -> (d0 * s0 + s1)>
module {
  func.func @main(%arg0: !disc_ral.context) attributes {tf.entry_function = {input_placements = "gpu", inputs = "input0", output_placements = "gpu", outputs = "output0"}} {
    %cst = arith.constant 0xFF800000 : f32
    %c4 = arith.constant 4 : index
    %c256 = arith.constant 256 : index
    %c64 = arith.constant 64 : index
    %c8 = arith.constant 8 : index
    %c32 = arith.constant 32 : index
    %c512 = arith.constant 512 : index
    %0 = llvm.mlir.constant(0 : i32) : i32
    %c0 = arith.constant 0 : index
    %c1 = arith.constant 1 : index
    %c2 = arith.constant 2 : index
    %1 = "disc_ral.dispatch"(%arg0, %c0) {backend_config = "", call_target_name = "ral_recv_input", device = "cpu", has_side_effect = false} : (!disc_ral.context, index) -> memref<?x?x?xf32, "gpu">
    %dim = memref.dim %1, %c2 : memref<?x?x?xf32, "gpu">
    %dim_0 = memref.dim %1, %c1 : memref<?x?x?xf32, "gpu">
    %dim_1 = memref.dim %1, %c0 : memref<?x?x?xf32, "gpu">
    %2 = arith.index_cast %dim_0 : index to i32
    %3 = arith.index_cast %dim : index to i32
    %4 = arith.muli %2, %3 : i32
    %5 = arith.index_cast %4 : i32 to index
    %alloc = memref.alloc(%5) {kDiscSymbolicDimAttr = [@S3]} : memref<?xf32, "gpu">
    %6 = arith.cmpi slt, %dim_1, %5 : index
    scf.if %6 {
      "lmhlo.fusion"() ({
        %c0_2 = arith.constant 0 : index
        %c512_3 = arith.constant 512 : index
        %9 = arith.muli %c1, %c512_3 : index
        %c1_4 = arith.constant 1 : index
        %10 = affine.apply #map(%5)[%c0, %9]
        %11 = affine.apply #map(%9)[%c0_2, %c1]
        gpu.launch blocks(%arg1, %arg2, %arg3) in (%arg7 = %10, %arg8 = %c1_4, %arg9 = %c1_4) threads(%arg4, %arg5, %arg6) in (%arg10 = %11, %arg11 = %c1_4, %arg12 = %c1_4) {
          %c0_11 = arith.constant 0 : index
          %c1_12 = arith.constant 1 : index
          %c0_13 = arith.constant 0 : index
          %cst_14 = arith.constant 0xFF800000 : f32
          %28 = affine.apply #map1(%arg1)[%9, %c0_11]
          %29 = affine.apply #map1(%arg4)[%c1_12, %c0_13]
          %30 = arith.addi %29, %28 : index
          %true = arith.constant true
          %31 = arith.muli %29, %c1_12 : index
          %32 = arith.addi %31, %28 : index
          %33 = arith.cmpi ult, %32, %5 : index
          %34 = arith.andi %true, %33 : i1
          scf.if %34 {
            %reinterpret_cast_15 = memref.reinterpret_cast %alloc to offset: [%c0_11], sizes: [%5], strides: [%c1_12] : memref<?xf32, "gpu"> to memref<?xf32, "gpu">
            memref.store %cst_14, %reinterpret_cast_15[%30] : memref<?xf32, "gpu">
          }
          gpu.terminator
        } {SCFToGPU_visited}
        %12 = arith.cmpi eq, %5, %c0 : index
        %13 = arith.subi %5, %c1 : index
        %14 = arith.divui %13, %c512 : index
        %15 = arith.addi %14, %c1 : index
        %16 = arith.select %12, %c0, %15 : index
        %17 = arith.cmpi eq, %dim_1, %c0 : index
        %18 = arith.subi %dim_1, %c1 : index
        %19 = arith.divui %18, %c32 : index
        %20 = arith.addi %19, %c1 : index
        %21 = arith.select %17, %c0, %20 : index
        %22 = arith.muli %16, %21 : index
        %c0_5 = arith.constant 0 : index
        %c1_6 = arith.constant 1 : index
        %c1_7 = arith.constant 1 : index
        %23 = arith.muli %c1_7, %22 : index
        %24 = arith.muli %23, %c512 : index
        %c0_8 = arith.constant 0 : index
        %c512_9 = arith.constant 512 : index
        %25 = arith.muli %c1_6, %c512_9 : index
        %c1_10 = arith.constant 1 : index
        %26 = affine.apply #map(%24)[%c0_5, %25]
        %27 = affine.apply #map(%25)[%c0_8, %c1_6]
        gpu.launch blocks(%arg1, %arg2, %arg3) in (%arg7 = %26, %arg8 = %c1_10, %arg9 = %c1_10) threads(%arg4, %arg5, %arg6) in (%arg10 = %27, %arg11 = %c1_10, %arg12 = %c1_10) {
          %c0_11 = arith.constant 0 : index
          %c1_12 = arith.constant 1 : index
          %c0_13 = arith.constant 0 : index
          %c512_14 = arith.constant 512 : index
          %c0_15 = arith.constant 0 : index
          %28 = arith.cmpi eq, %5, %c0_15 : index
          %c32_16 = arith.constant 32 : index
          %dim_17 = memref.dim %1, %c0_15 : memref<?x?x?xf32, "gpu">
          %c1_18 = arith.constant 1 : index
          %dim_19 = memref.dim %1, %c1_18 : memref<?x?x?xf32, "gpu">
          %c2_20 = arith.constant 2 : index
          %dim_21 = memref.dim %1, %c2_20 : memref<?x?x?xf32, "gpu">
          %cst_22 = arith.constant 0xFF800000 : f32
          %29 = affine.apply #map1(%arg1)[%25, %c0_11]
          %30 = affine.apply #map1(%arg4)[%c1_12, %c0_13]
          %31 = arith.addi %30, %29 : index
          %true = arith.constant true
          %32 = arith.muli %30, %c1_12 : index
          %33 = arith.addi %32, %29 : index
          %34 = arith.cmpi ult, %33, %24 : index
          %35 = arith.andi %true, %34 : i1
          scf.if %35 {
            %36 = arith.remsi %31, %c512_14 : index
            %37 = arith.divsi %31, %c512_14 : index
            %38 = arith.divui %37, %16 : index
            %39 = arith.remui %37, %16 : index
            %40 = arith.muli %39, %c512_14 : index
            %41 = arith.addi %40, %36 : index
            %42 = arith.cmpi ult, %41, %5 : index
            scf.if %42 {
              %43 = scf.for %arg13 = %c0_15 to %c32_16 step %c1_18 iter_args(%arg14 = %cst_22) -> (f32) {
                %45 = arith.muli %38, %c32_16 : index
                %46 = arith.addi %45, %arg13 : index
                %47 = arith.cmpi slt, %46, %dim_17 : index
                %48 = scf.if %47 -> (f32) {
                  %49 = arith.muli %46, %5 : index
                  %50 = arith.addi %49, %41 : index
                  %51 = arith.muli %dim_17, %dim_19 : index
                  %52 = arith.muli %51, %dim_21 : index
                  %reinterpret_cast_23 = memref.reinterpret_cast %1 to offset: [%c0_15], sizes: [%52], strides: [%c1_18] : memref<?x?x?xf32, "gpu"> to memref<?xf32, "gpu">
                  %53 = memref.load %reinterpret_cast_23[%50] : memref<?xf32, "gpu">
                  %54 = math.absf %53 : f32
                  %55 = arith.cmpf oge, %arg14, %54 : f32
                  %56 = arith.select %55, %arg14, %54 : f32
                  scf.yield %56 : f32
                } else {
                  scf.yield %arg14 : f32
                }
                scf.yield %48 : f32
              }
              %44 = memref.generic_atomic_rmw %alloc[%41] : memref<?xf32, "gpu"> {
              ^bb0(%arg13: f32):
                %45 = arith.cmpf ogt, %arg13, %43 : f32
                %46 = arith.select %45, %arg13, %43 : f32
                memref.atomic_yield %46 : f32
              }
            }
          }
          gpu.terminator
        } {SCFToGPU_visited}
        "lmhlo.terminator"() : () -> ()
      }) {disc.device = "gpu", disc.fusion.name = "main_kColReduction_reduce__4_1_0", disc.fusion.tag = "flat", disc.fusion_type = "kColReduction", disc_col_reduction_schedule_hint = 7 : i32, disc_thread_per_block_hint = 512 : i32} : () -> ()
    } else {
      "lmhlo.fusion"() ({
        %c0_2 = arith.constant 0 : index
        %c256_3 = arith.constant 256 : index
        %9 = arith.muli %c1, %c256_3 : index
        %c1_4 = arith.constant 1 : index
        %10 = affine.apply #map(%5)[%c0, %9]
        %11 = affine.apply #map(%9)[%c0_2, %c1]
        gpu.launch blocks(%arg1, %arg2, %arg3) in (%arg7 = %10, %arg8 = %c1_4, %arg9 = %c1_4) threads(%arg4, %arg5, %arg6) in (%arg10 = %11, %arg11 = %c1_4, %arg12 = %c1_4) {
          %c0_11 = arith.constant 0 : index
          %c1_12 = arith.constant 1 : index
          %c0_13 = arith.constant 0 : index
          %cst_14 = arith.constant 0xFF800000 : f32
          %28 = affine.apply #map1(%arg1)[%9, %c0_11]
          %29 = affine.apply #map1(%arg4)[%c1_12, %c0_13]
          %30 = arith.addi %29, %28 : index
          %true = arith.constant true
          %31 = arith.muli %29, %c1_12 : index
          %32 = arith.addi %31, %28 : index
          %33 = arith.cmpi ult, %32, %5 : index
          %34 = arith.andi %true, %33 : i1
          scf.if %34 {
            %reinterpret_cast_15 = memref.reinterpret_cast %alloc to offset: [%c0_11], sizes: [%5], strides: [%c1_12] : memref<?xf32, "gpu"> to memref<?xf32, "gpu">
            memref.store %cst_14, %reinterpret_cast_15[%30] : memref<?xf32, "gpu">
          }
          gpu.terminator
        } {SCFToGPU_visited}
        %12 = arith.cmpi eq, %5, %c0 : index
        %13 = arith.subi %5, %c1 : index
        %14 = arith.divui %13, %c32 : index
        %15 = arith.addi %14, %c1 : index
        %16 = arith.select %12, %c0, %15 : index
        %17 = arith.cmpi eq, %dim_1, %c0 : index
        %18 = arith.subi %dim_1, %c1 : index
        %19 = arith.divui %18, %c512 : index
        %20 = arith.addi %19, %c1 : index
        %21 = arith.select %17, %c0, %20 : index
        %22 = arith.muli %16, %21 : index
        %c0_5 = arith.constant 0 : index
        %c1_6 = arith.constant 1 : index
        %c1_7 = arith.constant 1 : index
        %23 = arith.muli %c1_7, %22 : index
        %24 = arith.muli %23, %c256 : index
        %c0_8 = arith.constant 0 : index
        %c256_9 = arith.constant 256 : index
        %25 = arith.muli %c1_6, %c256_9 : index
        %c1_10 = arith.constant 1 : index
        %26 = affine.apply #map(%24)[%c0_5, %25]
        %27 = affine.apply #map(%25)[%c0_8, %c1_6]
        gpu.launch blocks(%arg1, %arg2, %arg3) in (%arg7 = %26, %arg8 = %c1_10, %arg9 = %c1_10) threads(%arg4, %arg5, %arg6) in (%arg10 = %27, %arg11 = %c1_10, %arg12 = %c1_10) {
          %c0_11 = arith.constant 0 : index
          %c1_12 = arith.constant 1 : index
          %c0_13 = arith.constant 0 : index
          %c256_14 = arith.constant 256 : index
          %c0_15 = arith.constant 0 : index
          %28 = arith.cmpi eq, %5, %c0_15 : index
          %c32_16 = arith.constant 32 : index
          %c8_17 = arith.constant 8 : index
          %c64_18 = arith.constant 64 : index
          %dim_19 = memref.dim %1, %c0_15 : memref<?x?x?xf32, "gpu">
          %c1_20 = arith.constant 1 : index
          %dim_21 = memref.dim %1, %c1_20 : memref<?x?x?xf32, "gpu">
          %c2_22 = arith.constant 2 : index
          %dim_23 = memref.dim %1, %c2_22 : memref<?x?x?xf32, "gpu">
          %cst_24 = arith.constant 0xFF800000 : f32
          %c4_25 = arith.constant 4 : index
          %29 = affine.apply #map1(%arg1)[%25, %c0_11]
          %30 = affine.apply #map1(%arg4)[%c1_12, %c0_13]
          %31 = arith.addi %30, %29 : index
          %true = arith.constant true
          %32 = arith.muli %30, %c1_12 : index
          %33 = arith.addi %32, %29 : index
          %34 = arith.cmpi ult, %33, %24 : index
          %35 = arith.andi %true, %34 : i1
          scf.if %35 {
            %36 = arith.remsi %31, %c256_14 : index
            %37 = arith.divsi %31, %c256_14 : index
            %38 = arith.divui %37, %16 : index
            %39 = arith.remui %37, %16 : index
            %40 = arith.divui %36, %c32_16 : index
            %41 = arith.remui %36, %c32_16 : index
            %42 = arith.muli %41, %c8_17 : index
            %43 = arith.addi %40, %42 : index
            %44 = arith.muli %39, %c32_16 : index
            %45 = arith.addi %41, %44 : index
            %alloc_26 = memref.alloc() : memref<256xf32, #gpu.address_space<workgroup>>
            %46 = arith.cmpi ult, %45, %5 : index
            %47 = scf.if %46 -> (f32) {
              %52 = scf.for %arg13 = %c0_15 to %c64_18 step %c1_20 iter_args(%arg14 = %cst_24) -> (f32) {
                %53 = arith.muli %38, %c8_17 : index
                %54 = arith.addi %40, %53 : index
                %55 = arith.muli %54, %c64_18 : index
                %56 = arith.addi %arg13, %55 : index
                %57 = arith.cmpi slt, %56, %dim_19 : index
                %58 = scf.if %57 -> (f32) {
                  %59 = arith.muli %56, %5 : index
                  %60 = arith.addi %59, %45 : index
                  %61 = arith.muli %dim_19, %dim_21 : index
                  %62 = arith.muli %61, %dim_23 : index
                  %reinterpret_cast_28 = memref.reinterpret_cast %1 to offset: [%c0_15], sizes: [%62], strides: [%c1_20] : memref<?x?x?xf32, "gpu"> to memref<?xf32, "gpu">
                  %63 = memref.load %reinterpret_cast_28[%60] : memref<?xf32, "gpu">
                  %64 = math.absf %63 : f32
                  %65 = arith.cmpf ugt, %arg14, %64 : f32
                  %66 = arith.select %65, %arg14, %64 : f32
                  %67 = arith.cmpf uno, %64, %64 : f32
                  %68 = arith.select %67, %64, %66 : f32
                  scf.yield %68 : f32
                } else {
                  scf.yield %arg14 : f32
                }
                scf.yield %58 : f32
              }
              scf.yield %52 : f32
            } else {
              scf.yield %cst_24 : f32
            }
            %reinterpret_cast_27 = memref.reinterpret_cast %alloc_26 to offset: [0], sizes: [256], strides: [1] : memref<256xf32, #gpu.address_space<workgroup>> to memref<256xf32, #gpu.address_space<workgroup>>
            memref.store %47, %reinterpret_cast_27[%43] : memref<256xf32, #gpu.address_space<workgroup>>
            gpu.barrier
            %48 = arith.cmpi slt, %40, %c4_25 : index
            scf.if %48 {
              %52 = arith.addi %43, %c4_25 : index
              %53 = memref.load %reinterpret_cast_27[%43] : memref<256xf32, #gpu.address_space<workgroup>>
              %54 = memref.load %reinterpret_cast_27[%52] : memref<256xf32, #gpu.address_space<workgroup>>
              %55 = arith.cmpf ugt, %53, %54 : f32
              %56 = arith.select %55, %53, %54 : f32
              %57 = arith.cmpf uno, %54, %54 : f32
              %58 = arith.select %57, %54, %56 : f32
              memref.store %58, %reinterpret_cast_27[%43] : memref<256xf32, #gpu.address_space<workgroup>>
            }
            gpu.barrier
            %49 = arith.cmpi slt, %40, %c2_22 : index
            scf.if %49 {
              %52 = arith.addi %43, %c2_22 : index
              %53 = memref.load %reinterpret_cast_27[%43] : memref<256xf32, #gpu.address_space<workgroup>>
              %54 = memref.load %reinterpret_cast_27[%52] : memref<256xf32, #gpu.address_space<workgroup>>
              %55 = arith.cmpf ugt, %53, %54 : f32
              %56 = arith.select %55, %53, %54 : f32
              %57 = arith.cmpf uno, %54, %54 : f32
              %58 = arith.select %57, %54, %56 : f32
              memref.store %58, %reinterpret_cast_27[%43] : memref<256xf32, #gpu.address_space<workgroup>>
            }
            gpu.barrier
            %50 = arith.cmpi eq, %40, %c0_15 : index
            %51 = arith.andi %50, %46 : i1
            scf.if %51 {
              %52 = arith.addi %43, %c1_20 : index
              %53 = memref.load %reinterpret_cast_27[%43] : memref<256xf32, #gpu.address_space<workgroup>>
              %54 = memref.load %reinterpret_cast_27[%52] : memref<256xf32, #gpu.address_space<workgroup>>
              %55 = arith.cmpf ugt, %53, %54 : f32
              %56 = arith.select %55, %53, %54 : f32
              %57 = arith.cmpf uno, %54, %54 : f32
              %58 = arith.select %57, %54, %56 : f32
              %59 = memref.generic_atomic_rmw %alloc[%45] : memref<?xf32, "gpu"> {
              ^bb0(%arg13: f32):
                %60 = arith.cmpf ogt, %arg13, %58 : f32
                %61 = arith.select %60, %arg13, %58 : f32
                memref.atomic_yield %61 : f32
              }
            }
          }
          gpu.terminator
        } {SCFToGPU_visited}
        "lmhlo.terminator"() : () -> ()
      }) {disc.device = "gpu", disc.fusion.name = "main_kColReduction_reduce__4_1_0", disc.fusion.tag = "thin", disc.fusion_type = "kColReduction", disc_col_reduction_schedule_hint = 8 : i32, disc_thread_per_block_hint = 256 : i32} : () -> ()
    }
    %7 = llvm.inttoptr %0 : i32 to !llvm.ptr<i8>
    %alloca = memref.alloca() : memref<2xindex, "cpu">
    memref.store %dim_0, %alloca[%c0] : memref<2xindex, "cpu">
    memref.store %dim, %alloca[%c1] : memref<2xindex, "cpu">
    %8 = "disc_ral.dispatch"(%arg0, %7, %alloc, %alloca) {backend_config = "", call_target_name = "inc_ref", device = "gpu", has_side_effect = false} : (!disc_ral.context, !llvm.ptr<i8>, memref<?xf32, "gpu">, memref<2xindex, "cpu">) -> memref<?x?xf32, "gpu">
    %reinterpret_cast = memref.reinterpret_cast %8 to offset: [0], sizes: [%dim_0, %dim], strides: [%dim, 1] {kDiscSymbolicDimAttr = [@S1, @S2]} : memref<?x?xf32, "gpu"> to memref<?x?xf32, "gpu">
    memref.dealloc %alloc : memref<?xf32, "gpu">
    "disc_ral.dispatch"(%arg0, %c0, %reinterpret_cast) {backend_config = "", call_target_name = "ral_send_output", device = "cpu", has_side_effect = false} : (!disc_ral.context, index, memref<?x?xf32, "gpu">) -> ()
    return
  }
  "disc_shape.SymbolicDim"() {knownNegativeOne = false, knownNonNegative = true, knownNonSizeOne = false, knownNonSizeZero = false, sym_name = "S0", value = -9223372036854775808 : i64} : () -> ()
  "disc_shape.SymbolicDim"() {knownNegativeOne = false, knownNonNegative = true, knownNonSizeOne = false, knownNonSizeZero = false, sym_name = "S1", value = -9223372036854775808 : i64} : () -> ()
  "disc_shape.SymbolicDim"() {knownNegativeOne = false, knownNonNegative = true, knownNonSizeOne = false, knownNonSizeZero = false, sym_name = "S2", value = -9223372036854775808 : i64} : () -> ()
  "disc_shape.SymbolicDim"() {knownNegativeOne = false, knownNonNegative = true, knownNonSizeOne = false, knownNonSizeZero = false, sym_name = "S3", value = -9223372036854775808 : i64} : () -> ()
  func.func @shape_constraint_graph() {
    %0 = "disc_shape.dim"() {name = @S3} : () -> index
    %1 = "disc_shape.dim"() {name = @S1} : () -> index
    %2 = "disc_shape.dim"() {name = @S2} : () -> index
    "disc_shape.tie_product_equal"(%0, %1, %2) {operand_segment_sizes = array<i32: 1, 2>} : (index, index, index) -> ()
    return
  }
}


// -----// IR Dump After GpuKernelOutlining (gpu-kernel-outlining) //----- //
#map = affine_map<(d0)[s0, s1] -> ((d0 - s0) ceildiv s1)>
#map1 = affine_map<(d0)[s0, s1] -> (d0 * s0 + s1)>
module attributes {gpu.container_module} {
  func.func @main(%arg0: !disc_ral.context) attributes {tf.entry_function = {input_placements = "gpu", inputs = "input0", output_placements = "gpu", outputs = "output0"}} {
    %cst = arith.constant 0xFF800000 : f32
    %c4 = arith.constant 4 : index
    %c256 = arith.constant 256 : index
    %c64 = arith.constant 64 : index
    %c8 = arith.constant 8 : index
    %c32 = arith.constant 32 : index
    %c512 = arith.constant 512 : index
    %0 = llvm.mlir.constant(0 : i32) : i32
    %c0 = arith.constant 0 : index
    %c1 = arith.constant 1 : index
    %c2 = arith.constant 2 : index
    %1 = "disc_ral.dispatch"(%arg0, %c0) {backend_config = "", call_target_name = "ral_recv_input", device = "cpu", has_side_effect = false} : (!disc_ral.context, index) -> memref<?x?x?xf32, "gpu">
    %dim = memref.dim %1, %c2 : memref<?x?x?xf32, "gpu">
    %dim_0 = memref.dim %1, %c1 : memref<?x?x?xf32, "gpu">
    %dim_1 = memref.dim %1, %c0 : memref<?x?x?xf32, "gpu">
    %2 = arith.index_cast %dim_0 : index to i32
    %3 = arith.index_cast %dim : index to i32
    %4 = arith.muli %2, %3 : i32
    %5 = arith.index_cast %4 : i32 to index
    %alloc = memref.alloc(%5) {kDiscSymbolicDimAttr = [@S3]} : memref<?xf32, "gpu">
    %6 = arith.cmpi slt, %dim_1, %5 : index
    scf.if %6 {
      "lmhlo.fusion"() ({
        %c0_2 = arith.constant 0 : index
        %c512_3 = arith.constant 512 : index
        %9 = arith.muli %c1, %c512_3 : index
        %c1_4 = arith.constant 1 : index
        %10 = affine.apply #map(%5)[%c0, %9]
        %11 = affine.apply #map(%9)[%c0_2, %c1]
        gpu.launch_func  @main_kernel::@main_kernel blocks in (%10, %c1_4, %c1_4) threads in (%11, %c1_4, %c1_4) args(%9 : index, %5 : index, %alloc : memref<?xf32, "gpu">)
        %12 = arith.cmpi eq, %5, %c0 : index
        %13 = arith.subi %5, %c1 : index
        %14 = arith.divui %13, %c512 : index
        %15 = arith.addi %14, %c1 : index
        %16 = arith.select %12, %c0, %15 : index
        %17 = arith.cmpi eq, %dim_1, %c0 : index
        %18 = arith.subi %dim_1, %c1 : index
        %19 = arith.divui %18, %c32 : index
        %20 = arith.addi %19, %c1 : index
        %21 = arith.select %17, %c0, %20 : index
        %22 = arith.muli %16, %21 : index
        %c0_5 = arith.constant 0 : index
        %c1_6 = arith.constant 1 : index
        %c1_7 = arith.constant 1 : index
        %23 = arith.muli %c1_7, %22 : index
        %24 = arith.muli %23, %c512 : index
        %c0_8 = arith.constant 0 : index
        %c512_9 = arith.constant 512 : index
        %25 = arith.muli %c1_6, %c512_9 : index
        %c1_10 = arith.constant 1 : index
        %26 = affine.apply #map(%24)[%c0_5, %25]
        %27 = affine.apply #map(%25)[%c0_8, %c1_6]
        gpu.launch_func  @main_kernel_0::@main_kernel blocks in (%26, %c1_10, %c1_10) threads in (%27, %c1_10, %c1_10) args(%5 : index, %1 : memref<?x?x?xf32, "gpu">, %25 : index, %24 : index, %16 : index, %alloc : memref<?xf32, "gpu">)
        "lmhlo.terminator"() : () -> ()
      }) {disc.device = "gpu", disc.fusion.name = "main_kColReduction_reduce__4_1_0", disc.fusion.tag = "flat", disc.fusion_type = "kColReduction", disc_col_reduction_schedule_hint = 7 : i32, disc_thread_per_block_hint = 512 : i32} : () -> ()
    } else {
      "lmhlo.fusion"() ({
        %c0_2 = arith.constant 0 : index
        %c256_3 = arith.constant 256 : index
        %9 = arith.muli %c1, %c256_3 : index
        %c1_4 = arith.constant 1 : index
        %10 = affine.apply #map(%5)[%c0, %9]
        %11 = affine.apply #map(%9)[%c0_2, %c1]
        gpu.launch_func  @main_kernel_1::@main_kernel blocks in (%10, %c1_4, %c1_4) threads in (%11, %c1_4, %c1_4) args(%9 : index, %5 : index, %alloc : memref<?xf32, "gpu">)
        %12 = arith.cmpi eq, %5, %c0 : index
        %13 = arith.subi %5, %c1 : index
        %14 = arith.divui %13, %c32 : index
        %15 = arith.addi %14, %c1 : index
        %16 = arith.select %12, %c0, %15 : index
        %17 = arith.cmpi eq, %dim_1, %c0 : index
        %18 = arith.subi %dim_1, %c1 : index
        %19 = arith.divui %18, %c512 : index
        %20 = arith.addi %19, %c1 : index
        %21 = arith.select %17, %c0, %20 : index
        %22 = arith.muli %16, %21 : index
        %c0_5 = arith.constant 0 : index
        %c1_6 = arith.constant 1 : index
        %c1_7 = arith.constant 1 : index
        %23 = arith.muli %c1_7, %22 : index
        %24 = arith.muli %23, %c256 : index
        %c0_8 = arith.constant 0 : index
        %c256_9 = arith.constant 256 : index
        %25 = arith.muli %c1_6, %c256_9 : index
        %c1_10 = arith.constant 1 : index
        %26 = affine.apply #map(%24)[%c0_5, %25]
        %27 = affine.apply #map(%25)[%c0_8, %c1_6]
        gpu.launch_func  @main_kernel_2::@main_kernel blocks in (%26, %c1_10, %c1_10) threads in (%27, %c1_10, %c1_10) args(%5 : index, %1 : memref<?x?x?xf32, "gpu">, %25 : index, %24 : index, %16 : index, %alloc : memref<?xf32, "gpu">)
        "lmhlo.terminator"() : () -> ()
      }) {disc.device = "gpu", disc.fusion.name = "main_kColReduction_reduce__4_1_0", disc.fusion.tag = "thin", disc.fusion_type = "kColReduction", disc_col_reduction_schedule_hint = 8 : i32, disc_thread_per_block_hint = 256 : i32} : () -> ()
    }
    %7 = llvm.inttoptr %0 : i32 to !llvm.ptr<i8>
    %alloca = memref.alloca() : memref<2xindex, "cpu">
    memref.store %dim_0, %alloca[%c0] : memref<2xindex, "cpu">
    memref.store %dim, %alloca[%c1] : memref<2xindex, "cpu">
    %8 = "disc_ral.dispatch"(%arg0, %7, %alloc, %alloca) {backend_config = "", call_target_name = "inc_ref", device = "gpu", has_side_effect = false} : (!disc_ral.context, !llvm.ptr<i8>, memref<?xf32, "gpu">, memref<2xindex, "cpu">) -> memref<?x?xf32, "gpu">
    %reinterpret_cast = memref.reinterpret_cast %8 to offset: [0], sizes: [%dim_0, %dim], strides: [%dim, 1] {kDiscSymbolicDimAttr = [@S1, @S2]} : memref<?x?xf32, "gpu"> to memref<?x?xf32, "gpu">
    memref.dealloc %alloc : memref<?xf32, "gpu">
    "disc_ral.dispatch"(%arg0, %c0, %reinterpret_cast) {backend_config = "", call_target_name = "ral_send_output", device = "cpu", has_side_effect = false} : (!disc_ral.context, index, memref<?x?xf32, "gpu">) -> ()
    return
  }
  gpu.module @main_kernel {
    gpu.func @main_kernel(%arg0: index, %arg1: index, %arg2: memref<?xf32, "gpu">) kernel {
      %0 = gpu.block_id  x
      %1 = gpu.block_id  y
      %2 = gpu.block_id  z
      %3 = gpu.thread_id  x
      %4 = gpu.thread_id  y
      %5 = gpu.thread_id  z
      %6 = gpu.grid_dim  x
      %7 = gpu.grid_dim  y
      %8 = gpu.grid_dim  z
      %9 = gpu.block_dim  x
      %10 = gpu.block_dim  y
      %11 = gpu.block_dim  z
      cf.br ^bb1
    ^bb1:  // pred: ^bb0
      %c0 = arith.constant 0 : index
      %c1 = arith.constant 1 : index
      %c0_0 = arith.constant 0 : index
      %cst = arith.constant 0xFF800000 : f32
      %12 = affine.apply #map1(%0)[%arg0, %c0]
      %13 = affine.apply #map1(%3)[%c1, %c0_0]
      %14 = arith.addi %13, %12 : index
      %true = arith.constant true
      %15 = arith.muli %13, %c1 : index
      %16 = arith.addi %15, %12 : index
      %17 = arith.cmpi ult, %16, %arg1 : index
      %18 = arith.andi %true, %17 : i1
      scf.if %18 {
        %reinterpret_cast = memref.reinterpret_cast %arg2 to offset: [%c0], sizes: [%arg1], strides: [%c1] : memref<?xf32, "gpu"> to memref<?xf32, "gpu">
        memref.store %cst, %reinterpret_cast[%14] : memref<?xf32, "gpu">
      }
      gpu.return
    }
  }
  gpu.module @main_kernel_0 {
    gpu.func @main_kernel(%arg0: index, %arg1: memref<?x?x?xf32, "gpu">, %arg2: index, %arg3: index, %arg4: index, %arg5: memref<?xf32, "gpu">) kernel {
      %0 = gpu.block_id  x
      %1 = gpu.block_id  y
      %2 = gpu.block_id  z
      %3 = gpu.thread_id  x
      %4 = gpu.thread_id  y
      %5 = gpu.thread_id  z
      %6 = gpu.grid_dim  x
      %7 = gpu.grid_dim  y
      %8 = gpu.grid_dim  z
      %9 = gpu.block_dim  x
      %10 = gpu.block_dim  y
      %11 = gpu.block_dim  z
      cf.br ^bb1
    ^bb1:  // pred: ^bb0
      %c0 = arith.constant 0 : index
      %c1 = arith.constant 1 : index
      %c0_0 = arith.constant 0 : index
      %c512 = arith.constant 512 : index
      %c0_1 = arith.constant 0 : index
      %12 = arith.cmpi eq, %arg0, %c0_1 : index
      %c32 = arith.constant 32 : index
      %dim = memref.dim %arg1, %c0_1 : memref<?x?x?xf32, "gpu">
      %c1_2 = arith.constant 1 : index
      %dim_3 = memref.dim %arg1, %c1_2 : memref<?x?x?xf32, "gpu">
      %c2 = arith.constant 2 : index
      %dim_4 = memref.dim %arg1, %c2 : memref<?x?x?xf32, "gpu">
      %cst = arith.constant 0xFF800000 : f32
      %13 = affine.apply #map1(%0)[%arg2, %c0]
      %14 = affine.apply #map1(%3)[%c1, %c0_0]
      %15 = arith.addi %14, %13 : index
      %true = arith.constant true
      %16 = arith.muli %14, %c1 : index
      %17 = arith.addi %16, %13 : index
      %18 = arith.cmpi ult, %17, %arg3 : index
      %19 = arith.andi %true, %18 : i1
      scf.if %19 {
        %20 = arith.remsi %15, %c512 : index
        %21 = arith.divsi %15, %c512 : index
        %22 = arith.divui %21, %arg4 : index
        %23 = arith.remui %21, %arg4 : index
        %24 = arith.muli %23, %c512 : index
        %25 = arith.addi %24, %20 : index
        %26 = arith.cmpi ult, %25, %arg0 : index
        scf.if %26 {
          %27 = scf.for %arg6 = %c0_1 to %c32 step %c1_2 iter_args(%arg7 = %cst) -> (f32) {
            %29 = arith.muli %22, %c32 : index
            %30 = arith.addi %29, %arg6 : index
            %31 = arith.cmpi slt, %30, %dim : index
            %32 = scf.if %31 -> (f32) {
              %33 = arith.muli %30, %arg0 : index
              %34 = arith.addi %33, %25 : index
              %35 = arith.muli %dim, %dim_3 : index
              %36 = arith.muli %35, %dim_4 : index
              %reinterpret_cast = memref.reinterpret_cast %arg1 to offset: [%c0_1], sizes: [%36], strides: [%c1_2] : memref<?x?x?xf32, "gpu"> to memref<?xf32, "gpu">
              %37 = memref.load %reinterpret_cast[%34] : memref<?xf32, "gpu">
              %38 = math.absf %37 : f32
              %39 = arith.cmpf oge, %arg7, %38 : f32
              %40 = arith.select %39, %arg7, %38 : f32
              scf.yield %40 : f32
            } else {
              scf.yield %arg7 : f32
            }
            scf.yield %32 : f32
          }
          %28 = memref.generic_atomic_rmw %arg5[%25] : memref<?xf32, "gpu"> {
          ^bb0(%arg6: f32):
            %29 = arith.cmpf ogt, %arg6, %27 : f32
            %30 = arith.select %29, %arg6, %27 : f32
            memref.atomic_yield %30 : f32
          }
        }
      }
      gpu.return
    }
  }
  gpu.module @main_kernel_1 {
    gpu.func @main_kernel(%arg0: index, %arg1: index, %arg2: memref<?xf32, "gpu">) kernel {
      %0 = gpu.block_id  x
      %1 = gpu.block_id  y
      %2 = gpu.block_id  z
      %3 = gpu.thread_id  x
      %4 = gpu.thread_id  y
      %5 = gpu.thread_id  z
      %6 = gpu.grid_dim  x
      %7 = gpu.grid_dim  y
      %8 = gpu.grid_dim  z
      %9 = gpu.block_dim  x
      %10 = gpu.block_dim  y
      %11 = gpu.block_dim  z
      cf.br ^bb1
    ^bb1:  // pred: ^bb0
      %c0 = arith.constant 0 : index
      %c1 = arith.constant 1 : index
      %c0_0 = arith.constant 0 : index
      %cst = arith.constant 0xFF800000 : f32
      %12 = affine.apply #map1(%0)[%arg0, %c0]
      %13 = affine.apply #map1(%3)[%c1, %c0_0]
      %14 = arith.addi %13, %12 : index
      %true = arith.constant true
      %15 = arith.muli %13, %c1 : index
      %16 = arith.addi %15, %12 : index
      %17 = arith.cmpi ult, %16, %arg1 : index
      %18 = arith.andi %true, %17 : i1
      scf.if %18 {
        %reinterpret_cast = memref.reinterpret_cast %arg2 to offset: [%c0], sizes: [%arg1], strides: [%c1] : memref<?xf32, "gpu"> to memref<?xf32, "gpu">
        memref.store %cst, %reinterpret_cast[%14] : memref<?xf32, "gpu">
      }
      gpu.return
    }
  }
  gpu.module @main_kernel_2 {
    gpu.func @main_kernel(%arg0: index, %arg1: memref<?x?x?xf32, "gpu">, %arg2: index, %arg3: index, %arg4: index, %arg5: memref<?xf32, "gpu">) kernel {
      %0 = gpu.block_id  x
      %1 = gpu.block_id  y
      %2 = gpu.block_id  z
      %3 = gpu.thread_id  x
      %4 = gpu.thread_id  y
      %5 = gpu.thread_id  z
      %6 = gpu.grid_dim  x
      %7 = gpu.grid_dim  y
      %8 = gpu.grid_dim  z
      %9 = gpu.block_dim  x
      %10 = gpu.block_dim  y
      %11 = gpu.block_dim  z
      cf.br ^bb1
    ^bb1:  // pred: ^bb0
      %c0 = arith.constant 0 : index
      %c1 = arith.constant 1 : index
      %c0_0 = arith.constant 0 : index
      %c256 = arith.constant 256 : index
      %c0_1 = arith.constant 0 : index
      %12 = arith.cmpi eq, %arg0, %c0_1 : index
      %c32 = arith.constant 32 : index
      %c8 = arith.constant 8 : index
      %c64 = arith.constant 64 : index
      %dim = memref.dim %arg1, %c0_1 : memref<?x?x?xf32, "gpu">
      %c1_2 = arith.constant 1 : index
      %dim_3 = memref.dim %arg1, %c1_2 : memref<?x?x?xf32, "gpu">
      %c2 = arith.constant 2 : index
      %dim_4 = memref.dim %arg1, %c2 : memref<?x?x?xf32, "gpu">
      %cst = arith.constant 0xFF800000 : f32
      %c4 = arith.constant 4 : index
      %13 = affine.apply #map1(%0)[%arg2, %c0]
      %14 = affine.apply #map1(%3)[%c1, %c0_0]
      %15 = arith.addi %14, %13 : index
      %true = arith.constant true
      %16 = arith.muli %14, %c1 : index
      %17 = arith.addi %16, %13 : index
      %18 = arith.cmpi ult, %17, %arg3 : index
      %19 = arith.andi %true, %18 : i1
      scf.if %19 {
        %20 = arith.remsi %15, %c256 : index
        %21 = arith.divsi %15, %c256 : index
        %22 = arith.divui %21, %arg4 : index
        %23 = arith.remui %21, %arg4 : index
        %24 = arith.divui %20, %c32 : index
        %25 = arith.remui %20, %c32 : index
        %26 = arith.muli %25, %c8 : index
        %27 = arith.addi %24, %26 : index
        %28 = arith.muli %23, %c32 : index
        %29 = arith.addi %25, %28 : index
        %alloc = memref.alloc() : memref<256xf32, #gpu.address_space<workgroup>>
        %30 = arith.cmpi ult, %29, %arg0 : index
        %31 = scf.if %30 -> (f32) {
          %36 = scf.for %arg6 = %c0_1 to %c64 step %c1_2 iter_args(%arg7 = %cst) -> (f32) {
            %37 = arith.muli %22, %c8 : index
            %38 = arith.addi %24, %37 : index
            %39 = arith.muli %38, %c64 : index
            %40 = arith.addi %arg6, %39 : index
            %41 = arith.cmpi slt, %40, %dim : index
            %42 = scf.if %41 -> (f32) {
              %43 = arith.muli %40, %arg0 : index
              %44 = arith.addi %43, %29 : index
              %45 = arith.muli %dim, %dim_3 : index
              %46 = arith.muli %45, %dim_4 : index
              %reinterpret_cast_5 = memref.reinterpret_cast %arg1 to offset: [%c0_1], sizes: [%46], strides: [%c1_2] : memref<?x?x?xf32, "gpu"> to memref<?xf32, "gpu">
              %47 = memref.load %reinterpret_cast_5[%44] : memref<?xf32, "gpu">
              %48 = math.absf %47 : f32
              %49 = arith.cmpf ugt, %arg7, %48 : f32
              %50 = arith.select %49, %arg7, %48 : f32
              %51 = arith.cmpf uno, %48, %48 : f32
              %52 = arith.select %51, %48, %50 : f32
              scf.yield %52 : f32
            } else {
              scf.yield %arg7 : f32
            }
            scf.yield %42 : f32
          }
          scf.yield %36 : f32
        } else {
          scf.yield %cst : f32
        }
        %reinterpret_cast = memref.reinterpret_cast %alloc to offset: [0], sizes: [256], strides: [1] : memref<256xf32, #gpu.address_space<workgroup>> to memref<256xf32, #gpu.address_space<workgroup>>
        memref.store %31, %reinterpret_cast[%27] : memref<256xf32, #gpu.address_space<workgroup>>
        gpu.barrier
        %32 = arith.cmpi slt, %24, %c4 : index
        scf.if %32 {
          %36 = arith.addi %27, %c4 : index
          %37 = memref.load %reinterpret_cast[%27] : memref<256xf32, #gpu.address_space<workgroup>>
          %38 = memref.load %reinterpret_cast[%36] : memref<256xf32, #gpu.address_space<workgroup>>
          %39 = arith.cmpf ugt, %37, %38 : f32
          %40 = arith.select %39, %37, %38 : f32
          %41 = arith.cmpf uno, %38, %38 : f32
          %42 = arith.select %41, %38, %40 : f32
          memref.store %42, %reinterpret_cast[%27] : memref<256xf32, #gpu.address_space<workgroup>>
        }
        gpu.barrier
        %33 = arith.cmpi slt, %24, %c2 : index
        scf.if %33 {
          %36 = arith.addi %27, %c2 : index
          %37 = memref.load %reinterpret_cast[%27] : memref<256xf32, #gpu.address_space<workgroup>>
          %38 = memref.load %reinterpret_cast[%36] : memref<256xf32, #gpu.address_space<workgroup>>
          %39 = arith.cmpf ugt, %37, %38 : f32
          %40 = arith.select %39, %37, %38 : f32
          %41 = arith.cmpf uno, %38, %38 : f32
          %42 = arith.select %41, %38, %40 : f32
          memref.store %42, %reinterpret_cast[%27] : memref<256xf32, #gpu.address_space<workgroup>>
        }
        gpu.barrier
        %34 = arith.cmpi eq, %24, %c0_1 : index
        %35 = arith.andi %34, %30 : i1
        scf.if %35 {
          %36 = arith.addi %27, %c1_2 : index
          %37 = memref.load %reinterpret_cast[%27] : memref<256xf32, #gpu.address_space<workgroup>>
          %38 = memref.load %reinterpret_cast[%36] : memref<256xf32, #gpu.address_space<workgroup>>
          %39 = arith.cmpf ugt, %37, %38 : f32
          %40 = arith.select %39, %37, %38 : f32
          %41 = arith.cmpf uno, %38, %38 : f32
          %42 = arith.select %41, %38, %40 : f32
          %43 = memref.generic_atomic_rmw %arg5[%29] : memref<?xf32, "gpu"> {
          ^bb0(%arg6: f32):
            %44 = arith.cmpf ogt, %arg6, %42 : f32
            %45 = arith.select %44, %arg6, %42 : f32
            memref.atomic_yield %45 : f32
          }
        }
      }
      gpu.return
    }
  }
  "disc_shape.SymbolicDim"() {knownNegativeOne = false, knownNonNegative = true, knownNonSizeOne = false, knownNonSizeZero = false, sym_name = "S0", value = -9223372036854775808 : i64} : () -> ()
  "disc_shape.SymbolicDim"() {knownNegativeOne = false, knownNonNegative = true, knownNonSizeOne = false, knownNonSizeZero = false, sym_name = "S1", value = -9223372036854775808 : i64} : () -> ()
  "disc_shape.SymbolicDim"() {knownNegativeOne = false, knownNonNegative = true, knownNonSizeOne = false, knownNonSizeZero = false, sym_name = "S2", value = -9223372036854775808 : i64} : () -> ()
  "disc_shape.SymbolicDim"() {knownNegativeOne = false, knownNonNegative = true, knownNonSizeOne = false, knownNonSizeZero = false, sym_name = "S3", value = -9223372036854775808 : i64} : () -> ()
  func.func @shape_constraint_graph() {
    %0 = "disc_shape.dim"() {name = @S3} : () -> index
    %1 = "disc_shape.dim"() {name = @S1} : () -> index
    %2 = "disc_shape.dim"() {name = @S2} : () -> index
    "disc_shape.tie_product_equal"(%0, %1, %2) {operand_segment_sizes = array<i32: 1, 2>} : (index, index, index) -> ()
    return
  }
}


// -----// IR Dump After AssignKernelNamePass (disc-assign-kernel-name) //----- //
#map = affine_map<(d0)[s0, s1] -> ((d0 - s0) ceildiv s1)>
#map1 = affine_map<(d0)[s0, s1] -> (d0 * s0 + s1)>
module attributes {gpu.container_module} {
  func.func @main(%arg0: !disc_ral.context) attributes {tf.entry_function = {input_placements = "gpu", inputs = "input0", output_placements = "gpu", outputs = "output0"}} {
    %cst = arith.constant 0xFF800000 : f32
    %c4 = arith.constant 4 : index
    %c256 = arith.constant 256 : index
    %c64 = arith.constant 64 : index
    %c8 = arith.constant 8 : index
    %c32 = arith.constant 32 : index
    %c512 = arith.constant 512 : index
    %0 = llvm.mlir.constant(0 : i32) : i32
    %c0 = arith.constant 0 : index
    %c1 = arith.constant 1 : index
    %c2 = arith.constant 2 : index
    %1 = "disc_ral.dispatch"(%arg0, %c0) {backend_config = "", call_target_name = "ral_recv_input", device = "cpu", has_side_effect = false} : (!disc_ral.context, index) -> memref<?x?x?xf32, "gpu">
    %dim = memref.dim %1, %c2 : memref<?x?x?xf32, "gpu">
    %dim_0 = memref.dim %1, %c1 : memref<?x?x?xf32, "gpu">
    %dim_1 = memref.dim %1, %c0 : memref<?x?x?xf32, "gpu">
    %2 = arith.index_cast %dim_0 : index to i32
    %3 = arith.index_cast %dim : index to i32
    %4 = arith.muli %2, %3 : i32
    %5 = arith.index_cast %4 : i32 to index
    %alloc = memref.alloc(%5) {kDiscSymbolicDimAttr = [@S3]} : memref<?xf32, "gpu">
    %6 = arith.cmpi slt, %dim_1, %5 : index
    scf.if %6 {
      "lmhlo.fusion"() ({
        %c0_2 = arith.constant 0 : index
        %c512_3 = arith.constant 512 : index
        %9 = arith.muli %c1, %c512_3 : index
        %c1_4 = arith.constant 1 : index
        %10 = affine.apply #map(%5)[%c0, %9]
        %11 = affine.apply #map(%9)[%c0_2, %c1]
        gpu.launch_func  @main_kernel::@main_kColReduction_reduce__4_1_0___flat blocks in (%10, %c1_4, %c1_4) threads in (%11, %c1_4, %c1_4) args(%9 : index, %5 : index, %alloc : memref<?xf32, "gpu">)
        %12 = arith.cmpi eq, %5, %c0 : index
        %13 = arith.subi %5, %c1 : index
        %14 = arith.divui %13, %c512 : index
        %15 = arith.addi %14, %c1 : index
        %16 = arith.select %12, %c0, %15 : index
        %17 = arith.cmpi eq, %dim_1, %c0 : index
        %18 = arith.subi %dim_1, %c1 : index
        %19 = arith.divui %18, %c32 : index
        %20 = arith.addi %19, %c1 : index
        %21 = arith.select %17, %c0, %20 : index
        %22 = arith.muli %16, %21 : index
        %c0_5 = arith.constant 0 : index
        %c1_6 = arith.constant 1 : index
        %c1_7 = arith.constant 1 : index
        %23 = arith.muli %c1_7, %22 : index
        %24 = arith.muli %23, %c512 : index
        %c0_8 = arith.constant 0 : index
        %c512_9 = arith.constant 512 : index
        %25 = arith.muli %c1_6, %c512_9 : index
        %c1_10 = arith.constant 1 : index
        %26 = affine.apply #map(%24)[%c0_5, %25]
        %27 = affine.apply #map(%25)[%c0_8, %c1_6]
        gpu.launch_func  @main_kernel_0::@main_kColReduction_reduce__4_1_0___flat_1 blocks in (%26, %c1_10, %c1_10) threads in (%27, %c1_10, %c1_10) args(%5 : index, %1 : memref<?x?x?xf32, "gpu">, %25 : index, %24 : index, %16 : index, %alloc : memref<?xf32, "gpu">)
        "lmhlo.terminator"() : () -> ()
      }) {disc.device = "gpu", disc.fusion.name = "main_kColReduction_reduce__4_1_0", disc.fusion.tag = "flat", disc.fusion_type = "kColReduction", disc_col_reduction_schedule_hint = 7 : i32, disc_thread_per_block_hint = 512 : i32} : () -> ()
    } else {
      "lmhlo.fusion"() ({
        %c0_2 = arith.constant 0 : index
        %c256_3 = arith.constant 256 : index
        %9 = arith.muli %c1, %c256_3 : index
        %c1_4 = arith.constant 1 : index
        %10 = affine.apply #map(%5)[%c0, %9]
        %11 = affine.apply #map(%9)[%c0_2, %c1]
        gpu.launch_func  @main_kernel_1::@main_kColReduction_reduce__4_1_0___thin blocks in (%10, %c1_4, %c1_4) threads in (%11, %c1_4, %c1_4) args(%9 : index, %5 : index, %alloc : memref<?xf32, "gpu">)
        %12 = arith.cmpi eq, %5, %c0 : index
        %13 = arith.subi %5, %c1 : index
        %14 = arith.divui %13, %c32 : index
        %15 = arith.addi %14, %c1 : index
        %16 = arith.select %12, %c0, %15 : index
        %17 = arith.cmpi eq, %dim_1, %c0 : index
        %18 = arith.subi %dim_1, %c1 : index
        %19 = arith.divui %18, %c512 : index
        %20 = arith.addi %19, %c1 : index
        %21 = arith.select %17, %c0, %20 : index
        %22 = arith.muli %16, %21 : index
        %c0_5 = arith.constant 0 : index
        %c1_6 = arith.constant 1 : index
        %c1_7 = arith.constant 1 : index
        %23 = arith.muli %c1_7, %22 : index
        %24 = arith.muli %23, %c256 : index
        %c0_8 = arith.constant 0 : index
        %c256_9 = arith.constant 256 : index
        %25 = arith.muli %c1_6, %c256_9 : index
        %c1_10 = arith.constant 1 : index
        %26 = affine.apply #map(%24)[%c0_5, %25]
        %27 = affine.apply #map(%25)[%c0_8, %c1_6]
        gpu.launch_func  @main_kernel_2::@main_kColReduction_reduce__4_1_0___thin_1 blocks in (%26, %c1_10, %c1_10) threads in (%27, %c1_10, %c1_10) args(%5 : index, %1 : memref<?x?x?xf32, "gpu">, %25 : index, %24 : index, %16 : index, %alloc : memref<?xf32, "gpu">)
        "lmhlo.terminator"() : () -> ()
      }) {disc.device = "gpu", disc.fusion.name = "main_kColReduction_reduce__4_1_0", disc.fusion.tag = "thin", disc.fusion_type = "kColReduction", disc_col_reduction_schedule_hint = 8 : i32, disc_thread_per_block_hint = 256 : i32} : () -> ()
    }
    %7 = llvm.inttoptr %0 : i32 to !llvm.ptr<i8>
    %alloca = memref.alloca() : memref<2xindex, "cpu">
    memref.store %dim_0, %alloca[%c0] : memref<2xindex, "cpu">
    memref.store %dim, %alloca[%c1] : memref<2xindex, "cpu">
    %8 = "disc_ral.dispatch"(%arg0, %7, %alloc, %alloca) {backend_config = "", call_target_name = "inc_ref", device = "gpu", has_side_effect = false} : (!disc_ral.context, !llvm.ptr<i8>, memref<?xf32, "gpu">, memref<2xindex, "cpu">) -> memref<?x?xf32, "gpu">
    %reinterpret_cast = memref.reinterpret_cast %8 to offset: [0], sizes: [%dim_0, %dim], strides: [%dim, 1] {kDiscSymbolicDimAttr = [@S1, @S2]} : memref<?x?xf32, "gpu"> to memref<?x?xf32, "gpu">
    memref.dealloc %alloc : memref<?xf32, "gpu">
    "disc_ral.dispatch"(%arg0, %c0, %reinterpret_cast) {backend_config = "", call_target_name = "ral_send_output", device = "cpu", has_side_effect = false} : (!disc_ral.context, index, memref<?x?xf32, "gpu">) -> ()
    return
  }
  gpu.module @main_kernel {
    gpu.func @main_kColReduction_reduce__4_1_0___flat(%arg0: index, %arg1: index, %arg2: memref<?xf32, "gpu">) kernel {
      %0 = gpu.block_id  x
      %1 = gpu.block_id  y
      %2 = gpu.block_id  z
      %3 = gpu.thread_id  x
      %4 = gpu.thread_id  y
      %5 = gpu.thread_id  z
      %6 = gpu.grid_dim  x
      %7 = gpu.grid_dim  y
      %8 = gpu.grid_dim  z
      %9 = gpu.block_dim  x
      %10 = gpu.block_dim  y
      %11 = gpu.block_dim  z
      cf.br ^bb1
    ^bb1:  // pred: ^bb0
      %c0 = arith.constant 0 : index
      %c1 = arith.constant 1 : index
      %c0_0 = arith.constant 0 : index
      %cst = arith.constant 0xFF800000 : f32
      %12 = affine.apply #map1(%0)[%arg0, %c0]
      %13 = affine.apply #map1(%3)[%c1, %c0_0]
      %14 = arith.addi %13, %12 : index
      %true = arith.constant true
      %15 = arith.muli %13, %c1 : index
      %16 = arith.addi %15, %12 : index
      %17 = arith.cmpi ult, %16, %arg1 : index
      %18 = arith.andi %true, %17 : i1
      scf.if %18 {
        %reinterpret_cast = memref.reinterpret_cast %arg2 to offset: [%c0], sizes: [%arg1], strides: [%c1] : memref<?xf32, "gpu"> to memref<?xf32, "gpu">
        memref.store %cst, %reinterpret_cast[%14] : memref<?xf32, "gpu">
      }
      gpu.return
    }
  }
  gpu.module @main_kernel_0 {
    gpu.func @main_kColReduction_reduce__4_1_0___flat_1(%arg0: index, %arg1: memref<?x?x?xf32, "gpu">, %arg2: index, %arg3: index, %arg4: index, %arg5: memref<?xf32, "gpu">) kernel {
      %0 = gpu.block_id  x
      %1 = gpu.block_id  y
      %2 = gpu.block_id  z
      %3 = gpu.thread_id  x
      %4 = gpu.thread_id  y
      %5 = gpu.thread_id  z
      %6 = gpu.grid_dim  x
      %7 = gpu.grid_dim  y
      %8 = gpu.grid_dim  z
      %9 = gpu.block_dim  x
      %10 = gpu.block_dim  y
      %11 = gpu.block_dim  z
      cf.br ^bb1
    ^bb1:  // pred: ^bb0
      %c0 = arith.constant 0 : index
      %c1 = arith.constant 1 : index
      %c0_0 = arith.constant 0 : index
      %c512 = arith.constant 512 : index
      %c0_1 = arith.constant 0 : index
      %12 = arith.cmpi eq, %arg0, %c0_1 : index
      %c32 = arith.constant 32 : index
      %dim = memref.dim %arg1, %c0_1 : memref<?x?x?xf32, "gpu">
      %c1_2 = arith.constant 1 : index
      %dim_3 = memref.dim %arg1, %c1_2 : memref<?x?x?xf32, "gpu">
      %c2 = arith.constant 2 : index
      %dim_4 = memref.dim %arg1, %c2 : memref<?x?x?xf32, "gpu">
      %cst = arith.constant 0xFF800000 : f32
      %13 = affine.apply #map1(%0)[%arg2, %c0]
      %14 = affine.apply #map1(%3)[%c1, %c0_0]
      %15 = arith.addi %14, %13 : index
      %true = arith.constant true
      %16 = arith.muli %14, %c1 : index
      %17 = arith.addi %16, %13 : index
      %18 = arith.cmpi ult, %17, %arg3 : index
      %19 = arith.andi %true, %18 : i1
      scf.if %19 {
        %20 = arith.remsi %15, %c512 : index
        %21 = arith.divsi %15, %c512 : index
        %22 = arith.divui %21, %arg4 : index
        %23 = arith.remui %21, %arg4 : index
        %24 = arith.muli %23, %c512 : index
        %25 = arith.addi %24, %20 : index
        %26 = arith.cmpi ult, %25, %arg0 : index
        scf.if %26 {
          %27 = scf.for %arg6 = %c0_1 to %c32 step %c1_2 iter_args(%arg7 = %cst) -> (f32) {
            %29 = arith.muli %22, %c32 : index
            %30 = arith.addi %29, %arg6 : index
            %31 = arith.cmpi slt, %30, %dim : index
            %32 = scf.if %31 -> (f32) {
              %33 = arith.muli %30, %arg0 : index
              %34 = arith.addi %33, %25 : index
              %35 = arith.muli %dim, %dim_3 : index
              %36 = arith.muli %35, %dim_4 : index
              %reinterpret_cast = memref.reinterpret_cast %arg1 to offset: [%c0_1], sizes: [%36], strides: [%c1_2] : memref<?x?x?xf32, "gpu"> to memref<?xf32, "gpu">
              %37 = memref.load %reinterpret_cast[%34] : memref<?xf32, "gpu">
              %38 = math.absf %37 : f32
              %39 = arith.cmpf oge, %arg7, %38 : f32
              %40 = arith.select %39, %arg7, %38 : f32
              scf.yield %40 : f32
            } else {
              scf.yield %arg7 : f32
            }
            scf.yield %32 : f32
          }
          %28 = memref.generic_atomic_rmw %arg5[%25] : memref<?xf32, "gpu"> {
          ^bb0(%arg6: f32):
            %29 = arith.cmpf ogt, %arg6, %27 : f32
            %30 = arith.select %29, %arg6, %27 : f32
            memref.atomic_yield %30 : f32
          }
        }
      }
      gpu.return
    }
  }
  gpu.module @main_kernel_1 {
    gpu.func @main_kColReduction_reduce__4_1_0___thin(%arg0: index, %arg1: index, %arg2: memref<?xf32, "gpu">) kernel {
      %0 = gpu.block_id  x
      %1 = gpu.block_id  y
      %2 = gpu.block_id  z
      %3 = gpu.thread_id  x
      %4 = gpu.thread_id  y
      %5 = gpu.thread_id  z
      %6 = gpu.grid_dim  x
      %7 = gpu.grid_dim  y
      %8 = gpu.grid_dim  z
      %9 = gpu.block_dim  x
      %10 = gpu.block_dim  y
      %11 = gpu.block_dim  z
      cf.br ^bb1
    ^bb1:  // pred: ^bb0
      %c0 = arith.constant 0 : index
      %c1 = arith.constant 1 : index
      %c0_0 = arith.constant 0 : index
      %cst = arith.constant 0xFF800000 : f32
      %12 = affine.apply #map1(%0)[%arg0, %c0]
      %13 = affine.apply #map1(%3)[%c1, %c0_0]
      %14 = arith.addi %13, %12 : index
      %true = arith.constant true
      %15 = arith.muli %13, %c1 : index
      %16 = arith.addi %15, %12 : index
      %17 = arith.cmpi ult, %16, %arg1 : index
      %18 = arith.andi %true, %17 : i1
      scf.if %18 {
        %reinterpret_cast = memref.reinterpret_cast %arg2 to offset: [%c0], sizes: [%arg1], strides: [%c1] : memref<?xf32, "gpu"> to memref<?xf32, "gpu">
        memref.store %cst, %reinterpret_cast[%14] : memref<?xf32, "gpu">
      }
      gpu.return
    }
  }
  gpu.module @main_kernel_2 {
    gpu.func @main_kColReduction_reduce__4_1_0___thin_1(%arg0: index, %arg1: memref<?x?x?xf32, "gpu">, %arg2: index, %arg3: index, %arg4: index, %arg5: memref<?xf32, "gpu">) kernel {
      %0 = gpu.block_id  x
      %1 = gpu.block_id  y
      %2 = gpu.block_id  z
      %3 = gpu.thread_id  x
      %4 = gpu.thread_id  y
      %5 = gpu.thread_id  z
      %6 = gpu.grid_dim  x
      %7 = gpu.grid_dim  y
      %8 = gpu.grid_dim  z
      %9 = gpu.block_dim  x
      %10 = gpu.block_dim  y
      %11 = gpu.block_dim  z
      cf.br ^bb1
    ^bb1:  // pred: ^bb0
      %c0 = arith.constant 0 : index
      %c1 = arith.constant 1 : index
      %c0_0 = arith.constant 0 : index
      %c256 = arith.constant 256 : index
      %c0_1 = arith.constant 0 : index
      %12 = arith.cmpi eq, %arg0, %c0_1 : index
      %c32 = arith.constant 32 : index
      %c8 = arith.constant 8 : index
      %c64 = arith.constant 64 : index
      %dim = memref.dim %arg1, %c0_1 : memref<?x?x?xf32, "gpu">
      %c1_2 = arith.constant 1 : index
      %dim_3 = memref.dim %arg1, %c1_2 : memref<?x?x?xf32, "gpu">
      %c2 = arith.constant 2 : index
      %dim_4 = memref.dim %arg1, %c2 : memref<?x?x?xf32, "gpu">
      %cst = arith.constant 0xFF800000 : f32
      %c4 = arith.constant 4 : index
      %13 = affine.apply #map1(%0)[%arg2, %c0]
      %14 = affine.apply #map1(%3)[%c1, %c0_0]
      %15 = arith.addi %14, %13 : index
      %true = arith.constant true
      %16 = arith.muli %14, %c1 : index
      %17 = arith.addi %16, %13 : index
      %18 = arith.cmpi ult, %17, %arg3 : index
      %19 = arith.andi %true, %18 : i1
      scf.if %19 {
        %20 = arith.remsi %15, %c256 : index
        %21 = arith.divsi %15, %c256 : index
        %22 = arith.divui %21, %arg4 : index
        %23 = arith.remui %21, %arg4 : index
        %24 = arith.divui %20, %c32 : index
        %25 = arith.remui %20, %c32 : index
        %26 = arith.muli %25, %c8 : index
        %27 = arith.addi %24, %26 : index
        %28 = arith.muli %23, %c32 : index
        %29 = arith.addi %25, %28 : index
        %alloc = memref.alloc() : memref<256xf32, #gpu.address_space<workgroup>>
        %30 = arith.cmpi ult, %29, %arg0 : index
        %31 = scf.if %30 -> (f32) {
          %36 = scf.for %arg6 = %c0_1 to %c64 step %c1_2 iter_args(%arg7 = %cst) -> (f32) {
            %37 = arith.muli %22, %c8 : index
            %38 = arith.addi %24, %37 : index
            %39 = arith.muli %38, %c64 : index
            %40 = arith.addi %arg6, %39 : index
            %41 = arith.cmpi slt, %40, %dim : index
            %42 = scf.if %41 -> (f32) {
              %43 = arith.muli %40, %arg0 : index
              %44 = arith.addi %43, %29 : index
              %45 = arith.muli %dim, %dim_3 : index
              %46 = arith.muli %45, %dim_4 : index
              %reinterpret_cast_5 = memref.reinterpret_cast %arg1 to offset: [%c0_1], sizes: [%46], strides: [%c1_2] : memref<?x?x?xf32, "gpu"> to memref<?xf32, "gpu">
              %47 = memref.load %reinterpret_cast_5[%44] : memref<?xf32, "gpu">
              %48 = math.absf %47 : f32
              %49 = arith.cmpf ugt, %arg7, %48 : f32
              %50 = arith.select %49, %arg7, %48 : f32
              %51 = arith.cmpf uno, %48, %48 : f32
              %52 = arith.select %51, %48, %50 : f32
              scf.yield %52 : f32
            } else {
              scf.yield %arg7 : f32
            }
            scf.yield %42 : f32
          }
          scf.yield %36 : f32
        } else {
          scf.yield %cst : f32
        }
        %reinterpret_cast = memref.reinterpret_cast %alloc to offset: [0], sizes: [256], strides: [1] : memref<256xf32, #gpu.address_space<workgroup>> to memref<256xf32, #gpu.address_space<workgroup>>
        memref.store %31, %reinterpret_cast[%27] : memref<256xf32, #gpu.address_space<workgroup>>
        gpu.barrier
        %32 = arith.cmpi slt, %24, %c4 : index
        scf.if %32 {
          %36 = arith.addi %27, %c4 : index
          %37 = memref.load %reinterpret_cast[%27] : memref<256xf32, #gpu.address_space<workgroup>>
          %38 = memref.load %reinterpret_cast[%36] : memref<256xf32, #gpu.address_space<workgroup>>
          %39 = arith.cmpf ugt, %37, %38 : f32
          %40 = arith.select %39, %37, %38 : f32
          %41 = arith.cmpf uno, %38, %38 : f32
          %42 = arith.select %41, %38, %40 : f32
          memref.store %42, %reinterpret_cast[%27] : memref<256xf32, #gpu.address_space<workgroup>>
        }
        gpu.barrier
        %33 = arith.cmpi slt, %24, %c2 : index
        scf.if %33 {
          %36 = arith.addi %27, %c2 : index
          %37 = memref.load %reinterpret_cast[%27] : memref<256xf32, #gpu.address_space<workgroup>>
          %38 = memref.load %reinterpret_cast[%36] : memref<256xf32, #gpu.address_space<workgroup>>
          %39 = arith.cmpf ugt, %37, %38 : f32
          %40 = arith.select %39, %37, %38 : f32
          %41 = arith.cmpf uno, %38, %38 : f32
          %42 = arith.select %41, %38, %40 : f32
          memref.store %42, %reinterpret_cast[%27] : memref<256xf32, #gpu.address_space<workgroup>>
        }
        gpu.barrier
        %34 = arith.cmpi eq, %24, %c0_1 : index
        %35 = arith.andi %34, %30 : i1
        scf.if %35 {
          %36 = arith.addi %27, %c1_2 : index
          %37 = memref.load %reinterpret_cast[%27] : memref<256xf32, #gpu.address_space<workgroup>>
          %38 = memref.load %reinterpret_cast[%36] : memref<256xf32, #gpu.address_space<workgroup>>
          %39 = arith.cmpf ugt, %37, %38 : f32
          %40 = arith.select %39, %37, %38 : f32
          %41 = arith.cmpf uno, %38, %38 : f32
          %42 = arith.select %41, %38, %40 : f32
          %43 = memref.generic_atomic_rmw %arg5[%29] : memref<?xf32, "gpu"> {
          ^bb0(%arg6: f32):
            %44 = arith.cmpf ogt, %arg6, %42 : f32
            %45 = arith.select %44, %arg6, %42 : f32
            memref.atomic_yield %45 : f32
          }
        }
      }
      gpu.return
    }
  }
  "disc_shape.SymbolicDim"() {knownNegativeOne = false, knownNonNegative = true, knownNonSizeOne = false, knownNonSizeZero = false, sym_name = "S0", value = -9223372036854775808 : i64} : () -> ()
  "disc_shape.SymbolicDim"() {knownNegativeOne = false, knownNonNegative = true, knownNonSizeOne = false, knownNonSizeZero = false, sym_name = "S1", value = -9223372036854775808 : i64} : () -> ()
  "disc_shape.SymbolicDim"() {knownNegativeOne = false, knownNonNegative = true, knownNonSizeOne = false, knownNonSizeZero = false, sym_name = "S2", value = -9223372036854775808 : i64} : () -> ()
  "disc_shape.SymbolicDim"() {knownNegativeOne = false, knownNonNegative = true, knownNonSizeOne = false, knownNonSizeZero = false, sym_name = "S3", value = -9223372036854775808 : i64} : () -> ()
  func.func @shape_constraint_graph() {
    %0 = "disc_shape.dim"() {name = @S3} : () -> index
    %1 = "disc_shape.dim"() {name = @S1} : () -> index
    %2 = "disc_shape.dim"() {name = @S2} : () -> index
    "disc_shape.tie_product_equal"(%0, %1, %2) {operand_segment_sizes = array<i32: 1, 2>} : (index, index, index) -> ()
    return
  }
}


// -----// IR Dump After LhloFusionInlinerPass (lhlo-fusion-inliner) //----- //
func.func @main(%arg0: !disc_ral.context) attributes {tf.entry_function = {input_placements = "gpu", inputs = "input0", output_placements = "gpu", outputs = "output0"}} {
  %cst = arith.constant 0xFF800000 : f32
  %c4 = arith.constant 4 : index
  %c256 = arith.constant 256 : index
  %c64 = arith.constant 64 : index
  %c8 = arith.constant 8 : index
  %c32 = arith.constant 32 : index
  %c512 = arith.constant 512 : index
  %0 = llvm.mlir.constant(0 : i32) : i32
  %c0 = arith.constant 0 : index
  %c1 = arith.constant 1 : index
  %c2 = arith.constant 2 : index
  %1 = "disc_ral.dispatch"(%arg0, %c0) {backend_config = "", call_target_name = "ral_recv_input", device = "cpu", has_side_effect = false} : (!disc_ral.context, index) -> memref<?x?x?xf32, "gpu">
  %dim = memref.dim %1, %c2 : memref<?x?x?xf32, "gpu">
  %dim_0 = memref.dim %1, %c1 : memref<?x?x?xf32, "gpu">
  %dim_1 = memref.dim %1, %c0 : memref<?x?x?xf32, "gpu">
  %2 = arith.index_cast %dim_0 : index to i32
  %3 = arith.index_cast %dim : index to i32
  %4 = arith.muli %2, %3 : i32
  %5 = arith.index_cast %4 : i32 to index
  %alloc = memref.alloc(%5) {kDiscSymbolicDimAttr = [@S3]} : memref<?xf32, "gpu">
  %6 = arith.cmpi slt, %dim_1, %5 : index
  scf.if %6 {
    %c0_2 = arith.constant 0 : index
    %c512_3 = arith.constant 512 : index
    %9 = arith.muli %c1, %c512_3 : index
    %c1_4 = arith.constant 1 : index
    %10 = affine.apply affine_map<(d0)[s0, s1] -> ((d0 - s0) ceildiv s1)>(%5)[%c0, %9]
    %11 = affine.apply affine_map<(d0)[s0, s1] -> ((d0 - s0) ceildiv s1)>(%9)[%c0_2, %c1]
    gpu.launch_func  @main_kernel::@main_kColReduction_reduce__4_1_0___flat blocks in (%10, %c1_4, %c1_4) threads in (%11, %c1_4, %c1_4) args(%9 : index, %5 : index, %alloc : memref<?xf32, "gpu">)
    %12 = arith.cmpi eq, %5, %c0 : index
    %13 = arith.subi %5, %c1 : index
    %14 = arith.divui %13, %c512 : index
    %15 = arith.addi %14, %c1 : index
    %16 = arith.select %12, %c0, %15 : index
    %17 = arith.cmpi eq, %dim_1, %c0 : index
    %18 = arith.subi %dim_1, %c1 : index
    %19 = arith.divui %18, %c32 : index
    %20 = arith.addi %19, %c1 : index
    %21 = arith.select %17, %c0, %20 : index
    %22 = arith.muli %16, %21 : index
    %c0_5 = arith.constant 0 : index
    %c1_6 = arith.constant 1 : index
    %c1_7 = arith.constant 1 : index
    %23 = arith.muli %c1_7, %22 : index
    %24 = arith.muli %23, %c512 : index
    %c0_8 = arith.constant 0 : index
    %c512_9 = arith.constant 512 : index
    %25 = arith.muli %c1_6, %c512_9 : index
    %c1_10 = arith.constant 1 : index
    %26 = affine.apply affine_map<(d0)[s0, s1] -> ((d0 - s0) ceildiv s1)>(%24)[%c0_5, %25]
    %27 = affine.apply affine_map<(d0)[s0, s1] -> ((d0 - s0) ceildiv s1)>(%25)[%c0_8, %c1_6]
    gpu.launch_func  @main_kernel_0::@main_kColReduction_reduce__4_1_0___flat_1 blocks in (%26, %c1_10, %c1_10) threads in (%27, %c1_10, %c1_10) args(%5 : index, %1 : memref<?x?x?xf32, "gpu">, %25 : index, %24 : index, %16 : index, %alloc : memref<?xf32, "gpu">)
  } else {
    %c0_2 = arith.constant 0 : index
    %c256_3 = arith.constant 256 : index
    %9 = arith.muli %c1, %c256_3 : index
    %c1_4 = arith.constant 1 : index
    %10 = affine.apply affine_map<(d0)[s0, s1] -> ((d0 - s0) ceildiv s1)>(%5)[%c0, %9]
    %11 = affine.apply affine_map<(d0)[s0, s1] -> ((d0 - s0) ceildiv s1)>(%9)[%c0_2, %c1]
    gpu.launch_func  @main_kernel_1::@main_kColReduction_reduce__4_1_0___thin blocks in (%10, %c1_4, %c1_4) threads in (%11, %c1_4, %c1_4) args(%9 : index, %5 : index, %alloc : memref<?xf32, "gpu">)
    %12 = arith.cmpi eq, %5, %c0 : index
    %13 = arith.subi %5, %c1 : index
    %14 = arith.divui %13, %c32 : index
    %15 = arith.addi %14, %c1 : index
    %16 = arith.select %12, %c0, %15 : index
    %17 = arith.cmpi eq, %dim_1, %c0 : index
    %18 = arith.subi %dim_1, %c1 : index
    %19 = arith.divui %18, %c512 : index
    %20 = arith.addi %19, %c1 : index
    %21 = arith.select %17, %c0, %20 : index
    %22 = arith.muli %16, %21 : index
    %c0_5 = arith.constant 0 : index
    %c1_6 = arith.constant 1 : index
    %c1_7 = arith.constant 1 : index
    %23 = arith.muli %c1_7, %22 : index
    %24 = arith.muli %23, %c256 : index
    %c0_8 = arith.constant 0 : index
    %c256_9 = arith.constant 256 : index
    %25 = arith.muli %c1_6, %c256_9 : index
    %c1_10 = arith.constant 1 : index
    %26 = affine.apply affine_map<(d0)[s0, s1] -> ((d0 - s0) ceildiv s1)>(%24)[%c0_5, %25]
    %27 = affine.apply affine_map<(d0)[s0, s1] -> ((d0 - s0) ceildiv s1)>(%25)[%c0_8, %c1_6]
    gpu.launch_func  @main_kernel_2::@main_kColReduction_reduce__4_1_0___thin_1 blocks in (%26, %c1_10, %c1_10) threads in (%27, %c1_10, %c1_10) args(%5 : index, %1 : memref<?x?x?xf32, "gpu">, %25 : index, %24 : index, %16 : index, %alloc : memref<?xf32, "gpu">)
  }
  %7 = llvm.inttoptr %0 : i32 to !llvm.ptr<i8>
  %alloca = memref.alloca() : memref<2xindex, "cpu">
  memref.store %dim_0, %alloca[%c0] : memref<2xindex, "cpu">
  memref.store %dim, %alloca[%c1] : memref<2xindex, "cpu">
  %8 = "disc_ral.dispatch"(%arg0, %7, %alloc, %alloca) {backend_config = "", call_target_name = "inc_ref", device = "gpu", has_side_effect = false} : (!disc_ral.context, !llvm.ptr<i8>, memref<?xf32, "gpu">, memref<2xindex, "cpu">) -> memref<?x?xf32, "gpu">
  %reinterpret_cast = memref.reinterpret_cast %8 to offset: [0], sizes: [%dim_0, %dim], strides: [%dim, 1] {kDiscSymbolicDimAttr = [@S1, @S2]} : memref<?x?xf32, "gpu"> to memref<?x?xf32, "gpu">
  memref.dealloc %alloc : memref<?xf32, "gpu">
  "disc_ral.dispatch"(%arg0, %c0, %reinterpret_cast) {backend_config = "", call_target_name = "ral_send_output", device = "cpu", has_side_effect = false} : (!disc_ral.context, index, memref<?x?xf32, "gpu">) -> ()
  return
}

// -----// IR Dump After ReviseGpuKernelOutliningPass (disc-revise-gpu-kernel-outlining) //----- //
#map = affine_map<(d0)[s0, s1] -> ((d0 - s0) ceildiv s1)>
#map1 = affine_map<(d0)[s0, s1] -> (d0 * s0 + s1)>
module attributes {gpu.container_module} {
  func.func @main(%arg0: !disc_ral.context) attributes {tf.entry_function = {input_placements = "gpu", inputs = "input0", output_placements = "gpu", outputs = "output0"}} {
    %cst = arith.constant 0xFF800000 : f32
    %c4 = arith.constant 4 : index
    %c256 = arith.constant 256 : index
    %c64 = arith.constant 64 : index
    %c8 = arith.constant 8 : index
    %c32 = arith.constant 32 : index
    %c512 = arith.constant 512 : index
    %0 = llvm.mlir.constant(0 : i32) : i32
    %c0 = arith.constant 0 : index
    %c1 = arith.constant 1 : index
    %c2 = arith.constant 2 : index
    %1 = "disc_ral.dispatch"(%arg0, %c0) {backend_config = "", call_target_name = "ral_recv_input", device = "cpu", has_side_effect = false} : (!disc_ral.context, index) -> memref<?x?x?xf32, "gpu">
    %dim = memref.dim %1, %c2 : memref<?x?x?xf32, "gpu">
    %dim_0 = memref.dim %1, %c1 : memref<?x?x?xf32, "gpu">
    %dim_1 = memref.dim %1, %c0 : memref<?x?x?xf32, "gpu">
    %2 = arith.index_cast %dim_0 : index to i32
    %3 = arith.index_cast %dim : index to i32
    %4 = arith.muli %2, %3 : i32
    %5 = arith.index_cast %4 : i32 to index
    %alloc = memref.alloc(%5) {kDiscSymbolicDimAttr = [@S3]} : memref<?xf32, "gpu">
    %6 = arith.cmpi slt, %dim_1, %5 : index
    scf.if %6 {
      %c0_2 = arith.constant 0 : index
      %c512_3 = arith.constant 512 : index
      %9 = arith.muli %c1, %c512_3 : index
      %c1_4 = arith.constant 1 : index
      %10 = affine.apply #map(%5)[%c0, %9]
      %11 = affine.apply #map(%9)[%c0_2, %c1]
      gpu.launch_func  @main_kernel::@main_kColReduction_reduce__4_1_0___flat blocks in (%10, %c1_4, %c1_4) threads in (%11, %c1_4, %c1_4) args(%9 : index, %5 : index, %alloc : memref<?xf32, "gpu">)
      %12 = arith.cmpi eq, %5, %c0 : index
      %13 = arith.subi %5, %c1 : index
      %14 = arith.divui %13, %c512 : index
      %15 = arith.addi %14, %c1 : index
      %16 = arith.select %12, %c0, %15 : index
      %17 = arith.cmpi eq, %dim_1, %c0 : index
      %18 = arith.subi %dim_1, %c1 : index
      %19 = arith.divui %18, %c32 : index
      %20 = arith.addi %19, %c1 : index
      %21 = arith.select %17, %c0, %20 : index
      %22 = arith.muli %16, %21 : index
      %c0_5 = arith.constant 0 : index
      %c1_6 = arith.constant 1 : index
      %c1_7 = arith.constant 1 : index
      %23 = arith.muli %c1_7, %22 : index
      %24 = arith.muli %23, %c512 : index
      %c0_8 = arith.constant 0 : index
      %c512_9 = arith.constant 512 : index
      %25 = arith.muli %c1_6, %c512_9 : index
      %c1_10 = arith.constant 1 : index
      %26 = affine.apply #map(%24)[%c0_5, %25]
      %27 = affine.apply #map(%25)[%c0_8, %c1_6]
      gpu.launch_func  @main_kernel_0::@main_kColReduction_reduce__4_1_0___flat_1 blocks in (%26, %c1_10, %c1_10) threads in (%27, %c1_10, %c1_10) args(%5 : index, %1 : memref<?x?x?xf32, "gpu">, %25 : index, %24 : index, %16 : index, %alloc : memref<?xf32, "gpu">)
    } else {
      %c0_2 = arith.constant 0 : index
      %c256_3 = arith.constant 256 : index
      %9 = arith.muli %c1, %c256_3 : index
      %c1_4 = arith.constant 1 : index
      %10 = affine.apply #map(%5)[%c0, %9]
      %11 = affine.apply #map(%9)[%c0_2, %c1]
      gpu.launch_func  @main_kernel_1::@main_kColReduction_reduce__4_1_0___thin blocks in (%10, %c1_4, %c1_4) threads in (%11, %c1_4, %c1_4) args(%9 : index, %5 : index, %alloc : memref<?xf32, "gpu">)
      %12 = arith.cmpi eq, %5, %c0 : index
      %13 = arith.subi %5, %c1 : index
      %14 = arith.divui %13, %c32 : index
      %15 = arith.addi %14, %c1 : index
      %16 = arith.select %12, %c0, %15 : index
      %17 = arith.cmpi eq, %dim_1, %c0 : index
      %18 = arith.subi %dim_1, %c1 : index
      %19 = arith.divui %18, %c512 : index
      %20 = arith.addi %19, %c1 : index
      %21 = arith.select %17, %c0, %20 : index
      %22 = arith.muli %16, %21 : index
      %c0_5 = arith.constant 0 : index
      %c1_6 = arith.constant 1 : index
      %c1_7 = arith.constant 1 : index
      %23 = arith.muli %c1_7, %22 : index
      %24 = arith.muli %23, %c256 : index
      %c0_8 = arith.constant 0 : index
      %c256_9 = arith.constant 256 : index
      %25 = arith.muli %c1_6, %c256_9 : index
      %c1_10 = arith.constant 1 : index
      %26 = affine.apply #map(%24)[%c0_5, %25]
      %27 = affine.apply #map(%25)[%c0_8, %c1_6]
      gpu.launch_func  @main_kernel_2::@main_kColReduction_reduce__4_1_0___thin_1 blocks in (%26, %c1_10, %c1_10) threads in (%27, %c1_10, %c1_10) args(%5 : index, %1 : memref<?x?x?xf32, "gpu">, %25 : index, %24 : index, %16 : index, %alloc : memref<?xf32, "gpu">)
    }
    %7 = llvm.inttoptr %0 : i32 to !llvm.ptr<i8>
    %alloca = memref.alloca() : memref<2xindex, "cpu">
    memref.store %dim_0, %alloca[%c0] : memref<2xindex, "cpu">
    memref.store %dim, %alloca[%c1] : memref<2xindex, "cpu">
    %8 = "disc_ral.dispatch"(%arg0, %7, %alloc, %alloca) {backend_config = "", call_target_name = "inc_ref", device = "gpu", has_side_effect = false} : (!disc_ral.context, !llvm.ptr<i8>, memref<?xf32, "gpu">, memref<2xindex, "cpu">) -> memref<?x?xf32, "gpu">
    %reinterpret_cast = memref.reinterpret_cast %8 to offset: [0], sizes: [%dim_0, %dim], strides: [%dim, 1] {kDiscSymbolicDimAttr = [@S1, @S2]} : memref<?x?xf32, "gpu"> to memref<?x?xf32, "gpu">
    memref.dealloc %alloc : memref<?xf32, "gpu">
    "disc_ral.dispatch"(%arg0, %c0, %reinterpret_cast) {backend_config = "", call_target_name = "ral_send_output", device = "cpu", has_side_effect = false} : (!disc_ral.context, index, memref<?x?xf32, "gpu">) -> ()
    return
  }
  gpu.module @main_kernel {
    gpu.func @main_kColReduction_reduce__4_1_0___flat(%arg0: index, %arg1: index, %arg2: memref<?xf32, "gpu">) kernel {
      %0 = gpu.block_id  x
      %1 = gpu.block_id  y
      %2 = gpu.block_id  z
      %3 = gpu.thread_id  x
      %4 = gpu.thread_id  y
      %5 = gpu.thread_id  z
      %6 = gpu.grid_dim  x
      %7 = gpu.grid_dim  y
      %8 = gpu.grid_dim  z
      %9 = gpu.block_dim  x
      %10 = gpu.block_dim  y
      %11 = gpu.block_dim  z
      cf.br ^bb1
    ^bb1:  // pred: ^bb0
      %c0 = arith.constant 0 : index
      %c1 = arith.constant 1 : index
      %c0_0 = arith.constant 0 : index
      %cst = arith.constant 0xFF800000 : f32
      %12 = affine.apply #map1(%0)[%arg0, %c0]
      %13 = affine.apply #map1(%3)[%c1, %c0_0]
      %14 = arith.addi %13, %12 : index
      %true = arith.constant true
      %15 = arith.muli %13, %c1 : index
      %16 = arith.addi %15, %12 : index
      %17 = arith.cmpi ult, %16, %arg1 : index
      %18 = arith.andi %true, %17 : i1
      scf.if %18 {
        %reinterpret_cast = memref.reinterpret_cast %arg2 to offset: [%c0], sizes: [%arg1], strides: [%c1] : memref<?xf32, "gpu"> to memref<?xf32, "gpu">
        memref.store %cst, %reinterpret_cast[%14] : memref<?xf32, "gpu">
      }
      gpu.return
    }
  }
  gpu.module @main_kernel_0 {
    gpu.func @main_kColReduction_reduce__4_1_0___flat_1(%arg0: index, %arg1: memref<?x?x?xf32, "gpu">, %arg2: index, %arg3: index, %arg4: index, %arg5: memref<?xf32, "gpu">) kernel {
      %0 = gpu.block_id  x
      %1 = gpu.block_id  y
      %2 = gpu.block_id  z
      %3 = gpu.thread_id  x
      %4 = gpu.thread_id  y
      %5 = gpu.thread_id  z
      %6 = gpu.grid_dim  x
      %7 = gpu.grid_dim  y
      %8 = gpu.grid_dim  z
      %9 = gpu.block_dim  x
      %10 = gpu.block_dim  y
      %11 = gpu.block_dim  z
      cf.br ^bb1
    ^bb1:  // pred: ^bb0
      %c0 = arith.constant 0 : index
      %c1 = arith.constant 1 : index
      %c0_0 = arith.constant 0 : index
      %c512 = arith.constant 512 : index
      %c0_1 = arith.constant 0 : index
      %12 = arith.cmpi eq, %arg0, %c0_1 : index
      %c32 = arith.constant 32 : index
      %dim = memref.dim %arg1, %c0_1 : memref<?x?x?xf32, "gpu">
      %c1_2 = arith.constant 1 : index
      %dim_3 = memref.dim %arg1, %c1_2 : memref<?x?x?xf32, "gpu">
      %c2 = arith.constant 2 : index
      %dim_4 = memref.dim %arg1, %c2 : memref<?x?x?xf32, "gpu">
      %cst = arith.constant 0xFF800000 : f32
      %13 = affine.apply #map1(%0)[%arg2, %c0]
      %14 = affine.apply #map1(%3)[%c1, %c0_0]
      %15 = arith.addi %14, %13 : index
      %true = arith.constant true
      %16 = arith.muli %14, %c1 : index
      %17 = arith.addi %16, %13 : index
      %18 = arith.cmpi ult, %17, %arg3 : index
      %19 = arith.andi %true, %18 : i1
      scf.if %19 {
        %20 = arith.remsi %15, %c512 : index
        %21 = arith.divsi %15, %c512 : index
        %22 = arith.divui %21, %arg4 : index
        %23 = arith.remui %21, %arg4 : index
        %24 = arith.muli %23, %c512 : index
        %25 = arith.addi %24, %20 : index
        %26 = arith.cmpi ult, %25, %arg0 : index
        scf.if %26 {
          %27 = scf.for %arg6 = %c0_1 to %c32 step %c1_2 iter_args(%arg7 = %cst) -> (f32) {
            %29 = arith.muli %22, %c32 : index
            %30 = arith.addi %29, %arg6 : index
            %31 = arith.cmpi slt, %30, %dim : index
            %32 = scf.if %31 -> (f32) {
              %33 = arith.muli %30, %arg0 : index
              %34 = arith.addi %33, %25 : index
              %35 = arith.muli %dim, %dim_3 : index
              %36 = arith.muli %35, %dim_4 : index
              %reinterpret_cast = memref.reinterpret_cast %arg1 to offset: [%c0_1], sizes: [%36], strides: [%c1_2] : memref<?x?x?xf32, "gpu"> to memref<?xf32, "gpu">
              %37 = memref.load %reinterpret_cast[%34] : memref<?xf32, "gpu">
              %38 = math.absf %37 : f32
              %39 = arith.cmpf oge, %arg7, %38 : f32
              %40 = arith.select %39, %arg7, %38 : f32
              scf.yield %40 : f32
            } else {
              scf.yield %arg7 : f32
            }
            scf.yield %32 : f32
          }
          %28 = memref.generic_atomic_rmw %arg5[%25] : memref<?xf32, "gpu"> {
          ^bb0(%arg6: f32):
            %29 = arith.cmpf ogt, %arg6, %27 : f32
            %30 = arith.select %29, %arg6, %27 : f32
            memref.atomic_yield %30 : f32
          }
        }
      }
      gpu.return
    }
  }
  gpu.module @main_kernel_1 {
    gpu.func @main_kColReduction_reduce__4_1_0___thin(%arg0: index, %arg1: index, %arg2: memref<?xf32, "gpu">) kernel {
      %0 = gpu.block_id  x
      %1 = gpu.block_id  y
      %2 = gpu.block_id  z
      %3 = gpu.thread_id  x
      %4 = gpu.thread_id  y
      %5 = gpu.thread_id  z
      %6 = gpu.grid_dim  x
      %7 = gpu.grid_dim  y
      %8 = gpu.grid_dim  z
      %9 = gpu.block_dim  x
      %10 = gpu.block_dim  y
      %11 = gpu.block_dim  z
      cf.br ^bb1
    ^bb1:  // pred: ^bb0
      %c0 = arith.constant 0 : index
      %c1 = arith.constant 1 : index
      %c0_0 = arith.constant 0 : index
      %cst = arith.constant 0xFF800000 : f32
      %12 = affine.apply #map1(%0)[%arg0, %c0]
      %13 = affine.apply #map1(%3)[%c1, %c0_0]
      %14 = arith.addi %13, %12 : index
      %true = arith.constant true
      %15 = arith.muli %13, %c1 : index
      %16 = arith.addi %15, %12 : index
      %17 = arith.cmpi ult, %16, %arg1 : index
      %18 = arith.andi %true, %17 : i1
      scf.if %18 {
        %reinterpret_cast = memref.reinterpret_cast %arg2 to offset: [%c0], sizes: [%arg1], strides: [%c1] : memref<?xf32, "gpu"> to memref<?xf32, "gpu">
        memref.store %cst, %reinterpret_cast[%14] : memref<?xf32, "gpu">
      }
      gpu.return
    }
  }
  gpu.module @main_kernel_2 {
    gpu.func @main_kColReduction_reduce__4_1_0___thin_1(%arg0: index, %arg1: memref<?x?x?xf32, "gpu">, %arg2: index, %arg3: index, %arg4: index, %arg5: memref<?xf32, "gpu">) workgroup(%arg6 : memref<256xf32, #gpu.address_space<workgroup>>) kernel {
      %0 = gpu.block_id  x
      %1 = gpu.block_id  y
      %2 = gpu.block_id  z
      %3 = gpu.thread_id  x
      %4 = gpu.thread_id  y
      %5 = gpu.thread_id  z
      %6 = gpu.grid_dim  x
      %7 = gpu.grid_dim  y
      %8 = gpu.grid_dim  z
      %9 = gpu.block_dim  x
      %10 = gpu.block_dim  y
      %11 = gpu.block_dim  z
      cf.br ^bb1
    ^bb1:  // pred: ^bb0
      %c0 = arith.constant 0 : index
      %c1 = arith.constant 1 : index
      %c0_0 = arith.constant 0 : index
      %c256 = arith.constant 256 : index
      %c0_1 = arith.constant 0 : index
      %12 = arith.cmpi eq, %arg0, %c0_1 : index
      %c32 = arith.constant 32 : index
      %c8 = arith.constant 8 : index
      %c64 = arith.constant 64 : index
      %dim = memref.dim %arg1, %c0_1 : memref<?x?x?xf32, "gpu">
      %c1_2 = arith.constant 1 : index
      %dim_3 = memref.dim %arg1, %c1_2 : memref<?x?x?xf32, "gpu">
      %c2 = arith.constant 2 : index
      %dim_4 = memref.dim %arg1, %c2 : memref<?x?x?xf32, "gpu">
      %cst = arith.constant 0xFF800000 : f32
      %c4 = arith.constant 4 : index
      %13 = affine.apply #map1(%0)[%arg2, %c0]
      %14 = affine.apply #map1(%3)[%c1, %c0_0]
      %15 = arith.addi %14, %13 : index
      %true = arith.constant true
      %16 = arith.muli %14, %c1 : index
      %17 = arith.addi %16, %13 : index
      %18 = arith.cmpi ult, %17, %arg3 : index
      %19 = arith.andi %true, %18 : i1
      scf.if %19 {
        %20 = arith.remsi %15, %c256 : index
        %21 = arith.divsi %15, %c256 : index
        %22 = arith.divui %21, %arg4 : index
        %23 = arith.remui %21, %arg4 : index
        %24 = arith.divui %20, %c32 : index
        %25 = arith.remui %20, %c32 : index
        %26 = arith.muli %25, %c8 : index
        %27 = arith.addi %24, %26 : index
        %28 = arith.muli %23, %c32 : index
        %29 = arith.addi %25, %28 : index
        %30 = arith.cmpi ult, %29, %arg0 : index
        %31 = scf.if %30 -> (f32) {
          %36 = scf.for %arg7 = %c0_1 to %c64 step %c1_2 iter_args(%arg8 = %cst) -> (f32) {
            %37 = arith.muli %22, %c8 : index
            %38 = arith.addi %24, %37 : index
            %39 = arith.muli %38, %c64 : index
            %40 = arith.addi %arg7, %39 : index
            %41 = arith.cmpi slt, %40, %dim : index
            %42 = scf.if %41 -> (f32) {
              %43 = arith.muli %40, %arg0 : index
              %44 = arith.addi %43, %29 : index
              %45 = arith.muli %dim, %dim_3 : index
              %46 = arith.muli %45, %dim_4 : index
              %reinterpret_cast_5 = memref.reinterpret_cast %arg1 to offset: [%c0_1], sizes: [%46], strides: [%c1_2] : memref<?x?x?xf32, "gpu"> to memref<?xf32, "gpu">
              %47 = memref.load %reinterpret_cast_5[%44] : memref<?xf32, "gpu">
              %48 = math.absf %47 : f32
              %49 = arith.cmpf ugt, %arg8, %48 : f32
              %50 = arith.select %49, %arg8, %48 : f32
              %51 = arith.cmpf uno, %48, %48 : f32
              %52 = arith.select %51, %48, %50 : f32
              scf.yield %52 : f32
            } else {
              scf.yield %arg8 : f32
            }
            scf.yield %42 : f32
          }
          scf.yield %36 : f32
        } else {
          scf.yield %cst : f32
        }
        %reinterpret_cast = memref.reinterpret_cast %arg6 to offset: [0], sizes: [256], strides: [1] : memref<256xf32, #gpu.address_space<workgroup>> to memref<256xf32, #gpu.address_space<workgroup>>
        memref.store %31, %reinterpret_cast[%27] : memref<256xf32, #gpu.address_space<workgroup>>
        gpu.barrier
        %32 = arith.cmpi slt, %24, %c4 : index
        scf.if %32 {
          %36 = arith.addi %27, %c4 : index
          %37 = memref.load %reinterpret_cast[%27] : memref<256xf32, #gpu.address_space<workgroup>>
          %38 = memref.load %reinterpret_cast[%36] : memref<256xf32, #gpu.address_space<workgroup>>
          %39 = arith.cmpf ugt, %37, %38 : f32
          %40 = arith.select %39, %37, %38 : f32
          %41 = arith.cmpf uno, %38, %38 : f32
          %42 = arith.select %41, %38, %40 : f32
          memref.store %42, %reinterpret_cast[%27] : memref<256xf32, #gpu.address_space<workgroup>>
        }
        gpu.barrier
        %33 = arith.cmpi slt, %24, %c2 : index
        scf.if %33 {
          %36 = arith.addi %27, %c2 : index
          %37 = memref.load %reinterpret_cast[%27] : memref<256xf32, #gpu.address_space<workgroup>>
          %38 = memref.load %reinterpret_cast[%36] : memref<256xf32, #gpu.address_space<workgroup>>
          %39 = arith.cmpf ugt, %37, %38 : f32
          %40 = arith.select %39, %37, %38 : f32
          %41 = arith.cmpf uno, %38, %38 : f32
          %42 = arith.select %41, %38, %40 : f32
          memref.store %42, %reinterpret_cast[%27] : memref<256xf32, #gpu.address_space<workgroup>>
        }
        gpu.barrier
        %34 = arith.cmpi eq, %24, %c0_1 : index
        %35 = arith.andi %34, %30 : i1
        scf.if %35 {
          %36 = arith.addi %27, %c1_2 : index
          %37 = memref.load %reinterpret_cast[%27] : memref<256xf32, #gpu.address_space<workgroup>>
          %38 = memref.load %reinterpret_cast[%36] : memref<256xf32, #gpu.address_space<workgroup>>
          %39 = arith.cmpf ugt, %37, %38 : f32
          %40 = arith.select %39, %37, %38 : f32
          %41 = arith.cmpf uno, %38, %38 : f32
          %42 = arith.select %41, %38, %40 : f32
          %43 = memref.generic_atomic_rmw %arg5[%29] : memref<?xf32, "gpu"> {
          ^bb0(%arg7: f32):
            %44 = arith.cmpf ogt, %arg7, %42 : f32
            %45 = arith.select %44, %arg7, %42 : f32
            memref.atomic_yield %45 : f32
          }
        }
      }
      gpu.return
    }
  }
  "disc_shape.SymbolicDim"() {knownNegativeOne = false, knownNonNegative = true, knownNonSizeOne = false, knownNonSizeZero = false, sym_name = "S0", value = -9223372036854775808 : i64} : () -> ()
  "disc_shape.SymbolicDim"() {knownNegativeOne = false, knownNonNegative = true, knownNonSizeOne = false, knownNonSizeZero = false, sym_name = "S1", value = -9223372036854775808 : i64} : () -> ()
  "disc_shape.SymbolicDim"() {knownNegativeOne = false, knownNonNegative = true, knownNonSizeOne = false, knownNonSizeZero = false, sym_name = "S2", value = -9223372036854775808 : i64} : () -> ()
  "disc_shape.SymbolicDim"() {knownNegativeOne = false, knownNonNegative = true, knownNonSizeOne = false, knownNonSizeZero = false, sym_name = "S3", value = -9223372036854775808 : i64} : () -> ()
  func.func @shape_constraint_graph() {
    %0 = "disc_shape.dim"() {name = @S3} : () -> index
    %1 = "disc_shape.dim"() {name = @S1} : () -> index
    %2 = "disc_shape.dim"() {name = @S2} : () -> index
    "disc_shape.tie_product_equal"(%0, %1, %2) {operand_segment_sizes = array<i32: 1, 2>} : (index, index, index) -> ()
    return
  }
}


// -----// IR Dump After SideEffectLoopInvariantCodeMotionPass (disc-side-effect-loop-invariant-code-motion) //----- //
gpu.func @main_kColReduction_reduce__4_1_0___flat(%arg0: index, %arg1: index, %arg2: memref<?xf32, "gpu">) kernel {
  %cst = arith.constant 0xFF800000 : f32
  %c1 = arith.constant 1 : index
  %c0 = arith.constant 0 : index
  %0 = gpu.block_id  x
  %1 = gpu.thread_id  x
  cf.br ^bb1
^bb1:  // pred: ^bb0
  %2 = affine.apply affine_map<(d0)[s0, s1] -> (d0 * s0 + s1)>(%0)[%arg0, %c0]
  %3 = affine.apply affine_map<(d0)[s0, s1] -> (d0 * s0 + s1)>(%1)[%c1, %c0]
  %4 = arith.addi %3, %2 : index
  %5 = arith.addi %3, %2 : index
  %6 = arith.cmpi ult, %5, %arg1 : index
  scf.if %6 {
    %reinterpret_cast = memref.reinterpret_cast %arg2 to offset: [%c0], sizes: [%arg1], strides: [%c1] : memref<?xf32, "gpu"> to memref<?xf32, "gpu">
    memref.store %cst, %reinterpret_cast[%4] : memref<?xf32, "gpu">
  }
  gpu.return
}

// -----// IR Dump After CSE (cse) //----- //
gpu.module @main_kernel {
  gpu.func @main_kColReduction_reduce__4_1_0___flat(%arg0: index, %arg1: index, %arg2: memref<?xf32, "gpu">) kernel {
    %cst = arith.constant 0xFF800000 : f32
    %c1 = arith.constant 1 : index
    %c0 = arith.constant 0 : index
    %0 = gpu.block_id  x
    %1 = gpu.thread_id  x
    cf.br ^bb1
  ^bb1:  // pred: ^bb0
    %2 = affine.apply affine_map<(d0)[s0, s1] -> (d0 * s0 + s1)>(%0)[%arg0, %c0]
    %3 = affine.apply affine_map<(d0)[s0, s1] -> (d0 * s0 + s1)>(%1)[%c1, %c0]
    %4 = arith.addi %3, %2 : index
    %5 = arith.cmpi ult, %4, %arg1 : index
    scf.if %5 {
      %reinterpret_cast = memref.reinterpret_cast %arg2 to offset: [%c0], sizes: [%arg1], strides: [%c1] : memref<?xf32, "gpu"> to memref<?xf32, "gpu">
      memref.store %cst, %reinterpret_cast[%4] : memref<?xf32, "gpu">
    }
    gpu.return
  }
}

// -----// IR Dump After SCFToControlFlow (convert-scf-to-cf) //----- //
gpu.module @main_kernel {
  gpu.func @main_kColReduction_reduce__4_1_0___flat(%arg0: index, %arg1: index, %arg2: memref<?xf32, "gpu">) kernel {
    %cst = arith.constant 0xFF800000 : f32
    %c1 = arith.constant 1 : index
    %c0 = arith.constant 0 : index
    %0 = gpu.block_id  x
    %1 = gpu.thread_id  x
    cf.br ^bb1
  ^bb1:  // pred: ^bb0
    %2 = affine.apply affine_map<(d0)[s0, s1] -> (d0 * s0 + s1)>(%0)[%arg0, %c0]
    %3 = affine.apply affine_map<(d0)[s0, s1] -> (d0 * s0 + s1)>(%1)[%c1, %c0]
    %4 = arith.addi %3, %2 : index
    %5 = arith.cmpi ult, %4, %arg1 : index
    cf.cond_br %5, ^bb2, ^bb3
  ^bb2:  // pred: ^bb1
    %reinterpret_cast = memref.reinterpret_cast %arg2 to offset: [%c0], sizes: [%arg1], strides: [%c1] : memref<?xf32, "gpu"> to memref<?xf32, "gpu">
    memref.store %cst, %reinterpret_cast[%4] : memref<?xf32, "gpu">
    cf.br ^bb3
  ^bb3:  // 2 preds: ^bb1, ^bb2
    gpu.return
  }
}

// -----// IR Dump After ConvertAffineToStandard (lower-affine) //----- //
gpu.module @main_kernel {
  gpu.func @main_kColReduction_reduce__4_1_0___flat(%arg0: index, %arg1: index, %arg2: memref<?xf32, "gpu">) kernel {
    %cst = arith.constant 0xFF800000 : f32
    %c1 = arith.constant 1 : index
    %c0 = arith.constant 0 : index
    %0 = gpu.block_id  x
    %1 = gpu.thread_id  x
    cf.br ^bb1
  ^bb1:  // pred: ^bb0
    %2 = arith.muli %0, %arg0 : index
    %3 = arith.addi %2, %c0 : index
    %4 = arith.muli %1, %c1 : index
    %5 = arith.addi %4, %c0 : index
    %6 = arith.addi %5, %3 : index
    %7 = arith.cmpi ult, %6, %arg1 : index
    cf.cond_br %7, ^bb2, ^bb3
  ^bb2:  // pred: ^bb1
    %reinterpret_cast = memref.reinterpret_cast %arg2 to offset: [%c0], sizes: [%arg1], strides: [%c1] : memref<?xf32, "gpu"> to memref<?xf32, "gpu">
    memref.store %cst, %reinterpret_cast[%6] : memref<?xf32, "gpu">
    cf.br ^bb3
  ^bb3:  // 2 preds: ^bb1, ^bb2
    gpu.return
  }
}

// -----// IR Dump After StripDebugInfo (strip-debuginfo) //----- //
gpu.module @main_kernel {
  gpu.func @main_kColReduction_reduce__4_1_0___flat(%arg0: index, %arg1: index, %arg2: memref<?xf32, "gpu">) kernel {
    %cst = arith.constant 0xFF800000 : f32
    %c1 = arith.constant 1 : index
    %c0 = arith.constant 0 : index
    %0 = gpu.block_id  x
    %1 = gpu.thread_id  x
    cf.br ^bb1
  ^bb1:  // pred: ^bb0
    %2 = arith.muli %0, %arg0 : index
    %3 = arith.addi %2, %c0 : index
    %4 = arith.muli %1, %c1 : index
    %5 = arith.addi %4, %c0 : index
    %6 = arith.addi %5, %3 : index
    %7 = arith.cmpi ult, %6, %arg1 : index
    cf.cond_br %7, ^bb2, ^bb3
  ^bb2:  // pred: ^bb1
    %reinterpret_cast = memref.reinterpret_cast %arg2 to offset: [%c0], sizes: [%arg1], strides: [%c1] : memref<?xf32, "gpu"> to memref<?xf32, "gpu">
    memref.store %cst, %reinterpret_cast[%6] : memref<?xf32, "gpu">
    cf.br ^bb3
  ^bb3:  // 2 preds: ^bb1, ^bb2
    gpu.return
  }
}

// -----// IR Dump After DiscLowerGpuOpsToNVVMOpsPass (disc-convert-gpu-to-nvvm) //----- //
gpu.module @main_kernel {
  llvm.func @main_kColReduction_reduce__4_1_0___flat(%arg0: i32, %arg1: i32, %arg2: !llvm.ptr<f32>, %arg3: !llvm.ptr<f32>, %arg4: i32, %arg5: i32, %arg6: i32) attributes {gpu.kernel, nvvm.kernel} {
    %0 = llvm.mlir.undef : !llvm.struct<(ptr<f32>, ptr<f32>, i32, array<1 x i32>, array<1 x i32>)>
    %1 = llvm.insertvalue %arg2, %0[0] : !llvm.struct<(ptr<f32>, ptr<f32>, i32, array<1 x i32>, array<1 x i32>)> 
    %2 = llvm.insertvalue %arg3, %1[1] : !llvm.struct<(ptr<f32>, ptr<f32>, i32, array<1 x i32>, array<1 x i32>)> 
    %3 = llvm.insertvalue %arg4, %2[2] : !llvm.struct<(ptr<f32>, ptr<f32>, i32, array<1 x i32>, array<1 x i32>)> 
    %4 = llvm.insertvalue %arg5, %3[3, 0] : !llvm.struct<(ptr<f32>, ptr<f32>, i32, array<1 x i32>, array<1 x i32>)> 
    %5 = llvm.insertvalue %arg6, %4[4, 0] : !llvm.struct<(ptr<f32>, ptr<f32>, i32, array<1 x i32>, array<1 x i32>)> 
    %6 = llvm.mlir.constant(0xFF800000 : f32) : f32
    %7 = llvm.mlir.constant(1 : index) : i32
    %8 = llvm.mlir.constant(0 : index) : i32
    %9 = nvvm.read.ptx.sreg.ctaid.x : i32
    %10 = nvvm.read.ptx.sreg.tid.x : i32
    llvm.br ^bb1
  ^bb1:  // pred: ^bb0
    %11 = llvm.mul %9, %arg0  : i32
    %12 = llvm.add %10, %11  : i32
    %13 = llvm.icmp "ult" %12, %arg1 : i32
    llvm.cond_br %13, ^bb2, ^bb3
  ^bb2:  // pred: ^bb1
    %14 = llvm.mlir.undef : !llvm.struct<(ptr<f32>, ptr<f32>, i32, array<1 x i32>, array<1 x i32>)>
    %15 = llvm.extractvalue %5[0] : !llvm.struct<(ptr<f32>, ptr<f32>, i32, array<1 x i32>, array<1 x i32>)> 
    %16 = llvm.extractvalue %5[1] : !llvm.struct<(ptr<f32>, ptr<f32>, i32, array<1 x i32>, array<1 x i32>)> 
    %17 = llvm.insertvalue %15, %14[0] : !llvm.struct<(ptr<f32>, ptr<f32>, i32, array<1 x i32>, array<1 x i32>)> 
    %18 = llvm.insertvalue %16, %17[1] : !llvm.struct<(ptr<f32>, ptr<f32>, i32, array<1 x i32>, array<1 x i32>)> 
    %19 = llvm.insertvalue %8, %18[2] : !llvm.struct<(ptr<f32>, ptr<f32>, i32, array<1 x i32>, array<1 x i32>)> 
    %20 = llvm.insertvalue %arg1, %19[3, 0] : !llvm.struct<(ptr<f32>, ptr<f32>, i32, array<1 x i32>, array<1 x i32>)> 
    %21 = llvm.insertvalue %7, %20[4, 0] : !llvm.struct<(ptr<f32>, ptr<f32>, i32, array<1 x i32>, array<1 x i32>)> 
    %22 = llvm.extractvalue %21[1] : !llvm.struct<(ptr<f32>, ptr<f32>, i32, array<1 x i32>, array<1 x i32>)> 
    %23 = llvm.getelementptr %22[%12] : (!llvm.ptr<f32>, i32) -> !llvm.ptr<f32>
    llvm.store %6, %23 : !llvm.ptr<f32>
    llvm.br ^bb3
  ^bb3:  // 2 preds: ^bb1, ^bb2
    llvm.return
  }
}

// -----// IR Dump After LLVMInsertValueSimplifierPass (disc-llvm-insert-value-simplifier) //----- //
llvm.func @main_kColReduction_reduce__4_1_0___flat(%arg0: i32, %arg1: i32, %arg2: !llvm.ptr<f32>, %arg3: !llvm.ptr<f32>, %arg4: i32, %arg5: i32, %arg6: i32) attributes {gpu.kernel, nvvm.kernel} {
  %0 = llvm.mlir.constant(0xFF800000 : f32) : f32
  %1 = nvvm.read.ptx.sreg.ctaid.x : i32
  %2 = nvvm.read.ptx.sreg.tid.x : i32
  llvm.br ^bb1
^bb1:  // pred: ^bb0
  %3 = llvm.mul %1, %arg0  : i32
  %4 = llvm.add %2, %3  : i32
  %5 = llvm.icmp "ult" %4, %arg1 : i32
  llvm.cond_br %5, ^bb2, ^bb3
^bb2:  // pred: ^bb1
  %6 = llvm.getelementptr %arg3[%4] : (!llvm.ptr<f32>, i32) -> !llvm.ptr<f32>
  llvm.store %0, %6 : !llvm.ptr<f32>
  llvm.br ^bb3
^bb3:  // 2 preds: ^bb1, ^bb2
  llvm.return
}

// -----// IR Dump After FunctionDeadArgumentEliminationPass (disc-function-dead-argument-elimination) //----- //
gpu.module @main_kernel {
  llvm.func @main_kColReduction_reduce__4_1_0___flat(%arg0: i32, %arg1: i32, %arg2: !llvm.ptr<f32>) attributes {disc.elimargs = [2 : index, 4 : index, 5 : index, 6 : index], gpu.kernel, nvvm.kernel} {
    %0 = llvm.mlir.constant(0xFF800000 : f32) : f32
    %1 = nvvm.read.ptx.sreg.ctaid.x : i32
    %2 = nvvm.read.ptx.sreg.tid.x : i32
    llvm.br ^bb1
  ^bb1:  // pred: ^bb0
    %3 = llvm.mul %1, %arg0  : i32
    %4 = llvm.add %2, %3  : i32
    %5 = llvm.icmp "ult" %4, %arg1 : i32
    llvm.cond_br %5, ^bb2, ^bb3
  ^bb2:  // pred: ^bb1
    %6 = llvm.getelementptr %arg2[%4] : (!llvm.ptr<f32>, i32) -> !llvm.ptr<f32>
    llvm.store %0, %6 : !llvm.ptr<f32>
    llvm.br ^bb3
  ^bb3:  // 2 preds: ^bb1, ^bb2
    llvm.return
  }
}

// -----// IR Dump After GpuKernelToBlobPass (disc-gpu-kernel-to-blob) //----- //
gpu.module @main_kernel attributes {gpu.binary = "P\EDU\BA\01\00\10\008\04\00\00\00\00\00\00\02\00\01\01@\00\00\00\F8\03\00\00\00\00\00\00\F7\03\00\00\00\00\00\00\07\00\01\00P\00\00\00\00\00\00\00\00\00\00\00\11 \00\00\00\00\00\00\00\00\00\00\00\00\00\00\E8\0B\00\00\00\00\00\00\A2\7FELF\02\01\013\07\00\01\00f\02\00\BE\00q\00\01\00!@\0B\07\001\00\80\08\07\00\F5\0E\00P\05P\00@\008\00\03\00@\00\0B\00\01\00\00.shstrtab\00.\08\00'ym\08\00\F0\1B_shndx\00.nv.info\00.text.main_kColReduction_r\0A\00\F5\00e__4_1_0___flat7\00\0F1\00\1Aoshared3\00\1A\9Fconstant06\00\17\FA\01debug_frame\00.rel\11\00!nv\14\00\11aB\00\0F'\01 \0F\87\00\14\0FO\01\B6o_paramV\01\1C\0F\01\00\07\8CZ\00\00\00\03\00\0A\00\01\00\11\EC\18\00,\09\00\01\00 )\01\18\00,\04\00\01\00\11G\18\00,\07\00\01\00f2\00\00\00\12\10`\00!\80\01\07\00v\00\FF\FF\FF\FF(\00\0C\00\00\01\00@\03\00\04|\08\00\D1\0F\0C\81\80\80(\00\08\FF\81\80(\08\0B\00\06(\00/0\00\01\00\03\13pX\00\10\04\9B\00R\04\14\00\00\00E\00\01\0B\00\00\13\00p/\08\00\05\00\00\00\9F\03\22\04#\0C\00\00\01\00'\04\12\0C\00\17\11\0C\00!7\04\E0\03\F3\08\015\00\00\04\0A\08\00\02\00\00\00`\01\10\00\03\19\10\00\04\17\0C\04\04U\08\00\00\F0!\10\00\10\01\18\01%\F0\11\10\00\01\01\00\F2\02\F0\11\00\03\1B\FF\00\04\1C\08\00P\00\00\00\B0\00\01\00#K\00\01\00s\02\02\08\10\0A/\22\9B\00\00\07\00\03\08\00\13\10\08\00\13\18\08\00\13 \08\00\13(\08\00\130\08\00\228\08|\00\13\00\08\00\13\08\08\00\13\10\08\00\13\18\08\00\13 \08\00\13(\08\00\130\08\00\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\A2\00\00\00\14,\00\00\00H\00\01\00\000\01/\05\00\01\00\FF\B8A\02z\01\00'\04\B1\0F\00\00\00\C4\0F\00\19y\02\00\01\00\10%\93\02Q\0E\00\19y\03\0F\00\F5\1A\00!\00\00\00$\0E\00$z\02\02\00X\00\00\03\02\8E\07\00\CA\1F\00\0Cz\00\02\00Y\00\00p`\F0\03\00\DA\0F\00M[\04\A0\80\03\00\EA\0F\005t\03\FF\BB\03\10\FF\C8\03P\E2\0F\00\02x>\02B\80\FF\00\0F\10\00r\B9z\04\00\00F\00\84\00\94\D0\0F\00%v\02\02\00Z`\00`\0F\00\86y\00\022\00@\04\19\10\0C0\009My\00`\00PGy\00\00\F0A\04\A6\FF\83\03\00\C0\0F\00\18y\00\01\00\0F\10\00\90\0F\01\00-\00W\01.\03\00\01\00\22@\00\01\00='\01\000\00\08\01\00\1F\0B@\00\04\13g)\00\1FV@\00\0C\13\13\14\04\0C\01\00\13\C0\15\00&\90\008\04#\04\00\8D\04\00\FE\04\12\00\01\00\1F\FAT\00\00\00\01\00\13P\95\00/p\00\80\00\0B\1F)'\00\03#\00\C0@\00\04X\06\04\E4\00*\04\00\01\00\1F`@\00\04\13\F01\00&\\\00@\00\1F\0A@\00\00!\18\01D\01\0D@\00\13P)\00*\D8\00\01\00\1B\08\08\00?\07\01\00\8E\07\00Q\00\00(\05\00\01\00&\10\00\80\00\17\048\00\04\18\00\13\C4\14\01\0C\84\01*8\05@\07\1F\00\C0\00\04\132@\00+\06\00\01\00\1A\07\D8\07\12\03\F8\05:\08\80\00\01\00\13\06\10\06\04(\0B\0C\01\00*\A8\00\08\00\04\F8\00\13\018\00\04\A8\00\0C\01\009H\03\00\08\00\088\00/\06\00\01\00\17\80\08\00\00\00\00\00\00\00\00"} {
  llvm.func @main_kColReduction_reduce__4_1_0___flat(%arg0: i32, %arg1: i32, %arg2: !llvm.ptr<f32>) attributes {disc.elimargs = [2 : index, 4 : index, 5 : index, 6 : index], gpu.kernel, nvvm.kernel} {
    %0 = llvm.mlir.constant(0xFF800000 : f32) : f32
    %1 = nvvm.read.ptx.sreg.ctaid.x : i32
    %2 = nvvm.read.ptx.sreg.tid.x : i32
    llvm.br ^bb1
  ^bb1:  // pred: ^bb0
    %3 = llvm.mul %1, %arg0  : i32
    %4 = llvm.add %2, %3  : i32
    %5 = llvm.icmp "ult" %4, %arg1 : i32
    llvm.cond_br %5, ^bb2, ^bb3
  ^bb2:  // pred: ^bb1
    %6 = llvm.getelementptr %arg2[%4] : (!llvm.ptr<f32>, i32) -> !llvm.ptr<f32>
    llvm.store %0, %6 : !llvm.ptr<f32>
    llvm.br ^bb3
  ^bb3:  // 2 preds: ^bb1, ^bb2
    llvm.return
  }
}

// -----// IR Dump After SideEffectLoopInvariantCodeMotionPass (disc-side-effect-loop-invariant-code-motion) //----- //
gpu.func @main_kColReduction_reduce__4_1_0___flat_1(%arg0: index, %arg1: memref<?x?x?xf32, "gpu">, %arg2: index, %arg3: index, %arg4: index, %arg5: memref<?xf32, "gpu">) kernel {
  %cst = arith.constant 0xFF800000 : f32
  %c2 = arith.constant 2 : index
  %c32 = arith.constant 32 : index
  %c512 = arith.constant 512 : index
  %c1 = arith.constant 1 : index
  %c0 = arith.constant 0 : index
  %0 = gpu.block_id  x
  %1 = gpu.thread_id  x
  cf.br ^bb1
^bb1:  // pred: ^bb0
  %dim = memref.dim %arg1, %c0 : memref<?x?x?xf32, "gpu">
  %dim_0 = memref.dim %arg1, %c1 : memref<?x?x?xf32, "gpu">
  %dim_1 = memref.dim %arg1, %c2 : memref<?x?x?xf32, "gpu">
  %2 = affine.apply affine_map<(d0)[s0, s1] -> (d0 * s0 + s1)>(%0)[%arg2, %c0]
  %3 = affine.apply affine_map<(d0)[s0, s1] -> (d0 * s0 + s1)>(%1)[%c1, %c0]
  %4 = arith.addi %3, %2 : index
  %5 = arith.addi %3, %2 : index
  %6 = arith.cmpi ult, %5, %arg3 : index
  scf.if %6 {
    %7 = arith.remsi %4, %c512 : index
    %8 = arith.divsi %4, %c512 : index
    %9 = arith.divui %8, %arg4 : index
    %10 = arith.remui %8, %arg4 : index
    %11 = arith.muli %10, %c512 : index
    %12 = arith.addi %11, %7 : index
    %13 = arith.cmpi ult, %12, %arg0 : index
    scf.if %13 {
      %14 = arith.muli %9, %c32 : index
      %15 = scf.for %arg6 = %c0 to %c32 step %c1 iter_args(%arg7 = %cst) -> (f32) {
        %17 = arith.addi %14, %arg6 : index
        %18 = arith.cmpi slt, %17, %dim : index
        %19 = scf.if %18 -> (f32) {
          %20 = arith.muli %17, %arg0 : index
          %21 = arith.addi %20, %12 : index
          %22 = arith.muli %dim, %dim_0 : index
          %23 = arith.muli %22, %dim_1 : index
          %reinterpret_cast = memref.reinterpret_cast %arg1 to offset: [%c0], sizes: [%23], strides: [%c1] : memref<?x?x?xf32, "gpu"> to memref<?xf32, "gpu">
          %24 = memref.load %reinterpret_cast[%21] : memref<?xf32, "gpu">
          %25 = math.absf %24 : f32
          %26 = arith.cmpf oge, %arg7, %25 : f32
          %27 = arith.select %26, %arg7, %25 : f32
          scf.yield %27 : f32
        } else {
          scf.yield %arg7 : f32
        }
        scf.yield %19 : f32
      }
      %16 = memref.generic_atomic_rmw %arg5[%12] : memref<?xf32, "gpu"> {
      ^bb0(%arg6: f32):
        %17 = arith.cmpf ogt, %arg6, %15 : f32
        %18 = arith.select %17, %arg6, %15 : f32
        memref.atomic_yield %18 : f32
      }
    }
  }
  gpu.return
}

// -----// IR Dump After CSE (cse) //----- //
gpu.module @main_kernel_0 {
  gpu.func @main_kColReduction_reduce__4_1_0___flat_1(%arg0: index, %arg1: memref<?x?x?xf32, "gpu">, %arg2: index, %arg3: index, %arg4: index, %arg5: memref<?xf32, "gpu">) kernel {
    %cst = arith.constant 0xFF800000 : f32
    %c2 = arith.constant 2 : index
    %c32 = arith.constant 32 : index
    %c512 = arith.constant 512 : index
    %c1 = arith.constant 1 : index
    %c0 = arith.constant 0 : index
    %0 = gpu.block_id  x
    %1 = gpu.thread_id  x
    cf.br ^bb1
  ^bb1:  // pred: ^bb0
    %dim = memref.dim %arg1, %c0 : memref<?x?x?xf32, "gpu">
    %dim_0 = memref.dim %arg1, %c1 : memref<?x?x?xf32, "gpu">
    %dim_1 = memref.dim %arg1, %c2 : memref<?x?x?xf32, "gpu">
    %2 = affine.apply affine_map<(d0)[s0, s1] -> (d0 * s0 + s1)>(%0)[%arg2, %c0]
    %3 = affine.apply affine_map<(d0)[s0, s1] -> (d0 * s0 + s1)>(%1)[%c1, %c0]
    %4 = arith.addi %3, %2 : index
    %5 = arith.cmpi ult, %4, %arg3 : index
    scf.if %5 {
      %6 = arith.remsi %4, %c512 : index
      %7 = arith.divsi %4, %c512 : index
      %8 = arith.divui %7, %arg4 : index
      %9 = arith.remui %7, %arg4 : index
      %10 = arith.muli %9, %c512 : index
      %11 = arith.addi %10, %6 : index
      %12 = arith.cmpi ult, %11, %arg0 : index
      scf.if %12 {
        %13 = arith.muli %8, %c32 : index
        %14 = scf.for %arg6 = %c0 to %c32 step %c1 iter_args(%arg7 = %cst) -> (f32) {
          %16 = arith.addi %13, %arg6 : index
          %17 = arith.cmpi slt, %16, %dim : index
          %18 = scf.if %17 -> (f32) {
            %19 = arith.muli %16, %arg0 : index
            %20 = arith.addi %19, %11 : index
            %21 = arith.muli %dim, %dim_0 : index
            %22 = arith.muli %21, %dim_1 : index
            %reinterpret_cast = memref.reinterpret_cast %arg1 to offset: [%c0], sizes: [%22], strides: [%c1] : memref<?x?x?xf32, "gpu"> to memref<?xf32, "gpu">
            %23 = memref.load %reinterpret_cast[%20] : memref<?xf32, "gpu">
            %24 = math.absf %23 : f32
            %25 = arith.cmpf oge, %arg7, %24 : f32
            %26 = arith.select %25, %arg7, %24 : f32
            scf.yield %26 : f32
          } else {
            scf.yield %arg7 : f32
          }
          scf.yield %18 : f32
        }
        %15 = memref.generic_atomic_rmw %arg5[%11] : memref<?xf32, "gpu"> {
        ^bb0(%arg6: f32):
          %16 = arith.cmpf ogt, %arg6, %14 : f32
          %17 = arith.select %16, %arg6, %14 : f32
          memref.atomic_yield %17 : f32
        }
      }
    }
    gpu.return
  }
}

// -----// IR Dump After SCFToControlFlow (convert-scf-to-cf) //----- //
gpu.module @main_kernel_0 {
  gpu.func @main_kColReduction_reduce__4_1_0___flat_1(%arg0: index, %arg1: memref<?x?x?xf32, "gpu">, %arg2: index, %arg3: index, %arg4: index, %arg5: memref<?xf32, "gpu">) kernel {
    %cst = arith.constant 0xFF800000 : f32
    %c2 = arith.constant 2 : index
    %c32 = arith.constant 32 : index
    %c512 = arith.constant 512 : index
    %c1 = arith.constant 1 : index
    %c0 = arith.constant 0 : index
    %0 = gpu.block_id  x
    %1 = gpu.thread_id  x
    cf.br ^bb1
  ^bb1:  // pred: ^bb0
    %dim = memref.dim %arg1, %c0 : memref<?x?x?xf32, "gpu">
    %dim_0 = memref.dim %arg1, %c1 : memref<?x?x?xf32, "gpu">
    %dim_1 = memref.dim %arg1, %c2 : memref<?x?x?xf32, "gpu">
    %2 = affine.apply affine_map<(d0)[s0, s1] -> (d0 * s0 + s1)>(%0)[%arg2, %c0]
    %3 = affine.apply affine_map<(d0)[s0, s1] -> (d0 * s0 + s1)>(%1)[%c1, %c0]
    %4 = arith.addi %3, %2 : index
    %5 = arith.cmpi ult, %4, %arg3 : index
    cf.cond_br %5, ^bb2, ^bb12
  ^bb2:  // pred: ^bb1
    %6 = arith.remsi %4, %c512 : index
    %7 = arith.divsi %4, %c512 : index
    %8 = arith.divui %7, %arg4 : index
    %9 = arith.remui %7, %arg4 : index
    %10 = arith.muli %9, %c512 : index
    %11 = arith.addi %10, %6 : index
    %12 = arith.cmpi ult, %11, %arg0 : index
    cf.cond_br %12, ^bb3, ^bb11
  ^bb3:  // pred: ^bb2
    %13 = arith.muli %8, %c32 : index
    cf.br ^bb4(%c0, %cst : index, f32)
  ^bb4(%14: index, %15: f32):  // 2 preds: ^bb3, ^bb9
    %16 = arith.cmpi slt, %14, %c32 : index
    cf.cond_br %16, ^bb5, ^bb10
  ^bb5:  // pred: ^bb4
    %17 = arith.addi %13, %14 : index
    %18 = arith.cmpi slt, %17, %dim : index
    cf.cond_br %18, ^bb6, ^bb7
  ^bb6:  // pred: ^bb5
    %19 = arith.muli %17, %arg0 : index
    %20 = arith.addi %19, %11 : index
    %21 = arith.muli %dim, %dim_0 : index
    %22 = arith.muli %21, %dim_1 : index
    %reinterpret_cast = memref.reinterpret_cast %arg1 to offset: [%c0], sizes: [%22], strides: [%c1] : memref<?x?x?xf32, "gpu"> to memref<?xf32, "gpu">
    %23 = memref.load %reinterpret_cast[%20] : memref<?xf32, "gpu">
    %24 = math.absf %23 : f32
    %25 = arith.cmpf oge, %15, %24 : f32
    %26 = arith.select %25, %15, %24 : f32
    cf.br ^bb8(%26 : f32)
  ^bb7:  // pred: ^bb5
    cf.br ^bb8(%15 : f32)
  ^bb8(%27: f32):  // 2 preds: ^bb6, ^bb7
    cf.br ^bb9
  ^bb9:  // pred: ^bb8
    %28 = arith.addi %14, %c1 : index
    cf.br ^bb4(%28, %27 : index, f32)
  ^bb10:  // pred: ^bb4
    %29 = memref.generic_atomic_rmw %arg5[%11] : memref<?xf32, "gpu"> {
    ^bb0(%arg6: f32):
      %30 = arith.cmpf ogt, %arg6, %15 : f32
      %31 = arith.select %30, %arg6, %15 : f32
      memref.atomic_yield %31 : f32
    }
    cf.br ^bb11
  ^bb11:  // 2 preds: ^bb2, ^bb10
    cf.br ^bb12
  ^bb12:  // 2 preds: ^bb1, ^bb11
    gpu.return
  }
}

// -----// IR Dump After ConvertAffineToStandard (lower-affine) //----- //
gpu.module @main_kernel_0 {
  gpu.func @main_kColReduction_reduce__4_1_0___flat_1(%arg0: index, %arg1: memref<?x?x?xf32, "gpu">, %arg2: index, %arg3: index, %arg4: index, %arg5: memref<?xf32, "gpu">) kernel {
    %cst = arith.constant 0xFF800000 : f32
    %c2 = arith.constant 2 : index
    %c32 = arith.constant 32 : index
    %c512 = arith.constant 512 : index
    %c1 = arith.constant 1 : index
    %c0 = arith.constant 0 : index
    %0 = gpu.block_id  x
    %1 = gpu.thread_id  x
    cf.br ^bb1
  ^bb1:  // pred: ^bb0
    %dim = memref.dim %arg1, %c0 : memref<?x?x?xf32, "gpu">
    %dim_0 = memref.dim %arg1, %c1 : memref<?x?x?xf32, "gpu">
    %dim_1 = memref.dim %arg1, %c2 : memref<?x?x?xf32, "gpu">
    %2 = arith.muli %0, %arg2 : index
    %3 = arith.addi %2, %c0 : index
    %4 = arith.muli %1, %c1 : index
    %5 = arith.addi %4, %c0 : index
    %6 = arith.addi %5, %3 : index
    %7 = arith.cmpi ult, %6, %arg3 : index
    cf.cond_br %7, ^bb2, ^bb12
  ^bb2:  // pred: ^bb1
    %8 = arith.remsi %6, %c512 : index
    %9 = arith.divsi %6, %c512 : index
    %10 = arith.divui %9, %arg4 : index
    %11 = arith.remui %9, %arg4 : index
    %12 = arith.muli %11, %c512 : index
    %13 = arith.addi %12, %8 : index
    %14 = arith.cmpi ult, %13, %arg0 : index
    cf.cond_br %14, ^bb3, ^bb11
  ^bb3:  // pred: ^bb2
    %15 = arith.muli %10, %c32 : index
    cf.br ^bb4(%c0, %cst : index, f32)
  ^bb4(%16: index, %17: f32):  // 2 preds: ^bb3, ^bb9
    %18 = arith.cmpi slt, %16, %c32 : index
    cf.cond_br %18, ^bb5, ^bb10
  ^bb5:  // pred: ^bb4
    %19 = arith.addi %15, %16 : index
    %20 = arith.cmpi slt, %19, %dim : index
    cf.cond_br %20, ^bb6, ^bb7
  ^bb6:  // pred: ^bb5
    %21 = arith.muli %19, %arg0 : index
    %22 = arith.addi %21, %13 : index
    %23 = arith.muli %dim, %dim_0 : index
    %24 = arith.muli %23, %dim_1 : index
    %reinterpret_cast = memref.reinterpret_cast %arg1 to offset: [%c0], sizes: [%24], strides: [%c1] : memref<?x?x?xf32, "gpu"> to memref<?xf32, "gpu">
    %25 = memref.load %reinterpret_cast[%22] : memref<?xf32, "gpu">
    %26 = math.absf %25 : f32
    %27 = arith.cmpf oge, %17, %26 : f32
    %28 = arith.select %27, %17, %26 : f32
    cf.br ^bb8(%28 : f32)
  ^bb7:  // pred: ^bb5
    cf.br ^bb8(%17 : f32)
  ^bb8(%29: f32):  // 2 preds: ^bb6, ^bb7
    cf.br ^bb9
  ^bb9:  // pred: ^bb8
    %30 = arith.addi %16, %c1 : index
    cf.br ^bb4(%30, %29 : index, f32)
  ^bb10:  // pred: ^bb4
    %31 = memref.generic_atomic_rmw %arg5[%13] : memref<?xf32, "gpu"> {
    ^bb0(%arg6: f32):
      %32 = arith.cmpf ogt, %arg6, %17 : f32
      %33 = arith.select %32, %arg6, %17 : f32
      memref.atomic_yield %33 : f32
    }
    cf.br ^bb11
  ^bb11:  // 2 preds: ^bb2, ^bb10
    cf.br ^bb12
  ^bb12:  // 2 preds: ^bb1, ^bb11
    gpu.return
  }
}

// -----// IR Dump After StripDebugInfo (strip-debuginfo) //----- //
gpu.module @main_kernel_0 {
  gpu.func @main_kColReduction_reduce__4_1_0___flat_1(%arg0: index, %arg1: memref<?x?x?xf32, "gpu">, %arg2: index, %arg3: index, %arg4: index, %arg5: memref<?xf32, "gpu">) kernel {
    %cst = arith.constant 0xFF800000 : f32
    %c2 = arith.constant 2 : index
    %c32 = arith.constant 32 : index
    %c512 = arith.constant 512 : index
    %c1 = arith.constant 1 : index
    %c0 = arith.constant 0 : index
    %0 = gpu.block_id  x
    %1 = gpu.thread_id  x
    cf.br ^bb1
  ^bb1:  // pred: ^bb0
    %dim = memref.dim %arg1, %c0 : memref<?x?x?xf32, "gpu">
    %dim_0 = memref.dim %arg1, %c1 : memref<?x?x?xf32, "gpu">
    %dim_1 = memref.dim %arg1, %c2 : memref<?x?x?xf32, "gpu">
    %2 = arith.muli %0, %arg2 : index
    %3 = arith.addi %2, %c0 : index
    %4 = arith.muli %1, %c1 : index
    %5 = arith.addi %4, %c0 : index
    %6 = arith.addi %5, %3 : index
    %7 = arith.cmpi ult, %6, %arg3 : index
    cf.cond_br %7, ^bb2, ^bb12
  ^bb2:  // pred: ^bb1
    %8 = arith.remsi %6, %c512 : index
    %9 = arith.divsi %6, %c512 : index
    %10 = arith.divui %9, %arg4 : index
    %11 = arith.remui %9, %arg4 : index
    %12 = arith.muli %11, %c512 : index
    %13 = arith.addi %12, %8 : index
    %14 = arith.cmpi ult, %13, %arg0 : index
    cf.cond_br %14, ^bb3, ^bb11
  ^bb3:  // pred: ^bb2
    %15 = arith.muli %10, %c32 : index
    cf.br ^bb4(%c0, %cst : index, f32)
  ^bb4(%16: index, %17: f32):  // 2 preds: ^bb3, ^bb9
    %18 = arith.cmpi slt, %16, %c32 : index
    cf.cond_br %18, ^bb5, ^bb10
  ^bb5:  // pred: ^bb4
    %19 = arith.addi %15, %16 : index
    %20 = arith.cmpi slt, %19, %dim : index
    cf.cond_br %20, ^bb6, ^bb7
  ^bb6:  // pred: ^bb5
    %21 = arith.muli %19, %arg0 : index
    %22 = arith.addi %21, %13 : index
    %23 = arith.muli %dim, %dim_0 : index
    %24 = arith.muli %23, %dim_1 : index
    %reinterpret_cast = memref.reinterpret_cast %arg1 to offset: [%c0], sizes: [%24], strides: [%c1] : memref<?x?x?xf32, "gpu"> to memref<?xf32, "gpu">
    %25 = memref.load %reinterpret_cast[%22] : memref<?xf32, "gpu">
    %26 = math.absf %25 : f32
    %27 = arith.cmpf oge, %17, %26 : f32
    %28 = arith.select %27, %17, %26 : f32
    cf.br ^bb8(%28 : f32)
  ^bb7:  // pred: ^bb5
    cf.br ^bb8(%17 : f32)
  ^bb8(%29: f32):  // 2 preds: ^bb6, ^bb7
    cf.br ^bb9
  ^bb9:  // pred: ^bb8
    %30 = arith.addi %16, %c1 : index
    cf.br ^bb4(%30, %29 : index, f32)
  ^bb10:  // pred: ^bb4
    %31 = memref.generic_atomic_rmw %arg5[%13] : memref<?xf32, "gpu"> {
    ^bb0(%arg6: f32):
      %32 = arith.cmpf ogt, %arg6, %17 : f32
      %33 = arith.select %32, %arg6, %17 : f32
      memref.atomic_yield %33 : f32
    }
    cf.br ^bb11
  ^bb11:  // 2 preds: ^bb2, ^bb10
    cf.br ^bb12
  ^bb12:  // 2 preds: ^bb1, ^bb11
    gpu.return
  }
}

// -----// IR Dump After DiscLowerGpuOpsToNVVMOpsPass (disc-convert-gpu-to-nvvm) //----- //
gpu.module @main_kernel_0 {
  llvm.func @__nv_fabsf(f32) -> f32
  llvm.func @main_kColReduction_reduce__4_1_0___flat_1(%arg0: i32, %arg1: !llvm.ptr<f32>, %arg2: !llvm.ptr<f32>, %arg3: i32, %arg4: i32, %arg5: i32, %arg6: i32, %arg7: i32, %arg8: i32, %arg9: i32, %arg10: i32, %arg11: i32, %arg12: i32, %arg13: !llvm.ptr<f32>, %arg14: !llvm.ptr<f32>, %arg15: i32, %arg16: i32, %arg17: i32) attributes {gpu.kernel, nvvm.kernel} {
    %0 = llvm.mlir.undef : !llvm.struct<(ptr<f32>, ptr<f32>, i32, array<3 x i32>, array<3 x i32>)>
    %1 = llvm.insertvalue %arg1, %0[0] : !llvm.struct<(ptr<f32>, ptr<f32>, i32, array<3 x i32>, array<3 x i32>)> 
    %2 = llvm.insertvalue %arg2, %1[1] : !llvm.struct<(ptr<f32>, ptr<f32>, i32, array<3 x i32>, array<3 x i32>)> 
    %3 = llvm.insertvalue %arg3, %2[2] : !llvm.struct<(ptr<f32>, ptr<f32>, i32, array<3 x i32>, array<3 x i32>)> 
    %4 = llvm.insertvalue %arg4, %3[3, 0] : !llvm.struct<(ptr<f32>, ptr<f32>, i32, array<3 x i32>, array<3 x i32>)> 
    %5 = llvm.insertvalue %arg7, %4[4, 0] : !llvm.struct<(ptr<f32>, ptr<f32>, i32, array<3 x i32>, array<3 x i32>)> 
    %6 = llvm.insertvalue %arg5, %5[3, 1] : !llvm.struct<(ptr<f32>, ptr<f32>, i32, array<3 x i32>, array<3 x i32>)> 
    %7 = llvm.insertvalue %arg8, %6[4, 1] : !llvm.struct<(ptr<f32>, ptr<f32>, i32, array<3 x i32>, array<3 x i32>)> 
    %8 = llvm.insertvalue %arg6, %7[3, 2] : !llvm.struct<(ptr<f32>, ptr<f32>, i32, array<3 x i32>, array<3 x i32>)> 
    %9 = llvm.insertvalue %arg9, %8[4, 2] : !llvm.struct<(ptr<f32>, ptr<f32>, i32, array<3 x i32>, array<3 x i32>)> 
    %10 = llvm.mlir.undef : !llvm.struct<(ptr<f32>, ptr<f32>, i32, array<1 x i32>, array<1 x i32>)>
    %11 = llvm.insertvalue %arg13, %10[0] : !llvm.struct<(ptr<f32>, ptr<f32>, i32, array<1 x i32>, array<1 x i32>)> 
    %12 = llvm.insertvalue %arg14, %11[1] : !llvm.struct<(ptr<f32>, ptr<f32>, i32, array<1 x i32>, array<1 x i32>)> 
    %13 = llvm.insertvalue %arg15, %12[2] : !llvm.struct<(ptr<f32>, ptr<f32>, i32, array<1 x i32>, array<1 x i32>)> 
    %14 = llvm.insertvalue %arg16, %13[3, 0] : !llvm.struct<(ptr<f32>, ptr<f32>, i32, array<1 x i32>, array<1 x i32>)> 
    %15 = llvm.insertvalue %arg17, %14[4, 0] : !llvm.struct<(ptr<f32>, ptr<f32>, i32, array<1 x i32>, array<1 x i32>)> 
    %16 = llvm.mlir.constant(0xFF800000 : f32) : f32
    %17 = llvm.mlir.constant(2 : index) : i32
    %18 = llvm.mlir.constant(32 : index) : i32
    %19 = llvm.mlir.constant(512 : index) : i32
    %20 = llvm.mlir.constant(1 : index) : i32
    %21 = llvm.mlir.constant(0 : index) : i32
    %22 = nvvm.read.ptx.sreg.ctaid.x : i32
    %23 = nvvm.read.ptx.sreg.tid.x : i32
    llvm.br ^bb1
  ^bb1:  // pred: ^bb0
    %24 = llvm.extractvalue %9[3, 0] : !llvm.struct<(ptr<f32>, ptr<f32>, i32, array<3 x i32>, array<3 x i32>)> 
    %25 = llvm.extractvalue %9[3, 1] : !llvm.struct<(ptr<f32>, ptr<f32>, i32, array<3 x i32>, array<3 x i32>)> 
    %26 = llvm.extractvalue %9[3, 2] : !llvm.struct<(ptr<f32>, ptr<f32>, i32, array<3 x i32>, array<3 x i32>)> 
    %27 = llvm.mul %22, %arg10  : i32
    %28 = llvm.add %23, %27  : i32
    %29 = llvm.icmp "ult" %28, %arg11 : i32
    llvm.cond_br %29, ^bb2, ^bb14
  ^bb2:  // pred: ^bb1
    %30 = llvm.srem %28, %19  : i32
    %31 = llvm.sdiv %28, %19  : i32
    %32 = llvm.udiv %31, %arg12  : i32
    %33 = llvm.urem %31, %arg12  : i32
    %34 = llvm.mul %33, %19  : i32
    %35 = llvm.add %34, %30  : i32
    %36 = llvm.icmp "ult" %35, %arg0 : i32
    llvm.cond_br %36, ^bb3, ^bb13
  ^bb3:  // pred: ^bb2
    %37 = llvm.mul %32, %18  : i32
    llvm.br ^bb4(%21, %16 : i32, f32)
  ^bb4(%38: i32, %39: f32):  // 2 preds: ^bb3, ^bb9
    %40 = llvm.icmp "slt" %38, %18 : i32
    llvm.cond_br %40, ^bb5, ^bb10
  ^bb5:  // pred: ^bb4
    %41 = llvm.add %37, %38  : i32
    %42 = llvm.icmp "slt" %41, %24 : i32
    llvm.cond_br %42, ^bb6, ^bb7
  ^bb6:  // pred: ^bb5
    %43 = llvm.mul %41, %arg0  : i32
    %44 = llvm.add %43, %35  : i32
    %45 = llvm.mul %24, %25  : i32
    %46 = llvm.mul %45, %26  : i32
    %47 = llvm.mlir.undef : !llvm.struct<(ptr<f32>, ptr<f32>, i32, array<1 x i32>, array<1 x i32>)>
    %48 = llvm.extractvalue %9[0] : !llvm.struct<(ptr<f32>, ptr<f32>, i32, array<3 x i32>, array<3 x i32>)> 
    %49 = llvm.extractvalue %9[1] : !llvm.struct<(ptr<f32>, ptr<f32>, i32, array<3 x i32>, array<3 x i32>)> 
    %50 = llvm.insertvalue %48, %47[0] : !llvm.struct<(ptr<f32>, ptr<f32>, i32, array<1 x i32>, array<1 x i32>)> 
    %51 = llvm.insertvalue %49, %50[1] : !llvm.struct<(ptr<f32>, ptr<f32>, i32, array<1 x i32>, array<1 x i32>)> 
    %52 = llvm.insertvalue %21, %51[2] : !llvm.struct<(ptr<f32>, ptr<f32>, i32, array<1 x i32>, array<1 x i32>)> 
    %53 = llvm.insertvalue %46, %52[3, 0] : !llvm.struct<(ptr<f32>, ptr<f32>, i32, array<1 x i32>, array<1 x i32>)> 
    %54 = llvm.insertvalue %20, %53[4, 0] : !llvm.struct<(ptr<f32>, ptr<f32>, i32, array<1 x i32>, array<1 x i32>)> 
    %55 = llvm.extractvalue %54[1] : !llvm.struct<(ptr<f32>, ptr<f32>, i32, array<1 x i32>, array<1 x i32>)> 
    %56 = llvm.getelementptr %55[%44] : (!llvm.ptr<f32>, i32) -> !llvm.ptr<f32>
    %57 = llvm.load %56 : !llvm.ptr<f32>
    %58 = llvm.call @__nv_fabsf(%57) : (f32) -> f32
    %59 = llvm.fcmp "oge" %39, %58 : f32
    %60 = llvm.select %59, %39, %58 : i1, f32
    llvm.br ^bb8(%60 : f32)
  ^bb7:  // pred: ^bb5
    llvm.br ^bb8(%39 : f32)
  ^bb8(%61: f32):  // 2 preds: ^bb6, ^bb7
    llvm.br ^bb9
  ^bb9:  // pred: ^bb8
    %62 = llvm.add %38, %20  : i32
    llvm.br ^bb4(%62, %61 : i32, f32)
  ^bb10:  // pred: ^bb4
    %63 = llvm.extractvalue %15[1] : !llvm.struct<(ptr<f32>, ptr<f32>, i32, array<1 x i32>, array<1 x i32>)> 
    %64 = llvm.getelementptr %63[%35] : (!llvm.ptr<f32>, i32) -> !llvm.ptr<f32>
    %65 = llvm.load %64 : !llvm.ptr<f32>
    llvm.br ^bb11(%65 : f32)
  ^bb11(%66: f32):  // 2 preds: ^bb10, ^bb11
    %67 = llvm.fcmp "ogt" %66, %39 : f32
    %68 = llvm.select %67, %66, %39 : i1, f32
    %69 = llvm.bitcast %64 : !llvm.ptr<f32> to !llvm.ptr<i32>
    %70 = llvm.bitcast %66 : f32 to i32
    %71 = llvm.bitcast %68 : f32 to i32
    %72 = llvm.cmpxchg %69, %70, %71 acq_rel monotonic : !llvm.ptr<i32>, i32
    %73 = llvm.extractvalue %72[0] : !llvm.struct<(i32, i1)> 
    %74 = llvm.bitcast %73 : i32 to f32
    %75 = llvm.extractvalue %72[1] : !llvm.struct<(i32, i1)> 
    llvm.cond_br %75, ^bb12, ^bb11(%74 : f32)
  ^bb12:  // pred: ^bb11
    llvm.br ^bb13
  ^bb13:  // 2 preds: ^bb2, ^bb12
    llvm.br ^bb14
  ^bb14:  // 2 preds: ^bb1, ^bb13
    llvm.return
  }
}

// -----// IR Dump After LLVMInsertValueSimplifierPass (disc-llvm-insert-value-simplifier) //----- //
llvm.func @main_kColReduction_reduce__4_1_0___flat_1(%arg0: i32, %arg1: !llvm.ptr<f32>, %arg2: !llvm.ptr<f32>, %arg3: i32, %arg4: i32, %arg5: i32, %arg6: i32, %arg7: i32, %arg8: i32, %arg9: i32, %arg10: i32, %arg11: i32, %arg12: i32, %arg13: !llvm.ptr<f32>, %arg14: !llvm.ptr<f32>, %arg15: i32, %arg16: i32, %arg17: i32) attributes {gpu.kernel, nvvm.kernel} {
  %0 = llvm.mlir.constant(0 : index) : i32
  %1 = llvm.mlir.constant(1 : index) : i32
  %2 = llvm.mlir.constant(512 : index) : i32
  %3 = llvm.mlir.constant(32 : index) : i32
  %4 = llvm.mlir.constant(0xFF800000 : f32) : f32
  %5 = nvvm.read.ptx.sreg.ctaid.x : i32
  %6 = nvvm.read.ptx.sreg.tid.x : i32
  llvm.br ^bb1
^bb1:  // pred: ^bb0
  %7 = llvm.mul %5, %arg10  : i32
  %8 = llvm.add %6, %7  : i32
  %9 = llvm.icmp "ult" %8, %arg11 : i32
  llvm.cond_br %9, ^bb2, ^bb14
^bb2:  // pred: ^bb1
  %10 = llvm.srem %8, %2  : i32
  %11 = llvm.sdiv %8, %2  : i32
  %12 = llvm.udiv %11, %arg12  : i32
  %13 = llvm.urem %11, %arg12  : i32
  %14 = llvm.mul %13, %2  : i32
  %15 = llvm.add %14, %10  : i32
  %16 = llvm.icmp "ult" %15, %arg0 : i32
  llvm.cond_br %16, ^bb3, ^bb13
^bb3:  // pred: ^bb2
  %17 = llvm.mul %12, %3  : i32
  llvm.br ^bb4(%0, %4 : i32, f32)
^bb4(%18: i32, %19: f32):  // 2 preds: ^bb3, ^bb9
  %20 = llvm.icmp "slt" %18, %3 : i32
  llvm.cond_br %20, ^bb5, ^bb10
^bb5:  // pred: ^bb4
  %21 = llvm.add %17, %18  : i32
  %22 = llvm.icmp "slt" %21, %arg4 : i32
  llvm.cond_br %22, ^bb6, ^bb7
^bb6:  // pred: ^bb5
  %23 = llvm.mul %21, %arg0  : i32
  %24 = llvm.add %23, %15  : i32
  %25 = llvm.getelementptr %arg2[%24] : (!llvm.ptr<f32>, i32) -> !llvm.ptr<f32>
  %26 = llvm.load %25 : !llvm.ptr<f32>
  %27 = llvm.call @__nv_fabsf(%26) : (f32) -> f32
  %28 = llvm.fcmp "oge" %19, %27 : f32
  %29 = llvm.select %28, %19, %27 : i1, f32
  llvm.br ^bb8(%29 : f32)
^bb7:  // pred: ^bb5
  llvm.br ^bb8(%19 : f32)
^bb8(%30: f32):  // 2 preds: ^bb6, ^bb7
  llvm.br ^bb9
^bb9:  // pred: ^bb8
  %31 = llvm.add %18, %1  : i32
  llvm.br ^bb4(%31, %30 : i32, f32)
^bb10:  // pred: ^bb4
  %32 = llvm.getelementptr %arg14[%15] : (!llvm.ptr<f32>, i32) -> !llvm.ptr<f32>
  %33 = llvm.load %32 : !llvm.ptr<f32>
  llvm.br ^bb11(%33 : f32)
^bb11(%34: f32):  // 2 preds: ^bb10, ^bb11
  %35 = llvm.fcmp "ogt" %34, %19 : f32
  %36 = llvm.select %35, %34, %19 : i1, f32
  %37 = llvm.bitcast %32 : !llvm.ptr<f32> to !llvm.ptr<i32>
  %38 = llvm.bitcast %34 : f32 to i32
  %39 = llvm.bitcast %36 : f32 to i32
  %40 = llvm.cmpxchg %37, %38, %39 acq_rel monotonic : !llvm.ptr<i32>, i32
  %41 = llvm.extractvalue %40[0] : !llvm.struct<(i32, i1)> 
  %42 = llvm.bitcast %41 : i32 to f32
  %43 = llvm.extractvalue %40[1] : !llvm.struct<(i32, i1)> 
  llvm.cond_br %43, ^bb12, ^bb11(%42 : f32)
^bb12:  // pred: ^bb11
  llvm.br ^bb13
^bb13:  // 2 preds: ^bb2, ^bb12
  llvm.br ^bb14
^bb14:  // 2 preds: ^bb1, ^bb13
  llvm.return
}

// -----// IR Dump After FunctionDeadArgumentEliminationPass (disc-function-dead-argument-elimination) //----- //
gpu.module @main_kernel_0 {
  llvm.func @__nv_fabsf(f32) -> f32
  llvm.func @main_kColReduction_reduce__4_1_0___flat_1(%arg0: i32, %arg1: !llvm.ptr<f32>, %arg2: i32, %arg3: i32, %arg4: i32, %arg5: i32, %arg6: !llvm.ptr<f32>) attributes {disc.elimargs = [1 : index, 3 : index, 5 : index, 6 : index, 7 : index, 8 : index, 9 : index, 13 : index, 15 : index, 16 : index, 17 : index], gpu.kernel, nvvm.kernel} {
    %0 = llvm.mlir.constant(0 : index) : i32
    %1 = llvm.mlir.constant(1 : index) : i32
    %2 = llvm.mlir.constant(512 : index) : i32
    %3 = llvm.mlir.constant(32 : index) : i32
    %4 = llvm.mlir.constant(0xFF800000 : f32) : f32
    %5 = nvvm.read.ptx.sreg.ctaid.x : i32
    %6 = nvvm.read.ptx.sreg.tid.x : i32
    llvm.br ^bb1
  ^bb1:  // pred: ^bb0
    %7 = llvm.mul %5, %arg3  : i32
    %8 = llvm.add %6, %7  : i32
    %9 = llvm.icmp "ult" %8, %arg4 : i32
    llvm.cond_br %9, ^bb2, ^bb14
  ^bb2:  // pred: ^bb1
    %10 = llvm.srem %8, %2  : i32
    %11 = llvm.sdiv %8, %2  : i32
    %12 = llvm.udiv %11, %arg5  : i32
    %13 = llvm.urem %11, %arg5  : i32
    %14 = llvm.mul %13, %2  : i32
    %15 = llvm.add %14, %10  : i32
    %16 = llvm.icmp "ult" %15, %arg0 : i32
    llvm.cond_br %16, ^bb3, ^bb13
  ^bb3:  // pred: ^bb2
    %17 = llvm.mul %12, %3  : i32
    llvm.br ^bb4(%0, %4 : i32, f32)
  ^bb4(%18: i32, %19: f32):  // 2 preds: ^bb3, ^bb9
    %20 = llvm.icmp "slt" %18, %3 : i32
    llvm.cond_br %20, ^bb5, ^bb10
  ^bb5:  // pred: ^bb4
    %21 = llvm.add %17, %18  : i32
    %22 = llvm.icmp "slt" %21, %arg2 : i32
    llvm.cond_br %22, ^bb6, ^bb7
  ^bb6:  // pred: ^bb5
    %23 = llvm.mul %21, %arg0  : i32
    %24 = llvm.add %23, %15  : i32
    %25 = llvm.getelementptr %arg1[%24] : (!llvm.ptr<f32>, i32) -> !llvm.ptr<f32>
    %26 = llvm.load %25 : !llvm.ptr<f32>
    %27 = llvm.call @__nv_fabsf(%26) : (f32) -> f32
    %28 = llvm.fcmp "oge" %19, %27 : f32
    %29 = llvm.select %28, %19, %27 : i1, f32
    llvm.br ^bb8(%29 : f32)
  ^bb7:  // pred: ^bb5
    llvm.br ^bb8(%19 : f32)
  ^bb8(%30: f32):  // 2 preds: ^bb6, ^bb7
    llvm.br ^bb9
  ^bb9:  // pred: ^bb8
    %31 = llvm.add %18, %1  : i32
    llvm.br ^bb4(%31, %30 : i32, f32)
  ^bb10:  // pred: ^bb4
    %32 = llvm.getelementptr %arg6[%15] : (!llvm.ptr<f32>, i32) -> !llvm.ptr<f32>
    %33 = llvm.load %32 : !llvm.ptr<f32>
    llvm.br ^bb11(%33 : f32)
  ^bb11(%34: f32):  // 2 preds: ^bb10, ^bb11
    %35 = llvm.fcmp "ogt" %34, %19 : f32
    %36 = llvm.select %35, %34, %19 : i1, f32
    %37 = llvm.bitcast %32 : !llvm.ptr<f32> to !llvm.ptr<i32>
    %38 = llvm.bitcast %34 : f32 to i32
    %39 = llvm.bitcast %36 : f32 to i32
    %40 = llvm.cmpxchg %37, %38, %39 acq_rel monotonic : !llvm.ptr<i32>, i32
    %41 = llvm.extractvalue %40[0] : !llvm.struct<(i32, i1)> 
    %42 = llvm.bitcast %41 : i32 to f32
    %43 = llvm.extractvalue %40[1] : !llvm.struct<(i32, i1)> 
    llvm.cond_br %43, ^bb12, ^bb11(%42 : f32)
  ^bb12:  // pred: ^bb11
    llvm.br ^bb13
  ^bb13:  // 2 preds: ^bb2, ^bb12
    llvm.br ^bb14
  ^bb14:  // 2 preds: ^bb1, ^bb13
    llvm.return
  }
}

// -----// IR Dump After GpuKernelToBlobPass (disc-gpu-kernel-to-blob) //----- //
gpu.module @main_kernel_0 attributes {gpu.binary = "P\EDU\BA\01\00\10\00\90\0A\00\00\00\00\00\00\02\00\01\01@\00\00\00P\0A\00\00\00\00\00\00P\0A\00\00\00\00\00\00\07\00\01\00P\00\00\00\00\00\00\00\00\00\00\00\11 \00\00\00\00\00\00\00\00\00\00\00\00\00\00\E8\1C\00\00\00\00\00\00\A2\7FELF\02\01\013\07\00\01\00f\02\00\BE\00q\00\01\00!@\1C\07\001\00\80\19\07\00\F5\0E\00P\05P\00@\008\00\03\00@\00\0B\00\01\00\00.shstrtab\00.\08\00'ym\08\00\F0\1B_shndx\00.nv.info\00.text.main_kColReduction_r\0A\00\F5\02e__4_1_0___flat_19\00\0F3\00\1Coshared5\00\19Orela\9D\00\1E?rel\D1\00!\9Fconstant08\00\19\B2debug_framey\00\09\11\00!nv\14\00\11aD\00\0F\98\01 \0F\89\00\16\0F\C2\01\FF(o_param\C9\01\1C\0F\01\00\0B\8C\\\00\00\00\03\00\0A\00\01\00 ]\01\18\00,\09\00\01\00\11\9C\18\00,\04\00\01\00\11\BA\18\00,\07\00\01\00f2\00\00\00\12\10`\00!\80\11\07\00v\00\FF\FF\FF\FF(\00\0C\00\00\01\00@\03\00\04|\08\00\D1\0F\0C\81\80\80(\00\08\FF\81\80(\08\0B\00\06(\00/0\00\01\00\03\13pX\00\10\04\9B\00R\04\14\00\00\00E\00\22\04 \AC\00\90\04/\08\00\05\00\00\00\16\0C\00\12#\0C\00\00\01\00'\04\12\0C\00\17\11\0C\00!7\04\C8\04\F1\08\015\00\00\04\0A\08\00\02\00\00\00`\01(\00\03\19(\00\04\17\0C$\00u\06\00 \00\00\F0!\10\00u\05\00\1C\00\00\F0\11\10\009\04\00\18\10\009\03\00\14\10\009\02\00\10\10\009\01\00\08P\00\01\01\00\F2\0A\F0\11\00\03\1B\FF\00\04\1C\0C\00P\00\00\00\10\02\00\00\E0\10\00\00\04\1E\D8\00#K\00\01\00\B1\02\02\08\10\0A/\22\00\00\00\08\10\00#\00\08\08\00\13\10\08\00\13\18\08\00\13 \08\00\13(\08\00\130\08\00\228\08\84\00\13\00\08\00\13\08\08\00\13\10\08\00\13\18\08\00\13 \08\00\13(\08\00\130\08\00\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\A2\00\00\00\14,\00\00\00H\00\01\00\00x\01/\05\00\01\00\FF\88@$v\01\FF?\04\B2\FF\00\8E\07\00\C4\0F\00\19y\00\01\00\10%c\02Q\0E\00\19y\03\0F\00 \00!-\00@\0E\00$z]\04\B0\00\00\03\02\8E\07\00\CA\1F\00\0C\10\00\C5^\00\00p`\F0\03\00\DA\0F\00Ms\04\F0\0C\80\03\00\EA\0F\00\06{\04\00\00_\00\00\00\90 \00\00\22\0E\00\19x\05\FF\1F\1F\00\D3\14\01\00\00\E2\0F\00$r\02\FF\FF\00\80\00 \E2\0FP\00\10\FF0\00\B1pP\F4\03\00\E4\0F\00\11r\05?\02\B2\FFH\8F\07\00\C6\0F\00\08s\04\BE\04\10\10\90\00\F1\07\1E\00\10x\03\04\FE\FF\FF\0F\FF\E0\FF\07\00\CC\1F\00\05s\03\00\B2\00\22\F0!\B0\00!r\07`\00\C0\03\0A\8E\07\00\C8\1F\00$z\07\07`\00\10\FF\C0\00\81\C8\0F\00'r\07\03\07\A8\02\02\80\00@\19x\02\FF\C3\00\10\05\A0\00\E1\E4\0F\00\12x\05\05\00\FE\FF\FF\FF\C0\8E\80\00T'r\07\07\02\B0\00\10\C8\C0\00\11\09`\00#\07\0A\10\000z\03\09`\00\11\02 \01\01\D0\006\03\00_ \010\10\0A\03\10\00\11\80\B0\00\80\E4\0F\00\10\08\07\07\01P\00\03\10\00\060\00\12\F20\00\1A\18 \001\12\AA\07 \01\22\FF3`\00;$r\03\80\00%\02\03\80\00`\E4\0F\00$x\03\14\04$\00\05 \000x\03\02\95\03\18\03\A0\00\1FX\C0\01\07W$x\02\07 \A0\01\81\B9z\04\00\00F\00\00s\06\00\C0\01Dt\00\FF\04 \00\11\C6P\00\10\02\95\060pb\F8\C0\01U\04\10x\06\02\C0\00\11\C8 \00\12\06 \00\C3\FA\03\00\CE\0F\00$\CA\05\02\00X\90\00\90\C8\0F\00%\C6\04\05\00Z\91\01\03\B0\005\DA\07\06 \00a\E2\0F\00\81\C9\0A#\06\C5\00\19\1E\0C\00\A6\00\00%\D6\06\070\00u\CA\0F\00\81\D9\0D\06 \00\86\E2\02\00\10x\08\02\02@\01j\04\10x\10\02\03p\01\12\08\90\00#\F0\03\10\00\12\10\10\00\11\F2\10\00Y\10x\0C\02\04@\004\0E\02\05\10\00\11\C40\00\12\0C0\00 \F4\03\80\00E$\8A\09\08\B0\00w\E4\0F\10$\9A\0B\10\10\00U\00%\86\08\09\B0\00\02\F0\02\12\0E@\00 \F6\03\F0\00E\81\89\05\08\C0\00v\22\11\00%\96\06\0B0\00 /\00\C0\00\14\06\80\00w\C6\0F\00$\AA\0B\0C \01F\10\81\99\0F\00\01\B4\22\03\00$t\04\FF\00\00\80\FF\C0\01\00\D0\00\14\07@\00\00\C0\015\BA\09\0E@\00W\C8\1F\00%\B6\A0\00g\CC\0F\00\81\B9\09\90\00@\01\00\0B\C8\FA\01\D2\80\FF\00\C2\FC\03\00\C8O\00\08\C8\04\10\000\02\00\03P\007%\A6\0A\B0\00\08P\01\01\10\02\F5\06\00\0B\D2\00\04\0D\00\00@\00`\FC\03\00\C6\8F\00\81\A9\0B\0A`\00b\E2\04\00\08\D2\04 \00!\00\000\00\06`\01\12\FA`\01&\CA\07P\01z\C8/\00%\C6\06\070\02*\0D\0C0\02\16\11\10\01y\E6\02\00%\D6\0C\0D\E0\006\D9\0D\0C\80\00t\00\00\0B\82\00\04\05\A0\00\84\E4\0F\09\10x\0A\02\080\01\8A\E4O\00\08\82\04\04\05\A0\00\18\0A@\02V\0B\92\00\04\0F@\00\01\80\02\14\09@\00\8A\C4\1F\00\08\92\04\04\0F@\00\1A\08p\028\0E\02\0A\E0\016\8A\07\0A\D0\00\11/\C0\01\17\0B \00+%\86\00\03&\89\05\F0\00!\A2\000\02\14\0C0\00\00\A0\00D\A2\00\04\0B\A0\00\8A\C8\8F\00\08\A2\04\04\0B\90\00\15\0E\D0\02\000\00D\B2\00\04\090\00\000\02&\9A\0B\E0\02\00\90\05D\B2\04\04\09@\00\00\00\02(\96\08\00\02\16\08\C0\01\12\F6P\00D\C2\00\04\11P\00x\E4\0F\08\81\99\0F\08\B0\03H$\AA\0B\0E\C0\01Z\08\C2\04\04\11\A0\00\06P\02\0Fp\02\00\00@\01\17\0D\A0\03\09p\02f\E2\0F\08$\BA\07 \02*\E4\1F\80\02.\22\01\80\02\05\10\01\12\FAp\00\1B\B6p\028\CA\09\10\A0\01'\81\B9p\02{&\03\00%\C6\08\09\A0\02\1A\0E\A0\02\16\13\00\01/&\01\A0\02\0B+\22\03\A0\02 O\08\A0\02\15\0E\E0\01\1F\1F\A0\02\0E\00 \01\1B\0F\10\05\17\100\01\00`\02\14\11\10\00*\CE/\80\02\1F\C8p\02\0F\09\00\03/\C8\8F\F0\02\01\18\0E\F0\02\0A\90\02/\0F\01\90\02\00\1C\10\90\02\05@\02\00 \03)\9A\09P\01'\08\B20\02\11\C6\90\02\06\80\01\0F\90\02\04\14\13P\000\E4\0F\08 \01\14\12\00\01\11\E2`\05\18\10`\00\0C\B0\02\00\A0\02\1E\13\E0\00\0F\A0\02\06\09\90\02\01p\018\10\02\13p\00\08\A0\02*\E2\1F\90\02-\E2\0F\B0\02\06 \01\0F\A0\02\06\1F\0E\A0\02\1C\1F\10\A0\02=\1F\14\A0\02\1C\1B\15\A0\02\1B\16\A0\02\1F\17\A0\02\A7\19\10\A0\02\02@\00\06\90\02\1D\E2\B0\02\0F\A0\02\02\14\18\00\01\0F\A0\02\1D\1F\C4\A0\02\0E\19\08\A0\02\01p\00?\10\02\19\A0\02U\10\E4\A0\02\16\07\B0\01\1F$\A0\02\11\1F\11\A0\02.\1F\1A\A0\02\1A5\06\02\1B0\00\11/\10\0A\1B\1C\A0\02\14\1D\10\00\11\CE\10\0A\0B\A0\02+\08\09\A0\02\1C\08\B0\07\19\1E\00\036\02\02\1F\10\00\0F\C0\02\11\1F\06\C0\02\1C\1C\0E\C0\02\19\07\C0\02,\0B\06`\05\14\07@\00+\C6\0F\A0\0A\0F\F0\07\0C\00\80\07\08\E0\07\10\E4\B0\02,\07\06\B0\02\1E\11`\03\0FP\05\13\11\C6\D0\0A\0F\A0\02\18\08\B0\0C\04\A0\02\0Bp\02\17\CAp\02\1B\E4\10\0B\10$\B0\0C)\0C\0D0\009\DA\0F\020\009\C9\0D\0C0\00:\D6\0E\0F\A0\02(\0F\0E\90\00g%v\02\03\00` \025y\06\02 \00*b!\C0\02*\C8O\B0\02\12\C8 \02\02\C0\01\15\F0 \02\03\B0\018\00\00\C8\10\02\1C\F0\10\02\02 \00\05\F0\09\01 \00\08\E0\09\02 \00\14\C2p\01\03 \00\15\C2`\01\02 \00\14\D2\A0\02\03 \00\15\D2\A0\02\10\00`\0C2\0Br\00\F0\01 @\F0p\017\0EFyp\0ES\E6\0F\00\08r\10\02\00\01\00\80\CC\0F\00\A9s\07\02\06\E0\0E\B0\E1\1E\00\00\A4\0E\00\0Cr\00\07\10\00 pR@\00RO\00$r\06\00\0F\00\E0\0C\80\D8\0F\00G\09\00\00\90i\14!\FF\83\C0\0E*My\D0\0ETGy\00\00\F0 \00f\C0\0F\00\18y\00\01\00\0F\10\00`\0F\01\00-\00\\\0F.\03\00\01\00\02q\01]\00\00\98\01\000\00\08\01\00\1F\0B@\00\04\13\D8)\00\1F\C9@\00\0C\13\13\E4\13\0C\01\00\13\A8U\00\11\90\06\00\02$\00#\04\00]\14\00\CE\14\12\00\01\00.k\01T\00\00\01\00\138\05\02/p\00\80\00\0B\1F)'\00\03#\00\A8@\00\04p\16\04\E4\00*\04\00\01\00\1Fb@\00\04\13\D8)\00&\A8\00@\00\1F\0A@\00\00!\89\01D\01\0D@\00\11\80\B6\12J\00\00\D8\00\01\00\1B\08\08\00?x\01\00\A6\17\000\00\00X\E5\02\03\E7\12\04\80\00\17\048\00\04\18\00\133@\01\0C\84\01\13h@\00\17\881\01\0F\C0\00\01\132T\01+\06\00\01\00\1A\08\F0\17\12\03\C8\15:\16\80\00\01\00\13\06\E0\15\04(\1C\0D\88\01\1A\00\08\00\04\C0\00\13\018\00\04\A8\00\0C\01\009\18\13\00\08\00\088\00/\06\00\01\00\17\80\08\00\00\00\00\00\00\00"} {
  llvm.func @__nv_fabsf(f32) -> f32
  llvm.func @main_kColReduction_reduce__4_1_0___flat_1(%arg0: i32, %arg1: !llvm.ptr<f32>, %arg2: i32, %arg3: i32, %arg4: i32, %arg5: i32, %arg6: !llvm.ptr<f32>) attributes {disc.elimargs = [1 : index, 3 : index, 5 : index, 6 : index, 7 : index, 8 : index, 9 : index, 13 : index, 15 : index, 16 : index, 17 : index], gpu.kernel, nvvm.kernel} {
    %0 = llvm.mlir.constant(0 : index) : i32
    %1 = llvm.mlir.constant(1 : index) : i32
    %2 = llvm.mlir.constant(512 : index) : i32
    %3 = llvm.mlir.constant(32 : index) : i32
    %4 = llvm.mlir.constant(0xFF800000 : f32) : f32
    %5 = nvvm.read.ptx.sreg.ctaid.x : i32
    %6 = nvvm.read.ptx.sreg.tid.x : i32
    llvm.br ^bb1
  ^bb1:  // pred: ^bb0
    %7 = llvm.mul %5, %arg3  : i32
    %8 = llvm.add %6, %7  : i32
    %9 = llvm.icmp "ult" %8, %arg4 : i32
    llvm.cond_br %9, ^bb2, ^bb14
  ^bb2:  // pred: ^bb1
    %10 = llvm.srem %8, %2  : i32
    %11 = llvm.sdiv %8, %2  : i32
    %12 = llvm.udiv %11, %arg5  : i32
    %13 = llvm.urem %11, %arg5  : i32
    %14 = llvm.mul %13, %2  : i32
    %15 = llvm.add %14, %10  : i32
    %16 = llvm.icmp "ult" %15, %arg0 : i32
    llvm.cond_br %16, ^bb3, ^bb13
  ^bb3:  // pred: ^bb2
    %17 = llvm.mul %12, %3  : i32
    llvm.br ^bb4(%0, %4 : i32, f32)
  ^bb4(%18: i32, %19: f32):  // 2 preds: ^bb3, ^bb9
    %20 = llvm.icmp "slt" %18, %3 : i32
    llvm.cond_br %20, ^bb5, ^bb10
  ^bb5:  // pred: ^bb4
    %21 = llvm.add %17, %18  : i32
    %22 = llvm.icmp "slt" %21, %arg2 : i32
    llvm.cond_br %22, ^bb6, ^bb7
  ^bb6:  // pred: ^bb5
    %23 = llvm.mul %21, %arg0  : i32
    %24 = llvm.add %23, %15  : i32
    %25 = llvm.getelementptr %arg1[%24] : (!llvm.ptr<f32>, i32) -> !llvm.ptr<f32>
    %26 = llvm.load %25 : !llvm.ptr<f32>
    %27 = llvm.call @__nv_fabsf(%26) : (f32) -> f32
    %28 = llvm.fcmp "oge" %19, %27 : f32
    %29 = llvm.select %28, %19, %27 : i1, f32
    llvm.br ^bb8(%29 : f32)
  ^bb7:  // pred: ^bb5
    llvm.br ^bb8(%19 : f32)
  ^bb8(%30: f32):  // 2 preds: ^bb6, ^bb7
    llvm.br ^bb9
  ^bb9:  // pred: ^bb8
    %31 = llvm.add %18, %1  : i32
    llvm.br ^bb4(%31, %30 : i32, f32)
  ^bb10:  // pred: ^bb4
    %32 = llvm.getelementptr %arg6[%15] : (!llvm.ptr<f32>, i32) -> !llvm.ptr<f32>
    %33 = llvm.load %32 : !llvm.ptr<f32>
    llvm.br ^bb11(%33 : f32)
  ^bb11(%34: f32):  // 2 preds: ^bb10, ^bb11
    %35 = llvm.fcmp "ogt" %34, %19 : f32
    %36 = llvm.select %35, %34, %19 : i1, f32
    %37 = llvm.bitcast %32 : !llvm.ptr<f32> to !llvm.ptr<i32>
    %38 = llvm.bitcast %34 : f32 to i32
    %39 = llvm.bitcast %36 : f32 to i32
    %40 = llvm.cmpxchg %37, %38, %39 acq_rel monotonic : !llvm.ptr<i32>, i32
    %41 = llvm.extractvalue %40[0] : !llvm.struct<(i32, i1)> 
    %42 = llvm.bitcast %41 : i32 to f32
    %43 = llvm.extractvalue %40[1] : !llvm.struct<(i32, i1)> 
    llvm.cond_br %43, ^bb12, ^bb11(%42 : f32)
  ^bb12:  // pred: ^bb11
    llvm.br ^bb13
  ^bb13:  // 2 preds: ^bb2, ^bb12
    llvm.br ^bb14
  ^bb14:  // 2 preds: ^bb1, ^bb13
    llvm.return
  }
}

// -----// IR Dump After SideEffectLoopInvariantCodeMotionPass (disc-side-effect-loop-invariant-code-motion) //----- //
gpu.func @main_kColReduction_reduce__4_1_0___thin(%arg0: index, %arg1: index, %arg2: memref<?xf32, "gpu">) kernel {
  %cst = arith.constant 0xFF800000 : f32
  %c1 = arith.constant 1 : index
  %c0 = arith.constant 0 : index
  %0 = gpu.block_id  x
  %1 = gpu.thread_id  x
  cf.br ^bb1
^bb1:  // pred: ^bb0
  %2 = affine.apply affine_map<(d0)[s0, s1] -> (d0 * s0 + s1)>(%0)[%arg0, %c0]
  %3 = affine.apply affine_map<(d0)[s0, s1] -> (d0 * s0 + s1)>(%1)[%c1, %c0]
  %4 = arith.addi %3, %2 : index
  %5 = arith.addi %3, %2 : index
  %6 = arith.cmpi ult, %5, %arg1 : index
  scf.if %6 {
    %reinterpret_cast = memref.reinterpret_cast %arg2 to offset: [%c0], sizes: [%arg1], strides: [%c1] : memref<?xf32, "gpu"> to memref<?xf32, "gpu">
    memref.store %cst, %reinterpret_cast[%4] : memref<?xf32, "gpu">
  }
  gpu.return
}

// -----// IR Dump After CSE (cse) //----- //
gpu.module @main_kernel_1 {
  gpu.func @main_kColReduction_reduce__4_1_0___thin(%arg0: index, %arg1: index, %arg2: memref<?xf32, "gpu">) kernel {
    %cst = arith.constant 0xFF800000 : f32
    %c1 = arith.constant 1 : index
    %c0 = arith.constant 0 : index
    %0 = gpu.block_id  x
    %1 = gpu.thread_id  x
    cf.br ^bb1
  ^bb1:  // pred: ^bb0
    %2 = affine.apply affine_map<(d0)[s0, s1] -> (d0 * s0 + s1)>(%0)[%arg0, %c0]
    %3 = affine.apply affine_map<(d0)[s0, s1] -> (d0 * s0 + s1)>(%1)[%c1, %c0]
    %4 = arith.addi %3, %2 : index
    %5 = arith.cmpi ult, %4, %arg1 : index
    scf.if %5 {
      %reinterpret_cast = memref.reinterpret_cast %arg2 to offset: [%c0], sizes: [%arg1], strides: [%c1] : memref<?xf32, "gpu"> to memref<?xf32, "gpu">
      memref.store %cst, %reinterpret_cast[%4] : memref<?xf32, "gpu">
    }
    gpu.return
  }
}

// -----// IR Dump After SCFToControlFlow (convert-scf-to-cf) //----- //
gpu.module @main_kernel_1 {
  gpu.func @main_kColReduction_reduce__4_1_0___thin(%arg0: index, %arg1: index, %arg2: memref<?xf32, "gpu">) kernel {
    %cst = arith.constant 0xFF800000 : f32
    %c1 = arith.constant 1 : index
    %c0 = arith.constant 0 : index
    %0 = gpu.block_id  x
    %1 = gpu.thread_id  x
    cf.br ^bb1
  ^bb1:  // pred: ^bb0
    %2 = affine.apply affine_map<(d0)[s0, s1] -> (d0 * s0 + s1)>(%0)[%arg0, %c0]
    %3 = affine.apply affine_map<(d0)[s0, s1] -> (d0 * s0 + s1)>(%1)[%c1, %c0]
    %4 = arith.addi %3, %2 : index
    %5 = arith.cmpi ult, %4, %arg1 : index
    cf.cond_br %5, ^bb2, ^bb3
  ^bb2:  // pred: ^bb1
    %reinterpret_cast = memref.reinterpret_cast %arg2 to offset: [%c0], sizes: [%arg1], strides: [%c1] : memref<?xf32, "gpu"> to memref<?xf32, "gpu">
    memref.store %cst, %reinterpret_cast[%4] : memref<?xf32, "gpu">
    cf.br ^bb3
  ^bb3:  // 2 preds: ^bb1, ^bb2
    gpu.return
  }
}

// -----// IR Dump After ConvertAffineToStandard (lower-affine) //----- //
gpu.module @main_kernel_1 {
  gpu.func @main_kColReduction_reduce__4_1_0___thin(%arg0: index, %arg1: index, %arg2: memref<?xf32, "gpu">) kernel {
    %cst = arith.constant 0xFF800000 : f32
    %c1 = arith.constant 1 : index
    %c0 = arith.constant 0 : index
    %0 = gpu.block_id  x
    %1 = gpu.thread_id  x
    cf.br ^bb1
  ^bb1:  // pred: ^bb0
    %2 = arith.muli %0, %arg0 : index
    %3 = arith.addi %2, %c0 : index
    %4 = arith.muli %1, %c1 : index
    %5 = arith.addi %4, %c0 : index
    %6 = arith.addi %5, %3 : index
    %7 = arith.cmpi ult, %6, %arg1 : index
    cf.cond_br %7, ^bb2, ^bb3
  ^bb2:  // pred: ^bb1
    %reinterpret_cast = memref.reinterpret_cast %arg2 to offset: [%c0], sizes: [%arg1], strides: [%c1] : memref<?xf32, "gpu"> to memref<?xf32, "gpu">
    memref.store %cst, %reinterpret_cast[%6] : memref<?xf32, "gpu">
    cf.br ^bb3
  ^bb3:  // 2 preds: ^bb1, ^bb2
    gpu.return
  }
}

// -----// IR Dump After StripDebugInfo (strip-debuginfo) //----- //
gpu.module @main_kernel_1 {
  gpu.func @main_kColReduction_reduce__4_1_0___thin(%arg0: index, %arg1: index, %arg2: memref<?xf32, "gpu">) kernel {
    %cst = arith.constant 0xFF800000 : f32
    %c1 = arith.constant 1 : index
    %c0 = arith.constant 0 : index
    %0 = gpu.block_id  x
    %1 = gpu.thread_id  x
    cf.br ^bb1
  ^bb1:  // pred: ^bb0
    %2 = arith.muli %0, %arg0 : index
    %3 = arith.addi %2, %c0 : index
    %4 = arith.muli %1, %c1 : index
    %5 = arith.addi %4, %c0 : index
    %6 = arith.addi %5, %3 : index
    %7 = arith.cmpi ult, %6, %arg1 : index
    cf.cond_br %7, ^bb2, ^bb3
  ^bb2:  // pred: ^bb1
    %reinterpret_cast = memref.reinterpret_cast %arg2 to offset: [%c0], sizes: [%arg1], strides: [%c1] : memref<?xf32, "gpu"> to memref<?xf32, "gpu">
    memref.store %cst, %reinterpret_cast[%6] : memref<?xf32, "gpu">
    cf.br ^bb3
  ^bb3:  // 2 preds: ^bb1, ^bb2
    gpu.return
  }
}

// -----// IR Dump After DiscLowerGpuOpsToNVVMOpsPass (disc-convert-gpu-to-nvvm) //----- //
gpu.module @main_kernel_1 {
  llvm.func @main_kColReduction_reduce__4_1_0___thin(%arg0: i32, %arg1: i32, %arg2: !llvm.ptr<f32>, %arg3: !llvm.ptr<f32>, %arg4: i32, %arg5: i32, %arg6: i32) attributes {gpu.kernel, nvvm.kernel} {
    %0 = llvm.mlir.undef : !llvm.struct<(ptr<f32>, ptr<f32>, i32, array<1 x i32>, array<1 x i32>)>
    %1 = llvm.insertvalue %arg2, %0[0] : !llvm.struct<(ptr<f32>, ptr<f32>, i32, array<1 x i32>, array<1 x i32>)> 
    %2 = llvm.insertvalue %arg3, %1[1] : !llvm.struct<(ptr<f32>, ptr<f32>, i32, array<1 x i32>, array<1 x i32>)> 
    %3 = llvm.insertvalue %arg4, %2[2] : !llvm.struct<(ptr<f32>, ptr<f32>, i32, array<1 x i32>, array<1 x i32>)> 
    %4 = llvm.insertvalue %arg5, %3[3, 0] : !llvm.struct<(ptr<f32>, ptr<f32>, i32, array<1 x i32>, array<1 x i32>)> 
    %5 = llvm.insertvalue %arg6, %4[4, 0] : !llvm.struct<(ptr<f32>, ptr<f32>, i32, array<1 x i32>, array<1 x i32>)> 
    %6 = llvm.mlir.constant(0xFF800000 : f32) : f32
    %7 = llvm.mlir.constant(1 : index) : i32
    %8 = llvm.mlir.constant(0 : index) : i32
    %9 = nvvm.read.ptx.sreg.ctaid.x : i32
    %10 = nvvm.read.ptx.sreg.tid.x : i32
    llvm.br ^bb1
  ^bb1:  // pred: ^bb0
    %11 = llvm.mul %9, %arg0  : i32
    %12 = llvm.add %10, %11  : i32
    %13 = llvm.icmp "ult" %12, %arg1 : i32
    llvm.cond_br %13, ^bb2, ^bb3
  ^bb2:  // pred: ^bb1
    %14 = llvm.mlir.undef : !llvm.struct<(ptr<f32>, ptr<f32>, i32, array<1 x i32>, array<1 x i32>)>
    %15 = llvm.extractvalue %5[0] : !llvm.struct<(ptr<f32>, ptr<f32>, i32, array<1 x i32>, array<1 x i32>)> 
    %16 = llvm.extractvalue %5[1] : !llvm.struct<(ptr<f32>, ptr<f32>, i32, array<1 x i32>, array<1 x i32>)> 
    %17 = llvm.insertvalue %15, %14[0] : !llvm.struct<(ptr<f32>, ptr<f32>, i32, array<1 x i32>, array<1 x i32>)> 
    %18 = llvm.insertvalue %16, %17[1] : !llvm.struct<(ptr<f32>, ptr<f32>, i32, array<1 x i32>, array<1 x i32>)> 
    %19 = llvm.insertvalue %8, %18[2] : !llvm.struct<(ptr<f32>, ptr<f32>, i32, array<1 x i32>, array<1 x i32>)> 
    %20 = llvm.insertvalue %arg1, %19[3, 0] : !llvm.struct<(ptr<f32>, ptr<f32>, i32, array<1 x i32>, array<1 x i32>)> 
    %21 = llvm.insertvalue %7, %20[4, 0] : !llvm.struct<(ptr<f32>, ptr<f32>, i32, array<1 x i32>, array<1 x i32>)> 
    %22 = llvm.extractvalue %21[1] : !llvm.struct<(ptr<f32>, ptr<f32>, i32, array<1 x i32>, array<1 x i32>)> 
    %23 = llvm.getelementptr %22[%12] : (!llvm.ptr<f32>, i32) -> !llvm.ptr<f32>
    llvm.store %6, %23 : !llvm.ptr<f32>
    llvm.br ^bb3
  ^bb3:  // 2 preds: ^bb1, ^bb2
    llvm.return
  }
}

// -----// IR Dump After LLVMInsertValueSimplifierPass (disc-llvm-insert-value-simplifier) //----- //
llvm.func @main_kColReduction_reduce__4_1_0___thin(%arg0: i32, %arg1: i32, %arg2: !llvm.ptr<f32>, %arg3: !llvm.ptr<f32>, %arg4: i32, %arg5: i32, %arg6: i32) attributes {gpu.kernel, nvvm.kernel} {
  %0 = llvm.mlir.constant(0xFF800000 : f32) : f32
  %1 = nvvm.read.ptx.sreg.ctaid.x : i32
  %2 = nvvm.read.ptx.sreg.tid.x : i32
  llvm.br ^bb1
^bb1:  // pred: ^bb0
  %3 = llvm.mul %1, %arg0  : i32
  %4 = llvm.add %2, %3  : i32
  %5 = llvm.icmp "ult" %4, %arg1 : i32
  llvm.cond_br %5, ^bb2, ^bb3
^bb2:  // pred: ^bb1
  %6 = llvm.getelementptr %arg3[%4] : (!llvm.ptr<f32>, i32) -> !llvm.ptr<f32>
  llvm.store %0, %6 : !llvm.ptr<f32>
  llvm.br ^bb3
^bb3:  // 2 preds: ^bb1, ^bb2
  llvm.return
}

// -----// IR Dump After FunctionDeadArgumentEliminationPass (disc-function-dead-argument-elimination) //----- //
gpu.module @main_kernel_1 {
  llvm.func @main_kColReduction_reduce__4_1_0___thin(%arg0: i32, %arg1: i32, %arg2: !llvm.ptr<f32>) attributes {disc.elimargs = [2 : index, 4 : index, 5 : index, 6 : index], gpu.kernel, nvvm.kernel} {
    %0 = llvm.mlir.constant(0xFF800000 : f32) : f32
    %1 = nvvm.read.ptx.sreg.ctaid.x : i32
    %2 = nvvm.read.ptx.sreg.tid.x : i32
    llvm.br ^bb1
  ^bb1:  // pred: ^bb0
    %3 = llvm.mul %1, %arg0  : i32
    %4 = llvm.add %2, %3  : i32
    %5 = llvm.icmp "ult" %4, %arg1 : i32
    llvm.cond_br %5, ^bb2, ^bb3
  ^bb2:  // pred: ^bb1
    %6 = llvm.getelementptr %arg2[%4] : (!llvm.ptr<f32>, i32) -> !llvm.ptr<f32>
    llvm.store %0, %6 : !llvm.ptr<f32>
    llvm.br ^bb3
  ^bb3:  // 2 preds: ^bb1, ^bb2
    llvm.return
  }
}

// -----// IR Dump After GpuKernelToBlobPass (disc-gpu-kernel-to-blob) //----- //
gpu.module @main_kernel_1 attributes {gpu.binary = "P\EDU\BA\01\00\10\008\04\00\00\00\00\00\00\02\00\01\01@\00\00\00\F8\03\00\00\00\00\00\00\F7\03\00\00\00\00\00\00\07\00\01\00P\00\00\00\00\00\00\00\00\00\00\00\11 \00\00\00\00\00\00\00\00\00\00\00\00\00\00\E8\0B\00\00\00\00\00\00\A2\7FELF\02\01\013\07\00\01\00f\02\00\BE\00q\00\01\00!@\0B\07\001\00\80\08\07\00\F5\0E\00P\05P\00@\008\00\03\00@\00\0B\00\01\00\00.shstrtab\00.\08\00'ym\08\00\F0\1B_shndx\00.nv.info\00.text.main_kColReduction_r\0A\00\F5\00e__4_1_0___thin7\00\0F1\00\1Aoshared3\00\1A\9Fconstant06\00\17\FA\01debug_frame\00.rel\11\00!nv\14\00\11aB\00\0F'\01 \0F\87\00\14\0FO\01\B6o_paramV\01\1C\0F\01\00\07\8CZ\00\00\00\03\00\0A\00\01\00\11\EC\18\00,\09\00\01\00 )\01\18\00,\04\00\01\00\11G\18\00,\07\00\01\00f2\00\00\00\12\10`\00!\80\01\07\00v\00\FF\FF\FF\FF(\00\0C\00\00\01\00@\03\00\04|\08\00\D1\0F\0C\81\80\80(\00\08\FF\81\80(\08\0B\00\06(\00/0\00\01\00\03\13pX\00\10\04\9B\00R\04\14\00\00\00E\00\01\0B\00\00\13\00p/\08\00\05\00\00\00\9F\03\22\04#\0C\00\00\01\00'\04\12\0C\00\17\11\0C\00!7\04\E0\03\F3\08\015\00\00\04\0A\08\00\02\00\00\00`\01\10\00\03\19\10\00\04\17\0C\04\04U\08\00\00\F0!\10\00\10\01\18\01%\F0\11\10\00\01\01\00\F2\02\F0\11\00\03\1B\FF\00\04\1C\08\00P\00\00\00\B0\00\01\00#K\00\01\00s\02\02\08\10\0A/\22\9B\00\00\07\00\03\08\00\13\10\08\00\13\18\08\00\13 \08\00\13(\08\00\130\08\00\228\08|\00\13\00\08\00\13\08\08\00\13\10\08\00\13\18\08\00\13 \08\00\13(\08\00\130\08\00\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\A2\00\00\00\14,\00\00\00H\00\01\00\000\01/\05\00\01\00\FF\B8A\02z\01\00'\04\B1\0F\00\00\00\C4\0F\00\19y\02\00\01\00\10%\93\02Q\0E\00\19y\03\0F\00\F5\1A\00!\00\00\00$\0E\00$z\02\02\00X\00\00\03\02\8E\07\00\CA\1F\00\0Cz\00\02\00Y\00\00p`\F0\03\00\DA\0F\00M[\04\A0\80\03\00\EA\0F\005t\03\FF\BB\03\10\FF\C8\03P\E2\0F\00\02x>\02B\80\FF\00\0F\10\00r\B9z\04\00\00F\00\84\00\94\D0\0F\00%v\02\02\00Z`\00`\0F\00\86y\00\022\00@\04\19\10\0C0\009My\00`\00PGy\00\00\F0A\04\A6\FF\83\03\00\C0\0F\00\18y\00\01\00\0F\10\00\90\0F\01\00-\00W\01.\03\00\01\00\22@\00\01\00='\01\000\00\08\01\00\1F\0B@\00\04\13g)\00\1FV@\00\0C\13\13\14\04\0C\01\00\13\C0\15\00&\90\008\04#\04\00\8D\04\00\FE\04\12\00\01\00\1F\FAT\00\00\00\01\00\13P\95\00/p\00\80\00\0B\1F)'\00\03#\00\C0@\00\04X\06\04\E4\00*\04\00\01\00\1F`@\00\04\13\F01\00&\\\00@\00\1F\0A@\00\00!\18\01D\01\0D@\00\13P)\00*\D8\00\01\00\1B\08\08\00?\07\01\00\8E\07\00Q\00\00(\05\00\01\00&\10\00\80\00\17\048\00\04\18\00\13\C4\14\01\0C\84\01*8\05@\07\1F\00\C0\00\04\132@\00+\06\00\01\00\1A\07\D8\07\12\03\F8\05:\08\80\00\01\00\13\06\10\06\04(\0B\0C\01\00*\A8\00\08\00\04\F8\00\13\018\00\04\A8\00\0C\01\009H\03\00\08\00\088\00/\06\00\01\00\17\80\08\00\00\00\00\00\00\00\00"} {
  llvm.func @main_kColReduction_reduce__4_1_0___thin(%arg0: i32, %arg1: i32, %arg2: !llvm.ptr<f32>) attributes {disc.elimargs = [2 : index, 4 : index, 5 : index, 6 : index], gpu.kernel, nvvm.kernel} {
    %0 = llvm.mlir.constant(0xFF800000 : f32) : f32
    %1 = nvvm.read.ptx.sreg.ctaid.x : i32
    %2 = nvvm.read.ptx.sreg.tid.x : i32
    llvm.br ^bb1
  ^bb1:  // pred: ^bb0
    %3 = llvm.mul %1, %arg0  : i32
    %4 = llvm.add %2, %3  : i32
    %5 = llvm.icmp "ult" %4, %arg1 : i32
    llvm.cond_br %5, ^bb2, ^bb3
  ^bb2:  // pred: ^bb1
    %6 = llvm.getelementptr %arg2[%4] : (!llvm.ptr<f32>, i32) -> !llvm.ptr<f32>
    llvm.store %0, %6 : !llvm.ptr<f32>
    llvm.br ^bb3
  ^bb3:  // 2 preds: ^bb1, ^bb2
    llvm.return
  }
}

// -----// IR Dump After SideEffectLoopInvariantCodeMotionPass (disc-side-effect-loop-invariant-code-motion) //----- //
gpu.func @main_kColReduction_reduce__4_1_0___thin_1(%arg0: index, %arg1: memref<?x?x?xf32, "gpu">, %arg2: index, %arg3: index, %arg4: index, %arg5: memref<?xf32, "gpu">) workgroup(%arg6 : memref<256xf32, #gpu.address_space<workgroup>>) kernel {
  %c4 = arith.constant 4 : index
  %cst = arith.constant 0xFF800000 : f32
  %c2 = arith.constant 2 : index
  %c64 = arith.constant 64 : index
  %c8 = arith.constant 8 : index
  %c32 = arith.constant 32 : index
  %c256 = arith.constant 256 : index
  %c1 = arith.constant 1 : index
  %c0 = arith.constant 0 : index
  %0 = gpu.block_id  x
  %1 = gpu.thread_id  x
  cf.br ^bb1
^bb1:  // pred: ^bb0
  %dim = memref.dim %arg1, %c0 : memref<?x?x?xf32, "gpu">
  %dim_0 = memref.dim %arg1, %c1 : memref<?x?x?xf32, "gpu">
  %dim_1 = memref.dim %arg1, %c2 : memref<?x?x?xf32, "gpu">
  %2 = affine.apply affine_map<(d0)[s0, s1] -> (d0 * s0 + s1)>(%0)[%arg2, %c0]
  %3 = affine.apply affine_map<(d0)[s0, s1] -> (d0 * s0 + s1)>(%1)[%c1, %c0]
  %4 = arith.addi %3, %2 : index
  %5 = arith.addi %3, %2 : index
  %6 = arith.cmpi ult, %5, %arg3 : index
  scf.if %6 {
    %7 = arith.remsi %4, %c256 : index
    %8 = arith.divsi %4, %c256 : index
    %9 = arith.divui %8, %arg4 : index
    %10 = arith.remui %8, %arg4 : index
    %11 = arith.divui %7, %c32 : index
    %12 = arith.remui %7, %c32 : index
    %13 = arith.muli %12, %c8 : index
    %14 = arith.addi %11, %13 : index
    %15 = arith.muli %10, %c32 : index
    %16 = arith.addi %12, %15 : index
    %17 = arith.cmpi ult, %16, %arg0 : index
    %18 = scf.if %17 -> (f32) {
      %23 = arith.muli %9, %c8 : index
      %24 = arith.addi %11, %23 : index
      %25 = arith.muli %24, %c64 : index
      %26 = scf.for %arg7 = %c0 to %c64 step %c1 iter_args(%arg8 = %cst) -> (f32) {
        %27 = arith.addi %arg7, %25 : index
        %28 = arith.cmpi slt, %27, %dim : index
        %29 = scf.if %28 -> (f32) {
          %30 = arith.muli %27, %arg0 : index
          %31 = arith.addi %30, %16 : index
          %32 = arith.muli %dim, %dim_0 : index
          %33 = arith.muli %32, %dim_1 : index
          %reinterpret_cast_2 = memref.reinterpret_cast %arg1 to offset: [%c0], sizes: [%33], strides: [%c1] : memref<?x?x?xf32, "gpu"> to memref<?xf32, "gpu">
          %34 = memref.load %reinterpret_cast_2[%31] : memref<?xf32, "gpu">
          %35 = math.absf %34 : f32
          %36 = arith.cmpf ugt, %arg8, %35 : f32
          %37 = arith.select %36, %arg8, %35 : f32
          %38 = arith.cmpf uno, %35, %35 : f32
          %39 = arith.select %38, %35, %37 : f32
          scf.yield %39 : f32
        } else {
          scf.yield %arg8 : f32
        }
        scf.yield %29 : f32
      }
      scf.yield %26 : f32
    } else {
      scf.yield %cst : f32
    }
    %reinterpret_cast = memref.reinterpret_cast %arg6 to offset: [0], sizes: [256], strides: [1] : memref<256xf32, #gpu.address_space<workgroup>> to memref<256xf32, #gpu.address_space<workgroup>>
    memref.store %18, %reinterpret_cast[%14] : memref<256xf32, #gpu.address_space<workgroup>>
    gpu.barrier
    %19 = arith.cmpi slt, %11, %c4 : index
    scf.if %19 {
      %23 = arith.addi %14, %c4 : index
      %24 = memref.load %reinterpret_cast[%14] : memref<256xf32, #gpu.address_space<workgroup>>
      %25 = memref.load %reinterpret_cast[%23] : memref<256xf32, #gpu.address_space<workgroup>>
      %26 = arith.cmpf ugt, %24, %25 : f32
      %27 = arith.select %26, %24, %25 : f32
      %28 = arith.cmpf uno, %25, %25 : f32
      %29 = arith.select %28, %25, %27 : f32
      memref.store %29, %reinterpret_cast[%14] : memref<256xf32, #gpu.address_space<workgroup>>
    }
    gpu.barrier
    %20 = arith.cmpi slt, %11, %c2 : index
    scf.if %20 {
      %23 = arith.addi %14, %c2 : index
      %24 = memref.load %reinterpret_cast[%14] : memref<256xf32, #gpu.address_space<workgroup>>
      %25 = memref.load %reinterpret_cast[%23] : memref<256xf32, #gpu.address_space<workgroup>>
      %26 = arith.cmpf ugt, %24, %25 : f32
      %27 = arith.select %26, %24, %25 : f32
      %28 = arith.cmpf uno, %25, %25 : f32
      %29 = arith.select %28, %25, %27 : f32
      memref.store %29, %reinterpret_cast[%14] : memref<256xf32, #gpu.address_space<workgroup>>
    }
    gpu.barrier
    %21 = arith.cmpi eq, %11, %c0 : index
    %22 = arith.andi %21, %17 : i1
    scf.if %22 {
      %23 = arith.addi %14, %c1 : index
      %24 = memref.load %reinterpret_cast[%14] : memref<256xf32, #gpu.address_space<workgroup>>
      %25 = memref.load %reinterpret_cast[%23] : memref<256xf32, #gpu.address_space<workgroup>>
      %26 = arith.cmpf ugt, %24, %25 : f32
      %27 = arith.select %26, %24, %25 : f32
      %28 = arith.cmpf uno, %25, %25 : f32
      %29 = arith.select %28, %25, %27 : f32
      %30 = memref.generic_atomic_rmw %arg5[%16] : memref<?xf32, "gpu"> {
      ^bb0(%arg7: f32):
        %31 = arith.cmpf ogt, %arg7, %29 : f32
        %32 = arith.select %31, %arg7, %29 : f32
        memref.atomic_yield %32 : f32
      }
    }
  }
  gpu.return
}

// -----// IR Dump After CSE (cse) //----- //
gpu.module @main_kernel_2 {
  gpu.func @main_kColReduction_reduce__4_1_0___thin_1(%arg0: index, %arg1: memref<?x?x?xf32, "gpu">, %arg2: index, %arg3: index, %arg4: index, %arg5: memref<?xf32, "gpu">) workgroup(%arg6 : memref<256xf32, #gpu.address_space<workgroup>>) kernel {
    %c4 = arith.constant 4 : index
    %cst = arith.constant 0xFF800000 : f32
    %c2 = arith.constant 2 : index
    %c64 = arith.constant 64 : index
    %c8 = arith.constant 8 : index
    %c32 = arith.constant 32 : index
    %c256 = arith.constant 256 : index
    %c1 = arith.constant 1 : index
    %c0 = arith.constant 0 : index
    %0 = gpu.block_id  x
    %1 = gpu.thread_id  x
    cf.br ^bb1
  ^bb1:  // pred: ^bb0
    %dim = memref.dim %arg1, %c0 : memref<?x?x?xf32, "gpu">
    %dim_0 = memref.dim %arg1, %c1 : memref<?x?x?xf32, "gpu">
    %dim_1 = memref.dim %arg1, %c2 : memref<?x?x?xf32, "gpu">
    %2 = affine.apply affine_map<(d0)[s0, s1] -> (d0 * s0 + s1)>(%0)[%arg2, %c0]
    %3 = affine.apply affine_map<(d0)[s0, s1] -> (d0 * s0 + s1)>(%1)[%c1, %c0]
    %4 = arith.addi %3, %2 : index
    %5 = arith.cmpi ult, %4, %arg3 : index
    scf.if %5 {
      %6 = arith.remsi %4, %c256 : index
      %7 = arith.divsi %4, %c256 : index
      %8 = arith.divui %7, %arg4 : index
      %9 = arith.remui %7, %arg4 : index
      %10 = arith.divui %6, %c32 : index
      %11 = arith.remui %6, %c32 : index
      %12 = arith.muli %11, %c8 : index
      %13 = arith.addi %10, %12 : index
      %14 = arith.muli %9, %c32 : index
      %15 = arith.addi %11, %14 : index
      %16 = arith.cmpi ult, %15, %arg0 : index
      %17 = scf.if %16 -> (f32) {
        %22 = arith.muli %8, %c8 : index
        %23 = arith.addi %10, %22 : index
        %24 = arith.muli %23, %c64 : index
        %25 = scf.for %arg7 = %c0 to %c64 step %c1 iter_args(%arg8 = %cst) -> (f32) {
          %26 = arith.addi %arg7, %24 : index
          %27 = arith.cmpi slt, %26, %dim : index
          %28 = scf.if %27 -> (f32) {
            %29 = arith.muli %26, %arg0 : index
            %30 = arith.addi %29, %15 : index
            %31 = arith.muli %dim, %dim_0 : index
            %32 = arith.muli %31, %dim_1 : index
            %reinterpret_cast_2 = memref.reinterpret_cast %arg1 to offset: [%c0], sizes: [%32], strides: [%c1] : memref<?x?x?xf32, "gpu"> to memref<?xf32, "gpu">
            %33 = memref.load %reinterpret_cast_2[%30] : memref<?xf32, "gpu">
            %34 = math.absf %33 : f32
            %35 = arith.cmpf ugt, %arg8, %34 : f32
            %36 = arith.select %35, %arg8, %34 : f32
            %37 = arith.cmpf uno, %34, %34 : f32
            %38 = arith.select %37, %34, %36 : f32
            scf.yield %38 : f32
          } else {
            scf.yield %arg8 : f32
          }
          scf.yield %28 : f32
        }
        scf.yield %25 : f32
      } else {
        scf.yield %cst : f32
      }
      %reinterpret_cast = memref.reinterpret_cast %arg6 to offset: [0], sizes: [256], strides: [1] : memref<256xf32, #gpu.address_space<workgroup>> to memref<256xf32, #gpu.address_space<workgroup>>
      memref.store %17, %reinterpret_cast[%13] : memref<256xf32, #gpu.address_space<workgroup>>
      gpu.barrier
      %18 = arith.cmpi slt, %10, %c4 : index
      scf.if %18 {
        %22 = arith.addi %13, %c4 : index
        %23 = memref.load %reinterpret_cast[%13] : memref<256xf32, #gpu.address_space<workgroup>>
        %24 = memref.load %reinterpret_cast[%22] : memref<256xf32, #gpu.address_space<workgroup>>
        %25 = arith.cmpf ugt, %23, %24 : f32
        %26 = arith.select %25, %23, %24 : f32
        %27 = arith.cmpf uno, %24, %24 : f32
        %28 = arith.select %27, %24, %26 : f32
        memref.store %28, %reinterpret_cast[%13] : memref<256xf32, #gpu.address_space<workgroup>>
      }
      gpu.barrier
      %19 = arith.cmpi slt, %10, %c2 : index
      scf.if %19 {
        %22 = arith.addi %13, %c2 : index
        %23 = memref.load %reinterpret_cast[%13] : memref<256xf32, #gpu.address_space<workgroup>>
        %24 = memref.load %reinterpret_cast[%22] : memref<256xf32, #gpu.address_space<workgroup>>
        %25 = arith.cmpf ugt, %23, %24 : f32
        %26 = arith.select %25, %23, %24 : f32
        %27 = arith.cmpf uno, %24, %24 : f32
        %28 = arith.select %27, %24, %26 : f32
        memref.store %28, %reinterpret_cast[%13] : memref<256xf32, #gpu.address_space<workgroup>>
      }
      gpu.barrier
      %20 = arith.cmpi eq, %10, %c0 : index
      %21 = arith.andi %20, %16 : i1
      scf.if %21 {
        %22 = arith.addi %13, %c1 : index
        %23 = memref.load %reinterpret_cast[%13] : memref<256xf32, #gpu.address_space<workgroup>>
        %24 = memref.load %reinterpret_cast[%22] : memref<256xf32, #gpu.address_space<workgroup>>
        %25 = arith.cmpf ugt, %23, %24 : f32
        %26 = arith.select %25, %23, %24 : f32
        %27 = arith.cmpf uno, %24, %24 : f32
        %28 = arith.select %27, %24, %26 : f32
        %29 = memref.generic_atomic_rmw %arg5[%15] : memref<?xf32, "gpu"> {
        ^bb0(%arg7: f32):
          %30 = arith.cmpf ogt, %arg7, %28 : f32
          %31 = arith.select %30, %arg7, %28 : f32
          memref.atomic_yield %31 : f32
        }
      }
    }
    gpu.return
  }
}

// -----// IR Dump After SCFToControlFlow (convert-scf-to-cf) //----- //
gpu.module @main_kernel_2 {
  gpu.func @main_kColReduction_reduce__4_1_0___thin_1(%arg0: index, %arg1: memref<?x?x?xf32, "gpu">, %arg2: index, %arg3: index, %arg4: index, %arg5: memref<?xf32, "gpu">) workgroup(%arg6 : memref<256xf32, #gpu.address_space<workgroup>>) kernel {
    %c4 = arith.constant 4 : index
    %cst = arith.constant 0xFF800000 : f32
    %c2 = arith.constant 2 : index
    %c64 = arith.constant 64 : index
    %c8 = arith.constant 8 : index
    %c32 = arith.constant 32 : index
    %c256 = arith.constant 256 : index
    %c1 = arith.constant 1 : index
    %c0 = arith.constant 0 : index
    %0 = gpu.block_id  x
    %1 = gpu.thread_id  x
    cf.br ^bb1
  ^bb1:  // pred: ^bb0
    %dim = memref.dim %arg1, %c0 : memref<?x?x?xf32, "gpu">
    %dim_0 = memref.dim %arg1, %c1 : memref<?x?x?xf32, "gpu">
    %dim_1 = memref.dim %arg1, %c2 : memref<?x?x?xf32, "gpu">
    %2 = affine.apply affine_map<(d0)[s0, s1] -> (d0 * s0 + s1)>(%0)[%arg2, %c0]
    %3 = affine.apply affine_map<(d0)[s0, s1] -> (d0 * s0 + s1)>(%1)[%c1, %c0]
    %4 = arith.addi %3, %2 : index
    %5 = arith.cmpi ult, %4, %arg3 : index
    cf.cond_br %5, ^bb2, ^bb20
  ^bb2:  // pred: ^bb1
    %6 = arith.remsi %4, %c256 : index
    %7 = arith.divsi %4, %c256 : index
    %8 = arith.divui %7, %arg4 : index
    %9 = arith.remui %7, %arg4 : index
    %10 = arith.divui %6, %c32 : index
    %11 = arith.remui %6, %c32 : index
    %12 = arith.muli %11, %c8 : index
    %13 = arith.addi %10, %12 : index
    %14 = arith.muli %9, %c32 : index
    %15 = arith.addi %11, %14 : index
    %16 = arith.cmpi ult, %15, %arg0 : index
    cf.cond_br %16, ^bb3, ^bb11
  ^bb3:  // pred: ^bb2
    %17 = arith.muli %8, %c8 : index
    %18 = arith.addi %10, %17 : index
    %19 = arith.muli %18, %c64 : index
    cf.br ^bb4(%c0, %cst : index, f32)
  ^bb4(%20: index, %21: f32):  // 2 preds: ^bb3, ^bb9
    %22 = arith.cmpi slt, %20, %c64 : index
    cf.cond_br %22, ^bb5, ^bb10
  ^bb5:  // pred: ^bb4
    %23 = arith.addi %20, %19 : index
    %24 = arith.cmpi slt, %23, %dim : index
    cf.cond_br %24, ^bb6, ^bb7
  ^bb6:  // pred: ^bb5
    %25 = arith.muli %23, %arg0 : index
    %26 = arith.addi %25, %15 : index
    %27 = arith.muli %dim, %dim_0 : index
    %28 = arith.muli %27, %dim_1 : index
    %reinterpret_cast = memref.reinterpret_cast %arg1 to offset: [%c0], sizes: [%28], strides: [%c1] : memref<?x?x?xf32, "gpu"> to memref<?xf32, "gpu">
    %29 = memref.load %reinterpret_cast[%26] : memref<?xf32, "gpu">
    %30 = math.absf %29 : f32
    %31 = arith.cmpf ugt, %21, %30 : f32
    %32 = arith.select %31, %21, %30 : f32
    %33 = arith.cmpf uno, %30, %30 : f32
    %34 = arith.select %33, %30, %32 : f32
    cf.br ^bb8(%34 : f32)
  ^bb7:  // pred: ^bb5
    cf.br ^bb8(%21 : f32)
  ^bb8(%35: f32):  // 2 preds: ^bb6, ^bb7
    cf.br ^bb9
  ^bb9:  // pred: ^bb8
    %36 = arith.addi %20, %c1 : index
    cf.br ^bb4(%36, %35 : index, f32)
  ^bb10:  // pred: ^bb4
    cf.br ^bb12(%21 : f32)
  ^bb11:  // pred: ^bb2
    cf.br ^bb12(%cst : f32)
  ^bb12(%37: f32):  // 2 preds: ^bb10, ^bb11
    cf.br ^bb13
  ^bb13:  // pred: ^bb12
    %reinterpret_cast_2 = memref.reinterpret_cast %arg6 to offset: [0], sizes: [256], strides: [1] : memref<256xf32, #gpu.address_space<workgroup>> to memref<256xf32, #gpu.address_space<workgroup>>
    memref.store %37, %reinterpret_cast_2[%13] : memref<256xf32, #gpu.address_space<workgroup>>
    gpu.barrier
    %38 = arith.cmpi slt, %10, %c4 : index
    cf.cond_br %38, ^bb14, ^bb15
  ^bb14:  // pred: ^bb13
    %39 = arith.addi %13, %c4 : index
    %40 = memref.load %reinterpret_cast_2[%13] : memref<256xf32, #gpu.address_space<workgroup>>
    %41 = memref.load %reinterpret_cast_2[%39] : memref<256xf32, #gpu.address_space<workgroup>>
    %42 = arith.cmpf ugt, %40, %41 : f32
    %43 = arith.select %42, %40, %41 : f32
    %44 = arith.cmpf uno, %41, %41 : f32
    %45 = arith.select %44, %41, %43 : f32
    memref.store %45, %reinterpret_cast_2[%13] : memref<256xf32, #gpu.address_space<workgroup>>
    cf.br ^bb15
  ^bb15:  // 2 preds: ^bb13, ^bb14
    gpu.barrier
    %46 = arith.cmpi slt, %10, %c2 : index
    cf.cond_br %46, ^bb16, ^bb17
  ^bb16:  // pred: ^bb15
    %47 = arith.addi %13, %c2 : index
    %48 = memref.load %reinterpret_cast_2[%13] : memref<256xf32, #gpu.address_space<workgroup>>
    %49 = memref.load %reinterpret_cast_2[%47] : memref<256xf32, #gpu.address_space<workgroup>>
    %50 = arith.cmpf ugt, %48, %49 : f32
    %51 = arith.select %50, %48, %49 : f32
    %52 = arith.cmpf uno, %49, %49 : f32
    %53 = arith.select %52, %49, %51 : f32
    memref.store %53, %reinterpret_cast_2[%13] : memref<256xf32, #gpu.address_space<workgroup>>
    cf.br ^bb17
  ^bb17:  // 2 preds: ^bb15, ^bb16
    gpu.barrier
    %54 = arith.cmpi eq, %10, %c0 : index
    %55 = arith.andi %54, %16 : i1
    cf.cond_br %55, ^bb18, ^bb19
  ^bb18:  // pred: ^bb17
    %56 = arith.addi %13, %c1 : index
    %57 = memref.load %reinterpret_cast_2[%13] : memref<256xf32, #gpu.address_space<workgroup>>
    %58 = memref.load %reinterpret_cast_2[%56] : memref<256xf32, #gpu.address_space<workgroup>>
    %59 = arith.cmpf ugt, %57, %58 : f32
    %60 = arith.select %59, %57, %58 : f32
    %61 = arith.cmpf uno, %58, %58 : f32
    %62 = arith.select %61, %58, %60 : f32
    %63 = memref.generic_atomic_rmw %arg5[%15] : memref<?xf32, "gpu"> {
    ^bb0(%arg7: f32):
      %64 = arith.cmpf ogt, %arg7, %62 : f32
      %65 = arith.select %64, %arg7, %62 : f32
      memref.atomic_yield %65 : f32
    }
    cf.br ^bb19
  ^bb19:  // 2 preds: ^bb17, ^bb18
    cf.br ^bb20
  ^bb20:  // 2 preds: ^bb1, ^bb19
    gpu.return
  }
}

// -----// IR Dump After ConvertAffineToStandard (lower-affine) //----- //
gpu.module @main_kernel_2 {
  gpu.func @main_kColReduction_reduce__4_1_0___thin_1(%arg0: index, %arg1: memref<?x?x?xf32, "gpu">, %arg2: index, %arg3: index, %arg4: index, %arg5: memref<?xf32, "gpu">) workgroup(%arg6 : memref<256xf32, #gpu.address_space<workgroup>>) kernel {
    %c4 = arith.constant 4 : index
    %cst = arith.constant 0xFF800000 : f32
    %c2 = arith.constant 2 : index
    %c64 = arith.constant 64 : index
    %c8 = arith.constant 8 : index
    %c32 = arith.constant 32 : index
    %c256 = arith.constant 256 : index
    %c1 = arith.constant 1 : index
    %c0 = arith.constant 0 : index
    %0 = gpu.block_id  x
    %1 = gpu.thread_id  x
    cf.br ^bb1
  ^bb1:  // pred: ^bb0
    %dim = memref.dim %arg1, %c0 : memref<?x?x?xf32, "gpu">
    %dim_0 = memref.dim %arg1, %c1 : memref<?x?x?xf32, "gpu">
    %dim_1 = memref.dim %arg1, %c2 : memref<?x?x?xf32, "gpu">
    %2 = arith.muli %0, %arg2 : index
    %3 = arith.addi %2, %c0 : index
    %4 = arith.muli %1, %c1 : index
    %5 = arith.addi %4, %c0 : index
    %6 = arith.addi %5, %3 : index
    %7 = arith.cmpi ult, %6, %arg3 : index
    cf.cond_br %7, ^bb2, ^bb20
  ^bb2:  // pred: ^bb1
    %8 = arith.remsi %6, %c256 : index
    %9 = arith.divsi %6, %c256 : index
    %10 = arith.divui %9, %arg4 : index
    %11 = arith.remui %9, %arg4 : index
    %12 = arith.divui %8, %c32 : index
    %13 = arith.remui %8, %c32 : index
    %14 = arith.muli %13, %c8 : index
    %15 = arith.addi %12, %14 : index
    %16 = arith.muli %11, %c32 : index
    %17 = arith.addi %13, %16 : index
    %18 = arith.cmpi ult, %17, %arg0 : index
    cf.cond_br %18, ^bb3, ^bb11
  ^bb3:  // pred: ^bb2
    %19 = arith.muli %10, %c8 : index
    %20 = arith.addi %12, %19 : index
    %21 = arith.muli %20, %c64 : index
    cf.br ^bb4(%c0, %cst : index, f32)
  ^bb4(%22: index, %23: f32):  // 2 preds: ^bb3, ^bb9
    %24 = arith.cmpi slt, %22, %c64 : index
    cf.cond_br %24, ^bb5, ^bb10
  ^bb5:  // pred: ^bb4
    %25 = arith.addi %22, %21 : index
    %26 = arith.cmpi slt, %25, %dim : index
    cf.cond_br %26, ^bb6, ^bb7
  ^bb6:  // pred: ^bb5
    %27 = arith.muli %25, %arg0 : index
    %28 = arith.addi %27, %17 : index
    %29 = arith.muli %dim, %dim_0 : index
    %30 = arith.muli %29, %dim_1 : index
    %reinterpret_cast = memref.reinterpret_cast %arg1 to offset: [%c0], sizes: [%30], strides: [%c1] : memref<?x?x?xf32, "gpu"> to memref<?xf32, "gpu">
    %31 = memref.load %reinterpret_cast[%28] : memref<?xf32, "gpu">
    %32 = math.absf %31 : f32
    %33 = arith.cmpf ugt, %23, %32 : f32
    %34 = arith.select %33, %23, %32 : f32
    %35 = arith.cmpf uno, %32, %32 : f32
    %36 = arith.select %35, %32, %34 : f32
    cf.br ^bb8(%36 : f32)
  ^bb7:  // pred: ^bb5
    cf.br ^bb8(%23 : f32)
  ^bb8(%37: f32):  // 2 preds: ^bb6, ^bb7
    cf.br ^bb9
  ^bb9:  // pred: ^bb8
    %38 = arith.addi %22, %c1 : index
    cf.br ^bb4(%38, %37 : index, f32)
  ^bb10:  // pred: ^bb4
    cf.br ^bb12(%23 : f32)
  ^bb11:  // pred: ^bb2
    cf.br ^bb12(%cst : f32)
  ^bb12(%39: f32):  // 2 preds: ^bb10, ^bb11
    cf.br ^bb13
  ^bb13:  // pred: ^bb12
    %reinterpret_cast_2 = memref.reinterpret_cast %arg6 to offset: [0], sizes: [256], strides: [1] : memref<256xf32, #gpu.address_space<workgroup>> to memref<256xf32, #gpu.address_space<workgroup>>
    memref.store %39, %reinterpret_cast_2[%15] : memref<256xf32, #gpu.address_space<workgroup>>
    gpu.barrier
    %40 = arith.cmpi slt, %12, %c4 : index
    cf.cond_br %40, ^bb14, ^bb15
  ^bb14:  // pred: ^bb13
    %41 = arith.addi %15, %c4 : index
    %42 = memref.load %reinterpret_cast_2[%15] : memref<256xf32, #gpu.address_space<workgroup>>
    %43 = memref.load %reinterpret_cast_2[%41] : memref<256xf32, #gpu.address_space<workgroup>>
    %44 = arith.cmpf ugt, %42, %43 : f32
    %45 = arith.select %44, %42, %43 : f32
    %46 = arith.cmpf uno, %43, %43 : f32
    %47 = arith.select %46, %43, %45 : f32
    memref.store %47, %reinterpret_cast_2[%15] : memref<256xf32, #gpu.address_space<workgroup>>
    cf.br ^bb15
  ^bb15:  // 2 preds: ^bb13, ^bb14
    gpu.barrier
    %48 = arith.cmpi slt, %12, %c2 : index
    cf.cond_br %48, ^bb16, ^bb17
  ^bb16:  // pred: ^bb15
    %49 = arith.addi %15, %c2 : index
    %50 = memref.load %reinterpret_cast_2[%15] : memref<256xf32, #gpu.address_space<workgroup>>
    %51 = memref.load %reinterpret_cast_2[%49] : memref<256xf32, #gpu.address_space<workgroup>>
    %52 = arith.cmpf ugt, %50, %51 : f32
    %53 = arith.select %52, %50, %51 : f32
    %54 = arith.cmpf uno, %51, %51 : f32
    %55 = arith.select %54, %51, %53 : f32
    memref.store %55, %reinterpret_cast_2[%15] : memref<256xf32, #gpu.address_space<workgroup>>
    cf.br ^bb17
  ^bb17:  // 2 preds: ^bb15, ^bb16
    gpu.barrier
    %56 = arith.cmpi eq, %12, %c0 : index
    %57 = arith.andi %56, %18 : i1
    cf.cond_br %57, ^bb18, ^bb19
  ^bb18:  // pred: ^bb17
    %58 = arith.addi %15, %c1 : index
    %59 = memref.load %reinterpret_cast_2[%15] : memref<256xf32, #gpu.address_space<workgroup>>
    %60 = memref.load %reinterpret_cast_2[%58] : memref<256xf32, #gpu.address_space<workgroup>>
    %61 = arith.cmpf ugt, %59, %60 : f32
    %62 = arith.select %61, %59, %60 : f32
    %63 = arith.cmpf uno, %60, %60 : f32
    %64 = arith.select %63, %60, %62 : f32
    %65 = memref.generic_atomic_rmw %arg5[%17] : memref<?xf32, "gpu"> {
    ^bb0(%arg7: f32):
      %66 = arith.cmpf ogt, %arg7, %64 : f32
      %67 = arith.select %66, %arg7, %64 : f32
      memref.atomic_yield %67 : f32
    }
    cf.br ^bb19
  ^bb19:  // 2 preds: ^bb17, ^bb18
    cf.br ^bb20
  ^bb20:  // 2 preds: ^bb1, ^bb19
    gpu.return
  }
}

// -----// IR Dump After StripDebugInfo (strip-debuginfo) //----- //
gpu.module @main_kernel_2 {
  gpu.func @main_kColReduction_reduce__4_1_0___thin_1(%arg0: index, %arg1: memref<?x?x?xf32, "gpu">, %arg2: index, %arg3: index, %arg4: index, %arg5: memref<?xf32, "gpu">) workgroup(%arg6 : memref<256xf32, #gpu.address_space<workgroup>>) kernel {
    %c4 = arith.constant 4 : index
    %cst = arith.constant 0xFF800000 : f32
    %c2 = arith.constant 2 : index
    %c64 = arith.constant 64 : index
    %c8 = arith.constant 8 : index
    %c32 = arith.constant 32 : index
    %c256 = arith.constant 256 : index
    %c1 = arith.constant 1 : index
    %c0 = arith.constant 0 : index
    %0 = gpu.block_id  x
    %1 = gpu.thread_id  x
    cf.br ^bb1
  ^bb1:  // pred: ^bb0
    %dim = memref.dim %arg1, %c0 : memref<?x?x?xf32, "gpu">
    %dim_0 = memref.dim %arg1, %c1 : memref<?x?x?xf32, "gpu">
    %dim_1 = memref.dim %arg1, %c2 : memref<?x?x?xf32, "gpu">
    %2 = arith.muli %0, %arg2 : index
    %3 = arith.addi %2, %c0 : index
    %4 = arith.muli %1, %c1 : index
    %5 = arith.addi %4, %c0 : index
    %6 = arith.addi %5, %3 : index
    %7 = arith.cmpi ult, %6, %arg3 : index
    cf.cond_br %7, ^bb2, ^bb20
  ^bb2:  // pred: ^bb1
    %8 = arith.remsi %6, %c256 : index
    %9 = arith.divsi %6, %c256 : index
    %10 = arith.divui %9, %arg4 : index
    %11 = arith.remui %9, %arg4 : index
    %12 = arith.divui %8, %c32 : index
    %13 = arith.remui %8, %c32 : index
    %14 = arith.muli %13, %c8 : index
    %15 = arith.addi %12, %14 : index
    %16 = arith.muli %11, %c32 : index
    %17 = arith.addi %13, %16 : index
    %18 = arith.cmpi ult, %17, %arg0 : index
    cf.cond_br %18, ^bb3, ^bb11
  ^bb3:  // pred: ^bb2
    %19 = arith.muli %10, %c8 : index
    %20 = arith.addi %12, %19 : index
    %21 = arith.muli %20, %c64 : index
    cf.br ^bb4(%c0, %cst : index, f32)
  ^bb4(%22: index, %23: f32):  // 2 preds: ^bb3, ^bb9
    %24 = arith.cmpi slt, %22, %c64 : index
    cf.cond_br %24, ^bb5, ^bb10
  ^bb5:  // pred: ^bb4
    %25 = arith.addi %22, %21 : index
    %26 = arith.cmpi slt, %25, %dim : index
    cf.cond_br %26, ^bb6, ^bb7
  ^bb6:  // pred: ^bb5
    %27 = arith.muli %25, %arg0 : index
    %28 = arith.addi %27, %17 : index
    %29 = arith.muli %dim, %dim_0 : index
    %30 = arith.muli %29, %dim_1 : index
    %reinterpret_cast = memref.reinterpret_cast %arg1 to offset: [%c0], sizes: [%30], strides: [%c1] : memref<?x?x?xf32, "gpu"> to memref<?xf32, "gpu">
    %31 = memref.load %reinterpret_cast[%28] : memref<?xf32, "gpu">
    %32 = math.absf %31 : f32
    %33 = arith.cmpf ugt, %23, %32 : f32
    %34 = arith.select %33, %23, %32 : f32
    %35 = arith.cmpf uno, %32, %32 : f32
    %36 = arith.select %35, %32, %34 : f32
    cf.br ^bb8(%36 : f32)
  ^bb7:  // pred: ^bb5
    cf.br ^bb8(%23 : f32)
  ^bb8(%37: f32):  // 2 preds: ^bb6, ^bb7
    cf.br ^bb9
  ^bb9:  // pred: ^bb8
    %38 = arith.addi %22, %c1 : index
    cf.br ^bb4(%38, %37 : index, f32)
  ^bb10:  // pred: ^bb4
    cf.br ^bb12(%23 : f32)
  ^bb11:  // pred: ^bb2
    cf.br ^bb12(%cst : f32)
  ^bb12(%39: f32):  // 2 preds: ^bb10, ^bb11
    cf.br ^bb13
  ^bb13:  // pred: ^bb12
    %reinterpret_cast_2 = memref.reinterpret_cast %arg6 to offset: [0], sizes: [256], strides: [1] : memref<256xf32, #gpu.address_space<workgroup>> to memref<256xf32, #gpu.address_space<workgroup>>
    memref.store %39, %reinterpret_cast_2[%15] : memref<256xf32, #gpu.address_space<workgroup>>
    gpu.barrier
    %40 = arith.cmpi slt, %12, %c4 : index
    cf.cond_br %40, ^bb14, ^bb15
  ^bb14:  // pred: ^bb13
    %41 = arith.addi %15, %c4 : index
    %42 = memref.load %reinterpret_cast_2[%15] : memref<256xf32, #gpu.address_space<workgroup>>
    %43 = memref.load %reinterpret_cast_2[%41] : memref<256xf32, #gpu.address_space<workgroup>>
    %44 = arith.cmpf ugt, %42, %43 : f32
    %45 = arith.select %44, %42, %43 : f32
    %46 = arith.cmpf uno, %43, %43 : f32
    %47 = arith.select %46, %43, %45 : f32
    memref.store %47, %reinterpret_cast_2[%15] : memref<256xf32, #gpu.address_space<workgroup>>
    cf.br ^bb15
  ^bb15:  // 2 preds: ^bb13, ^bb14
    gpu.barrier
    %48 = arith.cmpi slt, %12, %c2 : index
    cf.cond_br %48, ^bb16, ^bb17
  ^bb16:  // pred: ^bb15
    %49 = arith.addi %15, %c2 : index
    %50 = memref.load %reinterpret_cast_2[%15] : memref<256xf32, #gpu.address_space<workgroup>>
    %51 = memref.load %reinterpret_cast_2[%49] : memref<256xf32, #gpu.address_space<workgroup>>
    %52 = arith.cmpf ugt, %50, %51 : f32
    %53 = arith.select %52, %50, %51 : f32
    %54 = arith.cmpf uno, %51, %51 : f32
    %55 = arith.select %54, %51, %53 : f32
    memref.store %55, %reinterpret_cast_2[%15] : memref<256xf32, #gpu.address_space<workgroup>>
    cf.br ^bb17
  ^bb17:  // 2 preds: ^bb15, ^bb16
    gpu.barrier
    %56 = arith.cmpi eq, %12, %c0 : index
    %57 = arith.andi %56, %18 : i1
    cf.cond_br %57, ^bb18, ^bb19
  ^bb18:  // pred: ^bb17
    %58 = arith.addi %15, %c1 : index
    %59 = memref.load %reinterpret_cast_2[%15] : memref<256xf32, #gpu.address_space<workgroup>>
    %60 = memref.load %reinterpret_cast_2[%58] : memref<256xf32, #gpu.address_space<workgroup>>
    %61 = arith.cmpf ugt, %59, %60 : f32
    %62 = arith.select %61, %59, %60 : f32
    %63 = arith.cmpf uno, %60, %60 : f32
    %64 = arith.select %63, %60, %62 : f32
    %65 = memref.generic_atomic_rmw %arg5[%17] : memref<?xf32, "gpu"> {
    ^bb0(%arg7: f32):
      %66 = arith.cmpf ogt, %arg7, %64 : f32
      %67 = arith.select %66, %arg7, %64 : f32
      memref.atomic_yield %67 : f32
    }
    cf.br ^bb19
  ^bb19:  // 2 preds: ^bb17, ^bb18
    cf.br ^bb20
  ^bb20:  // 2 preds: ^bb1, ^bb19
    gpu.return
  }
}

// -----// IR Dump After DiscLowerGpuOpsToNVVMOpsPass (disc-convert-gpu-to-nvvm) //----- //
gpu.module @main_kernel_2 {
  llvm.mlir.global internal @__wg_main_kColReduction_reduce__4_1_0___thin_1_0() {addr_space = 3 : i32} : !llvm.array<256 x f32>
  llvm.func @__nv_fabsf(f32) -> f32
  llvm.func @main_kColReduction_reduce__4_1_0___thin_1(%arg0: i32, %arg1: !llvm.ptr<f32>, %arg2: !llvm.ptr<f32>, %arg3: i32, %arg4: i32, %arg5: i32, %arg6: i32, %arg7: i32, %arg8: i32, %arg9: i32, %arg10: i32, %arg11: i32, %arg12: i32, %arg13: !llvm.ptr<f32>, %arg14: !llvm.ptr<f32>, %arg15: i32, %arg16: i32, %arg17: i32) attributes {gpu.kernel, nvvm.kernel} {
    %0 = llvm.mlir.undef : !llvm.struct<(ptr<f32>, ptr<f32>, i32, array<3 x i32>, array<3 x i32>)>
    %1 = llvm.insertvalue %arg1, %0[0] : !llvm.struct<(ptr<f32>, ptr<f32>, i32, array<3 x i32>, array<3 x i32>)> 
    %2 = llvm.insertvalue %arg2, %1[1] : !llvm.struct<(ptr<f32>, ptr<f32>, i32, array<3 x i32>, array<3 x i32>)> 
    %3 = llvm.insertvalue %arg3, %2[2] : !llvm.struct<(ptr<f32>, ptr<f32>, i32, array<3 x i32>, array<3 x i32>)> 
    %4 = llvm.insertvalue %arg4, %3[3, 0] : !llvm.struct<(ptr<f32>, ptr<f32>, i32, array<3 x i32>, array<3 x i32>)> 
    %5 = llvm.insertvalue %arg7, %4[4, 0] : !llvm.struct<(ptr<f32>, ptr<f32>, i32, array<3 x i32>, array<3 x i32>)> 
    %6 = llvm.insertvalue %arg5, %5[3, 1] : !llvm.struct<(ptr<f32>, ptr<f32>, i32, array<3 x i32>, array<3 x i32>)> 
    %7 = llvm.insertvalue %arg8, %6[4, 1] : !llvm.struct<(ptr<f32>, ptr<f32>, i32, array<3 x i32>, array<3 x i32>)> 
    %8 = llvm.insertvalue %arg6, %7[3, 2] : !llvm.struct<(ptr<f32>, ptr<f32>, i32, array<3 x i32>, array<3 x i32>)> 
    %9 = llvm.insertvalue %arg9, %8[4, 2] : !llvm.struct<(ptr<f32>, ptr<f32>, i32, array<3 x i32>, array<3 x i32>)> 
    %10 = llvm.mlir.undef : !llvm.struct<(ptr<f32>, ptr<f32>, i32, array<1 x i32>, array<1 x i32>)>
    %11 = llvm.insertvalue %arg13, %10[0] : !llvm.struct<(ptr<f32>, ptr<f32>, i32, array<1 x i32>, array<1 x i32>)> 
    %12 = llvm.insertvalue %arg14, %11[1] : !llvm.struct<(ptr<f32>, ptr<f32>, i32, array<1 x i32>, array<1 x i32>)> 
    %13 = llvm.insertvalue %arg15, %12[2] : !llvm.struct<(ptr<f32>, ptr<f32>, i32, array<1 x i32>, array<1 x i32>)> 
    %14 = llvm.insertvalue %arg16, %13[3, 0] : !llvm.struct<(ptr<f32>, ptr<f32>, i32, array<1 x i32>, array<1 x i32>)> 
    %15 = llvm.insertvalue %arg17, %14[4, 0] : !llvm.struct<(ptr<f32>, ptr<f32>, i32, array<1 x i32>, array<1 x i32>)> 
    %16 = llvm.mlir.addressof @__wg_main_kColReduction_reduce__4_1_0___thin_1_0 : !llvm.ptr<array<256 x f32>, 3>
    %17 = llvm.getelementptr %16[0, 0] : (!llvm.ptr<array<256 x f32>, 3>) -> !llvm.ptr<f32, 3>
    %18 = llvm.mlir.undef : !llvm.struct<(ptr<f32, 3>, ptr<f32, 3>, i32, array<1 x i32>, array<1 x i32>)>
    %19 = llvm.insertvalue %17, %18[0] : !llvm.struct<(ptr<f32, 3>, ptr<f32, 3>, i32, array<1 x i32>, array<1 x i32>)> 
    %20 = llvm.insertvalue %17, %19[1] : !llvm.struct<(ptr<f32, 3>, ptr<f32, 3>, i32, array<1 x i32>, array<1 x i32>)> 
    %21 = llvm.mlir.constant(0 : index) : i32
    %22 = llvm.insertvalue %21, %20[2] : !llvm.struct<(ptr<f32, 3>, ptr<f32, 3>, i32, array<1 x i32>, array<1 x i32>)> 
    %23 = llvm.mlir.constant(256 : index) : i32
    %24 = llvm.insertvalue %23, %22[3, 0] : !llvm.struct<(ptr<f32, 3>, ptr<f32, 3>, i32, array<1 x i32>, array<1 x i32>)> 
    %25 = llvm.mlir.constant(1 : index) : i32
    %26 = llvm.insertvalue %25, %24[4, 0] : !llvm.struct<(ptr<f32, 3>, ptr<f32, 3>, i32, array<1 x i32>, array<1 x i32>)> 
    %27 = llvm.mlir.constant(4 : index) : i32
    %28 = llvm.mlir.constant(0xFF800000 : f32) : f32
    %29 = llvm.mlir.constant(2 : index) : i32
    %30 = llvm.mlir.constant(64 : index) : i32
    %31 = llvm.mlir.constant(8 : index) : i32
    %32 = llvm.mlir.constant(32 : index) : i32
    %33 = llvm.mlir.constant(256 : index) : i32
    %34 = llvm.mlir.constant(1 : index) : i32
    %35 = llvm.mlir.constant(0 : index) : i32
    %36 = nvvm.read.ptx.sreg.ctaid.x : i32
    %37 = nvvm.read.ptx.sreg.tid.x : i32
    llvm.br ^bb1
  ^bb1:  // pred: ^bb0
    %38 = llvm.extractvalue %9[3, 0] : !llvm.struct<(ptr<f32>, ptr<f32>, i32, array<3 x i32>, array<3 x i32>)> 
    %39 = llvm.extractvalue %9[3, 1] : !llvm.struct<(ptr<f32>, ptr<f32>, i32, array<3 x i32>, array<3 x i32>)> 
    %40 = llvm.extractvalue %9[3, 2] : !llvm.struct<(ptr<f32>, ptr<f32>, i32, array<3 x i32>, array<3 x i32>)> 
    %41 = llvm.mul %36, %arg10  : i32
    %42 = llvm.add %37, %41  : i32
    %43 = llvm.icmp "ult" %42, %arg11 : i32
    llvm.cond_br %43, ^bb2, ^bb21
  ^bb2:  // pred: ^bb1
    %44 = llvm.srem %42, %33  : i32
    %45 = llvm.sdiv %42, %33  : i32
    %46 = llvm.udiv %45, %arg12  : i32
    %47 = llvm.urem %45, %arg12  : i32
    %48 = llvm.udiv %44, %32  : i32
    %49 = llvm.urem %44, %32  : i32
    %50 = llvm.mul %49, %31  : i32
    %51 = llvm.add %48, %50  : i32
    %52 = llvm.mul %47, %32  : i32
    %53 = llvm.add %49, %52  : i32
    %54 = llvm.icmp "ult" %53, %arg0 : i32
    llvm.cond_br %54, ^bb3, ^bb10(%28 : f32)
  ^bb3:  // pred: ^bb2
    %55 = llvm.mul %46, %31  : i32
    %56 = llvm.add %48, %55  : i32
    %57 = llvm.mul %56, %30  : i32
    llvm.br ^bb4(%35, %28 : i32, f32)
  ^bb4(%58: i32, %59: f32):  // 2 preds: ^bb3, ^bb9
    %60 = llvm.icmp "slt" %58, %30 : i32
    llvm.cond_br %60, ^bb5, ^bb10(%59 : f32)
  ^bb5:  // pred: ^bb4
    %61 = llvm.add %58, %57  : i32
    %62 = llvm.icmp "slt" %61, %38 : i32
    llvm.cond_br %62, ^bb6, ^bb7
  ^bb6:  // pred: ^bb5
    %63 = llvm.mul %61, %arg0  : i32
    %64 = llvm.add %63, %53  : i32
    %65 = llvm.mul %38, %39  : i32
    %66 = llvm.mul %65, %40  : i32
    %67 = llvm.mlir.undef : !llvm.struct<(ptr<f32>, ptr<f32>, i32, array<1 x i32>, array<1 x i32>)>
    %68 = llvm.extractvalue %9[0] : !llvm.struct<(ptr<f32>, ptr<f32>, i32, array<3 x i32>, array<3 x i32>)> 
    %69 = llvm.extractvalue %9[1] : !llvm.struct<(ptr<f32>, ptr<f32>, i32, array<3 x i32>, array<3 x i32>)> 
    %70 = llvm.insertvalue %68, %67[0] : !llvm.struct<(ptr<f32>, ptr<f32>, i32, array<1 x i32>, array<1 x i32>)> 
    %71 = llvm.insertvalue %69, %70[1] : !llvm.struct<(ptr<f32>, ptr<f32>, i32, array<1 x i32>, array<1 x i32>)> 
    %72 = llvm.insertvalue %35, %71[2] : !llvm.struct<(ptr<f32>, ptr<f32>, i32, array<1 x i32>, array<1 x i32>)> 
    %73 = llvm.insertvalue %66, %72[3, 0] : !llvm.struct<(ptr<f32>, ptr<f32>, i32, array<1 x i32>, array<1 x i32>)> 
    %74 = llvm.insertvalue %34, %73[4, 0] : !llvm.struct<(ptr<f32>, ptr<f32>, i32, array<1 x i32>, array<1 x i32>)> 
    %75 = llvm.extractvalue %74[1] : !llvm.struct<(ptr<f32>, ptr<f32>, i32, array<1 x i32>, array<1 x i32>)> 
    %76 = llvm.getelementptr %75[%64] : (!llvm.ptr<f32>, i32) -> !llvm.ptr<f32>
    %77 = llvm.load %76 : !llvm.ptr<f32>
    %78 = llvm.call @__nv_fabsf(%77) : (f32) -> f32
    %79 = llvm.fcmp "ugt" %59, %78 : f32
    %80 = llvm.select %79, %59, %78 : i1, f32
    %81 = llvm.fcmp "uno" %78, %78 : f32
    %82 = llvm.select %81, %78, %80 : i1, f32
    llvm.br ^bb8(%82 : f32)
  ^bb7:  // pred: ^bb5
    llvm.br ^bb8(%59 : f32)
  ^bb8(%83: f32):  // 2 preds: ^bb6, ^bb7
    llvm.br ^bb9
  ^bb9:  // pred: ^bb8
    %84 = llvm.add %58, %34  : i32
    llvm.br ^bb4(%84, %83 : i32, f32)
  ^bb10(%85: f32):  // 2 preds: ^bb2, ^bb4
    llvm.br ^bb11(%85 : f32)
  ^bb11(%86: f32):  // pred: ^bb10
    llvm.br ^bb12
  ^bb12:  // pred: ^bb11
    %87 = llvm.mlir.undef : !llvm.struct<(ptr<f32, 3>, ptr<f32, 3>, i32, array<1 x i32>, array<1 x i32>)>
    %88 = llvm.extractvalue %26[0] : !llvm.struct<(ptr<f32, 3>, ptr<f32, 3>, i32, array<1 x i32>, array<1 x i32>)> 
    %89 = llvm.extractvalue %26[1] : !llvm.struct<(ptr<f32, 3>, ptr<f32, 3>, i32, array<1 x i32>, array<1 x i32>)> 
    %90 = llvm.insertvalue %88, %87[0] : !llvm.struct<(ptr<f32, 3>, ptr<f32, 3>, i32, array<1 x i32>, array<1 x i32>)> 
    %91 = llvm.insertvalue %89, %90[1] : !llvm.struct<(ptr<f32, 3>, ptr<f32, 3>, i32, array<1 x i32>, array<1 x i32>)> 
    %92 = llvm.mlir.constant(0 : index) : i32
    %93 = llvm.insertvalue %92, %91[2] : !llvm.struct<(ptr<f32, 3>, ptr<f32, 3>, i32, array<1 x i32>, array<1 x i32>)> 
    %94 = llvm.mlir.constant(256 : index) : i32
    %95 = llvm.insertvalue %94, %93[3, 0] : !llvm.struct<(ptr<f32, 3>, ptr<f32, 3>, i32, array<1 x i32>, array<1 x i32>)> 
    %96 = llvm.mlir.constant(1 : index) : i32
    %97 = llvm.insertvalue %96, %95[4, 0] : !llvm.struct<(ptr<f32, 3>, ptr<f32, 3>, i32, array<1 x i32>, array<1 x i32>)> 
    %98 = llvm.extractvalue %97[1] : !llvm.struct<(ptr<f32, 3>, ptr<f32, 3>, i32, array<1 x i32>, array<1 x i32>)> 
    %99 = llvm.getelementptr %98[%51] : (!llvm.ptr<f32, 3>, i32) -> !llvm.ptr<f32, 3>
    llvm.store %86, %99 : !llvm.ptr<f32, 3>
    nvvm.barrier0
    %100 = llvm.icmp "slt" %48, %27 : i32
    llvm.cond_br %100, ^bb13, ^bb14
  ^bb13:  // pred: ^bb12
    %101 = llvm.add %51, %27  : i32
    %102 = llvm.extractvalue %97[1] : !llvm.struct<(ptr<f32, 3>, ptr<f32, 3>, i32, array<1 x i32>, array<1 x i32>)> 
    %103 = llvm.getelementptr %102[%51] : (!llvm.ptr<f32, 3>, i32) -> !llvm.ptr<f32, 3>
    %104 = llvm.load %103 : !llvm.ptr<f32, 3>
    %105 = llvm.extractvalue %97[1] : !llvm.struct<(ptr<f32, 3>, ptr<f32, 3>, i32, array<1 x i32>, array<1 x i32>)> 
    %106 = llvm.getelementptr %105[%101] : (!llvm.ptr<f32, 3>, i32) -> !llvm.ptr<f32, 3>
    %107 = llvm.load %106 : !llvm.ptr<f32, 3>
    %108 = llvm.fcmp "ugt" %104, %107 : f32
    %109 = llvm.select %108, %104, %107 : i1, f32
    %110 = llvm.fcmp "uno" %107, %107 : f32
    %111 = llvm.select %110, %107, %109 : i1, f32
    %112 = llvm.extractvalue %97[1] : !llvm.struct<(ptr<f32, 3>, ptr<f32, 3>, i32, array<1 x i32>, array<1 x i32>)> 
    %113 = llvm.getelementptr %112[%51] : (!llvm.ptr<f32, 3>, i32) -> !llvm.ptr<f32, 3>
    llvm.store %111, %113 : !llvm.ptr<f32, 3>
    llvm.br ^bb14
  ^bb14:  // 2 preds: ^bb12, ^bb13
    nvvm.barrier0
    %114 = llvm.icmp "slt" %48, %29 : i32
    llvm.cond_br %114, ^bb15, ^bb16
  ^bb15:  // pred: ^bb14
    %115 = llvm.add %51, %29  : i32
    %116 = llvm.extractvalue %97[1] : !llvm.struct<(ptr<f32, 3>, ptr<f32, 3>, i32, array<1 x i32>, array<1 x i32>)> 
    %117 = llvm.getelementptr %116[%51] : (!llvm.ptr<f32, 3>, i32) -> !llvm.ptr<f32, 3>
    %118 = llvm.load %117 : !llvm.ptr<f32, 3>
    %119 = llvm.extractvalue %97[1] : !llvm.struct<(ptr<f32, 3>, ptr<f32, 3>, i32, array<1 x i32>, array<1 x i32>)> 
    %120 = llvm.getelementptr %119[%115] : (!llvm.ptr<f32, 3>, i32) -> !llvm.ptr<f32, 3>
    %121 = llvm.load %120 : !llvm.ptr<f32, 3>
    %122 = llvm.fcmp "ugt" %118, %121 : f32
    %123 = llvm.select %122, %118, %121 : i1, f32
    %124 = llvm.fcmp "uno" %121, %121 : f32
    %125 = llvm.select %124, %121, %123 : i1, f32
    %126 = llvm.extractvalue %97[1] : !llvm.struct<(ptr<f32, 3>, ptr<f32, 3>, i32, array<1 x i32>, array<1 x i32>)> 
    %127 = llvm.getelementptr %126[%51] : (!llvm.ptr<f32, 3>, i32) -> !llvm.ptr<f32, 3>
    llvm.store %125, %127 : !llvm.ptr<f32, 3>
    llvm.br ^bb16
  ^bb16:  // 2 preds: ^bb14, ^bb15
    nvvm.barrier0
    %128 = llvm.icmp "eq" %48, %35 : i32
    %129 = llvm.and %128, %54  : i1
    llvm.cond_br %129, ^bb17, ^bb20
  ^bb17:  // pred: ^bb16
    %130 = llvm.add %51, %34  : i32
    %131 = llvm.extractvalue %97[1] : !llvm.struct<(ptr<f32, 3>, ptr<f32, 3>, i32, array<1 x i32>, array<1 x i32>)> 
    %132 = llvm.getelementptr %131[%51] : (!llvm.ptr<f32, 3>, i32) -> !llvm.ptr<f32, 3>
    %133 = llvm.load %132 : !llvm.ptr<f32, 3>
    %134 = llvm.extractvalue %97[1] : !llvm.struct<(ptr<f32, 3>, ptr<f32, 3>, i32, array<1 x i32>, array<1 x i32>)> 
    %135 = llvm.getelementptr %134[%130] : (!llvm.ptr<f32, 3>, i32) -> !llvm.ptr<f32, 3>
    %136 = llvm.load %135 : !llvm.ptr<f32, 3>
    %137 = llvm.fcmp "ugt" %133, %136 : f32
    %138 = llvm.select %137, %133, %136 : i1, f32
    %139 = llvm.fcmp "uno" %136, %136 : f32
    %140 = llvm.select %139, %136, %138 : i1, f32
    %141 = llvm.extractvalue %15[1] : !llvm.struct<(ptr<f32>, ptr<f32>, i32, array<1 x i32>, array<1 x i32>)> 
    %142 = llvm.getelementptr %141[%53] : (!llvm.ptr<f32>, i32) -> !llvm.ptr<f32>
    %143 = llvm.load %142 : !llvm.ptr<f32>
    llvm.br ^bb18(%143 : f32)
  ^bb18(%144: f32):  // 2 preds: ^bb17, ^bb18
    %145 = llvm.fcmp "ogt" %144, %140 : f32
    %146 = llvm.select %145, %144, %140 : i1, f32
    %147 = llvm.bitcast %142 : !llvm.ptr<f32> to !llvm.ptr<i32>
    %148 = llvm.bitcast %144 : f32 to i32
    %149 = llvm.bitcast %146 : f32 to i32
    %150 = llvm.cmpxchg %147, %148, %149 acq_rel monotonic : !llvm.ptr<i32>, i32
    %151 = llvm.extractvalue %150[0] : !llvm.struct<(i32, i1)> 
    %152 = llvm.bitcast %151 : i32 to f32
    %153 = llvm.extractvalue %150[1] : !llvm.struct<(i32, i1)> 
    llvm.cond_br %153, ^bb19, ^bb18(%152 : f32)
  ^bb19:  // pred: ^bb18
    llvm.br ^bb20
  ^bb20:  // 2 preds: ^bb16, ^bb19
    llvm.br ^bb21
  ^bb21:  // 2 preds: ^bb1, ^bb20
    llvm.return
  }
}

// -----// IR Dump After LLVMInsertValueSimplifierPass (disc-llvm-insert-value-simplifier) //----- //
llvm.func @main_kColReduction_reduce__4_1_0___thin_1(%arg0: i32, %arg1: !llvm.ptr<f32>, %arg2: !llvm.ptr<f32>, %arg3: i32, %arg4: i32, %arg5: i32, %arg6: i32, %arg7: i32, %arg8: i32, %arg9: i32, %arg10: i32, %arg11: i32, %arg12: i32, %arg13: !llvm.ptr<f32>, %arg14: !llvm.ptr<f32>, %arg15: i32, %arg16: i32, %arg17: i32) attributes {gpu.kernel, nvvm.kernel} {
  %0 = llvm.mlir.constant(32 : index) : i32
  %1 = llvm.mlir.constant(8 : index) : i32
  %2 = llvm.mlir.constant(64 : index) : i32
  %3 = llvm.mlir.constant(2 : index) : i32
  %4 = llvm.mlir.constant(0xFF800000 : f32) : f32
  %5 = llvm.mlir.constant(4 : index) : i32
  %6 = llvm.mlir.constant(1 : index) : i32
  %7 = llvm.mlir.constant(256 : index) : i32
  %8 = llvm.mlir.constant(0 : index) : i32
  %9 = llvm.mlir.addressof @__wg_main_kColReduction_reduce__4_1_0___thin_1_0 : !llvm.ptr<array<256 x f32>, 3>
  %10 = llvm.getelementptr %9[0, 0] : (!llvm.ptr<array<256 x f32>, 3>) -> !llvm.ptr<f32, 3>
  %11 = nvvm.read.ptx.sreg.ctaid.x : i32
  %12 = nvvm.read.ptx.sreg.tid.x : i32
  llvm.br ^bb1
^bb1:  // pred: ^bb0
  %13 = llvm.mul %11, %arg10  : i32
  %14 = llvm.add %12, %13  : i32
  %15 = llvm.icmp "ult" %14, %arg11 : i32
  llvm.cond_br %15, ^bb2, ^bb21
^bb2:  // pred: ^bb1
  %16 = llvm.srem %14, %7  : i32
  %17 = llvm.sdiv %14, %7  : i32
  %18 = llvm.udiv %17, %arg12  : i32
  %19 = llvm.urem %17, %arg12  : i32
  %20 = llvm.udiv %16, %0  : i32
  %21 = llvm.urem %16, %0  : i32
  %22 = llvm.mul %21, %1  : i32
  %23 = llvm.add %20, %22  : i32
  %24 = llvm.mul %19, %0  : i32
  %25 = llvm.add %21, %24  : i32
  %26 = llvm.icmp "ult" %25, %arg0 : i32
  llvm.cond_br %26, ^bb3, ^bb10(%4 : f32)
^bb3:  // pred: ^bb2
  %27 = llvm.mul %18, %1  : i32
  %28 = llvm.add %20, %27  : i32
  %29 = llvm.mul %28, %2  : i32
  llvm.br ^bb4(%8, %4 : i32, f32)
^bb4(%30: i32, %31: f32):  // 2 preds: ^bb3, ^bb9
  %32 = llvm.icmp "slt" %30, %2 : i32
  llvm.cond_br %32, ^bb5, ^bb10(%31 : f32)
^bb5:  // pred: ^bb4
  %33 = llvm.add %30, %29  : i32
  %34 = llvm.icmp "slt" %33, %arg4 : i32
  llvm.cond_br %34, ^bb6, ^bb7
^bb6:  // pred: ^bb5
  %35 = llvm.mul %33, %arg0  : i32
  %36 = llvm.add %35, %25  : i32
  %37 = llvm.getelementptr %arg2[%36] : (!llvm.ptr<f32>, i32) -> !llvm.ptr<f32>
  %38 = llvm.load %37 : !llvm.ptr<f32>
  %39 = llvm.call @__nv_fabsf(%38) : (f32) -> f32
  %40 = llvm.fcmp "ugt" %31, %39 : f32
  %41 = llvm.select %40, %31, %39 : i1, f32
  %42 = llvm.fcmp "uno" %39, %39 : f32
  %43 = llvm.select %42, %39, %41 : i1, f32
  llvm.br ^bb8(%43 : f32)
^bb7:  // pred: ^bb5
  llvm.br ^bb8(%31 : f32)
^bb8(%44: f32):  // 2 preds: ^bb6, ^bb7
  llvm.br ^bb9
^bb9:  // pred: ^bb8
  %45 = llvm.add %30, %6  : i32
  llvm.br ^bb4(%45, %44 : i32, f32)
^bb10(%46: f32):  // 2 preds: ^bb2, ^bb4
  llvm.br ^bb11(%46 : f32)
^bb11(%47: f32):  // pred: ^bb10
  llvm.br ^bb12
^bb12:  // pred: ^bb11
  %48 = llvm.getelementptr %10[%23] : (!llvm.ptr<f32, 3>, i32) -> !llvm.ptr<f32, 3>
  llvm.store %47, %48 : !llvm.ptr<f32, 3>
  nvvm.barrier0
  %49 = llvm.icmp "slt" %20, %5 : i32
  llvm.cond_br %49, ^bb13, ^bb14
^bb13:  // pred: ^bb12
  %50 = llvm.add %23, %5  : i32
  %51 = llvm.getelementptr %10[%23] : (!llvm.ptr<f32, 3>, i32) -> !llvm.ptr<f32, 3>
  %52 = llvm.load %51 : !llvm.ptr<f32, 3>
  %53 = llvm.getelementptr %10[%50] : (!llvm.ptr<f32, 3>, i32) -> !llvm.ptr<f32, 3>
  %54 = llvm.load %53 : !llvm.ptr<f32, 3>
  %55 = llvm.fcmp "ugt" %52, %54 : f32
  %56 = llvm.select %55, %52, %54 : i1, f32
  %57 = llvm.fcmp "uno" %54, %54 : f32
  %58 = llvm.select %57, %54, %56 : i1, f32
  %59 = llvm.getelementptr %10[%23] : (!llvm.ptr<f32, 3>, i32) -> !llvm.ptr<f32, 3>
  llvm.store %58, %59 : !llvm.ptr<f32, 3>
  llvm.br ^bb14
^bb14:  // 2 preds: ^bb12, ^bb13
  nvvm.barrier0
  %60 = llvm.icmp "slt" %20, %3 : i32
  llvm.cond_br %60, ^bb15, ^bb16
^bb15:  // pred: ^bb14
  %61 = llvm.add %23, %3  : i32
  %62 = llvm.getelementptr %10[%23] : (!llvm.ptr<f32, 3>, i32) -> !llvm.ptr<f32, 3>
  %63 = llvm.load %62 : !llvm.ptr<f32, 3>
  %64 = llvm.getelementptr %10[%61] : (!llvm.ptr<f32, 3>, i32) -> !llvm.ptr<f32, 3>
  %65 = llvm.load %64 : !llvm.ptr<f32, 3>
  %66 = llvm.fcmp "ugt" %63, %65 : f32
  %67 = llvm.select %66, %63, %65 : i1, f32
  %68 = llvm.fcmp "uno" %65, %65 : f32
  %69 = llvm.select %68, %65, %67 : i1, f32
  %70 = llvm.getelementptr %10[%23] : (!llvm.ptr<f32, 3>, i32) -> !llvm.ptr<f32, 3>
  llvm.store %69, %70 : !llvm.ptr<f32, 3>
  llvm.br ^bb16
^bb16:  // 2 preds: ^bb14, ^bb15
  nvvm.barrier0
  %71 = llvm.icmp "eq" %20, %8 : i32
  %72 = llvm.and %71, %26  : i1
  llvm.cond_br %72, ^bb17, ^bb20
^bb17:  // pred: ^bb16
  %73 = llvm.add %23, %6  : i32
  %74 = llvm.getelementptr %10[%23] : (!llvm.ptr<f32, 3>, i32) -> !llvm.ptr<f32, 3>
  %75 = llvm.load %74 : !llvm.ptr<f32, 3>
  %76 = llvm.getelementptr %10[%73] : (!llvm.ptr<f32, 3>, i32) -> !llvm.ptr<f32, 3>
  %77 = llvm.load %76 : !llvm.ptr<f32, 3>
  %78 = llvm.fcmp "ugt" %75, %77 : f32
  %79 = llvm.select %78, %75, %77 : i1, f32
  %80 = llvm.fcmp "uno" %77, %77 : f32
  %81 = llvm.select %80, %77, %79 : i1, f32
  %82 = llvm.getelementptr %arg14[%25] : (!llvm.ptr<f32>, i32) -> !llvm.ptr<f32>
  %83 = llvm.load %82 : !llvm.ptr<f32>
  llvm.br ^bb18(%83 : f32)
^bb18(%84: f32):  // 2 preds: ^bb17, ^bb18
  %85 = llvm.fcmp "ogt" %84, %81 : f32
  %86 = llvm.select %85, %84, %81 : i1, f32
  %87 = llvm.bitcast %82 : !llvm.ptr<f32> to !llvm.ptr<i32>
  %88 = llvm.bitcast %84 : f32 to i32
  %89 = llvm.bitcast %86 : f32 to i32
  %90 = llvm.cmpxchg %87, %88, %89 acq_rel monotonic : !llvm.ptr<i32>, i32
  %91 = llvm.extractvalue %90[0] : !llvm.struct<(i32, i1)> 
  %92 = llvm.bitcast %91 : i32 to f32
  %93 = llvm.extractvalue %90[1] : !llvm.struct<(i32, i1)> 
  llvm.cond_br %93, ^bb19, ^bb18(%92 : f32)
^bb19:  // pred: ^bb18
  llvm.br ^bb20
^bb20:  // 2 preds: ^bb16, ^bb19
  llvm.br ^bb21
^bb21:  // 2 preds: ^bb1, ^bb20
  llvm.return
}

// -----// IR Dump After FunctionDeadArgumentEliminationPass (disc-function-dead-argument-elimination) //----- //
gpu.module @main_kernel_2 {
  llvm.mlir.global internal @__wg_main_kColReduction_reduce__4_1_0___thin_1_0() {addr_space = 3 : i32} : !llvm.array<256 x f32>
  llvm.func @__nv_fabsf(f32) -> f32
  llvm.func @main_kColReduction_reduce__4_1_0___thin_1(%arg0: i32, %arg1: !llvm.ptr<f32>, %arg2: i32, %arg3: i32, %arg4: i32, %arg5: i32, %arg6: !llvm.ptr<f32>) attributes {disc.elimargs = [1 : index, 3 : index, 5 : index, 6 : index, 7 : index, 8 : index, 9 : index, 13 : index, 15 : index, 16 : index, 17 : index], gpu.kernel, nvvm.kernel} {
    %0 = llvm.mlir.constant(32 : index) : i32
    %1 = llvm.mlir.constant(8 : index) : i32
    %2 = llvm.mlir.constant(64 : index) : i32
    %3 = llvm.mlir.constant(2 : index) : i32
    %4 = llvm.mlir.constant(0xFF800000 : f32) : f32
    %5 = llvm.mlir.constant(4 : index) : i32
    %6 = llvm.mlir.constant(1 : index) : i32
    %7 = llvm.mlir.constant(256 : index) : i32
    %8 = llvm.mlir.constant(0 : index) : i32
    %9 = llvm.mlir.addressof @__wg_main_kColReduction_reduce__4_1_0___thin_1_0 : !llvm.ptr<array<256 x f32>, 3>
    %10 = llvm.getelementptr %9[0, 0] : (!llvm.ptr<array<256 x f32>, 3>) -> !llvm.ptr<f32, 3>
    %11 = nvvm.read.ptx.sreg.ctaid.x : i32
    %12 = nvvm.read.ptx.sreg.tid.x : i32
    llvm.br ^bb1
  ^bb1:  // pred: ^bb0
    %13 = llvm.mul %11, %arg3  : i32
    %14 = llvm.add %12, %13  : i32
    %15 = llvm.icmp "ult" %14, %arg4 : i32
    llvm.cond_br %15, ^bb2, ^bb21
  ^bb2:  // pred: ^bb1
    %16 = llvm.srem %14, %7  : i32
    %17 = llvm.sdiv %14, %7  : i32
    %18 = llvm.udiv %17, %arg5  : i32
    %19 = llvm.urem %17, %arg5  : i32
    %20 = llvm.udiv %16, %0  : i32
    %21 = llvm.urem %16, %0  : i32
    %22 = llvm.mul %21, %1  : i32
    %23 = llvm.add %20, %22  : i32
    %24 = llvm.mul %19, %0  : i32
    %25 = llvm.add %21, %24  : i32
    %26 = llvm.icmp "ult" %25, %arg0 : i32
    llvm.cond_br %26, ^bb3, ^bb10(%4 : f32)
  ^bb3:  // pred: ^bb2
    %27 = llvm.mul %18, %1  : i32
    %28 = llvm.add %20, %27  : i32
    %29 = llvm.mul %28, %2  : i32
    llvm.br ^bb4(%8, %4 : i32, f32)
  ^bb4(%30: i32, %31: f32):  // 2 preds: ^bb3, ^bb9
    %32 = llvm.icmp "slt" %30, %2 : i32
    llvm.cond_br %32, ^bb5, ^bb10(%31 : f32)
  ^bb5:  // pred: ^bb4
    %33 = llvm.add %30, %29  : i32
    %34 = llvm.icmp "slt" %33, %arg2 : i32
    llvm.cond_br %34, ^bb6, ^bb7
  ^bb6:  // pred: ^bb5
    %35 = llvm.mul %33, %arg0  : i32
    %36 = llvm.add %35, %25  : i32
    %37 = llvm.getelementptr %arg1[%36] : (!llvm.ptr<f32>, i32) -> !llvm.ptr<f32>
    %38 = llvm.load %37 : !llvm.ptr<f32>
    %39 = llvm.call @__nv_fabsf(%38) : (f32) -> f32
    %40 = llvm.fcmp "ugt" %31, %39 : f32
    %41 = llvm.select %40, %31, %39 : i1, f32
    %42 = llvm.fcmp "uno" %39, %39 : f32
    %43 = llvm.select %42, %39, %41 : i1, f32
    llvm.br ^bb8(%43 : f32)
  ^bb7:  // pred: ^bb5
    llvm.br ^bb8(%31 : f32)
  ^bb8(%44: f32):  // 2 preds: ^bb6, ^bb7
    llvm.br ^bb9
  ^bb9:  // pred: ^bb8
    %45 = llvm.add %30, %6  : i32
    llvm.br ^bb4(%45, %44 : i32, f32)
  ^bb10(%46: f32):  // 2 preds: ^bb2, ^bb4
    llvm.br ^bb11(%46 : f32)
  ^bb11(%47: f32):  // pred: ^bb10
    llvm.br ^bb12
  ^bb12:  // pred: ^bb11
    %48 = llvm.getelementptr %10[%23] : (!llvm.ptr<f32, 3>, i32) -> !llvm.ptr<f32, 3>
    llvm.store %47, %48 : !llvm.ptr<f32, 3>
    nvvm.barrier0
    %49 = llvm.icmp "slt" %20, %5 : i32
    llvm.cond_br %49, ^bb13, ^bb14
  ^bb13:  // pred: ^bb12
    %50 = llvm.add %23, %5  : i32
    %51 = llvm.getelementptr %10[%23] : (!llvm.ptr<f32, 3>, i32) -> !llvm.ptr<f32, 3>
    %52 = llvm.load %51 : !llvm.ptr<f32, 3>
    %53 = llvm.getelementptr %10[%50] : (!llvm.ptr<f32, 3>, i32) -> !llvm.ptr<f32, 3>
    %54 = llvm.load %53 : !llvm.ptr<f32, 3>
    %55 = llvm.fcmp "ugt" %52, %54 : f32
    %56 = llvm.select %55, %52, %54 : i1, f32
    %57 = llvm.fcmp "uno" %54, %54 : f32
    %58 = llvm.select %57, %54, %56 : i1, f32
    %59 = llvm.getelementptr %10[%23] : (!llvm.ptr<f32, 3>, i32) -> !llvm.ptr<f32, 3>
    llvm.store %58, %59 : !llvm.ptr<f32, 3>
    llvm.br ^bb14
  ^bb14:  // 2 preds: ^bb12, ^bb13
    nvvm.barrier0
    %60 = llvm.icmp "slt" %20, %3 : i32
    llvm.cond_br %60, ^bb15, ^bb16
  ^bb15:  // pred: ^bb14
    %61 = llvm.add %23, %3  : i32
    %62 = llvm.getelementptr %10[%23] : (!llvm.ptr<f32, 3>, i32) -> !llvm.ptr<f32, 3>
    %63 = llvm.load %62 : !llvm.ptr<f32, 3>
    %64 = llvm.getelementptr %10[%61] : (!llvm.ptr<f32, 3>, i32) -> !llvm.ptr<f32, 3>
    %65 = llvm.load %64 : !llvm.ptr<f32, 3>
    %66 = llvm.fcmp "ugt" %63, %65 : f32
    %67 = llvm.select %66, %63, %65 : i1, f32
    %68 = llvm.fcmp "uno" %65, %65 : f32
    %69 = llvm.select %68, %65, %67 : i1, f32
    %70 = llvm.getelementptr %10[%23] : (!llvm.ptr<f32, 3>, i32) -> !llvm.ptr<f32, 3>
    llvm.store %69, %70 : !llvm.ptr<f32, 3>
    llvm.br ^bb16
  ^bb16:  // 2 preds: ^bb14, ^bb15
    nvvm.barrier0
    %71 = llvm.icmp "eq" %20, %8 : i32
    %72 = llvm.and %71, %26  : i1
    llvm.cond_br %72, ^bb17, ^bb20
  ^bb17:  // pred: ^bb16
    %73 = llvm.add %23, %6  : i32
    %74 = llvm.getelementptr %10[%23] : (!llvm.ptr<f32, 3>, i32) -> !llvm.ptr<f32, 3>
    %75 = llvm.load %74 : !llvm.ptr<f32, 3>
    %76 = llvm.getelementptr %10[%73] : (!llvm.ptr<f32, 3>, i32) -> !llvm.ptr<f32, 3>
    %77 = llvm.load %76 : !llvm.ptr<f32, 3>
    %78 = llvm.fcmp "ugt" %75, %77 : f32
    %79 = llvm.select %78, %75, %77 : i1, f32
    %80 = llvm.fcmp "uno" %77, %77 : f32
    %81 = llvm.select %80, %77, %79 : i1, f32
    %82 = llvm.getelementptr %arg6[%25] : (!llvm.ptr<f32>, i32) -> !llvm.ptr<f32>
    %83 = llvm.load %82 : !llvm.ptr<f32>
    llvm.br ^bb18(%83 : f32)
  ^bb18(%84: f32):  // 2 preds: ^bb17, ^bb18
    %85 = llvm.fcmp "ogt" %84, %81 : f32
    %86 = llvm.select %85, %84, %81 : i1, f32
    %87 = llvm.bitcast %82 : !llvm.ptr<f32> to !llvm.ptr<i32>
    %88 = llvm.bitcast %84 : f32 to i32
    %89 = llvm.bitcast %86 : f32 to i32
    %90 = llvm.cmpxchg %87, %88, %89 acq_rel monotonic : !llvm.ptr<i32>, i32
    %91 = llvm.extractvalue %90[0] : !llvm.struct<(i32, i1)> 
    %92 = llvm.bitcast %91 : i32 to f32
    %93 = llvm.extractvalue %90[1] : !llvm.struct<(i32, i1)> 
    llvm.cond_br %93, ^bb19, ^bb18(%92 : f32)
  ^bb19:  // pred: ^bb18
    llvm.br ^bb20
  ^bb20:  // 2 preds: ^bb16, ^bb19
    llvm.br ^bb21
  ^bb21:  // 2 preds: ^bb1, ^bb20
    llvm.return
  }
}

// -----// IR Dump After GpuKernelToBlobPass (disc-gpu-kernel-to-blob) //----- //
gpu.module @main_kernel_2 attributes {gpu.binary = "P\EDU\BA\01\00\10\00(\0C\00\00\00\00\00\00\02\00\01\01@\00\00\00\E8\0B\00\00\00\00\00\00\E3\0B\00\00\00\00\00\00\07\00\01\00P\00\00\00\00\00\00\00\00\00\00\00\11 \00\00\00\00\00\00\00\00\00\00\00\00\00\00(:\00\00\00\00\00\00\A2\7FELF\02\01\013\07\00\01\00f\02\00\BE\00q\00\01\00#\809\08\00\116\07\00\F5\0E\00P\05P\00@\008\00\03\00@\00\0C\00\01\00\00.shstrtab\00.\08\00'ym\08\00\F0\1B_shndx\00.nv.info\00.text.main_kColReduction_r\0A\00\F5\02e__4_1_0___thin_19\00\0F3\00\1Coshared5\00\19Orela\9D\00\1E?rel\D1\00!\9Fconstant08\00\19\B2debug_framey\00\09\11\00!nv\14\00\11aD\00\0F\98\01 \0F\89\00\16\0F\C2\01\EF\8F$____wg_2\00\16\00\0B\00/27\FA\01&o_param\01\02\1C\0F\01\00\0B\8C\\\00\00\00\03\00\0A\00\01\00\11\BF\18\00,\0B\00\01\00 \95\01\18\00,\09\00\01\00\11\D4\18\00,\04\00\01\00\11\F2\18\00,\07\00\01\00g2\00\00\00\12\10x\00\11.\06\00v\00\FF\FF\FF\FF(\00\0C\00\00\01\00@\03\00\04|\08\00\D1\0F\0C\81\80\80(\00\08\FF\81\80(\08\0B\00\06(\00/0\00\01\00\03!\F0-\07\00 \00\04\9B\00R\04\14\00\00\00E\00\22\044\DC\00\90\04/\08\00\06\00\00\00\15\0C\00\12#\0C\00\00\01\00'\04\12\0C\00\17\11\0C\00!7\04\18\05\F1\08\015\00\00\04\0A\08\00\03\00\00\00`\01(\00\03\19(\00\04\17\0C$\00u\06\00 \00\00\F0!\10\00u\05\00\1C\00\00\F0\11\10\009\04\00\18\10\009\03\00\14\10\009\02\00\10\10\009\01\00\08P\00\01\01\00\F2\0A\F0\11\00\03\1B\FF\00\04\1C\0C\00P\00\00\000,\00\000-\00\00\04\1E\84\01#K\00\01\00\B1\02\02\08\10\0A/\22\00\00\00\08\10\00#\00\08\08\00\13\10\08\00\13\18\08\00\13 \08\00\13(\08\00\130\08\00\228\08\84\00\13\00\08\00\13\08\08\00\13\10\08\00\13\18\08\00\13 \08\00\13(\08\00\130\08\00\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\A2\00\00\00\14,\00\00\00H\00\01\00\11\02h\01\0F\01\00\FF\B8@$v\01\FF\87\04\B2\FF\00\8E\07\00\C4\0F\00\19y\00\01\00\10%\93\02a\0E\00\19y\03\00\01\00\10!-\00\F0\04\0E\00$z\00\00\00]\00\00\03\02\8E\07\00\CA\1F\00\0C\10\00\C5^\00\00p`\F0\03\00\DA\0F\00M\A3\04\F0\0C\80\03\00\EA\0F\00\06{\04\00\00_\00\00\00\90 \00\00\22\0E\00\19x\05\FF\1F\1F\00\F1\00\14\01\00\00\E2\0F\00\B9z\04\00\00F\00\00\0B\05 \E2\0FP\00\10\FF0\00@pP\F4\03\10\00cEy\00\00 *P\00p\E2\0F\00\11r\05\05\0E\00\B2\FF@\8F\07\00\C6\0F\00\08s\04\FE\04\10\10\A0\00\F1\06\1E\00\10x\02\04\FE\FF\FF\0F\FF\E0\FF\07\00\CC\1F\00\05s\03\B4\02\00\C8\03\A3\00d\00\00$r\02\FF\FF\00\F0\00a\E4\1F\00$r\07\10\00\C0\03\0A\8E\07\00\C8/\00$z\07\07\80\00\10\FF\E0\00\81\C8\0F\00'r\06\03\07\F8\02 \8E\07\80\00@\19x\03\FF\19\03\10\05\C0\00q\E4\0F\00\12x\05\05\09\05!\C0\8E\90\00T'r\06\06\03`\00a\C8\0F\00$r\04`\00#\06\0A\10\00Tz\02\04\00_@\01\01\F0\006\02\00_@\010\10\0A\02\10\00\11\80\C0\00\80\E4\0F\00\10\08\06\06\01P\00\03\10\00\060\00\11\F2\10\010$x\02$\04A\00\05\0A\8E \00\A2$t\05\FF\00\00\80\FF\FF\00\90\00@\12x\00\02p\01\22\FF\C0 \00@\19x\07\FFA\01@\02\16\01\00\80\00*\10\18`\001\12\AA\06\80\01\22\FF30\00\EF\0Cx\00\02\7F\00\00\00p@\F4\03\00\C6\D0\00\01\16\03\D0\00\00\10\00@x\03\03 p\01\12\02@\00\22$x\81\04\22\07\02\90\00h\0Cz\00\03\00X\C0\00\00\98\05\030\01\96\D8\0F\00G\19\00\00 (@\02E$x\04\06@\00\01`\004\04\04@0\00\02@\01\10\04M\07\C7pb\F8\03\00\E4\0F\04\10x\08\04\D0\00g\04\10x\0A\04\02\E0\00B\0Cz\00\080\00\A5\FA\03\00\CE\0F\00$\C4\07\FF\80\00\00\B0\00V\CA\06\04\00X\D0\00c%\C6\06\06\00Z\80\00\02\A0\02\12\0A@\00 \F0\03 \00@\81\C9\0C\06@\00\B6\00\19\1E\0C\00\A2\00\00$\D4\09P\00\84\E2\0F\00\10x\0E\04\03\80\00\00\F0\009\DA\08\08`\000\D6\08\08`\00\16\09`\00\12\0E`\00 \F6\03\E0\00E\81\D9\08\08`\00j\E2\02\00$\84\0B\B0\009\8A\0A\0AP\000\86\0A\0AP\00\10\0B\10\00u\CC\0F\00\81\89\0A\0A@\00X\22\0F\00$\B4\F0\00i\1F\00$\BA\06\0E@\00\17\B6\F0\00\00\80\00&\B9\0E\E0\00\88b\01\00\10x\10\04\04\D0\00\08p\02p\E2\0F\00\0B\C8\00\0C\10\00\B2\00b\FC\03\00\C8O\00\08\C8\09\10\00\B1\02\00\03\00\E4/\04\0B\C2\00\0CN\07\10\80 \00r\0F\00\08\C2\05\0C\09Y\06\13\03\A0\01\17\10\D0\01\F4\07\00\0B\D2\00\05\08\00\00@\00\C0\FC\03\00\E4\8F\08\10x\0C\04\05\80\00\82\E4\0F\00\08\D2\09\08\05@\00\00\10\003\04\0B\D2\E9\06\04`\005\D2\05\08`\00\1B\E2\F0\01\11\E2\C0\04\15\0C\10\02\11\E2\00\02\15\10\10\01\00\D0\00E\82\00\05\0A\80\00 \0F\09\00\01\17\06\00\01\0C \02I\08\82\09\0A\90\00D\82\00\0A\0A\90\00\10\C60\02\17\060\02i\0E\00\08\82\05\0A\A0\00\0B@\02\06\10\01#\F0\03@\02\19\0C\A0\00F\B2\00\05\0E\A0\00\10\0A \01\18\07\A0\00\0B`\02I\08\B2\0B\0E\A0\00H\B2\00\0E\0E\A0\00\09p\02x\0E\00\08\B2\05\0E\0B\A0\00\08\80\02'\E2\0F@\01\12\F6 \00?\8A\0A\10\90\02\16\10$\90\02\16\0DP\00\00\D0\02:\BA\0C\0C\90\02 \0C\0C@\00\14\0D@\005\B9\0C\0C@\00\12b`\03\14\08\F0\00\01\F0\036\10\04\09\10\00d\00\0B\C2\00\05\06 \01\00\90\029\C2\07\06\00\01D\C2\00\06\06\00\01\02\90\02$\06\07\F0\00\18\E4\90\03\0F\90\02\00o\C8\8F\00\08\D2\07\80\02\0E\1F\07\80\02\06\08\E0\01\05\80\02\06\90\03\0F\80\02\024\0E\04\0A\E0\00\0F\80\02S\1B\0E\80\02\1C\10\80\02\19\0C\80\02?\10\04\0B\80\02\0B\1B\0C\80\02/\0C\0C\80\02\0B\1F\0C\80\02\0C\1B\10\80\02\1F\0E\80\02,\1F\10\80\02\1D\14\0C\F0\00\03\80\02\1F\0D\80\02\CC\14\0E\E0\00\0F\80\02\84\1F\0F\80\02\DC\14\10\F0\00\03\80\02\1F\11\80\02\CC\14\12\E0\00\0F\80\02\84\1F\13\80\02\DC\14\14\F0\00\03\80\02\1F\15\80\02\CC\14\16\E0\00\0F\80\02\84\1F\17\80\02\DC\14\18\F0\00\03\80\02\1F\19\80\02\CC\14\1A\E0\00\0F\80\02\84\1F\1B\80\02\DC\14\1C\F0\00\03\80\02\1F\1D\80\02\CC\14\1E\E0\00\0F\80\02\84\1F\1F\80\02\DC\14 \F0\00\03\80\02\1F!\80\02\CC\14\22\E0\00\0F\80\02\84\1F#\80\02\DC\14$\F0\00\03\80\02\1F%\80\02\CC\14&\E0\00\0F\80\02\84\1F'\80\02\DC\14(\F0\00\03\80\02\1F)\80\02\CC\14*\E0\00\0F\80\02\84\1F+\80\02\DC\14,\F0\00\03\80\02\1F-\80\02\CC\14.\E0\00\0F\80\02\84\1F/\80\02\DC\140\F0\00\03\80\02\1F1\80\02\CC\142\E0\00\0F\80\02\84\1F3\80\02\DC\144\F0\00\03\80\02\1F5\80\02\CC\146\E0\00\0F\80\02\84\1F7\80\02\DC\148\F0\00\03\80\02\1F9\80\02\CC\14:\E0\00\0F\80\02\84\1F;\80\02\DC\14<\F0\00\03\80\02\19=\10\009\12\04>\10\00?\04\04?\A0\02\C5?\C6\0F\01\90\02\1AW\E4\0F\04\81\C9`$/\A4\00\90\02\11\1F\12\90\02\16?\C6\0F\02\80\02+\1F\02\80\02\00\0D@$\1A\04\80\02*\06\12\80\02\07\10\01\01\80\02\07\80\03W$\01\00$\B4\D0\02\01\80\02\1A\0A\80&\1B\B6\C0\028\B9\0A\0A\80\02\00@\02\19\0E@\02\17\09\E0#\01p%\06\E0#\02@\02\15\0E`\01\1E\C8`%\06 \02\01P\02\19\04@\02(\C8\1F0\02\11\02@\00\14\82\C0\02\10\FA0\004\09\0B\82\B0\02\03@\00\14\82\D0\02 \80\060\00I\08\82\05\06@\00\14\B2@\02\010\004\0A\0B\B2 \02\12\F0@\00$\B2\07@\02\02@\003\B2\05\0A@\00\10\00\B0*\14A\B0*\030( \88s\F8-\12\00u(f\E8\0F\00\1D{\00\01\00b\EC\0F\00\84\A9\07,*\00 \00Q\22\0E\00\1Cx\1A\00P\00p\F0\F0\03P\00\13\0C0)1pD\F2\90*!\84\A9n\01\020\00qb\0E\00\0B\A2\00\07\80\00\A1\80\F6\03\00\E4\1F\08\0B\A2\00p%\02\C0\00T/\00\1C\A8\00P\00\14\01@)\11?@)@\F6\03\00\D6\00\01#\07\07R\00\10\06 \00\09\80\00\8F\C6\0F\00\88\A3\00\00\07\C0\00\09\22\B9\05\1C\00\12\08\A0++\84\B9\A0\00\22\B2\00!+`\80\F4\03\00\C4\1F\10\00(\04\05\A0\00#\B8\00p\00Ap\01\00\DA\90\01\15\05\90\00_\CA\0F\00\88\B3@\01\0B9M\19\00p\017$t\02\10\03R\0F\00\84y\05\D1)\00@\00\90&\0E\00%v\02\03\00`\0F/\04 \00\00\1D\00\03`\00vh\0E\00\81y\06\02\D0\02D\05\00\0Br\C0\00\12\F0\C0\00\14r\C0\00W\F2\03\00\D6/\B0\00 \80\04\F0\022\0Br\00`\02\11@\10\04(\0EF\10\02S\E6\0F\00\08r\80\02\00\01\00p\CC\0F\00\A9s\07\02\A0\02\C0\07\E1\1E\00\00\A4\0E\00\0Cr\00\07\10\00 pR@\00QO\00$r\06\E0*\14\07\90*@\09\00\00\90\E0+!\FF\83\F0\00\1BMp\02TGy\00\00\F0 \00f\C0\0F\00\18y\00\01\00\0F\10\00\90\0F\01\00-\11\01H1\0E\01\00\22@\00\01\00=\98\01\000\00\08\01\00\1F\0B@\00\04\13\D8)\00?\01\02\00@\00\0A\13\13\0B\04\0C\01\00\13\E0U\00\03\8F\03\01/\04\13\05w\02\00\01\00\22\18\00\01\00.k\01T\00\00\01\00#\88\04(\03\1F\00\80\00\0B/)\00'\00\02#\00\F8@\00\04 3\04\E4\00*\04\00\01\00\1Fb@\00\04*(\05\C0\00\13\03#/\0C@\00!\89\01D\01\0D@\00\13\D0@\00*\D8\00\01\00\1B\08\08\00?x\01\00V4\000\00\00\A8\15\03\03W/\04\80\00\17\048\00\04\18\00\133@\01\0C\84\01\13\B8@\00\17\881\01\0F\C0\00\01\132T\01\15\06R\00\03^\03\1A\08\A04\11\03$\00J\00\15\80\00\01\00\13\95\94\00*\03\00\01\00\0409/\00\04\80\00\0B\13\06\18\02\04h9\0D\08\01\1A\00\08\00\04\97\00\13\018\00\04\E8\00\0C\01\009\C8/\00\08\00\088\00\18\06\A0\00\0F\01\00\05\03\A9\00\80\08\00\00\00\00\00\00\00\00\00\00\00\00"} {
  llvm.mlir.global internal @__wg_main_kColReduction_reduce__4_1_0___thin_1_0() {addr_space = 3 : i32} : !llvm.array<256 x f32>
  llvm.func @__nv_fabsf(f32) -> f32
  llvm.func @main_kColReduction_reduce__4_1_0___thin_1(%arg0: i32, %arg1: !llvm.ptr<f32>, %arg2: i32, %arg3: i32, %arg4: i32, %arg5: i32, %arg6: !llvm.ptr<f32>) attributes {disc.elimargs = [1 : index, 3 : index, 5 : index, 6 : index, 7 : index, 8 : index, 9 : index, 13 : index, 15 : index, 16 : index, 17 : index], gpu.kernel, nvvm.kernel} {
    %0 = llvm.mlir.constant(32 : index) : i32
    %1 = llvm.mlir.constant(8 : index) : i32
    %2 = llvm.mlir.constant(64 : index) : i32
    %3 = llvm.mlir.constant(2 : index) : i32
    %4 = llvm.mlir.constant(0xFF800000 : f32) : f32
    %5 = llvm.mlir.constant(4 : index) : i32
    %6 = llvm.mlir.constant(1 : index) : i32
    %7 = llvm.mlir.constant(256 : index) : i32
    %8 = llvm.mlir.constant(0 : index) : i32
    %9 = llvm.mlir.addressof @__wg_main_kColReduction_reduce__4_1_0___thin_1_0 : !llvm.ptr<array<256 x f32>, 3>
    %10 = llvm.getelementptr %9[0, 0] : (!llvm.ptr<array<256 x f32>, 3>) -> !llvm.ptr<f32, 3>
    %11 = nvvm.read.ptx.sreg.ctaid.x : i32
    %12 = nvvm.read.ptx.sreg.tid.x : i32
    llvm.br ^bb1
  ^bb1:  // pred: ^bb0
    %13 = llvm.mul %11, %arg3  : i32
    %14 = llvm.add %12, %13  : i32
    %15 = llvm.icmp "ult" %14, %arg4 : i32
    llvm.cond_br %15, ^bb2, ^bb21
  ^bb2:  // pred: ^bb1
    %16 = llvm.srem %14, %7  : i32
    %17 = llvm.sdiv %14, %7  : i32
    %18 = llvm.udiv %17, %arg5  : i32
    %19 = llvm.urem %17, %arg5  : i32
    %20 = llvm.udiv %16, %0  : i32
    %21 = llvm.urem %16, %0  : i32
    %22 = llvm.mul %21, %1  : i32
    %23 = llvm.add %20, %22  : i32
    %24 = llvm.mul %19, %0  : i32
    %25 = llvm.add %21, %24  : i32
    %26 = llvm.icmp "ult" %25, %arg0 : i32
    llvm.cond_br %26, ^bb3, ^bb10(%4 : f32)
  ^bb3:  // pred: ^bb2
    %27 = llvm.mul %18, %1  : i32
    %28 = llvm.add %20, %27  : i32
    %29 = llvm.mul %28, %2  : i32
    llvm.br ^bb4(%8, %4 : i32, f32)
  ^bb4(%30: i32, %31: f32):  // 2 preds: ^bb3, ^bb9
    %32 = llvm.icmp "slt" %30, %2 : i32
    llvm.cond_br %32, ^bb5, ^bb10(%31 : f32)
  ^bb5:  // pred: ^bb4
    %33 = llvm.add %30, %29  : i32
    %34 = llvm.icmp "slt" %33, %arg2 : i32
    llvm.cond_br %34, ^bb6, ^bb7
  ^bb6:  // pred: ^bb5
    %35 = llvm.mul %33, %arg0  : i32
    %36 = llvm.add %35, %25  : i32
    %37 = llvm.getelementptr %arg1[%36] : (!llvm.ptr<f32>, i32) -> !llvm.ptr<f32>
    %38 = llvm.load %37 : !llvm.ptr<f32>
    %39 = llvm.call @__nv_fabsf(%38) : (f32) -> f32
    %40 = llvm.fcmp "ugt" %31, %39 : f32
    %41 = llvm.select %40, %31, %39 : i1, f32
    %42 = llvm.fcmp "uno" %39, %39 : f32
    %43 = llvm.select %42, %39, %41 : i1, f32
    llvm.br ^bb8(%43 : f32)
  ^bb7:  // pred: ^bb5
    llvm.br ^bb8(%31 : f32)
  ^bb8(%44: f32):  // 2 preds: ^bb6, ^bb7
    llvm.br ^bb9
  ^bb9:  // pred: ^bb8
    %45 = llvm.add %30, %6  : i32
    llvm.br ^bb4(%45, %44 : i32, f32)
  ^bb10(%46: f32):  // 2 preds: ^bb2, ^bb4
    llvm.br ^bb11(%46 : f32)
  ^bb11(%47: f32):  // pred: ^bb10
    llvm.br ^bb12
  ^bb12:  // pred: ^bb11
    %48 = llvm.getelementptr %10[%23] : (!llvm.ptr<f32, 3>, i32) -> !llvm.ptr<f32, 3>
    llvm.store %47, %48 : !llvm.ptr<f32, 3>
    nvvm.barrier0
    %49 = llvm.icmp "slt" %20, %5 : i32
    llvm.cond_br %49, ^bb13, ^bb14
  ^bb13:  // pred: ^bb12
    %50 = llvm.add %23, %5  : i32
    %51 = llvm.getelementptr %10[%23] : (!llvm.ptr<f32, 3>, i32) -> !llvm.ptr<f32, 3>
    %52 = llvm.load %51 : !llvm.ptr<f32, 3>
    %53 = llvm.getelementptr %10[%50] : (!llvm.ptr<f32, 3>, i32) -> !llvm.ptr<f32, 3>
    %54 = llvm.load %53 : !llvm.ptr<f32, 3>
    %55 = llvm.fcmp "ugt" %52, %54 : f32
    %56 = llvm.select %55, %52, %54 : i1, f32
    %57 = llvm.fcmp "uno" %54, %54 : f32
    %58 = llvm.select %57, %54, %56 : i1, f32
    %59 = llvm.getelementptr %10[%23] : (!llvm.ptr<f32, 3>, i32) -> !llvm.ptr<f32, 3>
    llvm.store %58, %59 : !llvm.ptr<f32, 3>
    llvm.br ^bb14
  ^bb14:  // 2 preds: ^bb12, ^bb13
    nvvm.barrier0
    %60 = llvm.icmp "slt" %20, %3 : i32
    llvm.cond_br %60, ^bb15, ^bb16
  ^bb15:  // pred: ^bb14
    %61 = llvm.add %23, %3  : i32
    %62 = llvm.getelementptr %10[%23] : (!llvm.ptr<f32, 3>, i32) -> !llvm.ptr<f32, 3>
    %63 = llvm.load %62 : !llvm.ptr<f32, 3>
    %64 = llvm.getelementptr %10[%61] : (!llvm.ptr<f32, 3>, i32) -> !llvm.ptr<f32, 3>
    %65 = llvm.load %64 : !llvm.ptr<f32, 3>
    %66 = llvm.fcmp "ugt" %63, %65 : f32
    %67 = llvm.select %66, %63, %65 : i1, f32
    %68 = llvm.fcmp "uno" %65, %65 : f32
    %69 = llvm.select %68, %65, %67 : i1, f32
    %70 = llvm.getelementptr %10[%23] : (!llvm.ptr<f32, 3>, i32) -> !llvm.ptr<f32, 3>
    llvm.store %69, %70 : !llvm.ptr<f32, 3>
    llvm.br ^bb16
  ^bb16:  // 2 preds: ^bb14, ^bb15
    nvvm.barrier0
    %71 = llvm.icmp "eq" %20, %8 : i32
    %72 = llvm.and %71, %26  : i1
    llvm.cond_br %72, ^bb17, ^bb20
  ^bb17:  // pred: ^bb16
    %73 = llvm.add %23, %6  : i32
    %74 = llvm.getelementptr %10[%23] : (!llvm.ptr<f32, 3>, i32) -> !llvm.ptr<f32, 3>
    %75 = llvm.load %74 : !llvm.ptr<f32, 3>
    %76 = llvm.getelementptr %10[%73] : (!llvm.ptr<f32, 3>, i32) -> !llvm.ptr<f32, 3>
    %77 = llvm.load %76 : !llvm.ptr<f32, 3>
    %78 = llvm.fcmp "ugt" %75, %77 : f32
    %79 = llvm.select %78, %75, %77 : i1, f32
    %80 = llvm.fcmp "uno" %77, %77 : f32
    %81 = llvm.select %80, %77, %79 : i1, f32
    %82 = llvm.getelementptr %arg6[%25] : (!llvm.ptr<f32>, i32) -> !llvm.ptr<f32>
    %83 = llvm.load %82 : !llvm.ptr<f32>
    llvm.br ^bb18(%83 : f32)
  ^bb18(%84: f32):  // 2 preds: ^bb17, ^bb18
    %85 = llvm.fcmp "ogt" %84, %81 : f32
    %86 = llvm.select %85, %84, %81 : i1, f32
    %87 = llvm.bitcast %82 : !llvm.ptr<f32> to !llvm.ptr<i32>
    %88 = llvm.bitcast %84 : f32 to i32
    %89 = llvm.bitcast %86 : f32 to i32
    %90 = llvm.cmpxchg %87, %88, %89 acq_rel monotonic : !llvm.ptr<i32>, i32
    %91 = llvm.extractvalue %90[0] : !llvm.struct<(i32, i1)> 
    %92 = llvm.bitcast %91 : i32 to f32
    %93 = llvm.extractvalue %90[1] : !llvm.struct<(i32, i1)> 
    llvm.cond_br %93, ^bb19, ^bb18(%92 : f32)
  ^bb19:  // pred: ^bb18
    llvm.br ^bb20
  ^bb20:  // 2 preds: ^bb16, ^bb19
    llvm.br ^bb21
  ^bb21:  // 2 preds: ^bb1, ^bb20
    llvm.return
  }
}

// -----// IR Dump After Canonicalizer (canonicalize) //----- //
func.func @main(%arg0: !disc_ral.context) attributes {tf.entry_function = {input_placements = "gpu", inputs = "input0", output_placements = "gpu", outputs = "output0"}} {
  %c256 = arith.constant 256 : index
  %c32 = arith.constant 32 : index
  %c512 = arith.constant 512 : index
  %0 = llvm.mlir.constant(0 : i32) : i32
  %c0 = arith.constant 0 : index
  %c1 = arith.constant 1 : index
  %c2 = arith.constant 2 : index
  %1 = "disc_ral.dispatch"(%arg0, %c0) {backend_config = "", call_target_name = "ral_recv_input", device = "cpu", has_side_effect = false} : (!disc_ral.context, index) -> memref<?x?x?xf32, "gpu">
  %dim = memref.dim %1, %c2 : memref<?x?x?xf32, "gpu">
  %dim_0 = memref.dim %1, %c1 : memref<?x?x?xf32, "gpu">
  %dim_1 = memref.dim %1, %c0 : memref<?x?x?xf32, "gpu">
  %2 = arith.index_cast %dim_0 : index to i32
  %3 = arith.index_cast %dim : index to i32
  %4 = arith.muli %2, %3 : i32
  %5 = arith.index_cast %4 : i32 to index
  %alloc = memref.alloc(%5) {kDiscSymbolicDimAttr = [@S3]} : memref<?xf32, "gpu">
  %6 = arith.cmpi slt, %dim_1, %5 : index
  scf.if %6 {
    %9 = affine.apply affine_map<()[s0] -> (s0 ceildiv 512)>()[%5]
    gpu.launch_func  @main_kernel::@main_kColReduction_reduce__4_1_0___flat blocks in (%9, %c1, %c1) threads in (%c512, %c1, %c1) args(%c512 : index, %5 : index, %alloc : memref<?xf32, "gpu">)
    %10 = arith.cmpi eq, %5, %c0 : index
    %11 = arith.subi %5, %c1 : index
    %12 = arith.divui %11, %c512 : index
    %13 = arith.addi %12, %c1 : index
    %14 = arith.select %10, %c0, %13 : index
    %15 = arith.cmpi eq, %dim_1, %c0 : index
    %16 = arith.subi %dim_1, %c1 : index
    %17 = arith.divui %16, %c32 : index
    %18 = arith.addi %17, %c1 : index
    %19 = arith.select %15, %c0, %18 : index
    %20 = arith.muli %14, %19 : index
    %21 = arith.muli %20, %c512 : index
    %22 = affine.apply affine_map<(d0) -> (d0 ceildiv 512)>(%21)
    gpu.launch_func  @main_kernel_0::@main_kColReduction_reduce__4_1_0___flat_1 blocks in (%22, %c1, %c1) threads in (%c512, %c1, %c1) args(%5 : index, %1 : memref<?x?x?xf32, "gpu">, %c512 : index, %21 : index, %14 : index, %alloc : memref<?xf32, "gpu">)
  } else {
    %9 = affine.apply affine_map<()[s0] -> (s0 ceildiv 256)>()[%5]
    gpu.launch_func  @main_kernel_1::@main_kColReduction_reduce__4_1_0___thin blocks in (%9, %c1, %c1) threads in (%c256, %c1, %c1) args(%c256 : index, %5 : index, %alloc : memref<?xf32, "gpu">)
    %10 = arith.cmpi eq, %5, %c0 : index
    %11 = arith.subi %5, %c1 : index
    %12 = arith.divui %11, %c32 : index
    %13 = arith.addi %12, %c1 : index
    %14 = arith.select %10, %c0, %13 : index
    %15 = arith.cmpi eq, %dim_1, %c0 : index
    %16 = arith.subi %dim_1, %c1 : index
    %17 = arith.divui %16, %c512 : index
    %18 = arith.addi %17, %c1 : index
    %19 = arith.select %15, %c0, %18 : index
    %20 = arith.muli %14, %19 : index
    %21 = arith.muli %20, %c256 : index
    %22 = affine.apply affine_map<(d0) -> (d0 ceildiv 256)>(%21)
    gpu.launch_func  @main_kernel_2::@main_kColReduction_reduce__4_1_0___thin_1 blocks in (%22, %c1, %c1) threads in (%c256, %c1, %c1) args(%5 : index, %1 : memref<?x?x?xf32, "gpu">, %c256 : index, %21 : index, %14 : index, %alloc : memref<?xf32, "gpu">)
  }
  %7 = llvm.inttoptr %0 : i32 to !llvm.ptr<i8>
  %alloca = memref.alloca() : memref<2xindex, "cpu">
  memref.store %dim_0, %alloca[%c0] : memref<2xindex, "cpu">
  memref.store %dim, %alloca[%c1] : memref<2xindex, "cpu">
  %8 = "disc_ral.dispatch"(%arg0, %7, %alloc, %alloca) {backend_config = "", call_target_name = "inc_ref", device = "gpu", has_side_effect = false} : (!disc_ral.context, !llvm.ptr<i8>, memref<?xf32, "gpu">, memref<2xindex, "cpu">) -> memref<?x?xf32, "gpu">
  %reinterpret_cast = memref.reinterpret_cast %8 to offset: [0], sizes: [%dim_0, %dim], strides: [%dim, 1] {kDiscSymbolicDimAttr = [@S1, @S2]} : memref<?x?xf32, "gpu"> to memref<?x?xf32, "gpu">
  memref.dealloc %alloc : memref<?xf32, "gpu">
  "disc_ral.dispatch"(%arg0, %c0, %reinterpret_cast) {backend_config = "", call_target_name = "ral_send_output", device = "cpu", has_side_effect = false} : (!disc_ral.context, index, memref<?x?xf32, "gpu">) -> ()
  return
}

// -----// IR Dump After SCFToControlFlow (convert-scf-to-cf) //----- //
#map = affine_map<()[s0] -> (s0 ceildiv 512)>
#map1 = affine_map<(d0) -> (d0 ceildiv 512)>
#map2 = affine_map<()[s0] -> (s0 ceildiv 256)>
#map3 = affine_map<(d0) -> (d0 ceildiv 256)>
module attributes {gpu.container_module} {
  func.func @main(%arg0: !disc_ral.context) attributes {tf.entry_function = {input_placements = "gpu", inputs = "input0", output_placements = "gpu", outputs = "output0"}} {
    %c256 = arith.constant 256 : index
    %c32 = arith.constant 32 : index
    %c512 = arith.constant 512 : index
    %0 = llvm.mlir.constant(0 : i32) : i32
    %c0 = arith.constant 0 : index
    %c1 = arith.constant 1 : index
    %c2 = arith.constant 2 : index
    %1 = "disc_ral.dispatch"(%arg0, %c0) {backend_config = "", call_target_name = "ral_recv_input", device = "cpu", has_side_effect = false} : (!disc_ral.context, index) -> memref<?x?x?xf32, "gpu">
    %dim = memref.dim %1, %c2 : memref<?x?x?xf32, "gpu">
    %dim_0 = memref.dim %1, %c1 : memref<?x?x?xf32, "gpu">
    %dim_1 = memref.dim %1, %c0 : memref<?x?x?xf32, "gpu">
    %2 = arith.index_cast %dim_0 : index to i32
    %3 = arith.index_cast %dim : index to i32
    %4 = arith.muli %2, %3 : i32
    %5 = arith.index_cast %4 : i32 to index
    %alloc = memref.alloc(%5) {kDiscSymbolicDimAttr = [@S3]} : memref<?xf32, "gpu">
    %6 = arith.cmpi slt, %dim_1, %5 : index
    cf.cond_br %6, ^bb1, ^bb2
  ^bb1:  // pred: ^bb0
    %7 = affine.apply #map()[%5]
    gpu.launch_func  @main_kernel::@main_kColReduction_reduce__4_1_0___flat blocks in (%7, %c1, %c1) threads in (%c512, %c1, %c1) args(%c512 : index, %5 : index, %alloc : memref<?xf32, "gpu">)
    %8 = arith.cmpi eq, %5, %c0 : index
    %9 = arith.subi %5, %c1 : index
    %10 = arith.divui %9, %c512 : index
    %11 = arith.addi %10, %c1 : index
    %12 = arith.select %8, %c0, %11 : index
    %13 = arith.cmpi eq, %dim_1, %c0 : index
    %14 = arith.subi %dim_1, %c1 : index
    %15 = arith.divui %14, %c32 : index
    %16 = arith.addi %15, %c1 : index
    %17 = arith.select %13, %c0, %16 : index
    %18 = arith.muli %12, %17 : index
    %19 = arith.muli %18, %c512 : index
    %20 = affine.apply #map1(%19)
    gpu.launch_func  @main_kernel_0::@main_kColReduction_reduce__4_1_0___flat_1 blocks in (%20, %c1, %c1) threads in (%c512, %c1, %c1) args(%5 : index, %1 : memref<?x?x?xf32, "gpu">, %c512 : index, %19 : index, %12 : index, %alloc : memref<?xf32, "gpu">)
    cf.br ^bb3
  ^bb2:  // pred: ^bb0
    %21 = affine.apply #map2()[%5]
    gpu.launch_func  @main_kernel_1::@main_kColReduction_reduce__4_1_0___thin blocks in (%21, %c1, %c1) threads in (%c256, %c1, %c1) args(%c256 : index, %5 : index, %alloc : memref<?xf32, "gpu">)
    %22 = arith.cmpi eq, %5, %c0 : index
    %23 = arith.subi %5, %c1 : index
    %24 = arith.divui %23, %c32 : index
    %25 = arith.addi %24, %c1 : index
    %26 = arith.select %22, %c0, %25 : index
    %27 = arith.cmpi eq, %dim_1, %c0 : index
    %28 = arith.subi %dim_1, %c1 : index
    %29 = arith.divui %28, %c512 : index
    %30 = arith.addi %29, %c1 : index
    %31 = arith.select %27, %c0, %30 : index
    %32 = arith.muli %26, %31 : index
    %33 = arith.muli %32, %c256 : index
    %34 = affine.apply #map3(%33)
    gpu.launch_func  @main_kernel_2::@main_kColReduction_reduce__4_1_0___thin_1 blocks in (%34, %c1, %c1) threads in (%c256, %c1, %c1) args(%5 : index, %1 : memref<?x?x?xf32, "gpu">, %c256 : index, %33 : index, %26 : index, %alloc : memref<?xf32, "gpu">)
    cf.br ^bb3
  ^bb3:  // 2 preds: ^bb1, ^bb2
    %35 = llvm.inttoptr %0 : i32 to !llvm.ptr<i8>
    %alloca = memref.alloca() : memref<2xindex, "cpu">
    memref.store %dim_0, %alloca[%c0] : memref<2xindex, "cpu">
    memref.store %dim, %alloca[%c1] : memref<2xindex, "cpu">
    %36 = "disc_ral.dispatch"(%arg0, %35, %alloc, %alloca) {backend_config = "", call_target_name = "inc_ref", device = "gpu", has_side_effect = false} : (!disc_ral.context, !llvm.ptr<i8>, memref<?xf32, "gpu">, memref<2xindex, "cpu">) -> memref<?x?xf32, "gpu">
    %reinterpret_cast = memref.reinterpret_cast %36 to offset: [0], sizes: [%dim_0, %dim], strides: [%dim, 1] {kDiscSymbolicDimAttr = [@S1, @S2]} : memref<?x?xf32, "gpu"> to memref<?x?xf32, "gpu">
    memref.dealloc %alloc : memref<?xf32, "gpu">
    "disc_ral.dispatch"(%arg0, %c0, %reinterpret_cast) {backend_config = "", call_target_name = "ral_send_output", device = "cpu", has_side_effect = false} : (!disc_ral.context, index, memref<?x?xf32, "gpu">) -> ()
    return
  }
  gpu.module @main_kernel attributes {gpu.binary = "P\EDU\BA\01\00\10\008\04\00\00\00\00\00\00\02\00\01\01@\00\00\00\F8\03\00\00\00\00\00\00\F7\03\00\00\00\00\00\00\07\00\01\00P\00\00\00\00\00\00\00\00\00\00\00\11 \00\00\00\00\00\00\00\00\00\00\00\00\00\00\E8\0B\00\00\00\00\00\00\A2\7FELF\02\01\013\07\00\01\00f\02\00\BE\00q\00\01\00!@\0B\07\001\00\80\08\07\00\F5\0E\00P\05P\00@\008\00\03\00@\00\0B\00\01\00\00.shstrtab\00.\08\00'ym\08\00\F0\1B_shndx\00.nv.info\00.text.main_kColReduction_r\0A\00\F5\00e__4_1_0___flat7\00\0F1\00\1Aoshared3\00\1A\9Fconstant06\00\17\FA\01debug_frame\00.rel\11\00!nv\14\00\11aB\00\0F'\01 \0F\87\00\14\0FO\01\B6o_paramV\01\1C\0F\01\00\07\8CZ\00\00\00\03\00\0A\00\01\00\11\EC\18\00,\09\00\01\00 )\01\18\00,\04\00\01\00\11G\18\00,\07\00\01\00f2\00\00\00\12\10`\00!\80\01\07\00v\00\FF\FF\FF\FF(\00\0C\00\00\01\00@\03\00\04|\08\00\D1\0F\0C\81\80\80(\00\08\FF\81\80(\08\0B\00\06(\00/0\00\01\00\03\13pX\00\10\04\9B\00R\04\14\00\00\00E\00\01\0B\00\00\13\00p/\08\00\05\00\00\00\9F\03\22\04#\0C\00\00\01\00'\04\12\0C\00\17\11\0C\00!7\04\E0\03\F3\08\015\00\00\04\0A\08\00\02\00\00\00`\01\10\00\03\19\10\00\04\17\0C\04\04U\08\00\00\F0!\10\00\10\01\18\01%\F0\11\10\00\01\01\00\F2\02\F0\11\00\03\1B\FF\00\04\1C\08\00P\00\00\00\B0\00\01\00#K\00\01\00s\02\02\08\10\0A/\22\9B\00\00\07\00\03\08\00\13\10\08\00\13\18\08\00\13 \08\00\13(\08\00\130\08\00\228\08|\00\13\00\08\00\13\08\08\00\13\10\08\00\13\18\08\00\13 \08\00\13(\08\00\130\08\00\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\A2\00\00\00\14,\00\00\00H\00\01\00\000\01/\05\00\01\00\FF\B8A\02z\01\00'\04\B1\0F\00\00\00\C4\0F\00\19y\02\00\01\00\10%\93\02Q\0E\00\19y\03\0F\00\F5\1A\00!\00\00\00$\0E\00$z\02\02\00X\00\00\03\02\8E\07\00\CA\1F\00\0Cz\00\02\00Y\00\00p`\F0\03\00\DA\0F\00M[\04\A0\80\03\00\EA\0F\005t\03\FF\BB\03\10\FF\C8\03P\E2\0F\00\02x>\02B\80\FF\00\0F\10\00r\B9z\04\00\00F\00\84\00\94\D0\0F\00%v\02\02\00Z`\00`\0F\00\86y\00\022\00@\04\19\10\0C0\009My\00`\00PGy\00\00\F0A\04\A6\FF\83\03\00\C0\0F\00\18y\00\01\00\0F\10\00\90\0F\01\00-\00W\01.\03\00\01\00\22@\00\01\00='\01\000\00\08\01\00\1F\0B@\00\04\13g)\00\1FV@\00\0C\13\13\14\04\0C\01\00\13\C0\15\00&\90\008\04#\04\00\8D\04\00\FE\04\12\00\01\00\1F\FAT\00\00\00\01\00\13P\95\00/p\00\80\00\0B\1F)'\00\03#\00\C0@\00\04X\06\04\E4\00*\04\00\01\00\1F`@\00\04\13\F01\00&\\\00@\00\1F\0A@\00\00!\18\01D\01\0D@\00\13P)\00*\D8\00\01\00\1B\08\08\00?\07\01\00\8E\07\00Q\00\00(\05\00\01\00&\10\00\80\00\17\048\00\04\18\00\13\C4\14\01\0C\84\01*8\05@\07\1F\00\C0\00\04\132@\00+\06\00\01\00\1A\07\D8\07\12\03\F8\05:\08\80\00\01\00\13\06\10\06\04(\0B\0C\01\00*\A8\00\08\00\04\F8\00\13\018\00\04\A8\00\0C\01\009H\03\00\08\00\088\00/\06\00\01\00\17\80\08\00\00\00\00\00\00\00\00"} {
    llvm.func @main_kColReduction_reduce__4_1_0___flat(%arg0: i32, %arg1: i32, %arg2: !llvm.ptr<f32>) attributes {disc.elimargs = [2 : index, 4 : index, 5 : index, 6 : index], gpu.kernel, nvvm.kernel} {
      %0 = llvm.mlir.constant(0xFF800000 : f32) : f32
      %1 = nvvm.read.ptx.sreg.ctaid.x : i32
      %2 = nvvm.read.ptx.sreg.tid.x : i32
      llvm.br ^bb1
    ^bb1:  // pred: ^bb0
      %3 = llvm.mul %1, %arg0  : i32
      %4 = llvm.add %2, %3  : i32
      %5 = llvm.icmp "ult" %4, %arg1 : i32
      llvm.cond_br %5, ^bb2, ^bb3
    ^bb2:  // pred: ^bb1
      %6 = llvm.getelementptr %arg2[%4] : (!llvm.ptr<f32>, i32) -> !llvm.ptr<f32>
      llvm.store %0, %6 : !llvm.ptr<f32>
      llvm.br ^bb3
    ^bb3:  // 2 preds: ^bb1, ^bb2
      llvm.return
    }
  }
  gpu.module @main_kernel_0 attributes {gpu.binary = "P\EDU\BA\01\00\10\00\90\0A\00\00\00\00\00\00\02\00\01\01@\00\00\00P\0A\00\00\00\00\00\00P\0A\00\00\00\00\00\00\07\00\01\00P\00\00\00\00\00\00\00\00\00\00\00\11 \00\00\00\00\00\00\00\00\00\00\00\00\00\00\E8\1C\00\00\00\00\00\00\A2\7FELF\02\01\013\07\00\01\00f\02\00\BE\00q\00\01\00!@\1C\07\001\00\80\19\07\00\F5\0E\00P\05P\00@\008\00\03\00@\00\0B\00\01\00\00.shstrtab\00.\08\00'ym\08\00\F0\1B_shndx\00.nv.info\00.text.main_kColReduction_r\0A\00\F5\02e__4_1_0___flat_19\00\0F3\00\1Coshared5\00\19Orela\9D\00\1E?rel\D1\00!\9Fconstant08\00\19\B2debug_framey\00\09\11\00!nv\14\00\11aD\00\0F\98\01 \0F\89\00\16\0F\C2\01\FF(o_param\C9\01\1C\0F\01\00\0B\8C\\\00\00\00\03\00\0A\00\01\00 ]\01\18\00,\09\00\01\00\11\9C\18\00,\04\00\01\00\11\BA\18\00,\07\00\01\00f2\00\00\00\12\10`\00!\80\11\07\00v\00\FF\FF\FF\FF(\00\0C\00\00\01\00@\03\00\04|\08\00\D1\0F\0C\81\80\80(\00\08\FF\81\80(\08\0B\00\06(\00/0\00\01\00\03\13pX\00\10\04\9B\00R\04\14\00\00\00E\00\22\04 \AC\00\90\04/\08\00\05\00\00\00\16\0C\00\12#\0C\00\00\01\00'\04\12\0C\00\17\11\0C\00!7\04\C8\04\F1\08\015\00\00\04\0A\08\00\02\00\00\00`\01(\00\03\19(\00\04\17\0C$\00u\06\00 \00\00\F0!\10\00u\05\00\1C\00\00\F0\11\10\009\04\00\18\10\009\03\00\14\10\009\02\00\10\10\009\01\00\08P\00\01\01\00\F2\0A\F0\11\00\03\1B\FF\00\04\1C\0C\00P\00\00\00\10\02\00\00\E0\10\00\00\04\1E\D8\00#K\00\01\00\B1\02\02\08\10\0A/\22\00\00\00\08\10\00#\00\08\08\00\13\10\08\00\13\18\08\00\13 \08\00\13(\08\00\130\08\00\228\08\84\00\13\00\08\00\13\08\08\00\13\10\08\00\13\18\08\00\13 \08\00\13(\08\00\130\08\00\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\A2\00\00\00\14,\00\00\00H\00\01\00\00x\01/\05\00\01\00\FF\88@$v\01\FF?\04\B2\FF\00\8E\07\00\C4\0F\00\19y\00\01\00\10%c\02Q\0E\00\19y\03\0F\00 \00!-\00@\0E\00$z]\04\B0\00\00\03\02\8E\07\00\CA\1F\00\0C\10\00\C5^\00\00p`\F0\03\00\DA\0F\00Ms\04\F0\0C\80\03\00\EA\0F\00\06{\04\00\00_\00\00\00\90 \00\00\22\0E\00\19x\05\FF\1F\1F\00\D3\14\01\00\00\E2\0F\00$r\02\FF\FF\00\80\00 \E2\0FP\00\10\FF0\00\B1pP\F4\03\00\E4\0F\00\11r\05?\02\B2\FFH\8F\07\00\C6\0F\00\08s\04\BE\04\10\10\90\00\F1\07\1E\00\10x\03\04\FE\FF\FF\0F\FF\E0\FF\07\00\CC\1F\00\05s\03\00\B2\00\22\F0!\B0\00!r\07`\00\C0\03\0A\8E\07\00\C8\1F\00$z\07\07`\00\10\FF\C0\00\81\C8\0F\00'r\07\03\07\A8\02\02\80\00@\19x\02\FF\C3\00\10\05\A0\00\E1\E4\0F\00\12x\05\05\00\FE\FF\FF\FF\C0\8E\80\00T'r\07\07\02\B0\00\10\C8\C0\00\11\09`\00#\07\0A\10\000z\03\09`\00\11\02 \01\01\D0\006\03\00_ \010\10\0A\03\10\00\11\80\B0\00\80\E4\0F\00\10\08\07\07\01P\00\03\10\00\060\00\12\F20\00\1A\18 \001\12\AA\07 \01\22\FF3`\00;$r\03\80\00%\02\03\80\00`\E4\0F\00$x\03\14\04$\00\05 \000x\03\02\95\03\18\03\A0\00\1FX\C0\01\07W$x\02\07 \A0\01\81\B9z\04\00\00F\00\00s\06\00\C0\01Dt\00\FF\04 \00\11\C6P\00\10\02\95\060pb\F8\C0\01U\04\10x\06\02\C0\00\11\C8 \00\12\06 \00\C3\FA\03\00\CE\0F\00$\CA\05\02\00X\90\00\90\C8\0F\00%\C6\04\05\00Z\91\01\03\B0\005\DA\07\06 \00a\E2\0F\00\81\C9\0A#\06\C5\00\19\1E\0C\00\A6\00\00%\D6\06\070\00u\CA\0F\00\81\D9\0D\06 \00\86\E2\02\00\10x\08\02\02@\01j\04\10x\10\02\03p\01\12\08\90\00#\F0\03\10\00\12\10\10\00\11\F2\10\00Y\10x\0C\02\04@\004\0E\02\05\10\00\11\C40\00\12\0C0\00 \F4\03\80\00E$\8A\09\08\B0\00w\E4\0F\10$\9A\0B\10\10\00U\00%\86\08\09\B0\00\02\F0\02\12\0E@\00 \F6\03\F0\00E\81\89\05\08\C0\00v\22\11\00%\96\06\0B0\00 /\00\C0\00\14\06\80\00w\C6\0F\00$\AA\0B\0C \01F\10\81\99\0F\00\01\B4\22\03\00$t\04\FF\00\00\80\FF\C0\01\00\D0\00\14\07@\00\00\C0\015\BA\09\0E@\00W\C8\1F\00%\B6\A0\00g\CC\0F\00\81\B9\09\90\00@\01\00\0B\C8\FA\01\D2\80\FF\00\C2\FC\03\00\C8O\00\08\C8\04\10\000\02\00\03P\007%\A6\0A\B0\00\08P\01\01\10\02\F5\06\00\0B\D2\00\04\0D\00\00@\00`\FC\03\00\C6\8F\00\81\A9\0B\0A`\00b\E2\04\00\08\D2\04 \00!\00\000\00\06`\01\12\FA`\01&\CA\07P\01z\C8/\00%\C6\06\070\02*\0D\0C0\02\16\11\10\01y\E6\02\00%\D6\0C\0D\E0\006\D9\0D\0C\80\00t\00\00\0B\82\00\04\05\A0\00\84\E4\0F\09\10x\0A\02\080\01\8A\E4O\00\08\82\04\04\05\A0\00\18\0A@\02V\0B\92\00\04\0F@\00\01\80\02\14\09@\00\8A\C4\1F\00\08\92\04\04\0F@\00\1A\08p\028\0E\02\0A\E0\016\8A\07\0A\D0\00\11/\C0\01\17\0B \00+%\86\00\03&\89\05\F0\00!\A2\000\02\14\0C0\00\00\A0\00D\A2\00\04\0B\A0\00\8A\C8\8F\00\08\A2\04\04\0B\90\00\15\0E\D0\02\000\00D\B2\00\04\090\00\000\02&\9A\0B\E0\02\00\90\05D\B2\04\04\09@\00\00\00\02(\96\08\00\02\16\08\C0\01\12\F6P\00D\C2\00\04\11P\00x\E4\0F\08\81\99\0F\08\B0\03H$\AA\0B\0E\C0\01Z\08\C2\04\04\11\A0\00\06P\02\0Fp\02\00\00@\01\17\0D\A0\03\09p\02f\E2\0F\08$\BA\07 \02*\E4\1F\80\02.\22\01\80\02\05\10\01\12\FAp\00\1B\B6p\028\CA\09\10\A0\01'\81\B9p\02{&\03\00%\C6\08\09\A0\02\1A\0E\A0\02\16\13\00\01/&\01\A0\02\0B+\22\03\A0\02 O\08\A0\02\15\0E\E0\01\1F\1F\A0\02\0E\00 \01\1B\0F\10\05\17\100\01\00`\02\14\11\10\00*\CE/\80\02\1F\C8p\02\0F\09\00\03/\C8\8F\F0\02\01\18\0E\F0\02\0A\90\02/\0F\01\90\02\00\1C\10\90\02\05@\02\00 \03)\9A\09P\01'\08\B20\02\11\C6\90\02\06\80\01\0F\90\02\04\14\13P\000\E4\0F\08 \01\14\12\00\01\11\E2`\05\18\10`\00\0C\B0\02\00\A0\02\1E\13\E0\00\0F\A0\02\06\09\90\02\01p\018\10\02\13p\00\08\A0\02*\E2\1F\90\02-\E2\0F\B0\02\06 \01\0F\A0\02\06\1F\0E\A0\02\1C\1F\10\A0\02=\1F\14\A0\02\1C\1B\15\A0\02\1B\16\A0\02\1F\17\A0\02\A7\19\10\A0\02\02@\00\06\90\02\1D\E2\B0\02\0F\A0\02\02\14\18\00\01\0F\A0\02\1D\1F\C4\A0\02\0E\19\08\A0\02\01p\00?\10\02\19\A0\02U\10\E4\A0\02\16\07\B0\01\1F$\A0\02\11\1F\11\A0\02.\1F\1A\A0\02\1A5\06\02\1B0\00\11/\10\0A\1B\1C\A0\02\14\1D\10\00\11\CE\10\0A\0B\A0\02+\08\09\A0\02\1C\08\B0\07\19\1E\00\036\02\02\1F\10\00\0F\C0\02\11\1F\06\C0\02\1C\1C\0E\C0\02\19\07\C0\02,\0B\06`\05\14\07@\00+\C6\0F\A0\0A\0F\F0\07\0C\00\80\07\08\E0\07\10\E4\B0\02,\07\06\B0\02\1E\11`\03\0FP\05\13\11\C6\D0\0A\0F\A0\02\18\08\B0\0C\04\A0\02\0Bp\02\17\CAp\02\1B\E4\10\0B\10$\B0\0C)\0C\0D0\009\DA\0F\020\009\C9\0D\0C0\00:\D6\0E\0F\A0\02(\0F\0E\90\00g%v\02\03\00` \025y\06\02 \00*b!\C0\02*\C8O\B0\02\12\C8 \02\02\C0\01\15\F0 \02\03\B0\018\00\00\C8\10\02\1C\F0\10\02\02 \00\05\F0\09\01 \00\08\E0\09\02 \00\14\C2p\01\03 \00\15\C2`\01\02 \00\14\D2\A0\02\03 \00\15\D2\A0\02\10\00`\0C2\0Br\00\F0\01 @\F0p\017\0EFyp\0ES\E6\0F\00\08r\10\02\00\01\00\80\CC\0F\00\A9s\07\02\06\E0\0E\B0\E1\1E\00\00\A4\0E\00\0Cr\00\07\10\00 pR@\00RO\00$r\06\00\0F\00\E0\0C\80\D8\0F\00G\09\00\00\90i\14!\FF\83\C0\0E*My\D0\0ETGy\00\00\F0 \00f\C0\0F\00\18y\00\01\00\0F\10\00`\0F\01\00-\00\\\0F.\03\00\01\00\02q\01]\00\00\98\01\000\00\08\01\00\1F\0B@\00\04\13\D8)\00\1F\C9@\00\0C\13\13\E4\13\0C\01\00\13\A8U\00\11\90\06\00\02$\00#\04\00]\14\00\CE\14\12\00\01\00.k\01T\00\00\01\00\138\05\02/p\00\80\00\0B\1F)'\00\03#\00\A8@\00\04p\16\04\E4\00*\04\00\01\00\1Fb@\00\04\13\D8)\00&\A8\00@\00\1F\0A@\00\00!\89\01D\01\0D@\00\11\80\B6\12J\00\00\D8\00\01\00\1B\08\08\00?x\01\00\A6\17\000\00\00X\E5\02\03\E7\12\04\80\00\17\048\00\04\18\00\133@\01\0C\84\01\13h@\00\17\881\01\0F\C0\00\01\132T\01+\06\00\01\00\1A\08\F0\17\12\03\C8\15:\16\80\00\01\00\13\06\E0\15\04(\1C\0D\88\01\1A\00\08\00\04\C0\00\13\018\00\04\A8\00\0C\01\009\18\13\00\08\00\088\00/\06\00\01\00\17\80\08\00\00\00\00\00\00\00"} {
    llvm.func @__nv_fabsf(f32) -> f32
    llvm.func @main_kColReduction_reduce__4_1_0___flat_1(%arg0: i32, %arg1: !llvm.ptr<f32>, %arg2: i32, %arg3: i32, %arg4: i32, %arg5: i32, %arg6: !llvm.ptr<f32>) attributes {disc.elimargs = [1 : index, 3 : index, 5 : index, 6 : index, 7 : index, 8 : index, 9 : index, 13 : index, 15 : index, 16 : index, 17 : index], gpu.kernel, nvvm.kernel} {
      %0 = llvm.mlir.constant(0 : index) : i32
      %1 = llvm.mlir.constant(1 : index) : i32
      %2 = llvm.mlir.constant(512 : index) : i32
      %3 = llvm.mlir.constant(32 : index) : i32
      %4 = llvm.mlir.constant(0xFF800000 : f32) : f32
      %5 = nvvm.read.ptx.sreg.ctaid.x : i32
      %6 = nvvm.read.ptx.sreg.tid.x : i32
      llvm.br ^bb1
    ^bb1:  // pred: ^bb0
      %7 = llvm.mul %5, %arg3  : i32
      %8 = llvm.add %6, %7  : i32
      %9 = llvm.icmp "ult" %8, %arg4 : i32
      llvm.cond_br %9, ^bb2, ^bb14
    ^bb2:  // pred: ^bb1
      %10 = llvm.srem %8, %2  : i32
      %11 = llvm.sdiv %8, %2  : i32
      %12 = llvm.udiv %11, %arg5  : i32
      %13 = llvm.urem %11, %arg5  : i32
      %14 = llvm.mul %13, %2  : i32
      %15 = llvm.add %14, %10  : i32
      %16 = llvm.icmp "ult" %15, %arg0 : i32
      llvm.cond_br %16, ^bb3, ^bb13
    ^bb3:  // pred: ^bb2
      %17 = llvm.mul %12, %3  : i32
      llvm.br ^bb4(%0, %4 : i32, f32)
    ^bb4(%18: i32, %19: f32):  // 2 preds: ^bb3, ^bb9
      %20 = llvm.icmp "slt" %18, %3 : i32
      llvm.cond_br %20, ^bb5, ^bb10
    ^bb5:  // pred: ^bb4
      %21 = llvm.add %17, %18  : i32
      %22 = llvm.icmp "slt" %21, %arg2 : i32
      llvm.cond_br %22, ^bb6, ^bb7
    ^bb6:  // pred: ^bb5
      %23 = llvm.mul %21, %arg0  : i32
      %24 = llvm.add %23, %15  : i32
      %25 = llvm.getelementptr %arg1[%24] : (!llvm.ptr<f32>, i32) -> !llvm.ptr<f32>
      %26 = llvm.load %25 : !llvm.ptr<f32>
      %27 = llvm.call @__nv_fabsf(%26) : (f32) -> f32
      %28 = llvm.fcmp "oge" %19, %27 : f32
      %29 = llvm.select %28, %19, %27 : i1, f32
      llvm.br ^bb8(%29 : f32)
    ^bb7:  // pred: ^bb5
      llvm.br ^bb8(%19 : f32)
    ^bb8(%30: f32):  // 2 preds: ^bb6, ^bb7
      llvm.br ^bb9
    ^bb9:  // pred: ^bb8
      %31 = llvm.add %18, %1  : i32
      llvm.br ^bb4(%31, %30 : i32, f32)
    ^bb10:  // pred: ^bb4
      %32 = llvm.getelementptr %arg6[%15] : (!llvm.ptr<f32>, i32) -> !llvm.ptr<f32>
      %33 = llvm.load %32 : !llvm.ptr<f32>
      llvm.br ^bb11(%33 : f32)
    ^bb11(%34: f32):  // 2 preds: ^bb10, ^bb11
      %35 = llvm.fcmp "ogt" %34, %19 : f32
      %36 = llvm.select %35, %34, %19 : i1, f32
      %37 = llvm.bitcast %32 : !llvm.ptr<f32> to !llvm.ptr<i32>
      %38 = llvm.bitcast %34 : f32 to i32
      %39 = llvm.bitcast %36 : f32 to i32
      %40 = llvm.cmpxchg %37, %38, %39 acq_rel monotonic : !llvm.ptr<i32>, i32
      %41 = llvm.extractvalue %40[0] : !llvm.struct<(i32, i1)> 
      %42 = llvm.bitcast %41 : i32 to f32
      %43 = llvm.extractvalue %40[1] : !llvm.struct<(i32, i1)> 
      llvm.cond_br %43, ^bb12, ^bb11(%42 : f32)
    ^bb12:  // pred: ^bb11
      llvm.br ^bb13
    ^bb13:  // 2 preds: ^bb2, ^bb12
      llvm.br ^bb14
    ^bb14:  // 2 preds: ^bb1, ^bb13
      llvm.return
    }
  }
  gpu.module @main_kernel_1 attributes {gpu.binary = "P\EDU\BA\01\00\10\008\04\00\00\00\00\00\00\02\00\01\01@\00\00\00\F8\03\00\00\00\00\00\00\F7\03\00\00\00\00\00\00\07\00\01\00P\00\00\00\00\00\00\00\00\00\00\00\11 \00\00\00\00\00\00\00\00\00\00\00\00\00\00\E8\0B\00\00\00\00\00\00\A2\7FELF\02\01\013\07\00\01\00f\02\00\BE\00q\00\01\00!@\0B\07\001\00\80\08\07\00\F5\0E\00P\05P\00@\008\00\03\00@\00\0B\00\01\00\00.shstrtab\00.\08\00'ym\08\00\F0\1B_shndx\00.nv.info\00.text.main_kColReduction_r\0A\00\F5\00e__4_1_0___thin7\00\0F1\00\1Aoshared3\00\1A\9Fconstant06\00\17\FA\01debug_frame\00.rel\11\00!nv\14\00\11aB\00\0F'\01 \0F\87\00\14\0FO\01\B6o_paramV\01\1C\0F\01\00\07\8CZ\00\00\00\03\00\0A\00\01\00\11\EC\18\00,\09\00\01\00 )\01\18\00,\04\00\01\00\11G\18\00,\07\00\01\00f2\00\00\00\12\10`\00!\80\01\07\00v\00\FF\FF\FF\FF(\00\0C\00\00\01\00@\03\00\04|\08\00\D1\0F\0C\81\80\80(\00\08\FF\81\80(\08\0B\00\06(\00/0\00\01\00\03\13pX\00\10\04\9B\00R\04\14\00\00\00E\00\01\0B\00\00\13\00p/\08\00\05\00\00\00\9F\03\22\04#\0C\00\00\01\00'\04\12\0C\00\17\11\0C\00!7\04\E0\03\F3\08\015\00\00\04\0A\08\00\02\00\00\00`\01\10\00\03\19\10\00\04\17\0C\04\04U\08\00\00\F0!\10\00\10\01\18\01%\F0\11\10\00\01\01\00\F2\02\F0\11\00\03\1B\FF\00\04\1C\08\00P\00\00\00\B0\00\01\00#K\00\01\00s\02\02\08\10\0A/\22\9B\00\00\07\00\03\08\00\13\10\08\00\13\18\08\00\13 \08\00\13(\08\00\130\08\00\228\08|\00\13\00\08\00\13\08\08\00\13\10\08\00\13\18\08\00\13 \08\00\13(\08\00\130\08\00\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\A2\00\00\00\14,\00\00\00H\00\01\00\000\01/\05\00\01\00\FF\B8A\02z\01\00'\04\B1\0F\00\00\00\C4\0F\00\19y\02\00\01\00\10%\93\02Q\0E\00\19y\03\0F\00\F5\1A\00!\00\00\00$\0E\00$z\02\02\00X\00\00\03\02\8E\07\00\CA\1F\00\0Cz\00\02\00Y\00\00p`\F0\03\00\DA\0F\00M[\04\A0\80\03\00\EA\0F\005t\03\FF\BB\03\10\FF\C8\03P\E2\0F\00\02x>\02B\80\FF\00\0F\10\00r\B9z\04\00\00F\00\84\00\94\D0\0F\00%v\02\02\00Z`\00`\0F\00\86y\00\022\00@\04\19\10\0C0\009My\00`\00PGy\00\00\F0A\04\A6\FF\83\03\00\C0\0F\00\18y\00\01\00\0F\10\00\90\0F\01\00-\00W\01.\03\00\01\00\22@\00\01\00='\01\000\00\08\01\00\1F\0B@\00\04\13g)\00\1FV@\00\0C\13\13\14\04\0C\01\00\13\C0\15\00&\90\008\04#\04\00\8D\04\00\FE\04\12\00\01\00\1F\FAT\00\00\00\01\00\13P\95\00/p\00\80\00\0B\1F)'\00\03#\00\C0@\00\04X\06\04\E4\00*\04\00\01\00\1F`@\00\04\13\F01\00&\\\00@\00\1F\0A@\00\00!\18\01D\01\0D@\00\13P)\00*\D8\00\01\00\1B\08\08\00?\07\01\00\8E\07\00Q\00\00(\05\00\01\00&\10\00\80\00\17\048\00\04\18\00\13\C4\14\01\0C\84\01*8\05@\07\1F\00\C0\00\04\132@\00+\06\00\01\00\1A\07\D8\07\12\03\F8\05:\08\80\00\01\00\13\06\10\06\04(\0B\0C\01\00*\A8\00\08\00\04\F8\00\13\018\00\04\A8\00\0C\01\009H\03\00\08\00\088\00/\06\00\01\00\17\80\08\00\00\00\00\00\00\00\00"} {
    llvm.func @main_kColReduction_reduce__4_1_0___thin(%arg0: i32, %arg1: i32, %arg2: !llvm.ptr<f32>) attributes {disc.elimargs = [2 : index, 4 : index, 5 : index, 6 : index], gpu.kernel, nvvm.kernel} {
      %0 = llvm.mlir.constant(0xFF800000 : f32) : f32
      %1 = nvvm.read.ptx.sreg.ctaid.x : i32
      %2 = nvvm.read.ptx.sreg.tid.x : i32
      llvm.br ^bb1
    ^bb1:  // pred: ^bb0
      %3 = llvm.mul %1, %arg0  : i32
      %4 = llvm.add %2, %3  : i32
      %5 = llvm.icmp "ult" %4, %arg1 : i32
      llvm.cond_br %5, ^bb2, ^bb3
    ^bb2:  // pred: ^bb1
      %6 = llvm.getelementptr %arg2[%4] : (!llvm.ptr<f32>, i32) -> !llvm.ptr<f32>
      llvm.store %0, %6 : !llvm.ptr<f32>
      llvm.br ^bb3
    ^bb3:  // 2 preds: ^bb1, ^bb2
      llvm.return
    }
  }
  gpu.module @main_kernel_2 attributes {gpu.binary = "P\EDU\BA\01\00\10\00(\0C\00\00\00\00\00\00\02\00\01\01@\00\00\00\E8\0B\00\00\00\00\00\00\E3\0B\00\00\00\00\00\00\07\00\01\00P\00\00\00\00\00\00\00\00\00\00\00\11 \00\00\00\00\00\00\00\00\00\00\00\00\00\00(:\00\00\00\00\00\00\A2\7FELF\02\01\013\07\00\01\00f\02\00\BE\00q\00\01\00#\809\08\00\116\07\00\F5\0E\00P\05P\00@\008\00\03\00@\00\0C\00\01\00\00.shstrtab\00.\08\00'ym\08\00\F0\1B_shndx\00.nv.info\00.text.main_kColReduction_r\0A\00\F5\02e__4_1_0___thin_19\00\0F3\00\1Coshared5\00\19Orela\9D\00\1E?rel\D1\00!\9Fconstant08\00\19\B2debug_framey\00\09\11\00!nv\14\00\11aD\00\0F\98\01 \0F\89\00\16\0F\C2\01\EF\8F$____wg_2\00\16\00\0B\00/27\FA\01&o_param\01\02\1C\0F\01\00\0B\8C\\\00\00\00\03\00\0A\00\01\00\11\BF\18\00,\0B\00\01\00 \95\01\18\00,\09\00\01\00\11\D4\18\00,\04\00\01\00\11\F2\18\00,\07\00\01\00g2\00\00\00\12\10x\00\11.\06\00v\00\FF\FF\FF\FF(\00\0C\00\00\01\00@\03\00\04|\08\00\D1\0F\0C\81\80\80(\00\08\FF\81\80(\08\0B\00\06(\00/0\00\01\00\03!\F0-\07\00 \00\04\9B\00R\04\14\00\00\00E\00\22\044\DC\00\90\04/\08\00\06\00\00\00\15\0C\00\12#\0C\00\00\01\00'\04\12\0C\00\17\11\0C\00!7\04\18\05\F1\08\015\00\00\04\0A\08\00\03\00\00\00`\01(\00\03\19(\00\04\17\0C$\00u\06\00 \00\00\F0!\10\00u\05\00\1C\00\00\F0\11\10\009\04\00\18\10\009\03\00\14\10\009\02\00\10\10\009\01\00\08P\00\01\01\00\F2\0A\F0\11\00\03\1B\FF\00\04\1C\0C\00P\00\00\000,\00\000-\00\00\04\1E\84\01#K\00\01\00\B1\02\02\08\10\0A/\22\00\00\00\08\10\00#\00\08\08\00\13\10\08\00\13\18\08\00\13 \08\00\13(\08\00\130\08\00\228\08\84\00\13\00\08\00\13\08\08\00\13\10\08\00\13\18\08\00\13 \08\00\13(\08\00\130\08\00\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\A2\00\00\00\14,\00\00\00H\00\01\00\11\02h\01\0F\01\00\FF\B8@$v\01\FF\87\04\B2\FF\00\8E\07\00\C4\0F\00\19y\00\01\00\10%\93\02a\0E\00\19y\03\00\01\00\10!-\00\F0\04\0E\00$z\00\00\00]\00\00\03\02\8E\07\00\CA\1F\00\0C\10\00\C5^\00\00p`\F0\03\00\DA\0F\00M\A3\04\F0\0C\80\03\00\EA\0F\00\06{\04\00\00_\00\00\00\90 \00\00\22\0E\00\19x\05\FF\1F\1F\00\F1\00\14\01\00\00\E2\0F\00\B9z\04\00\00F\00\00\0B\05 \E2\0FP\00\10\FF0\00@pP\F4\03\10\00cEy\00\00 *P\00p\E2\0F\00\11r\05\05\0E\00\B2\FF@\8F\07\00\C6\0F\00\08s\04\FE\04\10\10\A0\00\F1\06\1E\00\10x\02\04\FE\FF\FF\0F\FF\E0\FF\07\00\CC\1F\00\05s\03\B4\02\00\C8\03\A3\00d\00\00$r\02\FF\FF\00\F0\00a\E4\1F\00$r\07\10\00\C0\03\0A\8E\07\00\C8/\00$z\07\07\80\00\10\FF\E0\00\81\C8\0F\00'r\06\03\07\F8\02 \8E\07\80\00@\19x\03\FF\19\03\10\05\C0\00q\E4\0F\00\12x\05\05\09\05!\C0\8E\90\00T'r\06\06\03`\00a\C8\0F\00$r\04`\00#\06\0A\10\00Tz\02\04\00_@\01\01\F0\006\02\00_@\010\10\0A\02\10\00\11\80\C0\00\80\E4\0F\00\10\08\06\06\01P\00\03\10\00\060\00\11\F2\10\010$x\02$\04A\00\05\0A\8E \00\A2$t\05\FF\00\00\80\FF\FF\00\90\00@\12x\00\02p\01\22\FF\C0 \00@\19x\07\FFA\01@\02\16\01\00\80\00*\10\18`\001\12\AA\06\80\01\22\FF30\00\EF\0Cx\00\02\7F\00\00\00p@\F4\03\00\C6\D0\00\01\16\03\D0\00\00\10\00@x\03\03 p\01\12\02@\00\22$x\81\04\22\07\02\90\00h\0Cz\00\03\00X\C0\00\00\98\05\030\01\96\D8\0F\00G\19\00\00 (@\02E$x\04\06@\00\01`\004\04\04@0\00\02@\01\10\04M\07\C7pb\F8\03\00\E4\0F\04\10x\08\04\D0\00g\04\10x\0A\04\02\E0\00B\0Cz\00\080\00\A5\FA\03\00\CE\0F\00$\C4\07\FF\80\00\00\B0\00V\CA\06\04\00X\D0\00c%\C6\06\06\00Z\80\00\02\A0\02\12\0A@\00 \F0\03 \00@\81\C9\0C\06@\00\B6\00\19\1E\0C\00\A2\00\00$\D4\09P\00\84\E2\0F\00\10x\0E\04\03\80\00\00\F0\009\DA\08\08`\000\D6\08\08`\00\16\09`\00\12\0E`\00 \F6\03\E0\00E\81\D9\08\08`\00j\E2\02\00$\84\0B\B0\009\8A\0A\0AP\000\86\0A\0AP\00\10\0B\10\00u\CC\0F\00\81\89\0A\0A@\00X\22\0F\00$\B4\F0\00i\1F\00$\BA\06\0E@\00\17\B6\F0\00\00\80\00&\B9\0E\E0\00\88b\01\00\10x\10\04\04\D0\00\08p\02p\E2\0F\00\0B\C8\00\0C\10\00\B2\00b\FC\03\00\C8O\00\08\C8\09\10\00\B1\02\00\03\00\E4/\04\0B\C2\00\0CN\07\10\80 \00r\0F\00\08\C2\05\0C\09Y\06\13\03\A0\01\17\10\D0\01\F4\07\00\0B\D2\00\05\08\00\00@\00\C0\FC\03\00\E4\8F\08\10x\0C\04\05\80\00\82\E4\0F\00\08\D2\09\08\05@\00\00\10\003\04\0B\D2\E9\06\04`\005\D2\05\08`\00\1B\E2\F0\01\11\E2\C0\04\15\0C\10\02\11\E2\00\02\15\10\10\01\00\D0\00E\82\00\05\0A\80\00 \0F\09\00\01\17\06\00\01\0C \02I\08\82\09\0A\90\00D\82\00\0A\0A\90\00\10\C60\02\17\060\02i\0E\00\08\82\05\0A\A0\00\0B@\02\06\10\01#\F0\03@\02\19\0C\A0\00F\B2\00\05\0E\A0\00\10\0A \01\18\07\A0\00\0B`\02I\08\B2\0B\0E\A0\00H\B2\00\0E\0E\A0\00\09p\02x\0E\00\08\B2\05\0E\0B\A0\00\08\80\02'\E2\0F@\01\12\F6 \00?\8A\0A\10\90\02\16\10$\90\02\16\0DP\00\00\D0\02:\BA\0C\0C\90\02 \0C\0C@\00\14\0D@\005\B9\0C\0C@\00\12b`\03\14\08\F0\00\01\F0\036\10\04\09\10\00d\00\0B\C2\00\05\06 \01\00\90\029\C2\07\06\00\01D\C2\00\06\06\00\01\02\90\02$\06\07\F0\00\18\E4\90\03\0F\90\02\00o\C8\8F\00\08\D2\07\80\02\0E\1F\07\80\02\06\08\E0\01\05\80\02\06\90\03\0F\80\02\024\0E\04\0A\E0\00\0F\80\02S\1B\0E\80\02\1C\10\80\02\19\0C\80\02?\10\04\0B\80\02\0B\1B\0C\80\02/\0C\0C\80\02\0B\1F\0C\80\02\0C\1B\10\80\02\1F\0E\80\02,\1F\10\80\02\1D\14\0C\F0\00\03\80\02\1F\0D\80\02\CC\14\0E\E0\00\0F\80\02\84\1F\0F\80\02\DC\14\10\F0\00\03\80\02\1F\11\80\02\CC\14\12\E0\00\0F\80\02\84\1F\13\80\02\DC\14\14\F0\00\03\80\02\1F\15\80\02\CC\14\16\E0\00\0F\80\02\84\1F\17\80\02\DC\14\18\F0\00\03\80\02\1F\19\80\02\CC\14\1A\E0\00\0F\80\02\84\1F\1B\80\02\DC\14\1C\F0\00\03\80\02\1F\1D\80\02\CC\14\1E\E0\00\0F\80\02\84\1F\1F\80\02\DC\14 \F0\00\03\80\02\1F!\80\02\CC\14\22\E0\00\0F\80\02\84\1F#\80\02\DC\14$\F0\00\03\80\02\1F%\80\02\CC\14&\E0\00\0F\80\02\84\1F'\80\02\DC\14(\F0\00\03\80\02\1F)\80\02\CC\14*\E0\00\0F\80\02\84\1F+\80\02\DC\14,\F0\00\03\80\02\1F-\80\02\CC\14.\E0\00\0F\80\02\84\1F/\80\02\DC\140\F0\00\03\80\02\1F1\80\02\CC\142\E0\00\0F\80\02\84\1F3\80\02\DC\144\F0\00\03\80\02\1F5\80\02\CC\146\E0\00\0F\80\02\84\1F7\80\02\DC\148\F0\00\03\80\02\1F9\80\02\CC\14:\E0\00\0F\80\02\84\1F;\80\02\DC\14<\F0\00\03\80\02\19=\10\009\12\04>\10\00?\04\04?\A0\02\C5?\C6\0F\01\90\02\1AW\E4\0F\04\81\C9`$/\A4\00\90\02\11\1F\12\90\02\16?\C6\0F\02\80\02+\1F\02\80\02\00\0D@$\1A\04\80\02*\06\12\80\02\07\10\01\01\80\02\07\80\03W$\01\00$\B4\D0\02\01\80\02\1A\0A\80&\1B\B6\C0\028\B9\0A\0A\80\02\00@\02\19\0E@\02\17\09\E0#\01p%\06\E0#\02@\02\15\0E`\01\1E\C8`%\06 \02\01P\02\19\04@\02(\C8\1F0\02\11\02@\00\14\82\C0\02\10\FA0\004\09\0B\82\B0\02\03@\00\14\82\D0\02 \80\060\00I\08\82\05\06@\00\14\B2@\02\010\004\0A\0B\B2 \02\12\F0@\00$\B2\07@\02\02@\003\B2\05\0A@\00\10\00\B0*\14A\B0*\030( \88s\F8-\12\00u(f\E8\0F\00\1D{\00\01\00b\EC\0F\00\84\A9\07,*\00 \00Q\22\0E\00\1Cx\1A\00P\00p\F0\F0\03P\00\13\0C0)1pD\F2\90*!\84\A9n\01\020\00qb\0E\00\0B\A2\00\07\80\00\A1\80\F6\03\00\E4\1F\08\0B\A2\00p%\02\C0\00T/\00\1C\A8\00P\00\14\01@)\11?@)@\F6\03\00\D6\00\01#\07\07R\00\10\06 \00\09\80\00\8F\C6\0F\00\88\A3\00\00\07\C0\00\09\22\B9\05\1C\00\12\08\A0++\84\B9\A0\00\22\B2\00!+`\80\F4\03\00\C4\1F\10\00(\04\05\A0\00#\B8\00p\00Ap\01\00\DA\90\01\15\05\90\00_\CA\0F\00\88\B3@\01\0B9M\19\00p\017$t\02\10\03R\0F\00\84y\05\D1)\00@\00\90&\0E\00%v\02\03\00`\0F/\04 \00\00\1D\00\03`\00vh\0E\00\81y\06\02\D0\02D\05\00\0Br\C0\00\12\F0\C0\00\14r\C0\00W\F2\03\00\D6/\B0\00 \80\04\F0\022\0Br\00`\02\11@\10\04(\0EF\10\02S\E6\0F\00\08r\80\02\00\01\00p\CC\0F\00\A9s\07\02\A0\02\C0\07\E1\1E\00\00\A4\0E\00\0Cr\00\07\10\00 pR@\00QO\00$r\06\E0*\14\07\90*@\09\00\00\90\E0+!\FF\83\F0\00\1BMp\02TGy\00\00\F0 \00f\C0\0F\00\18y\00\01\00\0F\10\00\90\0F\01\00-\11\01H1\0E\01\00\22@\00\01\00=\98\01\000\00\08\01\00\1F\0B@\00\04\13\D8)\00?\01\02\00@\00\0A\13\13\0B\04\0C\01\00\13\E0U\00\03\8F\03\01/\04\13\05w\02\00\01\00\22\18\00\01\00.k\01T\00\00\01\00#\88\04(\03\1F\00\80\00\0B/)\00'\00\02#\00\F8@\00\04 3\04\E4\00*\04\00\01\00\1Fb@\00\04*(\05\C0\00\13\03#/\0C@\00!\89\01D\01\0D@\00\13\D0@\00*\D8\00\01\00\1B\08\08\00?x\01\00V4\000\00\00\A8\15\03\03W/\04\80\00\17\048\00\04\18\00\133@\01\0C\84\01\13\B8@\00\17\881\01\0F\C0\00\01\132T\01\15\06R\00\03^\03\1A\08\A04\11\03$\00J\00\15\80\00\01\00\13\95\94\00*\03\00\01\00\0409/\00\04\80\00\0B\13\06\18\02\04h9\0D\08\01\1A\00\08\00\04\97\00\13\018\00\04\E8\00\0C\01\009\C8/\00\08\00\088\00\18\06\A0\00\0F\01\00\05\03\A9\00\80\08\00\00\00\00\00\00\00\00\00\00\00\00"} {
    llvm.mlir.global internal @__wg_main_kColReduction_reduce__4_1_0___thin_1_0() {addr_space = 3 : i32} : !llvm.array<256 x f32>
    llvm.func @__nv_fabsf(f32) -> f32
    llvm.func @main_kColReduction_reduce__4_1_0___thin_1(%arg0: i32, %arg1: !llvm.ptr<f32>, %arg2: i32, %arg3: i32, %arg4: i32, %arg5: i32, %arg6: !llvm.ptr<f32>) attributes {disc.elimargs = [1 : index, 3 : index, 5 : index, 6 : index, 7 : index, 8 : index, 9 : index, 13 : index, 15 : index, 16 : index, 17 : index], gpu.kernel, nvvm.kernel} {
      %0 = llvm.mlir.constant(32 : index) : i32
      %1 = llvm.mlir.constant(8 : index) : i32
      %2 = llvm.mlir.constant(64 : index) : i32
      %3 = llvm.mlir.constant(2 : index) : i32
      %4 = llvm.mlir.constant(0xFF800000 : f32) : f32
      %5 = llvm.mlir.constant(4 : index) : i32
      %6 = llvm.mlir.constant(1 : index) : i32
      %7 = llvm.mlir.constant(256 : index) : i32
      %8 = llvm.mlir.constant(0 : index) : i32
      %9 = llvm.mlir.addressof @__wg_main_kColReduction_reduce__4_1_0___thin_1_0 : !llvm.ptr<array<256 x f32>, 3>
      %10 = llvm.getelementptr %9[0, 0] : (!llvm.ptr<array<256 x f32>, 3>) -> !llvm.ptr<f32, 3>
      %11 = nvvm.read.ptx.sreg.ctaid.x : i32
      %12 = nvvm.read.ptx.sreg.tid.x : i32
      llvm.br ^bb1
    ^bb1:  // pred: ^bb0
      %13 = llvm.mul %11, %arg3  : i32
      %14 = llvm.add %12, %13  : i32
      %15 = llvm.icmp "ult" %14, %arg4 : i32
      llvm.cond_br %15, ^bb2, ^bb21
    ^bb2:  // pred: ^bb1
      %16 = llvm.srem %14, %7  : i32
      %17 = llvm.sdiv %14, %7  : i32
      %18 = llvm.udiv %17, %arg5  : i32
      %19 = llvm.urem %17, %arg5  : i32
      %20 = llvm.udiv %16, %0  : i32
      %21 = llvm.urem %16, %0  : i32
      %22 = llvm.mul %21, %1  : i32
      %23 = llvm.add %20, %22  : i32
      %24 = llvm.mul %19, %0  : i32
      %25 = llvm.add %21, %24  : i32
      %26 = llvm.icmp "ult" %25, %arg0 : i32
      llvm.cond_br %26, ^bb3, ^bb10(%4 : f32)
    ^bb3:  // pred: ^bb2
      %27 = llvm.mul %18, %1  : i32
      %28 = llvm.add %20, %27  : i32
      %29 = llvm.mul %28, %2  : i32
      llvm.br ^bb4(%8, %4 : i32, f32)
    ^bb4(%30: i32, %31: f32):  // 2 preds: ^bb3, ^bb9
      %32 = llvm.icmp "slt" %30, %2 : i32
      llvm.cond_br %32, ^bb5, ^bb10(%31 : f32)
    ^bb5:  // pred: ^bb4
      %33 = llvm.add %30, %29  : i32
      %34 = llvm.icmp "slt" %33, %arg2 : i32
      llvm.cond_br %34, ^bb6, ^bb7
    ^bb6:  // pred: ^bb5
      %35 = llvm.mul %33, %arg0  : i32
      %36 = llvm.add %35, %25  : i32
      %37 = llvm.getelementptr %arg1[%36] : (!llvm.ptr<f32>, i32) -> !llvm.ptr<f32>
      %38 = llvm.load %37 : !llvm.ptr<f32>
      %39 = llvm.call @__nv_fabsf(%38) : (f32) -> f32
      %40 = llvm.fcmp "ugt" %31, %39 : f32
      %41 = llvm.select %40, %31, %39 : i1, f32
      %42 = llvm.fcmp "uno" %39, %39 : f32
      %43 = llvm.select %42, %39, %41 : i1, f32
      llvm.br ^bb8(%43 : f32)
    ^bb7:  // pred: ^bb5
      llvm.br ^bb8(%31 : f32)
    ^bb8(%44: f32):  // 2 preds: ^bb6, ^bb7
      llvm.br ^bb9
    ^bb9:  // pred: ^bb8
      %45 = llvm.add %30, %6  : i32
      llvm.br ^bb4(%45, %44 : i32, f32)
    ^bb10(%46: f32):  // 2 preds: ^bb2, ^bb4
      llvm.br ^bb11(%46 : f32)
    ^bb11(%47: f32):  // pred: ^bb10
      llvm.br ^bb12
    ^bb12:  // pred: ^bb11
      %48 = llvm.getelementptr %10[%23] : (!llvm.ptr<f32, 3>, i32) -> !llvm.ptr<f32, 3>
      llvm.store %47, %48 : !llvm.ptr<f32, 3>
      nvvm.barrier0
      %49 = llvm.icmp "slt" %20, %5 : i32
      llvm.cond_br %49, ^bb13, ^bb14
    ^bb13:  // pred: ^bb12
      %50 = llvm.add %23, %5  : i32
      %51 = llvm.getelementptr %10[%23] : (!llvm.ptr<f32, 3>, i32) -> !llvm.ptr<f32, 3>
      %52 = llvm.load %51 : !llvm.ptr<f32, 3>
      %53 = llvm.getelementptr %10[%50] : (!llvm.ptr<f32, 3>, i32) -> !llvm.ptr<f32, 3>
      %54 = llvm.load %53 : !llvm.ptr<f32, 3>
      %55 = llvm.fcmp "ugt" %52, %54 : f32
      %56 = llvm.select %55, %52, %54 : i1, f32
      %57 = llvm.fcmp "uno" %54, %54 : f32
      %58 = llvm.select %57, %54, %56 : i1, f32
      %59 = llvm.getelementptr %10[%23] : (!llvm.ptr<f32, 3>, i32) -> !llvm.ptr<f32, 3>
      llvm.store %58, %59 : !llvm.ptr<f32, 3>
      llvm.br ^bb14
    ^bb14:  // 2 preds: ^bb12, ^bb13
      nvvm.barrier0
      %60 = llvm.icmp "slt" %20, %3 : i32
      llvm.cond_br %60, ^bb15, ^bb16
    ^bb15:  // pred: ^bb14
      %61 = llvm.add %23, %3  : i32
      %62 = llvm.getelementptr %10[%23] : (!llvm.ptr<f32, 3>, i32) -> !llvm.ptr<f32, 3>
      %63 = llvm.load %62 : !llvm.ptr<f32, 3>
      %64 = llvm.getelementptr %10[%61] : (!llvm.ptr<f32, 3>, i32) -> !llvm.ptr<f32, 3>
      %65 = llvm.load %64 : !llvm.ptr<f32, 3>
      %66 = llvm.fcmp "ugt" %63, %65 : f32
      %67 = llvm.select %66, %63, %65 : i1, f32
      %68 = llvm.fcmp "uno" %65, %65 : f32
      %69 = llvm.select %68, %65, %67 : i1, f32
      %70 = llvm.getelementptr %10[%23] : (!llvm.ptr<f32, 3>, i32) -> !llvm.ptr<f32, 3>
      llvm.store %69, %70 : !llvm.ptr<f32, 3>
      llvm.br ^bb16
    ^bb16:  // 2 preds: ^bb14, ^bb15
      nvvm.barrier0
      %71 = llvm.icmp "eq" %20, %8 : i32
      %72 = llvm.and %71, %26  : i1
      llvm.cond_br %72, ^bb17, ^bb20
    ^bb17:  // pred: ^bb16
      %73 = llvm.add %23, %6  : i32
      %74 = llvm.getelementptr %10[%23] : (!llvm.ptr<f32, 3>, i32) -> !llvm.ptr<f32, 3>
      %75 = llvm.load %74 : !llvm.ptr<f32, 3>
      %76 = llvm.getelementptr %10[%73] : (!llvm.ptr<f32, 3>, i32) -> !llvm.ptr<f32, 3>
      %77 = llvm.load %76 : !llvm.ptr<f32, 3>
      %78 = llvm.fcmp "ugt" %75, %77 : f32
      %79 = llvm.select %78, %75, %77 : i1, f32
      %80 = llvm.fcmp "uno" %77, %77 : f32
      %81 = llvm.select %80, %77, %79 : i1, f32
      %82 = llvm.getelementptr %arg6[%25] : (!llvm.ptr<f32>, i32) -> !llvm.ptr<f32>
      %83 = llvm.load %82 : !llvm.ptr<f32>
      llvm.br ^bb18(%83 : f32)
    ^bb18(%84: f32):  // 2 preds: ^bb17, ^bb18
      %85 = llvm.fcmp "ogt" %84, %81 : f32
      %86 = llvm.select %85, %84, %81 : i1, f32
      %87 = llvm.bitcast %82 : !llvm.ptr<f32> to !llvm.ptr<i32>
      %88 = llvm.bitcast %84 : f32 to i32
      %89 = llvm.bitcast %86 : f32 to i32
      %90 = llvm.cmpxchg %87, %88, %89 acq_rel monotonic : !llvm.ptr<i32>, i32
      %91 = llvm.extractvalue %90[0] : !llvm.struct<(i32, i1)> 
      %92 = llvm.bitcast %91 : i32 to f32
      %93 = llvm.extractvalue %90[1] : !llvm.struct<(i32, i1)> 
      llvm.cond_br %93, ^bb19, ^bb18(%92 : f32)
    ^bb19:  // pred: ^bb18
      llvm.br ^bb20
    ^bb20:  // 2 preds: ^bb16, ^bb19
      llvm.br ^bb21
    ^bb21:  // 2 preds: ^bb1, ^bb20
      llvm.return
    }
  }
  "disc_shape.SymbolicDim"() {knownNegativeOne = false, knownNonNegative = true, knownNonSizeOne = false, knownNonSizeZero = false, sym_name = "S0", value = -9223372036854775808 : i64} : () -> ()
  "disc_shape.SymbolicDim"() {knownNegativeOne = false, knownNonNegative = true, knownNonSizeOne = false, knownNonSizeZero = false, sym_name = "S1", value = -9223372036854775808 : i64} : () -> ()
  "disc_shape.SymbolicDim"() {knownNegativeOne = false, knownNonNegative = true, knownNonSizeOne = false, knownNonSizeZero = false, sym_name = "S2", value = -9223372036854775808 : i64} : () -> ()
  "disc_shape.SymbolicDim"() {knownNegativeOne = false, knownNonNegative = true, knownNonSizeOne = false, knownNonSizeZero = false, sym_name = "S3", value = -9223372036854775808 : i64} : () -> ()
  func.func @shape_constraint_graph() {
    %0 = "disc_shape.dim"() {name = @S3} : () -> index
    %1 = "disc_shape.dim"() {name = @S1} : () -> index
    %2 = "disc_shape.dim"() {name = @S2} : () -> index
    "disc_shape.tie_product_equal"(%0, %1, %2) {operand_segment_sizes = array<i32: 1, 2>} : (index, index, index) -> ()
    return
  }
}


// -----// IR Dump After Canonicalizer (canonicalize) //----- //
func.func @main(%arg0: !disc_ral.context) attributes {tf.entry_function = {input_placements = "gpu", inputs = "input0", output_placements = "gpu", outputs = "output0"}} {
  %c256 = arith.constant 256 : index
  %c32 = arith.constant 32 : index
  %c512 = arith.constant 512 : index
  %0 = llvm.mlir.constant(0 : i32) : i32
  %c0 = arith.constant 0 : index
  %c1 = arith.constant 1 : index
  %c2 = arith.constant 2 : index
  %1 = "disc_ral.dispatch"(%arg0, %c0) {backend_config = "", call_target_name = "ral_recv_input", device = "cpu", has_side_effect = false} : (!disc_ral.context, index) -> memref<?x?x?xf32, "gpu">
  %dim = memref.dim %1, %c2 : memref<?x?x?xf32, "gpu">
  %dim_0 = memref.dim %1, %c1 : memref<?x?x?xf32, "gpu">
  %dim_1 = memref.dim %1, %c0 : memref<?x?x?xf32, "gpu">
  %2 = arith.index_cast %dim_0 : index to i32
  %3 = arith.index_cast %dim : index to i32
  %4 = arith.muli %2, %3 : i32
  %5 = arith.index_cast %4 : i32 to index
  %alloc = memref.alloc(%5) {kDiscSymbolicDimAttr = [@S3]} : memref<?xf32, "gpu">
  %6 = arith.cmpi slt, %dim_1, %5 : index
  cf.cond_br %6, ^bb1, ^bb2
^bb1:  // pred: ^bb0
  %7 = affine.apply affine_map<()[s0] -> (s0 ceildiv 512)>()[%5]
  gpu.launch_func  @main_kernel::@main_kColReduction_reduce__4_1_0___flat blocks in (%7, %c1, %c1) threads in (%c512, %c1, %c1) args(%c512 : index, %5 : index, %alloc : memref<?xf32, "gpu">)
  %8 = arith.cmpi eq, %5, %c0 : index
  %9 = arith.subi %5, %c1 : index
  %10 = arith.divui %9, %c512 : index
  %11 = arith.addi %10, %c1 : index
  %12 = arith.select %8, %c0, %11 : index
  %13 = arith.cmpi eq, %dim_1, %c0 : index
  %14 = arith.subi %dim_1, %c1 : index
  %15 = arith.divui %14, %c32 : index
  %16 = arith.addi %15, %c1 : index
  %17 = arith.select %13, %c0, %16 : index
  %18 = arith.muli %12, %17 : index
  %19 = arith.muli %18, %c512 : index
  %20 = affine.apply affine_map<()[s0] -> (s0 ceildiv 512)>()[%19]
  gpu.launch_func  @main_kernel_0::@main_kColReduction_reduce__4_1_0___flat_1 blocks in (%20, %c1, %c1) threads in (%c512, %c1, %c1) args(%5 : index, %1 : memref<?x?x?xf32, "gpu">, %c512 : index, %19 : index, %12 : index, %alloc : memref<?xf32, "gpu">)
  cf.br ^bb3
^bb2:  // pred: ^bb0
  %21 = affine.apply affine_map<()[s0] -> (s0 ceildiv 256)>()[%5]
  gpu.launch_func  @main_kernel_1::@main_kColReduction_reduce__4_1_0___thin blocks in (%21, %c1, %c1) threads in (%c256, %c1, %c1) args(%c256 : index, %5 : index, %alloc : memref<?xf32, "gpu">)
  %22 = arith.cmpi eq, %5, %c0 : index
  %23 = arith.subi %5, %c1 : index
  %24 = arith.divui %23, %c32 : index
  %25 = arith.addi %24, %c1 : index
  %26 = arith.select %22, %c0, %25 : index
  %27 = arith.cmpi eq, %dim_1, %c0 : index
  %28 = arith.subi %dim_1, %c1 : index
  %29 = arith.divui %28, %c512 : index
  %30 = arith.addi %29, %c1 : index
  %31 = arith.select %27, %c0, %30 : index
  %32 = arith.muli %26, %31 : index
  %33 = arith.muli %32, %c256 : index
  %34 = affine.apply affine_map<()[s0] -> (s0 ceildiv 256)>()[%33]
  gpu.launch_func  @main_kernel_2::@main_kColReduction_reduce__4_1_0___thin_1 blocks in (%34, %c1, %c1) threads in (%c256, %c1, %c1) args(%5 : index, %1 : memref<?x?x?xf32, "gpu">, %c256 : index, %33 : index, %26 : index, %alloc : memref<?xf32, "gpu">)
  cf.br ^bb3
^bb3:  // 2 preds: ^bb1, ^bb2
  %35 = llvm.inttoptr %0 : i32 to !llvm.ptr<i8>
  %alloca = memref.alloca() : memref<2xindex, "cpu">
  memref.store %dim_0, %alloca[%c0] : memref<2xindex, "cpu">
  memref.store %dim, %alloca[%c1] : memref<2xindex, "cpu">
  %36 = "disc_ral.dispatch"(%arg0, %35, %alloc, %alloca) {backend_config = "", call_target_name = "inc_ref", device = "gpu", has_side_effect = false} : (!disc_ral.context, !llvm.ptr<i8>, memref<?xf32, "gpu">, memref<2xindex, "cpu">) -> memref<?x?xf32, "gpu">
  %reinterpret_cast = memref.reinterpret_cast %36 to offset: [0], sizes: [%dim_0, %dim], strides: [%dim, 1] {kDiscSymbolicDimAttr = [@S1, @S2]} : memref<?x?xf32, "gpu"> to memref<?x?xf32, "gpu">
  memref.dealloc %alloc : memref<?xf32, "gpu">
  "disc_ral.dispatch"(%arg0, %c0, %reinterpret_cast) {backend_config = "", call_target_name = "ral_send_output", device = "cpu", has_side_effect = false} : (!disc_ral.context, index, memref<?x?xf32, "gpu">) -> ()
  return
}

// -----// IR Dump After ConvertAffineToStandard (lower-affine) //----- //
module attributes {gpu.container_module} {
  func.func @main(%arg0: !disc_ral.context) attributes {tf.entry_function = {input_placements = "gpu", inputs = "input0", output_placements = "gpu", outputs = "output0"}} {
    %c256 = arith.constant 256 : index
    %c32 = arith.constant 32 : index
    %c512 = arith.constant 512 : index
    %0 = llvm.mlir.constant(0 : i32) : i32
    %c0 = arith.constant 0 : index
    %c1 = arith.constant 1 : index
    %c2 = arith.constant 2 : index
    %1 = "disc_ral.dispatch"(%arg0, %c0) {backend_config = "", call_target_name = "ral_recv_input", device = "cpu", has_side_effect = false} : (!disc_ral.context, index) -> memref<?x?x?xf32, "gpu">
    %dim = memref.dim %1, %c2 : memref<?x?x?xf32, "gpu">
    %dim_0 = memref.dim %1, %c1 : memref<?x?x?xf32, "gpu">
    %dim_1 = memref.dim %1, %c0 : memref<?x?x?xf32, "gpu">
    %2 = arith.index_cast %dim_0 : index to i32
    %3 = arith.index_cast %dim : index to i32
    %4 = arith.muli %2, %3 : i32
    %5 = arith.index_cast %4 : i32 to index
    %alloc = memref.alloc(%5) {kDiscSymbolicDimAttr = [@S3]} : memref<?xf32, "gpu">
    %6 = arith.cmpi slt, %dim_1, %5 : index
    cf.cond_br %6, ^bb1, ^bb2
  ^bb1:  // pred: ^bb0
    %c512_2 = arith.constant 512 : index
    %c0_3 = arith.constant 0 : index
    %c1_4 = arith.constant 1 : index
    %7 = arith.cmpi sle, %5, %c0_3 : index
    %8 = arith.subi %c0_3, %5 : index
    %9 = arith.subi %5, %c1_4 : index
    %10 = arith.select %7, %8, %9 : index
    %11 = arith.divsi %10, %c512_2 : index
    %12 = arith.subi %c0_3, %11 : index
    %13 = arith.addi %11, %c1_4 : index
    %14 = arith.select %7, %12, %13 : index
    gpu.launch_func  @main_kernel::@main_kColReduction_reduce__4_1_0___flat blocks in (%14, %c1, %c1) threads in (%c512, %c1, %c1) args(%c512 : index, %5 : index, %alloc : memref<?xf32, "gpu">)
    %15 = arith.cmpi eq, %5, %c0 : index
    %16 = arith.subi %5, %c1 : index
    %17 = arith.divui %16, %c512 : index
    %18 = arith.addi %17, %c1 : index
    %19 = arith.select %15, %c0, %18 : index
    %20 = arith.cmpi eq, %dim_1, %c0 : index
    %21 = arith.subi %dim_1, %c1 : index
    %22 = arith.divui %21, %c32 : index
    %23 = arith.addi %22, %c1 : index
    %24 = arith.select %20, %c0, %23 : index
    %25 = arith.muli %19, %24 : index
    %26 = arith.muli %25, %c512 : index
    %c512_5 = arith.constant 512 : index
    %c0_6 = arith.constant 0 : index
    %c1_7 = arith.constant 1 : index
    %27 = arith.cmpi sle, %26, %c0_6 : index
    %28 = arith.subi %c0_6, %26 : index
    %29 = arith.subi %26, %c1_7 : index
    %30 = arith.select %27, %28, %29 : index
    %31 = arith.divsi %30, %c512_5 : index
    %32 = arith.subi %c0_6, %31 : index
    %33 = arith.addi %31, %c1_7 : index
    %34 = arith.select %27, %32, %33 : index
    gpu.launch_func  @main_kernel_0::@main_kColReduction_reduce__4_1_0___flat_1 blocks in (%34, %c1, %c1) threads in (%c512, %c1, %c1) args(%5 : index, %1 : memref<?x?x?xf32, "gpu">, %c512 : index, %26 : index, %19 : index, %alloc : memref<?xf32, "gpu">)
    cf.br ^bb3
  ^bb2:  // pred: ^bb0
    %c256_8 = arith.constant 256 : index
    %c0_9 = arith.constant 0 : index
    %c1_10 = arith.constant 1 : index
    %35 = arith.cmpi sle, %5, %c0_9 : index
    %36 = arith.subi %c0_9, %5 : index
    %37 = arith.subi %5, %c1_10 : index
    %38 = arith.select %35, %36, %37 : index
    %39 = arith.divsi %38, %c256_8 : index
    %40 = arith.subi %c0_9, %39 : index
    %41 = arith.addi %39, %c1_10 : index
    %42 = arith.select %35, %40, %41 : index
    gpu.launch_func  @main_kernel_1::@main_kColReduction_reduce__4_1_0___thin blocks in (%42, %c1, %c1) threads in (%c256, %c1, %c1) args(%c256 : index, %5 : index, %alloc : memref<?xf32, "gpu">)
    %43 = arith.cmpi eq, %5, %c0 : index
    %44 = arith.subi %5, %c1 : index
    %45 = arith.divui %44, %c32 : index
    %46 = arith.addi %45, %c1 : index
    %47 = arith.select %43, %c0, %46 : index
    %48 = arith.cmpi eq, %dim_1, %c0 : index
    %49 = arith.subi %dim_1, %c1 : index
    %50 = arith.divui %49, %c512 : index
    %51 = arith.addi %50, %c1 : index
    %52 = arith.select %48, %c0, %51 : index
    %53 = arith.muli %47, %52 : index
    %54 = arith.muli %53, %c256 : index
    %c256_11 = arith.constant 256 : index
    %c0_12 = arith.constant 0 : index
    %c1_13 = arith.constant 1 : index
    %55 = arith.cmpi sle, %54, %c0_12 : index
    %56 = arith.subi %c0_12, %54 : index
    %57 = arith.subi %54, %c1_13 : index
    %58 = arith.select %55, %56, %57 : index
    %59 = arith.divsi %58, %c256_11 : index
    %60 = arith.subi %c0_12, %59 : index
    %61 = arith.addi %59, %c1_13 : index
    %62 = arith.select %55, %60, %61 : index
    gpu.launch_func  @main_kernel_2::@main_kColReduction_reduce__4_1_0___thin_1 blocks in (%62, %c1, %c1) threads in (%c256, %c1, %c1) args(%5 : index, %1 : memref<?x?x?xf32, "gpu">, %c256 : index, %54 : index, %47 : index, %alloc : memref<?xf32, "gpu">)
    cf.br ^bb3
  ^bb3:  // 2 preds: ^bb1, ^bb2
    %63 = llvm.inttoptr %0 : i32 to !llvm.ptr<i8>
    %alloca = memref.alloca() : memref<2xindex, "cpu">
    memref.store %dim_0, %alloca[%c0] : memref<2xindex, "cpu">
    memref.store %dim, %alloca[%c1] : memref<2xindex, "cpu">
    %64 = "disc_ral.dispatch"(%arg0, %63, %alloc, %alloca) {backend_config = "", call_target_name = "inc_ref", device = "gpu", has_side_effect = false} : (!disc_ral.context, !llvm.ptr<i8>, memref<?xf32, "gpu">, memref<2xindex, "cpu">) -> memref<?x?xf32, "gpu">
    %reinterpret_cast = memref.reinterpret_cast %64 to offset: [0], sizes: [%dim_0, %dim], strides: [%dim, 1] {kDiscSymbolicDimAttr = [@S1, @S2]} : memref<?x?xf32, "gpu"> to memref<?x?xf32, "gpu">
    memref.dealloc %alloc : memref<?xf32, "gpu">
    "disc_ral.dispatch"(%arg0, %c0, %reinterpret_cast) {backend_config = "", call_target_name = "ral_send_output", device = "cpu", has_side_effect = false} : (!disc_ral.context, index, memref<?x?xf32, "gpu">) -> ()
    return
  }
  gpu.module @main_kernel attributes {gpu.binary = "P\EDU\BA\01\00\10\008\04\00\00\00\00\00\00\02\00\01\01@\00\00\00\F8\03\00\00\00\00\00\00\F7\03\00\00\00\00\00\00\07\00\01\00P\00\00\00\00\00\00\00\00\00\00\00\11 \00\00\00\00\00\00\00\00\00\00\00\00\00\00\E8\0B\00\00\00\00\00\00\A2\7FELF\02\01\013\07\00\01\00f\02\00\BE\00q\00\01\00!@\0B\07\001\00\80\08\07\00\F5\0E\00P\05P\00@\008\00\03\00@\00\0B\00\01\00\00.shstrtab\00.\08\00'ym\08\00\F0\1B_shndx\00.nv.info\00.text.main_kColReduction_r\0A\00\F5\00e__4_1_0___flat7\00\0F1\00\1Aoshared3\00\1A\9Fconstant06\00\17\FA\01debug_frame\00.rel\11\00!nv\14\00\11aB\00\0F'\01 \0F\87\00\14\0FO\01\B6o_paramV\01\1C\0F\01\00\07\8CZ\00\00\00\03\00\0A\00\01\00\11\EC\18\00,\09\00\01\00 )\01\18\00,\04\00\01\00\11G\18\00,\07\00\01\00f2\00\00\00\12\10`\00!\80\01\07\00v\00\FF\FF\FF\FF(\00\0C\00\00\01\00@\03\00\04|\08\00\D1\0F\0C\81\80\80(\00\08\FF\81\80(\08\0B\00\06(\00/0\00\01\00\03\13pX\00\10\04\9B\00R\04\14\00\00\00E\00\01\0B\00\00\13\00p/\08\00\05\00\00\00\9F\03\22\04#\0C\00\00\01\00'\04\12\0C\00\17\11\0C\00!7\04\E0\03\F3\08\015\00\00\04\0A\08\00\02\00\00\00`\01\10\00\03\19\10\00\04\17\0C\04\04U\08\00\00\F0!\10\00\10\01\18\01%\F0\11\10\00\01\01\00\F2\02\F0\11\00\03\1B\FF\00\04\1C\08\00P\00\00\00\B0\00\01\00#K\00\01\00s\02\02\08\10\0A/\22\9B\00\00\07\00\03\08\00\13\10\08\00\13\18\08\00\13 \08\00\13(\08\00\130\08\00\228\08|\00\13\00\08\00\13\08\08\00\13\10\08\00\13\18\08\00\13 \08\00\13(\08\00\130\08\00\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\A2\00\00\00\14,\00\00\00H\00\01\00\000\01/\05\00\01\00\FF\B8A\02z\01\00'\04\B1\0F\00\00\00\C4\0F\00\19y\02\00\01\00\10%\93\02Q\0E\00\19y\03\0F\00\F5\1A\00!\00\00\00$\0E\00$z\02\02\00X\00\00\03\02\8E\07\00\CA\1F\00\0Cz\00\02\00Y\00\00p`\F0\03\00\DA\0F\00M[\04\A0\80\03\00\EA\0F\005t\03\FF\BB\03\10\FF\C8\03P\E2\0F\00\02x>\02B\80\FF\00\0F\10\00r\B9z\04\00\00F\00\84\00\94\D0\0F\00%v\02\02\00Z`\00`\0F\00\86y\00\022\00@\04\19\10\0C0\009My\00`\00PGy\00\00\F0A\04\A6\FF\83\03\00\C0\0F\00\18y\00\01\00\0F\10\00\90\0F\01\00-\00W\01.\03\00\01\00\22@\00\01\00='\01\000\00\08\01\00\1F\0B@\00\04\13g)\00\1FV@\00\0C\13\13\14\04\0C\01\00\13\C0\15\00&\90\008\04#\04\00\8D\04\00\FE\04\12\00\01\00\1F\FAT\00\00\00\01\00\13P\95\00/p\00\80\00\0B\1F)'\00\03#\00\C0@\00\04X\06\04\E4\00*\04\00\01\00\1F`@\00\04\13\F01\00&\\\00@\00\1F\0A@\00\00!\18\01D\01\0D@\00\13P)\00*\D8\00\01\00\1B\08\08\00?\07\01\00\8E\07\00Q\00\00(\05\00\01\00&\10\00\80\00\17\048\00\04\18\00\13\C4\14\01\0C\84\01*8\05@\07\1F\00\C0\00\04\132@\00+\06\00\01\00\1A\07\D8\07\12\03\F8\05:\08\80\00\01\00\13\06\10\06\04(\0B\0C\01\00*\A8\00\08\00\04\F8\00\13\018\00\04\A8\00\0C\01\009H\03\00\08\00\088\00/\06\00\01\00\17\80\08\00\00\00\00\00\00\00\00"} {
    llvm.func @main_kColReduction_reduce__4_1_0___flat(%arg0: i32, %arg1: i32, %arg2: !llvm.ptr<f32>) attributes {disc.elimargs = [2 : index, 4 : index, 5 : index, 6 : index], gpu.kernel, nvvm.kernel} {
      %0 = llvm.mlir.constant(0xFF800000 : f32) : f32
      %1 = nvvm.read.ptx.sreg.ctaid.x : i32
      %2 = nvvm.read.ptx.sreg.tid.x : i32
      llvm.br ^bb1
    ^bb1:  // pred: ^bb0
      %3 = llvm.mul %1, %arg0  : i32
      %4 = llvm.add %2, %3  : i32
      %5 = llvm.icmp "ult" %4, %arg1 : i32
      llvm.cond_br %5, ^bb2, ^bb3
    ^bb2:  // pred: ^bb1
      %6 = llvm.getelementptr %arg2[%4] : (!llvm.ptr<f32>, i32) -> !llvm.ptr<f32>
      llvm.store %0, %6 : !llvm.ptr<f32>
      llvm.br ^bb3
    ^bb3:  // 2 preds: ^bb1, ^bb2
      llvm.return
    }
  }
  gpu.module @main_kernel_0 attributes {gpu.binary = "P\EDU\BA\01\00\10\00\90\0A\00\00\00\00\00\00\02\00\01\01@\00\00\00P\0A\00\00\00\00\00\00P\0A\00\00\00\00\00\00\07\00\01\00P\00\00\00\00\00\00\00\00\00\00\00\11 \00\00\00\00\00\00\00\00\00\00\00\00\00\00\E8\1C\00\00\00\00\00\00\A2\7FELF\02\01\013\07\00\01\00f\02\00\BE\00q\00\01\00!@\1C\07\001\00\80\19\07\00\F5\0E\00P\05P\00@\008\00\03\00@\00\0B\00\01\00\00.shstrtab\00.\08\00'ym\08\00\F0\1B_shndx\00.nv.info\00.text.main_kColReduction_r\0A\00\F5\02e__4_1_0___flat_19\00\0F3\00\1Coshared5\00\19Orela\9D\00\1E?rel\D1\00!\9Fconstant08\00\19\B2debug_framey\00\09\11\00!nv\14\00\11aD\00\0F\98\01 \0F\89\00\16\0F\C2\01\FF(o_param\C9\01\1C\0F\01\00\0B\8C\\\00\00\00\03\00\0A\00\01\00 ]\01\18\00,\09\00\01\00\11\9C\18\00,\04\00\01\00\11\BA\18\00,\07\00\01\00f2\00\00\00\12\10`\00!\80\11\07\00v\00\FF\FF\FF\FF(\00\0C\00\00\01\00@\03\00\04|\08\00\D1\0F\0C\81\80\80(\00\08\FF\81\80(\08\0B\00\06(\00/0\00\01\00\03\13pX\00\10\04\9B\00R\04\14\00\00\00E\00\22\04 \AC\00\90\04/\08\00\05\00\00\00\16\0C\00\12#\0C\00\00\01\00'\04\12\0C\00\17\11\0C\00!7\04\C8\04\F1\08\015\00\00\04\0A\08\00\02\00\00\00`\01(\00\03\19(\00\04\17\0C$\00u\06\00 \00\00\F0!\10\00u\05\00\1C\00\00\F0\11\10\009\04\00\18\10\009\03\00\14\10\009\02\00\10\10\009\01\00\08P\00\01\01\00\F2\0A\F0\11\00\03\1B\FF\00\04\1C\0C\00P\00\00\00\10\02\00\00\E0\10\00\00\04\1E\D8\00#K\00\01\00\B1\02\02\08\10\0A/\22\00\00\00\08\10\00#\00\08\08\00\13\10\08\00\13\18\08\00\13 \08\00\13(\08\00\130\08\00\228\08\84\00\13\00\08\00\13\08\08\00\13\10\08\00\13\18\08\00\13 \08\00\13(\08\00\130\08\00\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\A2\00\00\00\14,\00\00\00H\00\01\00\00x\01/\05\00\01\00\FF\88@$v\01\FF?\04\B2\FF\00\8E\07\00\C4\0F\00\19y\00\01\00\10%c\02Q\0E\00\19y\03\0F\00 \00!-\00@\0E\00$z]\04\B0\00\00\03\02\8E\07\00\CA\1F\00\0C\10\00\C5^\00\00p`\F0\03\00\DA\0F\00Ms\04\F0\0C\80\03\00\EA\0F\00\06{\04\00\00_\00\00\00\90 \00\00\22\0E\00\19x\05\FF\1F\1F\00\D3\14\01\00\00\E2\0F\00$r\02\FF\FF\00\80\00 \E2\0FP\00\10\FF0\00\B1pP\F4\03\00\E4\0F\00\11r\05?\02\B2\FFH\8F\07\00\C6\0F\00\08s\04\BE\04\10\10\90\00\F1\07\1E\00\10x\03\04\FE\FF\FF\0F\FF\E0\FF\07\00\CC\1F\00\05s\03\00\B2\00\22\F0!\B0\00!r\07`\00\C0\03\0A\8E\07\00\C8\1F\00$z\07\07`\00\10\FF\C0\00\81\C8\0F\00'r\07\03\07\A8\02\02\80\00@\19x\02\FF\C3\00\10\05\A0\00\E1\E4\0F\00\12x\05\05\00\FE\FF\FF\FF\C0\8E\80\00T'r\07\07\02\B0\00\10\C8\C0\00\11\09`\00#\07\0A\10\000z\03\09`\00\11\02 \01\01\D0\006\03\00_ \010\10\0A\03\10\00\11\80\B0\00\80\E4\0F\00\10\08\07\07\01P\00\03\10\00\060\00\12\F20\00\1A\18 \001\12\AA\07 \01\22\FF3`\00;$r\03\80\00%\02\03\80\00`\E4\0F\00$x\03\14\04$\00\05 \000x\03\02\95\03\18\03\A0\00\1FX\C0\01\07W$x\02\07 \A0\01\81\B9z\04\00\00F\00\00s\06\00\C0\01Dt\00\FF\04 \00\11\C6P\00\10\02\95\060pb\F8\C0\01U\04\10x\06\02\C0\00\11\C8 \00\12\06 \00\C3\FA\03\00\CE\0F\00$\CA\05\02\00X\90\00\90\C8\0F\00%\C6\04\05\00Z\91\01\03\B0\005\DA\07\06 \00a\E2\0F\00\81\C9\0A#\06\C5\00\19\1E\0C\00\A6\00\00%\D6\06\070\00u\CA\0F\00\81\D9\0D\06 \00\86\E2\02\00\10x\08\02\02@\01j\04\10x\10\02\03p\01\12\08\90\00#\F0\03\10\00\12\10\10\00\11\F2\10\00Y\10x\0C\02\04@\004\0E\02\05\10\00\11\C40\00\12\0C0\00 \F4\03\80\00E$\8A\09\08\B0\00w\E4\0F\10$\9A\0B\10\10\00U\00%\86\08\09\B0\00\02\F0\02\12\0E@\00 \F6\03\F0\00E\81\89\05\08\C0\00v\22\11\00%\96\06\0B0\00 /\00\C0\00\14\06\80\00w\C6\0F\00$\AA\0B\0C \01F\10\81\99\0F\00\01\B4\22\03\00$t\04\FF\00\00\80\FF\C0\01\00\D0\00\14\07@\00\00\C0\015\BA\09\0E@\00W\C8\1F\00%\B6\A0\00g\CC\0F\00\81\B9\09\90\00@\01\00\0B\C8\FA\01\D2\80\FF\00\C2\FC\03\00\C8O\00\08\C8\04\10\000\02\00\03P\007%\A6\0A\B0\00\08P\01\01\10\02\F5\06\00\0B\D2\00\04\0D\00\00@\00`\FC\03\00\C6\8F\00\81\A9\0B\0A`\00b\E2\04\00\08\D2\04 \00!\00\000\00\06`\01\12\FA`\01&\CA\07P\01z\C8/\00%\C6\06\070\02*\0D\0C0\02\16\11\10\01y\E6\02\00%\D6\0C\0D\E0\006\D9\0D\0C\80\00t\00\00\0B\82\00\04\05\A0\00\84\E4\0F\09\10x\0A\02\080\01\8A\E4O\00\08\82\04\04\05\A0\00\18\0A@\02V\0B\92\00\04\0F@\00\01\80\02\14\09@\00\8A\C4\1F\00\08\92\04\04\0F@\00\1A\08p\028\0E\02\0A\E0\016\8A\07\0A\D0\00\11/\C0\01\17\0B \00+%\86\00\03&\89\05\F0\00!\A2\000\02\14\0C0\00\00\A0\00D\A2\00\04\0B\A0\00\8A\C8\8F\00\08\A2\04\04\0B\90\00\15\0E\D0\02\000\00D\B2\00\04\090\00\000\02&\9A\0B\E0\02\00\90\05D\B2\04\04\09@\00\00\00\02(\96\08\00\02\16\08\C0\01\12\F6P\00D\C2\00\04\11P\00x\E4\0F\08\81\99\0F\08\B0\03H$\AA\0B\0E\C0\01Z\08\C2\04\04\11\A0\00\06P\02\0Fp\02\00\00@\01\17\0D\A0\03\09p\02f\E2\0F\08$\BA\07 \02*\E4\1F\80\02.\22\01\80\02\05\10\01\12\FAp\00\1B\B6p\028\CA\09\10\A0\01'\81\B9p\02{&\03\00%\C6\08\09\A0\02\1A\0E\A0\02\16\13\00\01/&\01\A0\02\0B+\22\03\A0\02 O\08\A0\02\15\0E\E0\01\1F\1F\A0\02\0E\00 \01\1B\0F\10\05\17\100\01\00`\02\14\11\10\00*\CE/\80\02\1F\C8p\02\0F\09\00\03/\C8\8F\F0\02\01\18\0E\F0\02\0A\90\02/\0F\01\90\02\00\1C\10\90\02\05@\02\00 \03)\9A\09P\01'\08\B20\02\11\C6\90\02\06\80\01\0F\90\02\04\14\13P\000\E4\0F\08 \01\14\12\00\01\11\E2`\05\18\10`\00\0C\B0\02\00\A0\02\1E\13\E0\00\0F\A0\02\06\09\90\02\01p\018\10\02\13p\00\08\A0\02*\E2\1F\90\02-\E2\0F\B0\02\06 \01\0F\A0\02\06\1F\0E\A0\02\1C\1F\10\A0\02=\1F\14\A0\02\1C\1B\15\A0\02\1B\16\A0\02\1F\17\A0\02\A7\19\10\A0\02\02@\00\06\90\02\1D\E2\B0\02\0F\A0\02\02\14\18\00\01\0F\A0\02\1D\1F\C4\A0\02\0E\19\08\A0\02\01p\00?\10\02\19\A0\02U\10\E4\A0\02\16\07\B0\01\1F$\A0\02\11\1F\11\A0\02.\1F\1A\A0\02\1A5\06\02\1B0\00\11/\10\0A\1B\1C\A0\02\14\1D\10\00\11\CE\10\0A\0B\A0\02+\08\09\A0\02\1C\08\B0\07\19\1E\00\036\02\02\1F\10\00\0F\C0\02\11\1F\06\C0\02\1C\1C\0E\C0\02\19\07\C0\02,\0B\06`\05\14\07@\00+\C6\0F\A0\0A\0F\F0\07\0C\00\80\07\08\E0\07\10\E4\B0\02,\07\06\B0\02\1E\11`\03\0FP\05\13\11\C6\D0\0A\0F\A0\02\18\08\B0\0C\04\A0\02\0Bp\02\17\CAp\02\1B\E4\10\0B\10$\B0\0C)\0C\0D0\009\DA\0F\020\009\C9\0D\0C0\00:\D6\0E\0F\A0\02(\0F\0E\90\00g%v\02\03\00` \025y\06\02 \00*b!\C0\02*\C8O\B0\02\12\C8 \02\02\C0\01\15\F0 \02\03\B0\018\00\00\C8\10\02\1C\F0\10\02\02 \00\05\F0\09\01 \00\08\E0\09\02 \00\14\C2p\01\03 \00\15\C2`\01\02 \00\14\D2\A0\02\03 \00\15\D2\A0\02\10\00`\0C2\0Br\00\F0\01 @\F0p\017\0EFyp\0ES\E6\0F\00\08r\10\02\00\01\00\80\CC\0F\00\A9s\07\02\06\E0\0E\B0\E1\1E\00\00\A4\0E\00\0Cr\00\07\10\00 pR@\00RO\00$r\06\00\0F\00\E0\0C\80\D8\0F\00G\09\00\00\90i\14!\FF\83\C0\0E*My\D0\0ETGy\00\00\F0 \00f\C0\0F\00\18y\00\01\00\0F\10\00`\0F\01\00-\00\\\0F.\03\00\01\00\02q\01]\00\00\98\01\000\00\08\01\00\1F\0B@\00\04\13\D8)\00\1F\C9@\00\0C\13\13\E4\13\0C\01\00\13\A8U\00\11\90\06\00\02$\00#\04\00]\14\00\CE\14\12\00\01\00.k\01T\00\00\01\00\138\05\02/p\00\80\00\0B\1F)'\00\03#\00\A8@\00\04p\16\04\E4\00*\04\00\01\00\1Fb@\00\04\13\D8)\00&\A8\00@\00\1F\0A@\00\00!\89\01D\01\0D@\00\11\80\B6\12J\00\00\D8\00\01\00\1B\08\08\00?x\01\00\A6\17\000\00\00X\E5\02\03\E7\12\04\80\00\17\048\00\04\18\00\133@\01\0C\84\01\13h@\00\17\881\01\0F\C0\00\01\132T\01+\06\00\01\00\1A\08\F0\17\12\03\C8\15:\16\80\00\01\00\13\06\E0\15\04(\1C\0D\88\01\1A\00\08\00\04\C0\00\13\018\00\04\A8\00\0C\01\009\18\13\00\08\00\088\00/\06\00\01\00\17\80\08\00\00\00\00\00\00\00"} {
    llvm.func @__nv_fabsf(f32) -> f32
    llvm.func @main_kColReduction_reduce__4_1_0___flat_1(%arg0: i32, %arg1: !llvm.ptr<f32>, %arg2: i32, %arg3: i32, %arg4: i32, %arg5: i32, %arg6: !llvm.ptr<f32>) attributes {disc.elimargs = [1 : index, 3 : index, 5 : index, 6 : index, 7 : index, 8 : index, 9 : index, 13 : index, 15 : index, 16 : index, 17 : index], gpu.kernel, nvvm.kernel} {
      %0 = llvm.mlir.constant(0 : index) : i32
      %1 = llvm.mlir.constant(1 : index) : i32
      %2 = llvm.mlir.constant(512 : index) : i32
      %3 = llvm.mlir.constant(32 : index) : i32
      %4 = llvm.mlir.constant(0xFF800000 : f32) : f32
      %5 = nvvm.read.ptx.sreg.ctaid.x : i32
      %6 = nvvm.read.ptx.sreg.tid.x : i32
      llvm.br ^bb1
    ^bb1:  // pred: ^bb0
      %7 = llvm.mul %5, %arg3  : i32
      %8 = llvm.add %6, %7  : i32
      %9 = llvm.icmp "ult" %8, %arg4 : i32
      llvm.cond_br %9, ^bb2, ^bb14
    ^bb2:  // pred: ^bb1
      %10 = llvm.srem %8, %2  : i32
      %11 = llvm.sdiv %8, %2  : i32
      %12 = llvm.udiv %11, %arg5  : i32
      %13 = llvm.urem %11, %arg5  : i32
      %14 = llvm.mul %13, %2  : i32
      %15 = llvm.add %14, %10  : i32
      %16 = llvm.icmp "ult" %15, %arg0 : i32
      llvm.cond_br %16, ^bb3, ^bb13
    ^bb3:  // pred: ^bb2
      %17 = llvm.mul %12, %3  : i32
      llvm.br ^bb4(%0, %4 : i32, f32)
    ^bb4(%18: i32, %19: f32):  // 2 preds: ^bb3, ^bb9
      %20 = llvm.icmp "slt" %18, %3 : i32
      llvm.cond_br %20, ^bb5, ^bb10
    ^bb5:  // pred: ^bb4
      %21 = llvm.add %17, %18  : i32
      %22 = llvm.icmp "slt" %21, %arg2 : i32
      llvm.cond_br %22, ^bb6, ^bb7
    ^bb6:  // pred: ^bb5
      %23 = llvm.mul %21, %arg0  : i32
      %24 = llvm.add %23, %15  : i32
      %25 = llvm.getelementptr %arg1[%24] : (!llvm.ptr<f32>, i32) -> !llvm.ptr<f32>
      %26 = llvm.load %25 : !llvm.ptr<f32>
      %27 = llvm.call @__nv_fabsf(%26) : (f32) -> f32
      %28 = llvm.fcmp "oge" %19, %27 : f32
      %29 = llvm.select %28, %19, %27 : i1, f32
      llvm.br ^bb8(%29 : f32)
    ^bb7:  // pred: ^bb5
      llvm.br ^bb8(%19 : f32)
    ^bb8(%30: f32):  // 2 preds: ^bb6, ^bb7
      llvm.br ^bb9
    ^bb9:  // pred: ^bb8
      %31 = llvm.add %18, %1  : i32
      llvm.br ^bb4(%31, %30 : i32, f32)
    ^bb10:  // pred: ^bb4
      %32 = llvm.getelementptr %arg6[%15] : (!llvm.ptr<f32>, i32) -> !llvm.ptr<f32>
      %33 = llvm.load %32 : !llvm.ptr<f32>
      llvm.br ^bb11(%33 : f32)
    ^bb11(%34: f32):  // 2 preds: ^bb10, ^bb11
      %35 = llvm.fcmp "ogt" %34, %19 : f32
      %36 = llvm.select %35, %34, %19 : i1, f32
      %37 = llvm.bitcast %32 : !llvm.ptr<f32> to !llvm.ptr<i32>
      %38 = llvm.bitcast %34 : f32 to i32
      %39 = llvm.bitcast %36 : f32 to i32
      %40 = llvm.cmpxchg %37, %38, %39 acq_rel monotonic : !llvm.ptr<i32>, i32
      %41 = llvm.extractvalue %40[0] : !llvm.struct<(i32, i1)> 
      %42 = llvm.bitcast %41 : i32 to f32
      %43 = llvm.extractvalue %40[1] : !llvm.struct<(i32, i1)> 
      llvm.cond_br %43, ^bb12, ^bb11(%42 : f32)
    ^bb12:  // pred: ^bb11
      llvm.br ^bb13
    ^bb13:  // 2 preds: ^bb2, ^bb12
      llvm.br ^bb14
    ^bb14:  // 2 preds: ^bb1, ^bb13
      llvm.return
    }
  }
  gpu.module @main_kernel_1 attributes {gpu.binary = "P\EDU\BA\01\00\10\008\04\00\00\00\00\00\00\02\00\01\01@\00\00\00\F8\03\00\00\00\00\00\00\F7\03\00\00\00\00\00\00\07\00\01\00P\00\00\00\00\00\00\00\00\00\00\00\11 \00\00\00\00\00\00\00\00\00\00\00\00\00\00\E8\0B\00\00\00\00\00\00\A2\7FELF\02\01\013\07\00\01\00f\02\00\BE\00q\00\01\00!@\0B\07\001\00\80\08\07\00\F5\0E\00P\05P\00@\008\00\03\00@\00\0B\00\01\00\00.shstrtab\00.\08\00'ym\08\00\F0\1B_shndx\00.nv.info\00.text.main_kColReduction_r\0A\00\F5\00e__4_1_0___thin7\00\0F1\00\1Aoshared3\00\1A\9Fconstant06\00\17\FA\01debug_frame\00.rel\11\00!nv\14\00\11aB\00\0F'\01 \0F\87\00\14\0FO\01\B6o_paramV\01\1C\0F\01\00\07\8CZ\00\00\00\03\00\0A\00\01\00\11\EC\18\00,\09\00\01\00 )\01\18\00,\04\00\01\00\11G\18\00,\07\00\01\00f2\00\00\00\12\10`\00!\80\01\07\00v\00\FF\FF\FF\FF(\00\0C\00\00\01\00@\03\00\04|\08\00\D1\0F\0C\81\80\80(\00\08\FF\81\80(\08\0B\00\06(\00/0\00\01\00\03\13pX\00\10\04\9B\00R\04\14\00\00\00E\00\01\0B\00\00\13\00p/\08\00\05\00\00\00\9F\03\22\04#\0C\00\00\01\00'\04\12\0C\00\17\11\0C\00!7\04\E0\03\F3\08\015\00\00\04\0A\08\00\02\00\00\00`\01\10\00\03\19\10\00\04\17\0C\04\04U\08\00\00\F0!\10\00\10\01\18\01%\F0\11\10\00\01\01\00\F2\02\F0\11\00\03\1B\FF\00\04\1C\08\00P\00\00\00\B0\00\01\00#K\00\01\00s\02\02\08\10\0A/\22\9B\00\00\07\00\03\08\00\13\10\08\00\13\18\08\00\13 \08\00\13(\08\00\130\08\00\228\08|\00\13\00\08\00\13\08\08\00\13\10\08\00\13\18\08\00\13 \08\00\13(\08\00\130\08\00\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\A2\00\00\00\14,\00\00\00H\00\01\00\000\01/\05\00\01\00\FF\B8A\02z\01\00'\04\B1\0F\00\00\00\C4\0F\00\19y\02\00\01\00\10%\93\02Q\0E\00\19y\03\0F\00\F5\1A\00!\00\00\00$\0E\00$z\02\02\00X\00\00\03\02\8E\07\00\CA\1F\00\0Cz\00\02\00Y\00\00p`\F0\03\00\DA\0F\00M[\04\A0\80\03\00\EA\0F\005t\03\FF\BB\03\10\FF\C8\03P\E2\0F\00\02x>\02B\80\FF\00\0F\10\00r\B9z\04\00\00F\00\84\00\94\D0\0F\00%v\02\02\00Z`\00`\0F\00\86y\00\022\00@\04\19\10\0C0\009My\00`\00PGy\00\00\F0A\04\A6\FF\83\03\00\C0\0F\00\18y\00\01\00\0F\10\00\90\0F\01\00-\00W\01.\03\00\01\00\22@\00\01\00='\01\000\00\08\01\00\1F\0B@\00\04\13g)\00\1FV@\00\0C\13\13\14\04\0C\01\00\13\C0\15\00&\90\008\04#\04\00\8D\04\00\FE\04\12\00\01\00\1F\FAT\00\00\00\01\00\13P\95\00/p\00\80\00\0B\1F)'\00\03#\00\C0@\00\04X\06\04\E4\00*\04\00\01\00\1F`@\00\04\13\F01\00&\\\00@\00\1F\0A@\00\00!\18\01D\01\0D@\00\13P)\00*\D8\00\01\00\1B\08\08\00?\07\01\00\8E\07\00Q\00\00(\05\00\01\00&\10\00\80\00\17\048\00\04\18\00\13\C4\14\01\0C\84\01*8\05@\07\1F\00\C0\00\04\132@\00+\06\00\01\00\1A\07\D8\07\12\03\F8\05:\08\80\00\01\00\13\06\10\06\04(\0B\0C\01\00*\A8\00\08\00\04\F8\00\13\018\00\04\A8\00\0C\01\009H\03\00\08\00\088\00/\06\00\01\00\17\80\08\00\00\00\00\00\00\00\00"} {
    llvm.func @main_kColReduction_reduce__4_1_0___thin(%arg0: i32, %arg1: i32, %arg2: !llvm.ptr<f32>) attributes {disc.elimargs = [2 : index, 4 : index, 5 : index, 6 : index], gpu.kernel, nvvm.kernel} {
      %0 = llvm.mlir.constant(0xFF800000 : f32) : f32
      %1 = nvvm.read.ptx.sreg.ctaid.x : i32
      %2 = nvvm.read.ptx.sreg.tid.x : i32
      llvm.br ^bb1
    ^bb1:  // pred: ^bb0
      %3 = llvm.mul %1, %arg0  : i32
      %4 = llvm.add %2, %3  : i32
      %5 = llvm.icmp "ult" %4, %arg1 : i32
      llvm.cond_br %5, ^bb2, ^bb3
    ^bb2:  // pred: ^bb1
      %6 = llvm.getelementptr %arg2[%4] : (!llvm.ptr<f32>, i32) -> !llvm.ptr<f32>
      llvm.store %0, %6 : !llvm.ptr<f32>
      llvm.br ^bb3
    ^bb3:  // 2 preds: ^bb1, ^bb2
      llvm.return
    }
  }
  gpu.module @main_kernel_2 attributes {gpu.binary = "P\EDU\BA\01\00\10\00(\0C\00\00\00\00\00\00\02\00\01\01@\00\00\00\E8\0B\00\00\00\00\00\00\E3\0B\00\00\00\00\00\00\07\00\01\00P\00\00\00\00\00\00\00\00\00\00\00\11 \00\00\00\00\00\00\00\00\00\00\00\00\00\00(:\00\00\00\00\00\00\A2\7FELF\02\01\013\07\00\01\00f\02\00\BE\00q\00\01\00#\809\08\00\116\07\00\F5\0E\00P\05P\00@\008\00\03\00@\00\0C\00\01\00\00.shstrtab\00.\08\00'ym\08\00\F0\1B_shndx\00.nv.info\00.text.main_kColReduction_r\0A\00\F5\02e__4_1_0___thin_19\00\0F3\00\1Coshared5\00\19Orela\9D\00\1E?rel\D1\00!\9Fconstant08\00\19\B2debug_framey\00\09\11\00!nv\14\00\11aD\00\0F\98\01 \0F\89\00\16\0F\C2\01\EF\8F$____wg_2\00\16\00\0B\00/27\FA\01&o_param\01\02\1C\0F\01\00\0B\8C\\\00\00\00\03\00\0A\00\01\00\11\BF\18\00,\0B\00\01\00 \95\01\18\00,\09\00\01\00\11\D4\18\00,\04\00\01\00\11\F2\18\00,\07\00\01\00g2\00\00\00\12\10x\00\11.\06\00v\00\FF\FF\FF\FF(\00\0C\00\00\01\00@\03\00\04|\08\00\D1\0F\0C\81\80\80(\00\08\FF\81\80(\08\0B\00\06(\00/0\00\01\00\03!\F0-\07\00 \00\04\9B\00R\04\14\00\00\00E\00\22\044\DC\00\90\04/\08\00\06\00\00\00\15\0C\00\12#\0C\00\00\01\00'\04\12\0C\00\17\11\0C\00!7\04\18\05\F1\08\015\00\00\04\0A\08\00\03\00\00\00`\01(\00\03\19(\00\04\17\0C$\00u\06\00 \00\00\F0!\10\00u\05\00\1C\00\00\F0\11\10\009\04\00\18\10\009\03\00\14\10\009\02\00\10\10\009\01\00\08P\00\01\01\00\F2\0A\F0\11\00\03\1B\FF\00\04\1C\0C\00P\00\00\000,\00\000-\00\00\04\1E\84\01#K\00\01\00\B1\02\02\08\10\0A/\22\00\00\00\08\10\00#\00\08\08\00\13\10\08\00\13\18\08\00\13 \08\00\13(\08\00\130\08\00\228\08\84\00\13\00\08\00\13\08\08\00\13\10\08\00\13\18\08\00\13 \08\00\13(\08\00\130\08\00\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\A2\00\00\00\14,\00\00\00H\00\01\00\11\02h\01\0F\01\00\FF\B8@$v\01\FF\87\04\B2\FF\00\8E\07\00\C4\0F\00\19y\00\01\00\10%\93\02a\0E\00\19y\03\00\01\00\10!-\00\F0\04\0E\00$z\00\00\00]\00\00\03\02\8E\07\00\CA\1F\00\0C\10\00\C5^\00\00p`\F0\03\00\DA\0F\00M\A3\04\F0\0C\80\03\00\EA\0F\00\06{\04\00\00_\00\00\00\90 \00\00\22\0E\00\19x\05\FF\1F\1F\00\F1\00\14\01\00\00\E2\0F\00\B9z\04\00\00F\00\00\0B\05 \E2\0FP\00\10\FF0\00@pP\F4\03\10\00cEy\00\00 *P\00p\E2\0F\00\11r\05\05\0E\00\B2\FF@\8F\07\00\C6\0F\00\08s\04\FE\04\10\10\A0\00\F1\06\1E\00\10x\02\04\FE\FF\FF\0F\FF\E0\FF\07\00\CC\1F\00\05s\03\B4\02\00\C8\03\A3\00d\00\00$r\02\FF\FF\00\F0\00a\E4\1F\00$r\07\10\00\C0\03\0A\8E\07\00\C8/\00$z\07\07\80\00\10\FF\E0\00\81\C8\0F\00'r\06\03\07\F8\02 \8E\07\80\00@\19x\03\FF\19\03\10\05\C0\00q\E4\0F\00\12x\05\05\09\05!\C0\8E\90\00T'r\06\06\03`\00a\C8\0F\00$r\04`\00#\06\0A\10\00Tz\02\04\00_@\01\01\F0\006\02\00_@\010\10\0A\02\10\00\11\80\C0\00\80\E4\0F\00\10\08\06\06\01P\00\03\10\00\060\00\11\F2\10\010$x\02$\04A\00\05\0A\8E \00\A2$t\05\FF\00\00\80\FF\FF\00\90\00@\12x\00\02p\01\22\FF\C0 \00@\19x\07\FFA\01@\02\16\01\00\80\00*\10\18`\001\12\AA\06\80\01\22\FF30\00\EF\0Cx\00\02\7F\00\00\00p@\F4\03\00\C6\D0\00\01\16\03\D0\00\00\10\00@x\03\03 p\01\12\02@\00\22$x\81\04\22\07\02\90\00h\0Cz\00\03\00X\C0\00\00\98\05\030\01\96\D8\0F\00G\19\00\00 (@\02E$x\04\06@\00\01`\004\04\04@0\00\02@\01\10\04M\07\C7pb\F8\03\00\E4\0F\04\10x\08\04\D0\00g\04\10x\0A\04\02\E0\00B\0Cz\00\080\00\A5\FA\03\00\CE\0F\00$\C4\07\FF\80\00\00\B0\00V\CA\06\04\00X\D0\00c%\C6\06\06\00Z\80\00\02\A0\02\12\0A@\00 \F0\03 \00@\81\C9\0C\06@\00\B6\00\19\1E\0C\00\A2\00\00$\D4\09P\00\84\E2\0F\00\10x\0E\04\03\80\00\00\F0\009\DA\08\08`\000\D6\08\08`\00\16\09`\00\12\0E`\00 \F6\03\E0\00E\81\D9\08\08`\00j\E2\02\00$\84\0B\B0\009\8A\0A\0AP\000\86\0A\0AP\00\10\0B\10\00u\CC\0F\00\81\89\0A\0A@\00X\22\0F\00$\B4\F0\00i\1F\00$\BA\06\0E@\00\17\B6\F0\00\00\80\00&\B9\0E\E0\00\88b\01\00\10x\10\04\04\D0\00\08p\02p\E2\0F\00\0B\C8\00\0C\10\00\B2\00b\FC\03\00\C8O\00\08\C8\09\10\00\B1\02\00\03\00\E4/\04\0B\C2\00\0CN\07\10\80 \00r\0F\00\08\C2\05\0C\09Y\06\13\03\A0\01\17\10\D0\01\F4\07\00\0B\D2\00\05\08\00\00@\00\C0\FC\03\00\E4\8F\08\10x\0C\04\05\80\00\82\E4\0F\00\08\D2\09\08\05@\00\00\10\003\04\0B\D2\E9\06\04`\005\D2\05\08`\00\1B\E2\F0\01\11\E2\C0\04\15\0C\10\02\11\E2\00\02\15\10\10\01\00\D0\00E\82\00\05\0A\80\00 \0F\09\00\01\17\06\00\01\0C \02I\08\82\09\0A\90\00D\82\00\0A\0A\90\00\10\C60\02\17\060\02i\0E\00\08\82\05\0A\A0\00\0B@\02\06\10\01#\F0\03@\02\19\0C\A0\00F\B2\00\05\0E\A0\00\10\0A \01\18\07\A0\00\0B`\02I\08\B2\0B\0E\A0\00H\B2\00\0E\0E\A0\00\09p\02x\0E\00\08\B2\05\0E\0B\A0\00\08\80\02'\E2\0F@\01\12\F6 \00?\8A\0A\10\90\02\16\10$\90\02\16\0DP\00\00\D0\02:\BA\0C\0C\90\02 \0C\0C@\00\14\0D@\005\B9\0C\0C@\00\12b`\03\14\08\F0\00\01\F0\036\10\04\09\10\00d\00\0B\C2\00\05\06 \01\00\90\029\C2\07\06\00\01D\C2\00\06\06\00\01\02\90\02$\06\07\F0\00\18\E4\90\03\0F\90\02\00o\C8\8F\00\08\D2\07\80\02\0E\1F\07\80\02\06\08\E0\01\05\80\02\06\90\03\0F\80\02\024\0E\04\0A\E0\00\0F\80\02S\1B\0E\80\02\1C\10\80\02\19\0C\80\02?\10\04\0B\80\02\0B\1B\0C\80\02/\0C\0C\80\02\0B\1F\0C\80\02\0C\1B\10\80\02\1F\0E\80\02,\1F\10\80\02\1D\14\0C\F0\00\03\80\02\1F\0D\80\02\CC\14\0E\E0\00\0F\80\02\84\1F\0F\80\02\DC\14\10\F0\00\03\80\02\1F\11\80\02\CC\14\12\E0\00\0F\80\02\84\1F\13\80\02\DC\14\14\F0\00\03\80\02\1F\15\80\02\CC\14\16\E0\00\0F\80\02\84\1F\17\80\02\DC\14\18\F0\00\03\80\02\1F\19\80\02\CC\14\1A\E0\00\0F\80\02\84\1F\1B\80\02\DC\14\1C\F0\00\03\80\02\1F\1D\80\02\CC\14\1E\E0\00\0F\80\02\84\1F\1F\80\02\DC\14 \F0\00\03\80\02\1F!\80\02\CC\14\22\E0\00\0F\80\02\84\1F#\80\02\DC\14$\F0\00\03\80\02\1F%\80\02\CC\14&\E0\00\0F\80\02\84\1F'\80\02\DC\14(\F0\00\03\80\02\1F)\80\02\CC\14*\E0\00\0F\80\02\84\1F+\80\02\DC\14,\F0\00\03\80\02\1F-\80\02\CC\14.\E0\00\0F\80\02\84\1F/\80\02\DC\140\F0\00\03\80\02\1F1\80\02\CC\142\E0\00\0F\80\02\84\1F3\80\02\DC\144\F0\00\03\80\02\1F5\80\02\CC\146\E0\00\0F\80\02\84\1F7\80\02\DC\148\F0\00\03\80\02\1F9\80\02\CC\14:\E0\00\0F\80\02\84\1F;\80\02\DC\14<\F0\00\03\80\02\19=\10\009\12\04>\10\00?\04\04?\A0\02\C5?\C6\0F\01\90\02\1AW\E4\0F\04\81\C9`$/\A4\00\90\02\11\1F\12\90\02\16?\C6\0F\02\80\02+\1F\02\80\02\00\0D@$\1A\04\80\02*\06\12\80\02\07\10\01\01\80\02\07\80\03W$\01\00$\B4\D0\02\01\80\02\1A\0A\80&\1B\B6\C0\028\B9\0A\0A\80\02\00@\02\19\0E@\02\17\09\E0#\01p%\06\E0#\02@\02\15\0E`\01\1E\C8`%\06 \02\01P\02\19\04@\02(\C8\1F0\02\11\02@\00\14\82\C0\02\10\FA0\004\09\0B\82\B0\02\03@\00\14\82\D0\02 \80\060\00I\08\82\05\06@\00\14\B2@\02\010\004\0A\0B\B2 \02\12\F0@\00$\B2\07@\02\02@\003\B2\05\0A@\00\10\00\B0*\14A\B0*\030( \88s\F8-\12\00u(f\E8\0F\00\1D{\00\01\00b\EC\0F\00\84\A9\07,*\00 \00Q\22\0E\00\1Cx\1A\00P\00p\F0\F0\03P\00\13\0C0)1pD\F2\90*!\84\A9n\01\020\00qb\0E\00\0B\A2\00\07\80\00\A1\80\F6\03\00\E4\1F\08\0B\A2\00p%\02\C0\00T/\00\1C\A8\00P\00\14\01@)\11?@)@\F6\03\00\D6\00\01#\07\07R\00\10\06 \00\09\80\00\8F\C6\0F\00\88\A3\00\00\07\C0\00\09\22\B9\05\1C\00\12\08\A0++\84\B9\A0\00\22\B2\00!+`\80\F4\03\00\C4\1F\10\00(\04\05\A0\00#\B8\00p\00Ap\01\00\DA\90\01\15\05\90\00_\CA\0F\00\88\B3@\01\0B9M\19\00p\017$t\02\10\03R\0F\00\84y\05\D1)\00@\00\90&\0E\00%v\02\03\00`\0F/\04 \00\00\1D\00\03`\00vh\0E\00\81y\06\02\D0\02D\05\00\0Br\C0\00\12\F0\C0\00\14r\C0\00W\F2\03\00\D6/\B0\00 \80\04\F0\022\0Br\00`\02\11@\10\04(\0EF\10\02S\E6\0F\00\08r\80\02\00\01\00p\CC\0F\00\A9s\07\02\A0\02\C0\07\E1\1E\00\00\A4\0E\00\0Cr\00\07\10\00 pR@\00QO\00$r\06\E0*\14\07\90*@\09\00\00\90\E0+!\FF\83\F0\00\1BMp\02TGy\00\00\F0 \00f\C0\0F\00\18y\00\01\00\0F\10\00\90\0F\01\00-\11\01H1\0E\01\00\22@\00\01\00=\98\01\000\00\08\01\00\1F\0B@\00\04\13\D8)\00?\01\02\00@\00\0A\13\13\0B\04\0C\01\00\13\E0U\00\03\8F\03\01/\04\13\05w\02\00\01\00\22\18\00\01\00.k\01T\00\00\01\00#\88\04(\03\1F\00\80\00\0B/)\00'\00\02#\00\F8@\00\04 3\04\E4\00*\04\00\01\00\1Fb@\00\04*(\05\C0\00\13\03#/\0C@\00!\89\01D\01\0D@\00\13\D0@\00*\D8\00\01\00\1B\08\08\00?x\01\00V4\000\00\00\A8\15\03\03W/\04\80\00\17\048\00\04\18\00\133@\01\0C\84\01\13\B8@\00\17\881\01\0F\C0\00\01\132T\01\15\06R\00\03^\03\1A\08\A04\11\03$\00J\00\15\80\00\01\00\13\95\94\00*\03\00\01\00\0409/\00\04\80\00\0B\13\06\18\02\04h9\0D\08\01\1A\00\08\00\04\97\00\13\018\00\04\E8\00\0C\01\009\C8/\00\08\00\088\00\18\06\A0\00\0F\01\00\05\03\A9\00\80\08\00\00\00\00\00\00\00\00\00\00\00\00"} {
    llvm.mlir.global internal @__wg_main_kColReduction_reduce__4_1_0___thin_1_0() {addr_space = 3 : i32} : !llvm.array<256 x f32>
    llvm.func @__nv_fabsf(f32) -> f32
    llvm.func @main_kColReduction_reduce__4_1_0___thin_1(%arg0: i32, %arg1: !llvm.ptr<f32>, %arg2: i32, %arg3: i32, %arg4: i32, %arg5: i32, %arg6: !llvm.ptr<f32>) attributes {disc.elimargs = [1 : index, 3 : index, 5 : index, 6 : index, 7 : index, 8 : index, 9 : index, 13 : index, 15 : index, 16 : index, 17 : index], gpu.kernel, nvvm.kernel} {
      %0 = llvm.mlir.constant(32 : index) : i32
      %1 = llvm.mlir.constant(8 : index) : i32
      %2 = llvm.mlir.constant(64 : index) : i32
      %3 = llvm.mlir.constant(2 : index) : i32
      %4 = llvm.mlir.constant(0xFF800000 : f32) : f32
      %5 = llvm.mlir.constant(4 : index) : i32
      %6 = llvm.mlir.constant(1 : index) : i32
      %7 = llvm.mlir.constant(256 : index) : i32
      %8 = llvm.mlir.constant(0 : index) : i32
      %9 = llvm.mlir.addressof @__wg_main_kColReduction_reduce__4_1_0___thin_1_0 : !llvm.ptr<array<256 x f32>, 3>
      %10 = llvm.getelementptr %9[0, 0] : (!llvm.ptr<array<256 x f32>, 3>) -> !llvm.ptr<f32, 3>
      %11 = nvvm.read.ptx.sreg.ctaid.x : i32
      %12 = nvvm.read.ptx.sreg.tid.x : i32
      llvm.br ^bb1
    ^bb1:  // pred: ^bb0
      %13 = llvm.mul %11, %arg3  : i32
      %14 = llvm.add %12, %13  : i32
      %15 = llvm.icmp "ult" %14, %arg4 : i32
      llvm.cond_br %15, ^bb2, ^bb21
    ^bb2:  // pred: ^bb1
      %16 = llvm.srem %14, %7  : i32
      %17 = llvm.sdiv %14, %7  : i32
      %18 = llvm.udiv %17, %arg5  : i32
      %19 = llvm.urem %17, %arg5  : i32
      %20 = llvm.udiv %16, %0  : i32
      %21 = llvm.urem %16, %0  : i32
      %22 = llvm.mul %21, %1  : i32
      %23 = llvm.add %20, %22  : i32
      %24 = llvm.mul %19, %0  : i32
      %25 = llvm.add %21, %24  : i32
      %26 = llvm.icmp "ult" %25, %arg0 : i32
      llvm.cond_br %26, ^bb3, ^bb10(%4 : f32)
    ^bb3:  // pred: ^bb2
      %27 = llvm.mul %18, %1  : i32
      %28 = llvm.add %20, %27  : i32
      %29 = llvm.mul %28, %2  : i32
      llvm.br ^bb4(%8, %4 : i32, f32)
    ^bb4(%30: i32, %31: f32):  // 2 preds: ^bb3, ^bb9
      %32 = llvm.icmp "slt" %30, %2 : i32
      llvm.cond_br %32, ^bb5, ^bb10(%31 : f32)
    ^bb5:  // pred: ^bb4
      %33 = llvm.add %30, %29  : i32
      %34 = llvm.icmp "slt" %33, %arg2 : i32
      llvm.cond_br %34, ^bb6, ^bb7
    ^bb6:  // pred: ^bb5
      %35 = llvm.mul %33, %arg0  : i32
      %36 = llvm.add %35, %25  : i32
      %37 = llvm.getelementptr %arg1[%36] : (!llvm.ptr<f32>, i32) -> !llvm.ptr<f32>
      %38 = llvm.load %37 : !llvm.ptr<f32>
      %39 = llvm.call @__nv_fabsf(%38) : (f32) -> f32
      %40 = llvm.fcmp "ugt" %31, %39 : f32
      %41 = llvm.select %40, %31, %39 : i1, f32
      %42 = llvm.fcmp "uno" %39, %39 : f32
      %43 = llvm.select %42, %39, %41 : i1, f32
      llvm.br ^bb8(%43 : f32)
    ^bb7:  // pred: ^bb5
      llvm.br ^bb8(%31 : f32)
    ^bb8(%44: f32):  // 2 preds: ^bb6, ^bb7
      llvm.br ^bb9
    ^bb9:  // pred: ^bb8
      %45 = llvm.add %30, %6  : i32
      llvm.br ^bb4(%45, %44 : i32, f32)
    ^bb10(%46: f32):  // 2 preds: ^bb2, ^bb4
      llvm.br ^bb11(%46 : f32)
    ^bb11(%47: f32):  // pred: ^bb10
      llvm.br ^bb12
    ^bb12:  // pred: ^bb11
      %48 = llvm.getelementptr %10[%23] : (!llvm.ptr<f32, 3>, i32) -> !llvm.ptr<f32, 3>
      llvm.store %47, %48 : !llvm.ptr<f32, 3>
      nvvm.barrier0
      %49 = llvm.icmp "slt" %20, %5 : i32
      llvm.cond_br %49, ^bb13, ^bb14
    ^bb13:  // pred: ^bb12
      %50 = llvm.add %23, %5  : i32
      %51 = llvm.getelementptr %10[%23] : (!llvm.ptr<f32, 3>, i32) -> !llvm.ptr<f32, 3>
      %52 = llvm.load %51 : !llvm.ptr<f32, 3>
      %53 = llvm.getelementptr %10[%50] : (!llvm.ptr<f32, 3>, i32) -> !llvm.ptr<f32, 3>
      %54 = llvm.load %53 : !llvm.ptr<f32, 3>
      %55 = llvm.fcmp "ugt" %52, %54 : f32
      %56 = llvm.select %55, %52, %54 : i1, f32
      %57 = llvm.fcmp "uno" %54, %54 : f32
      %58 = llvm.select %57, %54, %56 : i1, f32
      %59 = llvm.getelementptr %10[%23] : (!llvm.ptr<f32, 3>, i32) -> !llvm.ptr<f32, 3>
      llvm.store %58, %59 : !llvm.ptr<f32, 3>
      llvm.br ^bb14
    ^bb14:  // 2 preds: ^bb12, ^bb13
      nvvm.barrier0
      %60 = llvm.icmp "slt" %20, %3 : i32
      llvm.cond_br %60, ^bb15, ^bb16
    ^bb15:  // pred: ^bb14
      %61 = llvm.add %23, %3  : i32
      %62 = llvm.getelementptr %10[%23] : (!llvm.ptr<f32, 3>, i32) -> !llvm.ptr<f32, 3>
      %63 = llvm.load %62 : !llvm.ptr<f32, 3>
      %64 = llvm.getelementptr %10[%61] : (!llvm.ptr<f32, 3>, i32) -> !llvm.ptr<f32, 3>
      %65 = llvm.load %64 : !llvm.ptr<f32, 3>
      %66 = llvm.fcmp "ugt" %63, %65 : f32
      %67 = llvm.select %66, %63, %65 : i1, f32
      %68 = llvm.fcmp "uno" %65, %65 : f32
      %69 = llvm.select %68, %65, %67 : i1, f32
      %70 = llvm.getelementptr %10[%23] : (!llvm.ptr<f32, 3>, i32) -> !llvm.ptr<f32, 3>
      llvm.store %69, %70 : !llvm.ptr<f32, 3>
      llvm.br ^bb16
    ^bb16:  // 2 preds: ^bb14, ^bb15
      nvvm.barrier0
      %71 = llvm.icmp "eq" %20, %8 : i32
      %72 = llvm.and %71, %26  : i1
      llvm.cond_br %72, ^bb17, ^bb20
    ^bb17:  // pred: ^bb16
      %73 = llvm.add %23, %6  : i32
      %74 = llvm.getelementptr %10[%23] : (!llvm.ptr<f32, 3>, i32) -> !llvm.ptr<f32, 3>
      %75 = llvm.load %74 : !llvm.ptr<f32, 3>
      %76 = llvm.getelementptr %10[%73] : (!llvm.ptr<f32, 3>, i32) -> !llvm.ptr<f32, 3>
      %77 = llvm.load %76 : !llvm.ptr<f32, 3>
      %78 = llvm.fcmp "ugt" %75, %77 : f32
      %79 = llvm.select %78, %75, %77 : i1, f32
      %80 = llvm.fcmp "uno" %77, %77 : f32
      %81 = llvm.select %80, %77, %79 : i1, f32
      %82 = llvm.getelementptr %arg6[%25] : (!llvm.ptr<f32>, i32) -> !llvm.ptr<f32>
      %83 = llvm.load %82 : !llvm.ptr<f32>
      llvm.br ^bb18(%83 : f32)
    ^bb18(%84: f32):  // 2 preds: ^bb17, ^bb18
      %85 = llvm.fcmp "ogt" %84, %81 : f32
      %86 = llvm.select %85, %84, %81 : i1, f32
      %87 = llvm.bitcast %82 : !llvm.ptr<f32> to !llvm.ptr<i32>
      %88 = llvm.bitcast %84 : f32 to i32
      %89 = llvm.bitcast %86 : f32 to i32
      %90 = llvm.cmpxchg %87, %88, %89 acq_rel monotonic : !llvm.ptr<i32>, i32
      %91 = llvm.extractvalue %90[0] : !llvm.struct<(i32, i1)> 
      %92 = llvm.bitcast %91 : i32 to f32
      %93 = llvm.extractvalue %90[1] : !llvm.struct<(i32, i1)> 
      llvm.cond_br %93, ^bb19, ^bb18(%92 : f32)
    ^bb19:  // pred: ^bb18
      llvm.br ^bb20
    ^bb20:  // 2 preds: ^bb16, ^bb19
      llvm.br ^bb21
    ^bb21:  // 2 preds: ^bb1, ^bb20
      llvm.return
    }
  }
  "disc_shape.SymbolicDim"() {knownNegativeOne = false, knownNonNegative = true, knownNonSizeOne = false, knownNonSizeZero = false, sym_name = "S0", value = -9223372036854775808 : i64} : () -> ()
  "disc_shape.SymbolicDim"() {knownNegativeOne = false, knownNonNegative = true, knownNonSizeOne = false, knownNonSizeZero = false, sym_name = "S1", value = -9223372036854775808 : i64} : () -> ()
  "disc_shape.SymbolicDim"() {knownNegativeOne = false, knownNonNegative = true, knownNonSizeOne = false, knownNonSizeZero = false, sym_name = "S2", value = -9223372036854775808 : i64} : () -> ()
  "disc_shape.SymbolicDim"() {knownNegativeOne = false, knownNonNegative = true, knownNonSizeOne = false, knownNonSizeZero = false, sym_name = "S3", value = -9223372036854775808 : i64} : () -> ()
  func.func @shape_constraint_graph() {
    %0 = "disc_shape.dim"() {name = @S3} : () -> index
    %1 = "disc_shape.dim"() {name = @S1} : () -> index
    %2 = "disc_shape.dim"() {name = @S2} : () -> index
    "disc_shape.tie_product_equal"(%0, %1, %2) {operand_segment_sizes = array<i32: 1, 2>} : (index, index, index) -> ()
    return
  }
}


// -----// IR Dump After StripDebugInfo (strip-debuginfo) //----- //
module attributes {gpu.container_module} {
  func.func @main(%arg0: !disc_ral.context) attributes {tf.entry_function = {input_placements = "gpu", inputs = "input0", output_placements = "gpu", outputs = "output0"}} {
    %c256 = arith.constant 256 : index
    %c32 = arith.constant 32 : index
    %c512 = arith.constant 512 : index
    %0 = llvm.mlir.constant(0 : i32) : i32
    %c0 = arith.constant 0 : index
    %c1 = arith.constant 1 : index
    %c2 = arith.constant 2 : index
    %1 = "disc_ral.dispatch"(%arg0, %c0) {backend_config = "", call_target_name = "ral_recv_input", device = "cpu", has_side_effect = false} : (!disc_ral.context, index) -> memref<?x?x?xf32, "gpu">
    %dim = memref.dim %1, %c2 : memref<?x?x?xf32, "gpu">
    %dim_0 = memref.dim %1, %c1 : memref<?x?x?xf32, "gpu">
    %dim_1 = memref.dim %1, %c0 : memref<?x?x?xf32, "gpu">
    %2 = arith.index_cast %dim_0 : index to i32
    %3 = arith.index_cast %dim : index to i32
    %4 = arith.muli %2, %3 : i32
    %5 = arith.index_cast %4 : i32 to index
    %alloc = memref.alloc(%5) {kDiscSymbolicDimAttr = [@S3]} : memref<?xf32, "gpu">
    %6 = arith.cmpi slt, %dim_1, %5 : index
    cf.cond_br %6, ^bb1, ^bb2
  ^bb1:  // pred: ^bb0
    %c512_2 = arith.constant 512 : index
    %c0_3 = arith.constant 0 : index
    %c1_4 = arith.constant 1 : index
    %7 = arith.cmpi sle, %5, %c0_3 : index
    %8 = arith.subi %c0_3, %5 : index
    %9 = arith.subi %5, %c1_4 : index
    %10 = arith.select %7, %8, %9 : index
    %11 = arith.divsi %10, %c512_2 : index
    %12 = arith.subi %c0_3, %11 : index
    %13 = arith.addi %11, %c1_4 : index
    %14 = arith.select %7, %12, %13 : index
    gpu.launch_func  @main_kernel::@main_kColReduction_reduce__4_1_0___flat blocks in (%14, %c1, %c1) threads in (%c512, %c1, %c1) args(%c512 : index, %5 : index, %alloc : memref<?xf32, "gpu">)
    %15 = arith.cmpi eq, %5, %c0 : index
    %16 = arith.subi %5, %c1 : index
    %17 = arith.divui %16, %c512 : index
    %18 = arith.addi %17, %c1 : index
    %19 = arith.select %15, %c0, %18 : index
    %20 = arith.cmpi eq, %dim_1, %c0 : index
    %21 = arith.subi %dim_1, %c1 : index
    %22 = arith.divui %21, %c32 : index
    %23 = arith.addi %22, %c1 : index
    %24 = arith.select %20, %c0, %23 : index
    %25 = arith.muli %19, %24 : index
    %26 = arith.muli %25, %c512 : index
    %c512_5 = arith.constant 512 : index
    %c0_6 = arith.constant 0 : index
    %c1_7 = arith.constant 1 : index
    %27 = arith.cmpi sle, %26, %c0_6 : index
    %28 = arith.subi %c0_6, %26 : index
    %29 = arith.subi %26, %c1_7 : index
    %30 = arith.select %27, %28, %29 : index
    %31 = arith.divsi %30, %c512_5 : index
    %32 = arith.subi %c0_6, %31 : index
    %33 = arith.addi %31, %c1_7 : index
    %34 = arith.select %27, %32, %33 : index
    gpu.launch_func  @main_kernel_0::@main_kColReduction_reduce__4_1_0___flat_1 blocks in (%34, %c1, %c1) threads in (%c512, %c1, %c1) args(%5 : index, %1 : memref<?x?x?xf32, "gpu">, %c512 : index, %26 : index, %19 : index, %alloc : memref<?xf32, "gpu">)
    cf.br ^bb3
  ^bb2:  // pred: ^bb0
    %c256_8 = arith.constant 256 : index
    %c0_9 = arith.constant 0 : index
    %c1_10 = arith.constant 1 : index
    %35 = arith.cmpi sle, %5, %c0_9 : index
    %36 = arith.subi %c0_9, %5 : index
    %37 = arith.subi %5, %c1_10 : index
    %38 = arith.select %35, %36, %37 : index
    %39 = arith.divsi %38, %c256_8 : index
    %40 = arith.subi %c0_9, %39 : index
    %41 = arith.addi %39, %c1_10 : index
    %42 = arith.select %35, %40, %41 : index
    gpu.launch_func  @main_kernel_1::@main_kColReduction_reduce__4_1_0___thin blocks in (%42, %c1, %c1) threads in (%c256, %c1, %c1) args(%c256 : index, %5 : index, %alloc : memref<?xf32, "gpu">)
    %43 = arith.cmpi eq, %5, %c0 : index
    %44 = arith.subi %5, %c1 : index
    %45 = arith.divui %44, %c32 : index
    %46 = arith.addi %45, %c1 : index
    %47 = arith.select %43, %c0, %46 : index
    %48 = arith.cmpi eq, %dim_1, %c0 : index
    %49 = arith.subi %dim_1, %c1 : index
    %50 = arith.divui %49, %c512 : index
    %51 = arith.addi %50, %c1 : index
    %52 = arith.select %48, %c0, %51 : index
    %53 = arith.muli %47, %52 : index
    %54 = arith.muli %53, %c256 : index
    %c256_11 = arith.constant 256 : index
    %c0_12 = arith.constant 0 : index
    %c1_13 = arith.constant 1 : index
    %55 = arith.cmpi sle, %54, %c0_12 : index
    %56 = arith.subi %c0_12, %54 : index
    %57 = arith.subi %54, %c1_13 : index
    %58 = arith.select %55, %56, %57 : index
    %59 = arith.divsi %58, %c256_11 : index
    %60 = arith.subi %c0_12, %59 : index
    %61 = arith.addi %59, %c1_13 : index
    %62 = arith.select %55, %60, %61 : index
    gpu.launch_func  @main_kernel_2::@main_kColReduction_reduce__4_1_0___thin_1 blocks in (%62, %c1, %c1) threads in (%c256, %c1, %c1) args(%5 : index, %1 : memref<?x?x?xf32, "gpu">, %c256 : index, %54 : index, %47 : index, %alloc : memref<?xf32, "gpu">)
    cf.br ^bb3
  ^bb3:  // 2 preds: ^bb1, ^bb2
    %63 = llvm.inttoptr %0 : i32 to !llvm.ptr<i8>
    %alloca = memref.alloca() : memref<2xindex, "cpu">
    memref.store %dim_0, %alloca[%c0] : memref<2xindex, "cpu">
    memref.store %dim, %alloca[%c1] : memref<2xindex, "cpu">
    %64 = "disc_ral.dispatch"(%arg0, %63, %alloc, %alloca) {backend_config = "", call_target_name = "inc_ref", device = "gpu", has_side_effect = false} : (!disc_ral.context, !llvm.ptr<i8>, memref<?xf32, "gpu">, memref<2xindex, "cpu">) -> memref<?x?xf32, "gpu">
    %reinterpret_cast = memref.reinterpret_cast %64 to offset: [0], sizes: [%dim_0, %dim], strides: [%dim, 1] {kDiscSymbolicDimAttr = [@S1, @S2]} : memref<?x?xf32, "gpu"> to memref<?x?xf32, "gpu">
    memref.dealloc %alloc : memref<?xf32, "gpu">
    "disc_ral.dispatch"(%arg0, %c0, %reinterpret_cast) {backend_config = "", call_target_name = "ral_send_output", device = "cpu", has_side_effect = false} : (!disc_ral.context, index, memref<?x?xf32, "gpu">) -> ()
    return
  }
  gpu.module @main_kernel attributes {gpu.binary = "P\EDU\BA\01\00\10\008\04\00\00\00\00\00\00\02\00\01\01@\00\00\00\F8\03\00\00\00\00\00\00\F7\03\00\00\00\00\00\00\07\00\01\00P\00\00\00\00\00\00\00\00\00\00\00\11 \00\00\00\00\00\00\00\00\00\00\00\00\00\00\E8\0B\00\00\00\00\00\00\A2\7FELF\02\01\013\07\00\01\00f\02\00\BE\00q\00\01\00!@\0B\07\001\00\80\08\07\00\F5\0E\00P\05P\00@\008\00\03\00@\00\0B\00\01\00\00.shstrtab\00.\08\00'ym\08\00\F0\1B_shndx\00.nv.info\00.text.main_kColReduction_r\0A\00\F5\00e__4_1_0___flat7\00\0F1\00\1Aoshared3\00\1A\9Fconstant06\00\17\FA\01debug_frame\00.rel\11\00!nv\14\00\11aB\00\0F'\01 \0F\87\00\14\0FO\01\B6o_paramV\01\1C\0F\01\00\07\8CZ\00\00\00\03\00\0A\00\01\00\11\EC\18\00,\09\00\01\00 )\01\18\00,\04\00\01\00\11G\18\00,\07\00\01\00f2\00\00\00\12\10`\00!\80\01\07\00v\00\FF\FF\FF\FF(\00\0C\00\00\01\00@\03\00\04|\08\00\D1\0F\0C\81\80\80(\00\08\FF\81\80(\08\0B\00\06(\00/0\00\01\00\03\13pX\00\10\04\9B\00R\04\14\00\00\00E\00\01\0B\00\00\13\00p/\08\00\05\00\00\00\9F\03\22\04#\0C\00\00\01\00'\04\12\0C\00\17\11\0C\00!7\04\E0\03\F3\08\015\00\00\04\0A\08\00\02\00\00\00`\01\10\00\03\19\10\00\04\17\0C\04\04U\08\00\00\F0!\10\00\10\01\18\01%\F0\11\10\00\01\01\00\F2\02\F0\11\00\03\1B\FF\00\04\1C\08\00P\00\00\00\B0\00\01\00#K\00\01\00s\02\02\08\10\0A/\22\9B\00\00\07\00\03\08\00\13\10\08\00\13\18\08\00\13 \08\00\13(\08\00\130\08\00\228\08|\00\13\00\08\00\13\08\08\00\13\10\08\00\13\18\08\00\13 \08\00\13(\08\00\130\08\00\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\A2\00\00\00\14,\00\00\00H\00\01\00\000\01/\05\00\01\00\FF\B8A\02z\01\00'\04\B1\0F\00\00\00\C4\0F\00\19y\02\00\01\00\10%\93\02Q\0E\00\19y\03\0F\00\F5\1A\00!\00\00\00$\0E\00$z\02\02\00X\00\00\03\02\8E\07\00\CA\1F\00\0Cz\00\02\00Y\00\00p`\F0\03\00\DA\0F\00M[\04\A0\80\03\00\EA\0F\005t\03\FF\BB\03\10\FF\C8\03P\E2\0F\00\02x>\02B\80\FF\00\0F\10\00r\B9z\04\00\00F\00\84\00\94\D0\0F\00%v\02\02\00Z`\00`\0F\00\86y\00\022\00@\04\19\10\0C0\009My\00`\00PGy\00\00\F0A\04\A6\FF\83\03\00\C0\0F\00\18y\00\01\00\0F\10\00\90\0F\01\00-\00W\01.\03\00\01\00\22@\00\01\00='\01\000\00\08\01\00\1F\0B@\00\04\13g)\00\1FV@\00\0C\13\13\14\04\0C\01\00\13\C0\15\00&\90\008\04#\04\00\8D\04\00\FE\04\12\00\01\00\1F\FAT\00\00\00\01\00\13P\95\00/p\00\80\00\0B\1F)'\00\03#\00\C0@\00\04X\06\04\E4\00*\04\00\01\00\1F`@\00\04\13\F01\00&\\\00@\00\1F\0A@\00\00!\18\01D\01\0D@\00\13P)\00*\D8\00\01\00\1B\08\08\00?\07\01\00\8E\07\00Q\00\00(\05\00\01\00&\10\00\80\00\17\048\00\04\18\00\13\C4\14\01\0C\84\01*8\05@\07\1F\00\C0\00\04\132@\00+\06\00\01\00\1A\07\D8\07\12\03\F8\05:\08\80\00\01\00\13\06\10\06\04(\0B\0C\01\00*\A8\00\08\00\04\F8\00\13\018\00\04\A8\00\0C\01\009H\03\00\08\00\088\00/\06\00\01\00\17\80\08\00\00\00\00\00\00\00\00"} {
    llvm.func @main_kColReduction_reduce__4_1_0___flat(%arg0: i32, %arg1: i32, %arg2: !llvm.ptr<f32>) attributes {disc.elimargs = [2 : index, 4 : index, 5 : index, 6 : index], gpu.kernel, nvvm.kernel} {
      %0 = llvm.mlir.constant(0xFF800000 : f32) : f32
      %1 = nvvm.read.ptx.sreg.ctaid.x : i32
      %2 = nvvm.read.ptx.sreg.tid.x : i32
      llvm.br ^bb1
    ^bb1:  // pred: ^bb0
      %3 = llvm.mul %1, %arg0  : i32
      %4 = llvm.add %2, %3  : i32
      %5 = llvm.icmp "ult" %4, %arg1 : i32
      llvm.cond_br %5, ^bb2, ^bb3
    ^bb2:  // pred: ^bb1
      %6 = llvm.getelementptr %arg2[%4] : (!llvm.ptr<f32>, i32) -> !llvm.ptr<f32>
      llvm.store %0, %6 : !llvm.ptr<f32>
      llvm.br ^bb3
    ^bb3:  // 2 preds: ^bb1, ^bb2
      llvm.return
    }
  }
  gpu.module @main_kernel_0 attributes {gpu.binary = "P\EDU\BA\01\00\10\00\90\0A\00\00\00\00\00\00\02\00\01\01@\00\00\00P\0A\00\00\00\00\00\00P\0A\00\00\00\00\00\00\07\00\01\00P\00\00\00\00\00\00\00\00\00\00\00\11 \00\00\00\00\00\00\00\00\00\00\00\00\00\00\E8\1C\00\00\00\00\00\00\A2\7FELF\02\01\013\07\00\01\00f\02\00\BE\00q\00\01\00!@\1C\07\001\00\80\19\07\00\F5\0E\00P\05P\00@\008\00\03\00@\00\0B\00\01\00\00.shstrtab\00.\08\00'ym\08\00\F0\1B_shndx\00.nv.info\00.text.main_kColReduction_r\0A\00\F5\02e__4_1_0___flat_19\00\0F3\00\1Coshared5\00\19Orela\9D\00\1E?rel\D1\00!\9Fconstant08\00\19\B2debug_framey\00\09\11\00!nv\14\00\11aD\00\0F\98\01 \0F\89\00\16\0F\C2\01\FF(o_param\C9\01\1C\0F\01\00\0B\8C\\\00\00\00\03\00\0A\00\01\00 ]\01\18\00,\09\00\01\00\11\9C\18\00,\04\00\01\00\11\BA\18\00,\07\00\01\00f2\00\00\00\12\10`\00!\80\11\07\00v\00\FF\FF\FF\FF(\00\0C\00\00\01\00@\03\00\04|\08\00\D1\0F\0C\81\80\80(\00\08\FF\81\80(\08\0B\00\06(\00/0\00\01\00\03\13pX\00\10\04\9B\00R\04\14\00\00\00E\00\22\04 \AC\00\90\04/\08\00\05\00\00\00\16\0C\00\12#\0C\00\00\01\00'\04\12\0C\00\17\11\0C\00!7\04\C8\04\F1\08\015\00\00\04\0A\08\00\02\00\00\00`\01(\00\03\19(\00\04\17\0C$\00u\06\00 \00\00\F0!\10\00u\05\00\1C\00\00\F0\11\10\009\04\00\18\10\009\03\00\14\10\009\02\00\10\10\009\01\00\08P\00\01\01\00\F2\0A\F0\11\00\03\1B\FF\00\04\1C\0C\00P\00\00\00\10\02\00\00\E0\10\00\00\04\1E\D8\00#K\00\01\00\B1\02\02\08\10\0A/\22\00\00\00\08\10\00#\00\08\08\00\13\10\08\00\13\18\08\00\13 \08\00\13(\08\00\130\08\00\228\08\84\00\13\00\08\00\13\08\08\00\13\10\08\00\13\18\08\00\13 \08\00\13(\08\00\130\08\00\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\A2\00\00\00\14,\00\00\00H\00\01\00\00x\01/\05\00\01\00\FF\88@$v\01\FF?\04\B2\FF\00\8E\07\00\C4\0F\00\19y\00\01\00\10%c\02Q\0E\00\19y\03\0F\00 \00!-\00@\0E\00$z]\04\B0\00\00\03\02\8E\07\00\CA\1F\00\0C\10\00\C5^\00\00p`\F0\03\00\DA\0F\00Ms\04\F0\0C\80\03\00\EA\0F\00\06{\04\00\00_\00\00\00\90 \00\00\22\0E\00\19x\05\FF\1F\1F\00\D3\14\01\00\00\E2\0F\00$r\02\FF\FF\00\80\00 \E2\0FP\00\10\FF0\00\B1pP\F4\03\00\E4\0F\00\11r\05?\02\B2\FFH\8F\07\00\C6\0F\00\08s\04\BE\04\10\10\90\00\F1\07\1E\00\10x\03\04\FE\FF\FF\0F\FF\E0\FF\07\00\CC\1F\00\05s\03\00\B2\00\22\F0!\B0\00!r\07`\00\C0\03\0A\8E\07\00\C8\1F\00$z\07\07`\00\10\FF\C0\00\81\C8\0F\00'r\07\03\07\A8\02\02\80\00@\19x\02\FF\C3\00\10\05\A0\00\E1\E4\0F\00\12x\05\05\00\FE\FF\FF\FF\C0\8E\80\00T'r\07\07\02\B0\00\10\C8\C0\00\11\09`\00#\07\0A\10\000z\03\09`\00\11\02 \01\01\D0\006\03\00_ \010\10\0A\03\10\00\11\80\B0\00\80\E4\0F\00\10\08\07\07\01P\00\03\10\00\060\00\12\F20\00\1A\18 \001\12\AA\07 \01\22\FF3`\00;$r\03\80\00%\02\03\80\00`\E4\0F\00$x\03\14\04$\00\05 \000x\03\02\95\03\18\03\A0\00\1FX\C0\01\07W$x\02\07 \A0\01\81\B9z\04\00\00F\00\00s\06\00\C0\01Dt\00\FF\04 \00\11\C6P\00\10\02\95\060pb\F8\C0\01U\04\10x\06\02\C0\00\11\C8 \00\12\06 \00\C3\FA\03\00\CE\0F\00$\CA\05\02\00X\90\00\90\C8\0F\00%\C6\04\05\00Z\91\01\03\B0\005\DA\07\06 \00a\E2\0F\00\81\C9\0A#\06\C5\00\19\1E\0C\00\A6\00\00%\D6\06\070\00u\CA\0F\00\81\D9\0D\06 \00\86\E2\02\00\10x\08\02\02@\01j\04\10x\10\02\03p\01\12\08\90\00#\F0\03\10\00\12\10\10\00\11\F2\10\00Y\10x\0C\02\04@\004\0E\02\05\10\00\11\C40\00\12\0C0\00 \F4\03\80\00E$\8A\09\08\B0\00w\E4\0F\10$\9A\0B\10\10\00U\00%\86\08\09\B0\00\02\F0\02\12\0E@\00 \F6\03\F0\00E\81\89\05\08\C0\00v\22\11\00%\96\06\0B0\00 /\00\C0\00\14\06\80\00w\C6\0F\00$\AA\0B\0C \01F\10\81\99\0F\00\01\B4\22\03\00$t\04\FF\00\00\80\FF\C0\01\00\D0\00\14\07@\00\00\C0\015\BA\09\0E@\00W\C8\1F\00%\B6\A0\00g\CC\0F\00\81\B9\09\90\00@\01\00\0B\C8\FA\01\D2\80\FF\00\C2\FC\03\00\C8O\00\08\C8\04\10\000\02\00\03P\007%\A6\0A\B0\00\08P\01\01\10\02\F5\06\00\0B\D2\00\04\0D\00\00@\00`\FC\03\00\C6\8F\00\81\A9\0B\0A`\00b\E2\04\00\08\D2\04 \00!\00\000\00\06`\01\12\FA`\01&\CA\07P\01z\C8/\00%\C6\06\070\02*\0D\0C0\02\16\11\10\01y\E6\02\00%\D6\0C\0D\E0\006\D9\0D\0C\80\00t\00\00\0B\82\00\04\05\A0\00\84\E4\0F\09\10x\0A\02\080\01\8A\E4O\00\08\82\04\04\05\A0\00\18\0A@\02V\0B\92\00\04\0F@\00\01\80\02\14\09@\00\8A\C4\1F\00\08\92\04\04\0F@\00\1A\08p\028\0E\02\0A\E0\016\8A\07\0A\D0\00\11/\C0\01\17\0B \00+%\86\00\03&\89\05\F0\00!\A2\000\02\14\0C0\00\00\A0\00D\A2\00\04\0B\A0\00\8A\C8\8F\00\08\A2\04\04\0B\90\00\15\0E\D0\02\000\00D\B2\00\04\090\00\000\02&\9A\0B\E0\02\00\90\05D\B2\04\04\09@\00\00\00\02(\96\08\00\02\16\08\C0\01\12\F6P\00D\C2\00\04\11P\00x\E4\0F\08\81\99\0F\08\B0\03H$\AA\0B\0E\C0\01Z\08\C2\04\04\11\A0\00\06P\02\0Fp\02\00\00@\01\17\0D\A0\03\09p\02f\E2\0F\08$\BA\07 \02*\E4\1F\80\02.\22\01\80\02\05\10\01\12\FAp\00\1B\B6p\028\CA\09\10\A0\01'\81\B9p\02{&\03\00%\C6\08\09\A0\02\1A\0E\A0\02\16\13\00\01/&\01\A0\02\0B+\22\03\A0\02 O\08\A0\02\15\0E\E0\01\1F\1F\A0\02\0E\00 \01\1B\0F\10\05\17\100\01\00`\02\14\11\10\00*\CE/\80\02\1F\C8p\02\0F\09\00\03/\C8\8F\F0\02\01\18\0E\F0\02\0A\90\02/\0F\01\90\02\00\1C\10\90\02\05@\02\00 \03)\9A\09P\01'\08\B20\02\11\C6\90\02\06\80\01\0F\90\02\04\14\13P\000\E4\0F\08 \01\14\12\00\01\11\E2`\05\18\10`\00\0C\B0\02\00\A0\02\1E\13\E0\00\0F\A0\02\06\09\90\02\01p\018\10\02\13p\00\08\A0\02*\E2\1F\90\02-\E2\0F\B0\02\06 \01\0F\A0\02\06\1F\0E\A0\02\1C\1F\10\A0\02=\1F\14\A0\02\1C\1B\15\A0\02\1B\16\A0\02\1F\17\A0\02\A7\19\10\A0\02\02@\00\06\90\02\1D\E2\B0\02\0F\A0\02\02\14\18\00\01\0F\A0\02\1D\1F\C4\A0\02\0E\19\08\A0\02\01p\00?\10\02\19\A0\02U\10\E4\A0\02\16\07\B0\01\1F$\A0\02\11\1F\11\A0\02.\1F\1A\A0\02\1A5\06\02\1B0\00\11/\10\0A\1B\1C\A0\02\14\1D\10\00\11\CE\10\0A\0B\A0\02+\08\09\A0\02\1C\08\B0\07\19\1E\00\036\02\02\1F\10\00\0F\C0\02\11\1F\06\C0\02\1C\1C\0E\C0\02\19\07\C0\02,\0B\06`\05\14\07@\00+\C6\0F\A0\0A\0F\F0\07\0C\00\80\07\08\E0\07\10\E4\B0\02,\07\06\B0\02\1E\11`\03\0FP\05\13\11\C6\D0\0A\0F\A0\02\18\08\B0\0C\04\A0\02\0Bp\02\17\CAp\02\1B\E4\10\0B\10$\B0\0C)\0C\0D0\009\DA\0F\020\009\C9\0D\0C0\00:\D6\0E\0F\A0\02(\0F\0E\90\00g%v\02\03\00` \025y\06\02 \00*b!\C0\02*\C8O\B0\02\12\C8 \02\02\C0\01\15\F0 \02\03\B0\018\00\00\C8\10\02\1C\F0\10\02\02 \00\05\F0\09\01 \00\08\E0\09\02 \00\14\C2p\01\03 \00\15\C2`\01\02 \00\14\D2\A0\02\03 \00\15\D2\A0\02\10\00`\0C2\0Br\00\F0\01 @\F0p\017\0EFyp\0ES\E6\0F\00\08r\10\02\00\01\00\80\CC\0F\00\A9s\07\02\06\E0\0E\B0\E1\1E\00\00\A4\0E\00\0Cr\00\07\10\00 pR@\00RO\00$r\06\00\0F\00\E0\0C\80\D8\0F\00G\09\00\00\90i\14!\FF\83\C0\0E*My\D0\0ETGy\00\00\F0 \00f\C0\0F\00\18y\00\01\00\0F\10\00`\0F\01\00-\00\\\0F.\03\00\01\00\02q\01]\00\00\98\01\000\00\08\01\00\1F\0B@\00\04\13\D8)\00\1F\C9@\00\0C\13\13\E4\13\0C\01\00\13\A8U\00\11\90\06\00\02$\00#\04\00]\14\00\CE\14\12\00\01\00.k\01T\00\00\01\00\138\05\02/p\00\80\00\0B\1F)'\00\03#\00\A8@\00\04p\16\04\E4\00*\04\00\01\00\1Fb@\00\04\13\D8)\00&\A8\00@\00\1F\0A@\00\00!\89\01D\01\0D@\00\11\80\B6\12J\00\00\D8\00\01\00\1B\08\08\00?x\01\00\A6\17\000\00\00X\E5\02\03\E7\12\04\80\00\17\048\00\04\18\00\133@\01\0C\84\01\13h@\00\17\881\01\0F\C0\00\01\132T\01+\06\00\01\00\1A\08\F0\17\12\03\C8\15:\16\80\00\01\00\13\06\E0\15\04(\1C\0D\88\01\1A\00\08\00\04\C0\00\13\018\00\04\A8\00\0C\01\009\18\13\00\08\00\088\00/\06\00\01\00\17\80\08\00\00\00\00\00\00\00"} {
    llvm.func @__nv_fabsf(f32) -> f32
    llvm.func @main_kColReduction_reduce__4_1_0___flat_1(%arg0: i32, %arg1: !llvm.ptr<f32>, %arg2: i32, %arg3: i32, %arg4: i32, %arg5: i32, %arg6: !llvm.ptr<f32>) attributes {disc.elimargs = [1 : index, 3 : index, 5 : index, 6 : index, 7 : index, 8 : index, 9 : index, 13 : index, 15 : index, 16 : index, 17 : index], gpu.kernel, nvvm.kernel} {
      %0 = llvm.mlir.constant(0 : index) : i32
      %1 = llvm.mlir.constant(1 : index) : i32
      %2 = llvm.mlir.constant(512 : index) : i32
      %3 = llvm.mlir.constant(32 : index) : i32
      %4 = llvm.mlir.constant(0xFF800000 : f32) : f32
      %5 = nvvm.read.ptx.sreg.ctaid.x : i32
      %6 = nvvm.read.ptx.sreg.tid.x : i32
      llvm.br ^bb1
    ^bb1:  // pred: ^bb0
      %7 = llvm.mul %5, %arg3  : i32
      %8 = llvm.add %6, %7  : i32
      %9 = llvm.icmp "ult" %8, %arg4 : i32
      llvm.cond_br %9, ^bb2, ^bb14
    ^bb2:  // pred: ^bb1
      %10 = llvm.srem %8, %2  : i32
      %11 = llvm.sdiv %8, %2  : i32
      %12 = llvm.udiv %11, %arg5  : i32
      %13 = llvm.urem %11, %arg5  : i32
      %14 = llvm.mul %13, %2  : i32
      %15 = llvm.add %14, %10  : i32
      %16 = llvm.icmp "ult" %15, %arg0 : i32
      llvm.cond_br %16, ^bb3, ^bb13
    ^bb3:  // pred: ^bb2
      %17 = llvm.mul %12, %3  : i32
      llvm.br ^bb4(%0, %4 : i32, f32)
    ^bb4(%18: i32, %19: f32):  // 2 preds: ^bb3, ^bb9
      %20 = llvm.icmp "slt" %18, %3 : i32
      llvm.cond_br %20, ^bb5, ^bb10
    ^bb5:  // pred: ^bb4
      %21 = llvm.add %17, %18  : i32
      %22 = llvm.icmp "slt" %21, %arg2 : i32
      llvm.cond_br %22, ^bb6, ^bb7
    ^bb6:  // pred: ^bb5
      %23 = llvm.mul %21, %arg0  : i32
      %24 = llvm.add %23, %15  : i32
      %25 = llvm.getelementptr %arg1[%24] : (!llvm.ptr<f32>, i32) -> !llvm.ptr<f32>
      %26 = llvm.load %25 : !llvm.ptr<f32>
      %27 = llvm.call @__nv_fabsf(%26) : (f32) -> f32
      %28 = llvm.fcmp "oge" %19, %27 : f32
      %29 = llvm.select %28, %19, %27 : i1, f32
      llvm.br ^bb8(%29 : f32)
    ^bb7:  // pred: ^bb5
      llvm.br ^bb8(%19 : f32)
    ^bb8(%30: f32):  // 2 preds: ^bb6, ^bb7
      llvm.br ^bb9
    ^bb9:  // pred: ^bb8
      %31 = llvm.add %18, %1  : i32
      llvm.br ^bb4(%31, %30 : i32, f32)
    ^bb10:  // pred: ^bb4
      %32 = llvm.getelementptr %arg6[%15] : (!llvm.ptr<f32>, i32) -> !llvm.ptr<f32>
      %33 = llvm.load %32 : !llvm.ptr<f32>
      llvm.br ^bb11(%33 : f32)
    ^bb11(%34: f32):  // 2 preds: ^bb10, ^bb11
      %35 = llvm.fcmp "ogt" %34, %19 : f32
      %36 = llvm.select %35, %34, %19 : i1, f32
      %37 = llvm.bitcast %32 : !llvm.ptr<f32> to !llvm.ptr<i32>
      %38 = llvm.bitcast %34 : f32 to i32
      %39 = llvm.bitcast %36 : f32 to i32
      %40 = llvm.cmpxchg %37, %38, %39 acq_rel monotonic : !llvm.ptr<i32>, i32
      %41 = llvm.extractvalue %40[0] : !llvm.struct<(i32, i1)> 
      %42 = llvm.bitcast %41 : i32 to f32
      %43 = llvm.extractvalue %40[1] : !llvm.struct<(i32, i1)> 
      llvm.cond_br %43, ^bb12, ^bb11(%42 : f32)
    ^bb12:  // pred: ^bb11
      llvm.br ^bb13
    ^bb13:  // 2 preds: ^bb2, ^bb12
      llvm.br ^bb14
    ^bb14:  // 2 preds: ^bb1, ^bb13
      llvm.return
    }
  }
  gpu.module @main_kernel_1 attributes {gpu.binary = "P\EDU\BA\01\00\10\008\04\00\00\00\00\00\00\02\00\01\01@\00\00\00\F8\03\00\00\00\00\00\00\F7\03\00\00\00\00\00\00\07\00\01\00P\00\00\00\00\00\00\00\00\00\00\00\11 \00\00\00\00\00\00\00\00\00\00\00\00\00\00\E8\0B\00\00\00\00\00\00\A2\7FELF\02\01\013\07\00\01\00f\02\00\BE\00q\00\01\00!@\0B\07\001\00\80\08\07\00\F5\0E\00P\05P\00@\008\00\03\00@\00\0B\00\01\00\00.shstrtab\00.\08\00'ym\08\00\F0\1B_shndx\00.nv.info\00.text.main_kColReduction_r\0A\00\F5\00e__4_1_0___thin7\00\0F1\00\1Aoshared3\00\1A\9Fconstant06\00\17\FA\01debug_frame\00.rel\11\00!nv\14\00\11aB\00\0F'\01 \0F\87\00\14\0FO\01\B6o_paramV\01\1C\0F\01\00\07\8CZ\00\00\00\03\00\0A\00\01\00\11\EC\18\00,\09\00\01\00 )\01\18\00,\04\00\01\00\11G\18\00,\07\00\01\00f2\00\00\00\12\10`\00!\80\01\07\00v\00\FF\FF\FF\FF(\00\0C\00\00\01\00@\03\00\04|\08\00\D1\0F\0C\81\80\80(\00\08\FF\81\80(\08\0B\00\06(\00/0\00\01\00\03\13pX\00\10\04\9B\00R\04\14\00\00\00E\00\01\0B\00\00\13\00p/\08\00\05\00\00\00\9F\03\22\04#\0C\00\00\01\00'\04\12\0C\00\17\11\0C\00!7\04\E0\03\F3\08\015\00\00\04\0A\08\00\02\00\00\00`\01\10\00\03\19\10\00\04\17\0C\04\04U\08\00\00\F0!\10\00\10\01\18\01%\F0\11\10\00\01\01\00\F2\02\F0\11\00\03\1B\FF\00\04\1C\08\00P\00\00\00\B0\00\01\00#K\00\01\00s\02\02\08\10\0A/\22\9B\00\00\07\00\03\08\00\13\10\08\00\13\18\08\00\13 \08\00\13(\08\00\130\08\00\228\08|\00\13\00\08\00\13\08\08\00\13\10\08\00\13\18\08\00\13 \08\00\13(\08\00\130\08\00\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\A2\00\00\00\14,\00\00\00H\00\01\00\000\01/\05\00\01\00\FF\B8A\02z\01\00'\04\B1\0F\00\00\00\C4\0F\00\19y\02\00\01\00\10%\93\02Q\0E\00\19y\03\0F\00\F5\1A\00!\00\00\00$\0E\00$z\02\02\00X\00\00\03\02\8E\07\00\CA\1F\00\0Cz\00\02\00Y\00\00p`\F0\03\00\DA\0F\00M[\04\A0\80\03\00\EA\0F\005t\03\FF\BB\03\10\FF\C8\03P\E2\0F\00\02x>\02B\80\FF\00\0F\10\00r\B9z\04\00\00F\00\84\00\94\D0\0F\00%v\02\02\00Z`\00`\0F\00\86y\00\022\00@\04\19\10\0C0\009My\00`\00PGy\00\00\F0A\04\A6\FF\83\03\00\C0\0F\00\18y\00\01\00\0F\10\00\90\0F\01\00-\00W\01.\03\00\01\00\22@\00\01\00='\01\000\00\08\01\00\1F\0B@\00\04\13g)\00\1FV@\00\0C\13\13\14\04\0C\01\00\13\C0\15\00&\90\008\04#\04\00\8D\04\00\FE\04\12\00\01\00\1F\FAT\00\00\00\01\00\13P\95\00/p\00\80\00\0B\1F)'\00\03#\00\C0@\00\04X\06\04\E4\00*\04\00\01\00\1F`@\00\04\13\F01\00&\\\00@\00\1F\0A@\00\00!\18\01D\01\0D@\00\13P)\00*\D8\00\01\00\1B\08\08\00?\07\01\00\8E\07\00Q\00\00(\05\00\01\00&\10\00\80\00\17\048\00\04\18\00\13\C4\14\01\0C\84\01*8\05@\07\1F\00\C0\00\04\132@\00+\06\00\01\00\1A\07\D8\07\12\03\F8\05:\08\80\00\01\00\13\06\10\06\04(\0B\0C\01\00*\A8\00\08\00\04\F8\00\13\018\00\04\A8\00\0C\01\009H\03\00\08\00\088\00/\06\00\01\00\17\80\08\00\00\00\00\00\00\00\00"} {
    llvm.func @main_kColReduction_reduce__4_1_0___thin(%arg0: i32, %arg1: i32, %arg2: !llvm.ptr<f32>) attributes {disc.elimargs = [2 : index, 4 : index, 5 : index, 6 : index], gpu.kernel, nvvm.kernel} {
      %0 = llvm.mlir.constant(0xFF800000 : f32) : f32
      %1 = nvvm.read.ptx.sreg.ctaid.x : i32
      %2 = nvvm.read.ptx.sreg.tid.x : i32
      llvm.br ^bb1
    ^bb1:  // pred: ^bb0
      %3 = llvm.mul %1, %arg0  : i32
      %4 = llvm.add %2, %3  : i32
      %5 = llvm.icmp "ult" %4, %arg1 : i32
      llvm.cond_br %5, ^bb2, ^bb3
    ^bb2:  // pred: ^bb1
      %6 = llvm.getelementptr %arg2[%4] : (!llvm.ptr<f32>, i32) -> !llvm.ptr<f32>
      llvm.store %0, %6 : !llvm.ptr<f32>
      llvm.br ^bb3
    ^bb3:  // 2 preds: ^bb1, ^bb2
      llvm.return
    }
  }
  gpu.module @main_kernel_2 attributes {gpu.binary = "P\EDU\BA\01\00\10\00(\0C\00\00\00\00\00\00\02\00\01\01@\00\00\00\E8\0B\00\00\00\00\00\00\E3\0B\00\00\00\00\00\00\07\00\01\00P\00\00\00\00\00\00\00\00\00\00\00\11 \00\00\00\00\00\00\00\00\00\00\00\00\00\00(:\00\00\00\00\00\00\A2\7FELF\02\01\013\07\00\01\00f\02\00\BE\00q\00\01\00#\809\08\00\116\07\00\F5\0E\00P\05P\00@\008\00\03\00@\00\0C\00\01\00\00.shstrtab\00.\08\00'ym\08\00\F0\1B_shndx\00.nv.info\00.text.main_kColReduction_r\0A\00\F5\02e__4_1_0___thin_19\00\0F3\00\1Coshared5\00\19Orela\9D\00\1E?rel\D1\00!\9Fconstant08\00\19\B2debug_framey\00\09\11\00!nv\14\00\11aD\00\0F\98\01 \0F\89\00\16\0F\C2\01\EF\8F$____wg_2\00\16\00\0B\00/27\FA\01&o_param\01\02\1C\0F\01\00\0B\8C\\\00\00\00\03\00\0A\00\01\00\11\BF\18\00,\0B\00\01\00 \95\01\18\00,\09\00\01\00\11\D4\18\00,\04\00\01\00\11\F2\18\00,\07\00\01\00g2\00\00\00\12\10x\00\11.\06\00v\00\FF\FF\FF\FF(\00\0C\00\00\01\00@\03\00\04|\08\00\D1\0F\0C\81\80\80(\00\08\FF\81\80(\08\0B\00\06(\00/0\00\01\00\03!\F0-\07\00 \00\04\9B\00R\04\14\00\00\00E\00\22\044\DC\00\90\04/\08\00\06\00\00\00\15\0C\00\12#\0C\00\00\01\00'\04\12\0C\00\17\11\0C\00!7\04\18\05\F1\08\015\00\00\04\0A\08\00\03\00\00\00`\01(\00\03\19(\00\04\17\0C$\00u\06\00 \00\00\F0!\10\00u\05\00\1C\00\00\F0\11\10\009\04\00\18\10\009\03\00\14\10\009\02\00\10\10\009\01\00\08P\00\01\01\00\F2\0A\F0\11\00\03\1B\FF\00\04\1C\0C\00P\00\00\000,\00\000-\00\00\04\1E\84\01#K\00\01\00\B1\02\02\08\10\0A/\22\00\00\00\08\10\00#\00\08\08\00\13\10\08\00\13\18\08\00\13 \08\00\13(\08\00\130\08\00\228\08\84\00\13\00\08\00\13\08\08\00\13\10\08\00\13\18\08\00\13 \08\00\13(\08\00\130\08\00\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\A2\00\00\00\14,\00\00\00H\00\01\00\11\02h\01\0F\01\00\FF\B8@$v\01\FF\87\04\B2\FF\00\8E\07\00\C4\0F\00\19y\00\01\00\10%\93\02a\0E\00\19y\03\00\01\00\10!-\00\F0\04\0E\00$z\00\00\00]\00\00\03\02\8E\07\00\CA\1F\00\0C\10\00\C5^\00\00p`\F0\03\00\DA\0F\00M\A3\04\F0\0C\80\03\00\EA\0F\00\06{\04\00\00_\00\00\00\90 \00\00\22\0E\00\19x\05\FF\1F\1F\00\F1\00\14\01\00\00\E2\0F\00\B9z\04\00\00F\00\00\0B\05 \E2\0FP\00\10\FF0\00@pP\F4\03\10\00cEy\00\00 *P\00p\E2\0F\00\11r\05\05\0E\00\B2\FF@\8F\07\00\C6\0F\00\08s\04\FE\04\10\10\A0\00\F1\06\1E\00\10x\02\04\FE\FF\FF\0F\FF\E0\FF\07\00\CC\1F\00\05s\03\B4\02\00\C8\03\A3\00d\00\00$r\02\FF\FF\00\F0\00a\E4\1F\00$r\07\10\00\C0\03\0A\8E\07\00\C8/\00$z\07\07\80\00\10\FF\E0\00\81\C8\0F\00'r\06\03\07\F8\02 \8E\07\80\00@\19x\03\FF\19\03\10\05\C0\00q\E4\0F\00\12x\05\05\09\05!\C0\8E\90\00T'r\06\06\03`\00a\C8\0F\00$r\04`\00#\06\0A\10\00Tz\02\04\00_@\01\01\F0\006\02\00_@\010\10\0A\02\10\00\11\80\C0\00\80\E4\0F\00\10\08\06\06\01P\00\03\10\00\060\00\11\F2\10\010$x\02$\04A\00\05\0A\8E \00\A2$t\05\FF\00\00\80\FF\FF\00\90\00@\12x\00\02p\01\22\FF\C0 \00@\19x\07\FFA\01@\02\16\01\00\80\00*\10\18`\001\12\AA\06\80\01\22\FF30\00\EF\0Cx\00\02\7F\00\00\00p@\F4\03\00\C6\D0\00\01\16\03\D0\00\00\10\00@x\03\03 p\01\12\02@\00\22$x\81\04\22\07\02\90\00h\0Cz\00\03\00X\C0\00\00\98\05\030\01\96\D8\0F\00G\19\00\00 (@\02E$x\04\06@\00\01`\004\04\04@0\00\02@\01\10\04M\07\C7pb\F8\03\00\E4\0F\04\10x\08\04\D0\00g\04\10x\0A\04\02\E0\00B\0Cz\00\080\00\A5\FA\03\00\CE\0F\00$\C4\07\FF\80\00\00\B0\00V\CA\06\04\00X\D0\00c%\C6\06\06\00Z\80\00\02\A0\02\12\0A@\00 \F0\03 \00@\81\C9\0C\06@\00\B6\00\19\1E\0C\00\A2\00\00$\D4\09P\00\84\E2\0F\00\10x\0E\04\03\80\00\00\F0\009\DA\08\08`\000\D6\08\08`\00\16\09`\00\12\0E`\00 \F6\03\E0\00E\81\D9\08\08`\00j\E2\02\00$\84\0B\B0\009\8A\0A\0AP\000\86\0A\0AP\00\10\0B\10\00u\CC\0F\00\81\89\0A\0A@\00X\22\0F\00$\B4\F0\00i\1F\00$\BA\06\0E@\00\17\B6\F0\00\00\80\00&\B9\0E\E0\00\88b\01\00\10x\10\04\04\D0\00\08p\02p\E2\0F\00\0B\C8\00\0C\10\00\B2\00b\FC\03\00\C8O\00\08\C8\09\10\00\B1\02\00\03\00\E4/\04\0B\C2\00\0CN\07\10\80 \00r\0F\00\08\C2\05\0C\09Y\06\13\03\A0\01\17\10\D0\01\F4\07\00\0B\D2\00\05\08\00\00@\00\C0\FC\03\00\E4\8F\08\10x\0C\04\05\80\00\82\E4\0F\00\08\D2\09\08\05@\00\00\10\003\04\0B\D2\E9\06\04`\005\D2\05\08`\00\1B\E2\F0\01\11\E2\C0\04\15\0C\10\02\11\E2\00\02\15\10\10\01\00\D0\00E\82\00\05\0A\80\00 \0F\09\00\01\17\06\00\01\0C \02I\08\82\09\0A\90\00D\82\00\0A\0A\90\00\10\C60\02\17\060\02i\0E\00\08\82\05\0A\A0\00\0B@\02\06\10\01#\F0\03@\02\19\0C\A0\00F\B2\00\05\0E\A0\00\10\0A \01\18\07\A0\00\0B`\02I\08\B2\0B\0E\A0\00H\B2\00\0E\0E\A0\00\09p\02x\0E\00\08\B2\05\0E\0B\A0\00\08\80\02'\E2\0F@\01\12\F6 \00?\8A\0A\10\90\02\16\10$\90\02\16\0DP\00\00\D0\02:\BA\0C\0C\90\02 \0C\0C@\00\14\0D@\005\B9\0C\0C@\00\12b`\03\14\08\F0\00\01\F0\036\10\04\09\10\00d\00\0B\C2\00\05\06 \01\00\90\029\C2\07\06\00\01D\C2\00\06\06\00\01\02\90\02$\06\07\F0\00\18\E4\90\03\0F\90\02\00o\C8\8F\00\08\D2\07\80\02\0E\1F\07\80\02\06\08\E0\01\05\80\02\06\90\03\0F\80\02\024\0E\04\0A\E0\00\0F\80\02S\1B\0E\80\02\1C\10\80\02\19\0C\80\02?\10\04\0B\80\02\0B\1B\0C\80\02/\0C\0C\80\02\0B\1F\0C\80\02\0C\1B\10\80\02\1F\0E\80\02,\1F\10\80\02\1D\14\0C\F0\00\03\80\02\1F\0D\80\02\CC\14\0E\E0\00\0F\80\02\84\1F\0F\80\02\DC\14\10\F0\00\03\80\02\1F\11\80\02\CC\14\12\E0\00\0F\80\02\84\1F\13\80\02\DC\14\14\F0\00\03\80\02\1F\15\80\02\CC\14\16\E0\00\0F\80\02\84\1F\17\80\02\DC\14\18\F0\00\03\80\02\1F\19\80\02\CC\14\1A\E0\00\0F\80\02\84\1F\1B\80\02\DC\14\1C\F0\00\03\80\02\1F\1D\80\02\CC\14\1E\E0\00\0F\80\02\84\1F\1F\80\02\DC\14 \F0\00\03\80\02\1F!\80\02\CC\14\22\E0\00\0F\80\02\84\1F#\80\02\DC\14$\F0\00\03\80\02\1F%\80\02\CC\14&\E0\00\0F\80\02\84\1F'\80\02\DC\14(\F0\00\03\80\02\1F)\80\02\CC\14*\E0\00\0F\80\02\84\1F+\80\02\DC\14,\F0\00\03\80\02\1F-\80\02\CC\14.\E0\00\0F\80\02\84\1F/\80\02\DC\140\F0\00\03\80\02\1F1\80\02\CC\142\E0\00\0F\80\02\84\1F3\80\02\DC\144\F0\00\03\80\02\1F5\80\02\CC\146\E0\00\0F\80\02\84\1F7\80\02\DC\148\F0\00\03\80\02\1F9\80\02\CC\14:\E0\00\0F\80\02\84\1F;\80\02\DC\14<\F0\00\03\80\02\19=\10\009\12\04>\10\00?\04\04?\A0\02\C5?\C6\0F\01\90\02\1AW\E4\0F\04\81\C9`$/\A4\00\90\02\11\1F\12\90\02\16?\C6\0F\02\80\02+\1F\02\80\02\00\0D@$\1A\04\80\02*\06\12\80\02\07\10\01\01\80\02\07\80\03W$\01\00$\B4\D0\02\01\80\02\1A\0A\80&\1B\B6\C0\028\B9\0A\0A\80\02\00@\02\19\0E@\02\17\09\E0#\01p%\06\E0#\02@\02\15\0E`\01\1E\C8`%\06 \02\01P\02\19\04@\02(\C8\1F0\02\11\02@\00\14\82\C0\02\10\FA0\004\09\0B\82\B0\02\03@\00\14\82\D0\02 \80\060\00I\08\82\05\06@\00\14\B2@\02\010\004\0A\0B\B2 \02\12\F0@\00$\B2\07@\02\02@\003\B2\05\0A@\00\10\00\B0*\14A\B0*\030( \88s\F8-\12\00u(f\E8\0F\00\1D{\00\01\00b\EC\0F\00\84\A9\07,*\00 \00Q\22\0E\00\1Cx\1A\00P\00p\F0\F0\03P\00\13\0C0)1pD\F2\90*!\84\A9n\01\020\00qb\0E\00\0B\A2\00\07\80\00\A1\80\F6\03\00\E4\1F\08\0B\A2\00p%\02\C0\00T/\00\1C\A8\00P\00\14\01@)\11?@)@\F6\03\00\D6\00\01#\07\07R\00\10\06 \00\09\80\00\8F\C6\0F\00\88\A3\00\00\07\C0\00\09\22\B9\05\1C\00\12\08\A0++\84\B9\A0\00\22\B2\00!+`\80\F4\03\00\C4\1F\10\00(\04\05\A0\00#\B8\00p\00Ap\01\00\DA\90\01\15\05\90\00_\CA\0F\00\88\B3@\01\0B9M\19\00p\017$t\02\10\03R\0F\00\84y\05\D1)\00@\00\90&\0E\00%v\02\03\00`\0F/\04 \00\00\1D\00\03`\00vh\0E\00\81y\06\02\D0\02D\05\00\0Br\C0\00\12\F0\C0\00\14r\C0\00W\F2\03\00\D6/\B0\00 \80\04\F0\022\0Br\00`\02\11@\10\04(\0EF\10\02S\E6\0F\00\08r\80\02\00\01\00p\CC\0F\00\A9s\07\02\A0\02\C0\07\E1\1E\00\00\A4\0E\00\0Cr\00\07\10\00 pR@\00QO\00$r\06\E0*\14\07\90*@\09\00\00\90\E0+!\FF\83\F0\00\1BMp\02TGy\00\00\F0 \00f\C0\0F\00\18y\00\01\00\0F\10\00\90\0F\01\00-\11\01H1\0E\01\00\22@\00\01\00=\98\01\000\00\08\01\00\1F\0B@\00\04\13\D8)\00?\01\02\00@\00\0A\13\13\0B\04\0C\01\00\13\E0U\00\03\8F\03\01/\04\13\05w\02\00\01\00\22\18\00\01\00.k\01T\00\00\01\00#\88\04(\03\1F\00\80\00\0B/)\00'\00\02#\00\F8@\00\04 3\04\E4\00*\04\00\01\00\1Fb@\00\04*(\05\C0\00\13\03#/\0C@\00!\89\01D\01\0D@\00\13\D0@\00*\D8\00\01\00\1B\08\08\00?x\01\00V4\000\00\00\A8\15\03\03W/\04\80\00\17\048\00\04\18\00\133@\01\0C\84\01\13\B8@\00\17\881\01\0F\C0\00\01\132T\01\15\06R\00\03^\03\1A\08\A04\11\03$\00J\00\15\80\00\01\00\13\95\94\00*\03\00\01\00\0409/\00\04\80\00\0B\13\06\18\02\04h9\0D\08\01\1A\00\08\00\04\97\00\13\018\00\04\E8\00\0C\01\009\C8/\00\08\00\088\00\18\06\A0\00\0F\01\00\05\03\A9\00\80\08\00\00\00\00\00\00\00\00\00\00\00\00"} {
    llvm.mlir.global internal @__wg_main_kColReduction_reduce__4_1_0___thin_1_0() {addr_space = 3 : i32} : !llvm.array<256 x f32>
    llvm.func @__nv_fabsf(f32) -> f32
    llvm.func @main_kColReduction_reduce__4_1_0___thin_1(%arg0: i32, %arg1: !llvm.ptr<f32>, %arg2: i32, %arg3: i32, %arg4: i32, %arg5: i32, %arg6: !llvm.ptr<f32>) attributes {disc.elimargs = [1 : index, 3 : index, 5 : index, 6 : index, 7 : index, 8 : index, 9 : index, 13 : index, 15 : index, 16 : index, 17 : index], gpu.kernel, nvvm.kernel} {
      %0 = llvm.mlir.constant(32 : index) : i32
      %1 = llvm.mlir.constant(8 : index) : i32
      %2 = llvm.mlir.constant(64 : index) : i32
      %3 = llvm.mlir.constant(2 : index) : i32
      %4 = llvm.mlir.constant(0xFF800000 : f32) : f32
      %5 = llvm.mlir.constant(4 : index) : i32
      %6 = llvm.mlir.constant(1 : index) : i32
      %7 = llvm.mlir.constant(256 : index) : i32
      %8 = llvm.mlir.constant(0 : index) : i32
      %9 = llvm.mlir.addressof @__wg_main_kColReduction_reduce__4_1_0___thin_1_0 : !llvm.ptr<array<256 x f32>, 3>
      %10 = llvm.getelementptr %9[0, 0] : (!llvm.ptr<array<256 x f32>, 3>) -> !llvm.ptr<f32, 3>
      %11 = nvvm.read.ptx.sreg.ctaid.x : i32
      %12 = nvvm.read.ptx.sreg.tid.x : i32
      llvm.br ^bb1
    ^bb1:  // pred: ^bb0
      %13 = llvm.mul %11, %arg3  : i32
      %14 = llvm.add %12, %13  : i32
      %15 = llvm.icmp "ult" %14, %arg4 : i32
      llvm.cond_br %15, ^bb2, ^bb21
    ^bb2:  // pred: ^bb1
      %16 = llvm.srem %14, %7  : i32
      %17 = llvm.sdiv %14, %7  : i32
      %18 = llvm.udiv %17, %arg5  : i32
      %19 = llvm.urem %17, %arg5  : i32
      %20 = llvm.udiv %16, %0  : i32
      %21 = llvm.urem %16, %0  : i32
      %22 = llvm.mul %21, %1  : i32
      %23 = llvm.add %20, %22  : i32
      %24 = llvm.mul %19, %0  : i32
      %25 = llvm.add %21, %24  : i32
      %26 = llvm.icmp "ult" %25, %arg0 : i32
      llvm.cond_br %26, ^bb3, ^bb10(%4 : f32)
    ^bb3:  // pred: ^bb2
      %27 = llvm.mul %18, %1  : i32
      %28 = llvm.add %20, %27  : i32
      %29 = llvm.mul %28, %2  : i32
      llvm.br ^bb4(%8, %4 : i32, f32)
    ^bb4(%30: i32, %31: f32):  // 2 preds: ^bb3, ^bb9
      %32 = llvm.icmp "slt" %30, %2 : i32
      llvm.cond_br %32, ^bb5, ^bb10(%31 : f32)
    ^bb5:  // pred: ^bb4
      %33 = llvm.add %30, %29  : i32
      %34 = llvm.icmp "slt" %33, %arg2 : i32
      llvm.cond_br %34, ^bb6, ^bb7
    ^bb6:  // pred: ^bb5
      %35 = llvm.mul %33, %arg0  : i32
      %36 = llvm.add %35, %25  : i32
      %37 = llvm.getelementptr %arg1[%36] : (!llvm.ptr<f32>, i32) -> !llvm.ptr<f32>
      %38 = llvm.load %37 : !llvm.ptr<f32>
      %39 = llvm.call @__nv_fabsf(%38) : (f32) -> f32
      %40 = llvm.fcmp "ugt" %31, %39 : f32
      %41 = llvm.select %40, %31, %39 : i1, f32
      %42 = llvm.fcmp "uno" %39, %39 : f32
      %43 = llvm.select %42, %39, %41 : i1, f32
      llvm.br ^bb8(%43 : f32)
    ^bb7:  // pred: ^bb5
      llvm.br ^bb8(%31 : f32)
    ^bb8(%44: f32):  // 2 preds: ^bb6, ^bb7
      llvm.br ^bb9
    ^bb9:  // pred: ^bb8
      %45 = llvm.add %30, %6  : i32
      llvm.br ^bb4(%45, %44 : i32, f32)
    ^bb10(%46: f32):  // 2 preds: ^bb2, ^bb4
      llvm.br ^bb11(%46 : f32)
    ^bb11(%47: f32):  // pred: ^bb10
      llvm.br ^bb12
    ^bb12:  // pred: ^bb11
      %48 = llvm.getelementptr %10[%23] : (!llvm.ptr<f32, 3>, i32) -> !llvm.ptr<f32, 3>
      llvm.store %47, %48 : !llvm.ptr<f32, 3>
      nvvm.barrier0
      %49 = llvm.icmp "slt" %20, %5 : i32
      llvm.cond_br %49, ^bb13, ^bb14
    ^bb13:  // pred: ^bb12
      %50 = llvm.add %23, %5  : i32
      %51 = llvm.getelementptr %10[%23] : (!llvm.ptr<f32, 3>, i32) -> !llvm.ptr<f32, 3>
      %52 = llvm.load %51 : !llvm.ptr<f32, 3>
      %53 = llvm.getelementptr %10[%50] : (!llvm.ptr<f32, 3>, i32) -> !llvm.ptr<f32, 3>
      %54 = llvm.load %53 : !llvm.ptr<f32, 3>
      %55 = llvm.fcmp "ugt" %52, %54 : f32
      %56 = llvm.select %55, %52, %54 : i1, f32
      %57 = llvm.fcmp "uno" %54, %54 : f32
      %58 = llvm.select %57, %54, %56 : i1, f32
      %59 = llvm.getelementptr %10[%23] : (!llvm.ptr<f32, 3>, i32) -> !llvm.ptr<f32, 3>
      llvm.store %58, %59 : !llvm.ptr<f32, 3>
      llvm.br ^bb14
    ^bb14:  // 2 preds: ^bb12, ^bb13
      nvvm.barrier0
      %60 = llvm.icmp "slt" %20, %3 : i32
      llvm.cond_br %60, ^bb15, ^bb16
    ^bb15:  // pred: ^bb14
      %61 = llvm.add %23, %3  : i32
      %62 = llvm.getelementptr %10[%23] : (!llvm.ptr<f32, 3>, i32) -> !llvm.ptr<f32, 3>
      %63 = llvm.load %62 : !llvm.ptr<f32, 3>
      %64 = llvm.getelementptr %10[%61] : (!llvm.ptr<f32, 3>, i32) -> !llvm.ptr<f32, 3>
      %65 = llvm.load %64 : !llvm.ptr<f32, 3>
      %66 = llvm.fcmp "ugt" %63, %65 : f32
      %67 = llvm.select %66, %63, %65 : i1, f32
      %68 = llvm.fcmp "uno" %65, %65 : f32
      %69 = llvm.select %68, %65, %67 : i1, f32
      %70 = llvm.getelementptr %10[%23] : (!llvm.ptr<f32, 3>, i32) -> !llvm.ptr<f32, 3>
      llvm.store %69, %70 : !llvm.ptr<f32, 3>
      llvm.br ^bb16
    ^bb16:  // 2 preds: ^bb14, ^bb15
      nvvm.barrier0
      %71 = llvm.icmp "eq" %20, %8 : i32
      %72 = llvm.and %71, %26  : i1
      llvm.cond_br %72, ^bb17, ^bb20
    ^bb17:  // pred: ^bb16
      %73 = llvm.add %23, %6  : i32
      %74 = llvm.getelementptr %10[%23] : (!llvm.ptr<f32, 3>, i32) -> !llvm.ptr<f32, 3>
      %75 = llvm.load %74 : !llvm.ptr<f32, 3>
      %76 = llvm.getelementptr %10[%73] : (!llvm.ptr<f32, 3>, i32) -> !llvm.ptr<f32, 3>
      %77 = llvm.load %76 : !llvm.ptr<f32, 3>
      %78 = llvm.fcmp "ugt" %75, %77 : f32
      %79 = llvm.select %78, %75, %77 : i1, f32
      %80 = llvm.fcmp "uno" %77, %77 : f32
      %81 = llvm.select %80, %77, %79 : i1, f32
      %82 = llvm.getelementptr %arg6[%25] : (!llvm.ptr<f32>, i32) -> !llvm.ptr<f32>
      %83 = llvm.load %82 : !llvm.ptr<f32>
      llvm.br ^bb18(%83 : f32)
    ^bb18(%84: f32):  // 2 preds: ^bb17, ^bb18
      %85 = llvm.fcmp "ogt" %84, %81 : f32
      %86 = llvm.select %85, %84, %81 : i1, f32
      %87 = llvm.bitcast %82 : !llvm.ptr<f32> to !llvm.ptr<i32>
      %88 = llvm.bitcast %84 : f32 to i32
      %89 = llvm.bitcast %86 : f32 to i32
      %90 = llvm.cmpxchg %87, %88, %89 acq_rel monotonic : !llvm.ptr<i32>, i32
      %91 = llvm.extractvalue %90[0] : !llvm.struct<(i32, i1)> 
      %92 = llvm.bitcast %91 : i32 to f32
      %93 = llvm.extractvalue %90[1] : !llvm.struct<(i32, i1)> 
      llvm.cond_br %93, ^bb19, ^bb18(%92 : f32)
    ^bb19:  // pred: ^bb18
      llvm.br ^bb20
    ^bb20:  // 2 preds: ^bb16, ^bb19
      llvm.br ^bb21
    ^bb21:  // 2 preds: ^bb1, ^bb20
      llvm.return
    }
  }
  "disc_shape.SymbolicDim"() {knownNegativeOne = false, knownNonNegative = true, knownNonSizeOne = false, knownNonSizeZero = false, sym_name = "S0", value = -9223372036854775808 : i64} : () -> ()
  "disc_shape.SymbolicDim"() {knownNegativeOne = false, knownNonNegative = true, knownNonSizeOne = false, knownNonSizeZero = false, sym_name = "S1", value = -9223372036854775808 : i64} : () -> ()
  "disc_shape.SymbolicDim"() {knownNegativeOne = false, knownNonNegative = true, knownNonSizeOne = false, knownNonSizeZero = false, sym_name = "S2", value = -9223372036854775808 : i64} : () -> ()
  "disc_shape.SymbolicDim"() {knownNegativeOne = false, knownNonNegative = true, knownNonSizeOne = false, knownNonSizeZero = false, sym_name = "S3", value = -9223372036854775808 : i64} : () -> ()
  func.func @shape_constraint_graph() {
    %0 = "disc_shape.dim"() {name = @S3} : () -> index
    %1 = "disc_shape.dim"() {name = @S1} : () -> index
    %2 = "disc_shape.dim"() {name = @S2} : () -> index
    "disc_shape.tie_product_equal"(%0, %1, %2) {operand_segment_sizes = array<i32: 1, 2>} : (index, index, index) -> ()
    return
  }
}


// -----// IR Dump After DiscStripShapeConstraintOpsPass (disc-strip-shape-constraint-ops) //----- //
module attributes {gpu.container_module} {
  func.func @main(%arg0: !disc_ral.context) attributes {tf.entry_function = {input_placements = "gpu", inputs = "input0", output_placements = "gpu", outputs = "output0"}} {
    %c256 = arith.constant 256 : index
    %c32 = arith.constant 32 : index
    %c512 = arith.constant 512 : index
    %0 = llvm.mlir.constant(0 : i32) : i32
    %c0 = arith.constant 0 : index
    %c1 = arith.constant 1 : index
    %c2 = arith.constant 2 : index
    %1 = "disc_ral.dispatch"(%arg0, %c0) {backend_config = "", call_target_name = "ral_recv_input", device = "cpu", has_side_effect = false} : (!disc_ral.context, index) -> memref<?x?x?xf32, "gpu">
    %dim = memref.dim %1, %c2 : memref<?x?x?xf32, "gpu">
    %dim_0 = memref.dim %1, %c1 : memref<?x?x?xf32, "gpu">
    %dim_1 = memref.dim %1, %c0 : memref<?x?x?xf32, "gpu">
    %2 = arith.index_cast %dim_0 : index to i32
    %3 = arith.index_cast %dim : index to i32
    %4 = arith.muli %2, %3 : i32
    %5 = arith.index_cast %4 : i32 to index
    %alloc = memref.alloc(%5) {kDiscSymbolicDimAttr = [@S3]} : memref<?xf32, "gpu">
    %6 = arith.cmpi slt, %dim_1, %5 : index
    cf.cond_br %6, ^bb1, ^bb2
  ^bb1:  // pred: ^bb0
    %c512_2 = arith.constant 512 : index
    %c0_3 = arith.constant 0 : index
    %c1_4 = arith.constant 1 : index
    %7 = arith.cmpi sle, %5, %c0_3 : index
    %8 = arith.subi %c0_3, %5 : index
    %9 = arith.subi %5, %c1_4 : index
    %10 = arith.select %7, %8, %9 : index
    %11 = arith.divsi %10, %c512_2 : index
    %12 = arith.subi %c0_3, %11 : index
    %13 = arith.addi %11, %c1_4 : index
    %14 = arith.select %7, %12, %13 : index
    gpu.launch_func  @main_kernel::@main_kColReduction_reduce__4_1_0___flat blocks in (%14, %c1, %c1) threads in (%c512, %c1, %c1) args(%c512 : index, %5 : index, %alloc : memref<?xf32, "gpu">)
    %15 = arith.cmpi eq, %5, %c0 : index
    %16 = arith.subi %5, %c1 : index
    %17 = arith.divui %16, %c512 : index
    %18 = arith.addi %17, %c1 : index
    %19 = arith.select %15, %c0, %18 : index
    %20 = arith.cmpi eq, %dim_1, %c0 : index
    %21 = arith.subi %dim_1, %c1 : index
    %22 = arith.divui %21, %c32 : index
    %23 = arith.addi %22, %c1 : index
    %24 = arith.select %20, %c0, %23 : index
    %25 = arith.muli %19, %24 : index
    %26 = arith.muli %25, %c512 : index
    %c512_5 = arith.constant 512 : index
    %c0_6 = arith.constant 0 : index
    %c1_7 = arith.constant 1 : index
    %27 = arith.cmpi sle, %26, %c0_6 : index
    %28 = arith.subi %c0_6, %26 : index
    %29 = arith.subi %26, %c1_7 : index
    %30 = arith.select %27, %28, %29 : index
    %31 = arith.divsi %30, %c512_5 : index
    %32 = arith.subi %c0_6, %31 : index
    %33 = arith.addi %31, %c1_7 : index
    %34 = arith.select %27, %32, %33 : index
    gpu.launch_func  @main_kernel_0::@main_kColReduction_reduce__4_1_0___flat_1 blocks in (%34, %c1, %c1) threads in (%c512, %c1, %c1) args(%5 : index, %1 : memref<?x?x?xf32, "gpu">, %c512 : index, %26 : index, %19 : index, %alloc : memref<?xf32, "gpu">)
    cf.br ^bb3
  ^bb2:  // pred: ^bb0
    %c256_8 = arith.constant 256 : index
    %c0_9 = arith.constant 0 : index
    %c1_10 = arith.constant 1 : index
    %35 = arith.cmpi sle, %5, %c0_9 : index
    %36 = arith.subi %c0_9, %5 : index
    %37 = arith.subi %5, %c1_10 : index
    %38 = arith.select %35, %36, %37 : index
    %39 = arith.divsi %38, %c256_8 : index
    %40 = arith.subi %c0_9, %39 : index
    %41 = arith.addi %39, %c1_10 : index
    %42 = arith.select %35, %40, %41 : index
    gpu.launch_func  @main_kernel_1::@main_kColReduction_reduce__4_1_0___thin blocks in (%42, %c1, %c1) threads in (%c256, %c1, %c1) args(%c256 : index, %5 : index, %alloc : memref<?xf32, "gpu">)
    %43 = arith.cmpi eq, %5, %c0 : index
    %44 = arith.subi %5, %c1 : index
    %45 = arith.divui %44, %c32 : index
    %46 = arith.addi %45, %c1 : index
    %47 = arith.select %43, %c0, %46 : index
    %48 = arith.cmpi eq, %dim_1, %c0 : index
    %49 = arith.subi %dim_1, %c1 : index
    %50 = arith.divui %49, %c512 : index
    %51 = arith.addi %50, %c1 : index
    %52 = arith.select %48, %c0, %51 : index
    %53 = arith.muli %47, %52 : index
    %54 = arith.muli %53, %c256 : index
    %c256_11 = arith.constant 256 : index
    %c0_12 = arith.constant 0 : index
    %c1_13 = arith.constant 1 : index
    %55 = arith.cmpi sle, %54, %c0_12 : index
    %56 = arith.subi %c0_12, %54 : index
    %57 = arith.subi %54, %c1_13 : index
    %58 = arith.select %55, %56, %57 : index
    %59 = arith.divsi %58, %c256_11 : index
    %60 = arith.subi %c0_12, %59 : index
    %61 = arith.addi %59, %c1_13 : index
    %62 = arith.select %55, %60, %61 : index
    gpu.launch_func  @main_kernel_2::@main_kColReduction_reduce__4_1_0___thin_1 blocks in (%62, %c1, %c1) threads in (%c256, %c1, %c1) args(%5 : index, %1 : memref<?x?x?xf32, "gpu">, %c256 : index, %54 : index, %47 : index, %alloc : memref<?xf32, "gpu">)
    cf.br ^bb3
  ^bb3:  // 2 preds: ^bb1, ^bb2
    %63 = llvm.inttoptr %0 : i32 to !llvm.ptr<i8>
    %alloca = memref.alloca() : memref<2xindex, "cpu">
    memref.store %dim_0, %alloca[%c0] : memref<2xindex, "cpu">
    memref.store %dim, %alloca[%c1] : memref<2xindex, "cpu">
    %64 = "disc_ral.dispatch"(%arg0, %63, %alloc, %alloca) {backend_config = "", call_target_name = "inc_ref", device = "gpu", has_side_effect = false} : (!disc_ral.context, !llvm.ptr<i8>, memref<?xf32, "gpu">, memref<2xindex, "cpu">) -> memref<?x?xf32, "gpu">
    %reinterpret_cast = memref.reinterpret_cast %64 to offset: [0], sizes: [%dim_0, %dim], strides: [%dim, 1] {kDiscSymbolicDimAttr = [@S1, @S2]} : memref<?x?xf32, "gpu"> to memref<?x?xf32, "gpu">
    memref.dealloc %alloc : memref<?xf32, "gpu">
    "disc_ral.dispatch"(%arg0, %c0, %reinterpret_cast) {backend_config = "", call_target_name = "ral_send_output", device = "cpu", has_side_effect = false} : (!disc_ral.context, index, memref<?x?xf32, "gpu">) -> ()
    return
  }
  gpu.module @main_kernel attributes {gpu.binary = "P\EDU\BA\01\00\10\008\04\00\00\00\00\00\00\02\00\01\01@\00\00\00\F8\03\00\00\00\00\00\00\F7\03\00\00\00\00\00\00\07\00\01\00P\00\00\00\00\00\00\00\00\00\00\00\11 \00\00\00\00\00\00\00\00\00\00\00\00\00\00\E8\0B\00\00\00\00\00\00\A2\7FELF\02\01\013\07\00\01\00f\02\00\BE\00q\00\01\00!@\0B\07\001\00\80\08\07\00\F5\0E\00P\05P\00@\008\00\03\00@\00\0B\00\01\00\00.shstrtab\00.\08\00'ym\08\00\F0\1B_shndx\00.nv.info\00.text.main_kColReduction_r\0A\00\F5\00e__4_1_0___flat7\00\0F1\00\1Aoshared3\00\1A\9Fconstant06\00\17\FA\01debug_frame\00.rel\11\00!nv\14\00\11aB\00\0F'\01 \0F\87\00\14\0FO\01\B6o_paramV\01\1C\0F\01\00\07\8CZ\00\00\00\03\00\0A\00\01\00\11\EC\18\00,\09\00\01\00 )\01\18\00,\04\00\01\00\11G\18\00,\07\00\01\00f2\00\00\00\12\10`\00!\80\01\07\00v\00\FF\FF\FF\FF(\00\0C\00\00\01\00@\03\00\04|\08\00\D1\0F\0C\81\80\80(\00\08\FF\81\80(\08\0B\00\06(\00/0\00\01\00\03\13pX\00\10\04\9B\00R\04\14\00\00\00E\00\01\0B\00\00\13\00p/\08\00\05\00\00\00\9F\03\22\04#\0C\00\00\01\00'\04\12\0C\00\17\11\0C\00!7\04\E0\03\F3\08\015\00\00\04\0A\08\00\02\00\00\00`\01\10\00\03\19\10\00\04\17\0C\04\04U\08\00\00\F0!\10\00\10\01\18\01%\F0\11\10\00\01\01\00\F2\02\F0\11\00\03\1B\FF\00\04\1C\08\00P\00\00\00\B0\00\01\00#K\00\01\00s\02\02\08\10\0A/\22\9B\00\00\07\00\03\08\00\13\10\08\00\13\18\08\00\13 \08\00\13(\08\00\130\08\00\228\08|\00\13\00\08\00\13\08\08\00\13\10\08\00\13\18\08\00\13 \08\00\13(\08\00\130\08\00\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\A2\00\00\00\14,\00\00\00H\00\01\00\000\01/\05\00\01\00\FF\B8A\02z\01\00'\04\B1\0F\00\00\00\C4\0F\00\19y\02\00\01\00\10%\93\02Q\0E\00\19y\03\0F\00\F5\1A\00!\00\00\00$\0E\00$z\02\02\00X\00\00\03\02\8E\07\00\CA\1F\00\0Cz\00\02\00Y\00\00p`\F0\03\00\DA\0F\00M[\04\A0\80\03\00\EA\0F\005t\03\FF\BB\03\10\FF\C8\03P\E2\0F\00\02x>\02B\80\FF\00\0F\10\00r\B9z\04\00\00F\00\84\00\94\D0\0F\00%v\02\02\00Z`\00`\0F\00\86y\00\022\00@\04\19\10\0C0\009My\00`\00PGy\00\00\F0A\04\A6\FF\83\03\00\C0\0F\00\18y\00\01\00\0F\10\00\90\0F\01\00-\00W\01.\03\00\01\00\22@\00\01\00='\01\000\00\08\01\00\1F\0B@\00\04\13g)\00\1FV@\00\0C\13\13\14\04\0C\01\00\13\C0\15\00&\90\008\04#\04\00\8D\04\00\FE\04\12\00\01\00\1F\FAT\00\00\00\01\00\13P\95\00/p\00\80\00\0B\1F)'\00\03#\00\C0@\00\04X\06\04\E4\00*\04\00\01\00\1F`@\00\04\13\F01\00&\\\00@\00\1F\0A@\00\00!\18\01D\01\0D@\00\13P)\00*\D8\00\01\00\1B\08\08\00?\07\01\00\8E\07\00Q\00\00(\05\00\01\00&\10\00\80\00\17\048\00\04\18\00\13\C4\14\01\0C\84\01*8\05@\07\1F\00\C0\00\04\132@\00+\06\00\01\00\1A\07\D8\07\12\03\F8\05:\08\80\00\01\00\13\06\10\06\04(\0B\0C\01\00*\A8\00\08\00\04\F8\00\13\018\00\04\A8\00\0C\01\009H\03\00\08\00\088\00/\06\00\01\00\17\80\08\00\00\00\00\00\00\00\00"} {
    llvm.func @main_kColReduction_reduce__4_1_0___flat(%arg0: i32, %arg1: i32, %arg2: !llvm.ptr<f32>) attributes {disc.elimargs = [2 : index, 4 : index, 5 : index, 6 : index], gpu.kernel, nvvm.kernel} {
      %0 = llvm.mlir.constant(0xFF800000 : f32) : f32
      %1 = nvvm.read.ptx.sreg.ctaid.x : i32
      %2 = nvvm.read.ptx.sreg.tid.x : i32
      llvm.br ^bb1
    ^bb1:  // pred: ^bb0
      %3 = llvm.mul %1, %arg0  : i32
      %4 = llvm.add %2, %3  : i32
      %5 = llvm.icmp "ult" %4, %arg1 : i32
      llvm.cond_br %5, ^bb2, ^bb3
    ^bb2:  // pred: ^bb1
      %6 = llvm.getelementptr %arg2[%4] : (!llvm.ptr<f32>, i32) -> !llvm.ptr<f32>
      llvm.store %0, %6 : !llvm.ptr<f32>
      llvm.br ^bb3
    ^bb3:  // 2 preds: ^bb1, ^bb2
      llvm.return
    }
  }
  gpu.module @main_kernel_0 attributes {gpu.binary = "P\EDU\BA\01\00\10\00\90\0A\00\00\00\00\00\00\02\00\01\01@\00\00\00P\0A\00\00\00\00\00\00P\0A\00\00\00\00\00\00\07\00\01\00P\00\00\00\00\00\00\00\00\00\00\00\11 \00\00\00\00\00\00\00\00\00\00\00\00\00\00\E8\1C\00\00\00\00\00\00\A2\7FELF\02\01\013\07\00\01\00f\02\00\BE\00q\00\01\00!@\1C\07\001\00\80\19\07\00\F5\0E\00P\05P\00@\008\00\03\00@\00\0B\00\01\00\00.shstrtab\00.\08\00'ym\08\00\F0\1B_shndx\00.nv.info\00.text.main_kColReduction_r\0A\00\F5\02e__4_1_0___flat_19\00\0F3\00\1Coshared5\00\19Orela\9D\00\1E?rel\D1\00!\9Fconstant08\00\19\B2debug_framey\00\09\11\00!nv\14\00\11aD\00\0F\98\01 \0F\89\00\16\0F\C2\01\FF(o_param\C9\01\1C\0F\01\00\0B\8C\\\00\00\00\03\00\0A\00\01\00 ]\01\18\00,\09\00\01\00\11\9C\18\00,\04\00\01\00\11\BA\18\00,\07\00\01\00f2\00\00\00\12\10`\00!\80\11\07\00v\00\FF\FF\FF\FF(\00\0C\00\00\01\00@\03\00\04|\08\00\D1\0F\0C\81\80\80(\00\08\FF\81\80(\08\0B\00\06(\00/0\00\01\00\03\13pX\00\10\04\9B\00R\04\14\00\00\00E\00\22\04 \AC\00\90\04/\08\00\05\00\00\00\16\0C\00\12#\0C\00\00\01\00'\04\12\0C\00\17\11\0C\00!7\04\C8\04\F1\08\015\00\00\04\0A\08\00\02\00\00\00`\01(\00\03\19(\00\04\17\0C$\00u\06\00 \00\00\F0!\10\00u\05\00\1C\00\00\F0\11\10\009\04\00\18\10\009\03\00\14\10\009\02\00\10\10\009\01\00\08P\00\01\01\00\F2\0A\F0\11\00\03\1B\FF\00\04\1C\0C\00P\00\00\00\10\02\00\00\E0\10\00\00\04\1E\D8\00#K\00\01\00\B1\02\02\08\10\0A/\22\00\00\00\08\10\00#\00\08\08\00\13\10\08\00\13\18\08\00\13 \08\00\13(\08\00\130\08\00\228\08\84\00\13\00\08\00\13\08\08\00\13\10\08\00\13\18\08\00\13 \08\00\13(\08\00\130\08\00\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\A2\00\00\00\14,\00\00\00H\00\01\00\00x\01/\05\00\01\00\FF\88@$v\01\FF?\04\B2\FF\00\8E\07\00\C4\0F\00\19y\00\01\00\10%c\02Q\0E\00\19y\03\0F\00 \00!-\00@\0E\00$z]\04\B0\00\00\03\02\8E\07\00\CA\1F\00\0C\10\00\C5^\00\00p`\F0\03\00\DA\0F\00Ms\04\F0\0C\80\03\00\EA\0F\00\06{\04\00\00_\00\00\00\90 \00\00\22\0E\00\19x\05\FF\1F\1F\00\D3\14\01\00\00\E2\0F\00$r\02\FF\FF\00\80\00 \E2\0FP\00\10\FF0\00\B1pP\F4\03\00\E4\0F\00\11r\05?\02\B2\FFH\8F\07\00\C6\0F\00\08s\04\BE\04\10\10\90\00\F1\07\1E\00\10x\03\04\FE\FF\FF\0F\FF\E0\FF\07\00\CC\1F\00\05s\03\00\B2\00\22\F0!\B0\00!r\07`\00\C0\03\0A\8E\07\00\C8\1F\00$z\07\07`\00\10\FF\C0\00\81\C8\0F\00'r\07\03\07\A8\02\02\80\00@\19x\02\FF\C3\00\10\05\A0\00\E1\E4\0F\00\12x\05\05\00\FE\FF\FF\FF\C0\8E\80\00T'r\07\07\02\B0\00\10\C8\C0\00\11\09`\00#\07\0A\10\000z\03\09`\00\11\02 \01\01\D0\006\03\00_ \010\10\0A\03\10\00\11\80\B0\00\80\E4\0F\00\10\08\07\07\01P\00\03\10\00\060\00\12\F20\00\1A\18 \001\12\AA\07 \01\22\FF3`\00;$r\03\80\00%\02\03\80\00`\E4\0F\00$x\03\14\04$\00\05 \000x\03\02\95\03\18\03\A0\00\1FX\C0\01\07W$x\02\07 \A0\01\81\B9z\04\00\00F\00\00s\06\00\C0\01Dt\00\FF\04 \00\11\C6P\00\10\02\95\060pb\F8\C0\01U\04\10x\06\02\C0\00\11\C8 \00\12\06 \00\C3\FA\03\00\CE\0F\00$\CA\05\02\00X\90\00\90\C8\0F\00%\C6\04\05\00Z\91\01\03\B0\005\DA\07\06 \00a\E2\0F\00\81\C9\0A#\06\C5\00\19\1E\0C\00\A6\00\00%\D6\06\070\00u\CA\0F\00\81\D9\0D\06 \00\86\E2\02\00\10x\08\02\02@\01j\04\10x\10\02\03p\01\12\08\90\00#\F0\03\10\00\12\10\10\00\11\F2\10\00Y\10x\0C\02\04@\004\0E\02\05\10\00\11\C40\00\12\0C0\00 \F4\03\80\00E$\8A\09\08\B0\00w\E4\0F\10$\9A\0B\10\10\00U\00%\86\08\09\B0\00\02\F0\02\12\0E@\00 \F6\03\F0\00E\81\89\05\08\C0\00v\22\11\00%\96\06\0B0\00 /\00\C0\00\14\06\80\00w\C6\0F\00$\AA\0B\0C \01F\10\81\99\0F\00\01\B4\22\03\00$t\04\FF\00\00\80\FF\C0\01\00\D0\00\14\07@\00\00\C0\015\BA\09\0E@\00W\C8\1F\00%\B6\A0\00g\CC\0F\00\81\B9\09\90\00@\01\00\0B\C8\FA\01\D2\80\FF\00\C2\FC\03\00\C8O\00\08\C8\04\10\000\02\00\03P\007%\A6\0A\B0\00\08P\01\01\10\02\F5\06\00\0B\D2\00\04\0D\00\00@\00`\FC\03\00\C6\8F\00\81\A9\0B\0A`\00b\E2\04\00\08\D2\04 \00!\00\000\00\06`\01\12\FA`\01&\CA\07P\01z\C8/\00%\C6\06\070\02*\0D\0C0\02\16\11\10\01y\E6\02\00%\D6\0C\0D\E0\006\D9\0D\0C\80\00t\00\00\0B\82\00\04\05\A0\00\84\E4\0F\09\10x\0A\02\080\01\8A\E4O\00\08\82\04\04\05\A0\00\18\0A@\02V\0B\92\00\04\0F@\00\01\80\02\14\09@\00\8A\C4\1F\00\08\92\04\04\0F@\00\1A\08p\028\0E\02\0A\E0\016\8A\07\0A\D0\00\11/\C0\01\17\0B \00+%\86\00\03&\89\05\F0\00!\A2\000\02\14\0C0\00\00\A0\00D\A2\00\04\0B\A0\00\8A\C8\8F\00\08\A2\04\04\0B\90\00\15\0E\D0\02\000\00D\B2\00\04\090\00\000\02&\9A\0B\E0\02\00\90\05D\B2\04\04\09@\00\00\00\02(\96\08\00\02\16\08\C0\01\12\F6P\00D\C2\00\04\11P\00x\E4\0F\08\81\99\0F\08\B0\03H$\AA\0B\0E\C0\01Z\08\C2\04\04\11\A0\00\06P\02\0Fp\02\00\00@\01\17\0D\A0\03\09p\02f\E2\0F\08$\BA\07 \02*\E4\1F\80\02.\22\01\80\02\05\10\01\12\FAp\00\1B\B6p\028\CA\09\10\A0\01'\81\B9p\02{&\03\00%\C6\08\09\A0\02\1A\0E\A0\02\16\13\00\01/&\01\A0\02\0B+\22\03\A0\02 O\08\A0\02\15\0E\E0\01\1F\1F\A0\02\0E\00 \01\1B\0F\10\05\17\100\01\00`\02\14\11\10\00*\CE/\80\02\1F\C8p\02\0F\09\00\03/\C8\8F\F0\02\01\18\0E\F0\02\0A\90\02/\0F\01\90\02\00\1C\10\90\02\05@\02\00 \03)\9A\09P\01'\08\B20\02\11\C6\90\02\06\80\01\0F\90\02\04\14\13P\000\E4\0F\08 \01\14\12\00\01\11\E2`\05\18\10`\00\0C\B0\02\00\A0\02\1E\13\E0\00\0F\A0\02\06\09\90\02\01p\018\10\02\13p\00\08\A0\02*\E2\1F\90\02-\E2\0F\B0\02\06 \01\0F\A0\02\06\1F\0E\A0\02\1C\1F\10\A0\02=\1F\14\A0\02\1C\1B\15\A0\02\1B\16\A0\02\1F\17\A0\02\A7\19\10\A0\02\02@\00\06\90\02\1D\E2\B0\02\0F\A0\02\02\14\18\00\01\0F\A0\02\1D\1F\C4\A0\02\0E\19\08\A0\02\01p\00?\10\02\19\A0\02U\10\E4\A0\02\16\07\B0\01\1F$\A0\02\11\1F\11\A0\02.\1F\1A\A0\02\1A5\06\02\1B0\00\11/\10\0A\1B\1C\A0\02\14\1D\10\00\11\CE\10\0A\0B\A0\02+\08\09\A0\02\1C\08\B0\07\19\1E\00\036\02\02\1F\10\00\0F\C0\02\11\1F\06\C0\02\1C\1C\0E\C0\02\19\07\C0\02,\0B\06`\05\14\07@\00+\C6\0F\A0\0A\0F\F0\07\0C\00\80\07\08\E0\07\10\E4\B0\02,\07\06\B0\02\1E\11`\03\0FP\05\13\11\C6\D0\0A\0F\A0\02\18\08\B0\0C\04\A0\02\0Bp\02\17\CAp\02\1B\E4\10\0B\10$\B0\0C)\0C\0D0\009\DA\0F\020\009\C9\0D\0C0\00:\D6\0E\0F\A0\02(\0F\0E\90\00g%v\02\03\00` \025y\06\02 \00*b!\C0\02*\C8O\B0\02\12\C8 \02\02\C0\01\15\F0 \02\03\B0\018\00\00\C8\10\02\1C\F0\10\02\02 \00\05\F0\09\01 \00\08\E0\09\02 \00\14\C2p\01\03 \00\15\C2`\01\02 \00\14\D2\A0\02\03 \00\15\D2\A0\02\10\00`\0C2\0Br\00\F0\01 @\F0p\017\0EFyp\0ES\E6\0F\00\08r\10\02\00\01\00\80\CC\0F\00\A9s\07\02\06\E0\0E\B0\E1\1E\00\00\A4\0E\00\0Cr\00\07\10\00 pR@\00RO\00$r\06\00\0F\00\E0\0C\80\D8\0F\00G\09\00\00\90i\14!\FF\83\C0\0E*My\D0\0ETGy\00\00\F0 \00f\C0\0F\00\18y\00\01\00\0F\10\00`\0F\01\00-\00\\\0F.\03\00\01\00\02q\01]\00\00\98\01\000\00\08\01\00\1F\0B@\00\04\13\D8)\00\1F\C9@\00\0C\13\13\E4\13\0C\01\00\13\A8U\00\11\90\06\00\02$\00#\04\00]\14\00\CE\14\12\00\01\00.k\01T\00\00\01\00\138\05\02/p\00\80\00\0B\1F)'\00\03#\00\A8@\00\04p\16\04\E4\00*\04\00\01\00\1Fb@\00\04\13\D8)\00&\A8\00@\00\1F\0A@\00\00!\89\01D\01\0D@\00\11\80\B6\12J\00\00\D8\00\01\00\1B\08\08\00?x\01\00\A6\17\000\00\00X\E5\02\03\E7\12\04\80\00\17\048\00\04\18\00\133@\01\0C\84\01\13h@\00\17\881\01\0F\C0\00\01\132T\01+\06\00\01\00\1A\08\F0\17\12\03\C8\15:\16\80\00\01\00\13\06\E0\15\04(\1C\0D\88\01\1A\00\08\00\04\C0\00\13\018\00\04\A8\00\0C\01\009\18\13\00\08\00\088\00/\06\00\01\00\17\80\08\00\00\00\00\00\00\00"} {
    llvm.func @__nv_fabsf(f32) -> f32
    llvm.func @main_kColReduction_reduce__4_1_0___flat_1(%arg0: i32, %arg1: !llvm.ptr<f32>, %arg2: i32, %arg3: i32, %arg4: i32, %arg5: i32, %arg6: !llvm.ptr<f32>) attributes {disc.elimargs = [1 : index, 3 : index, 5 : index, 6 : index, 7 : index, 8 : index, 9 : index, 13 : index, 15 : index, 16 : index, 17 : index], gpu.kernel, nvvm.kernel} {
      %0 = llvm.mlir.constant(0 : index) : i32
      %1 = llvm.mlir.constant(1 : index) : i32
      %2 = llvm.mlir.constant(512 : index) : i32
      %3 = llvm.mlir.constant(32 : index) : i32
      %4 = llvm.mlir.constant(0xFF800000 : f32) : f32
      %5 = nvvm.read.ptx.sreg.ctaid.x : i32
      %6 = nvvm.read.ptx.sreg.tid.x : i32
      llvm.br ^bb1
    ^bb1:  // pred: ^bb0
      %7 = llvm.mul %5, %arg3  : i32
      %8 = llvm.add %6, %7  : i32
      %9 = llvm.icmp "ult" %8, %arg4 : i32
      llvm.cond_br %9, ^bb2, ^bb14
    ^bb2:  // pred: ^bb1
      %10 = llvm.srem %8, %2  : i32
      %11 = llvm.sdiv %8, %2  : i32
      %12 = llvm.udiv %11, %arg5  : i32
      %13 = llvm.urem %11, %arg5  : i32
      %14 = llvm.mul %13, %2  : i32
      %15 = llvm.add %14, %10  : i32
      %16 = llvm.icmp "ult" %15, %arg0 : i32
      llvm.cond_br %16, ^bb3, ^bb13
    ^bb3:  // pred: ^bb2
      %17 = llvm.mul %12, %3  : i32
      llvm.br ^bb4(%0, %4 : i32, f32)
    ^bb4(%18: i32, %19: f32):  // 2 preds: ^bb3, ^bb9
      %20 = llvm.icmp "slt" %18, %3 : i32
      llvm.cond_br %20, ^bb5, ^bb10
    ^bb5:  // pred: ^bb4
      %21 = llvm.add %17, %18  : i32
      %22 = llvm.icmp "slt" %21, %arg2 : i32
      llvm.cond_br %22, ^bb6, ^bb7
    ^bb6:  // pred: ^bb5
      %23 = llvm.mul %21, %arg0  : i32
      %24 = llvm.add %23, %15  : i32
      %25 = llvm.getelementptr %arg1[%24] : (!llvm.ptr<f32>, i32) -> !llvm.ptr<f32>
      %26 = llvm.load %25 : !llvm.ptr<f32>
      %27 = llvm.call @__nv_fabsf(%26) : (f32) -> f32
      %28 = llvm.fcmp "oge" %19, %27 : f32
      %29 = llvm.select %28, %19, %27 : i1, f32
      llvm.br ^bb8(%29 : f32)
    ^bb7:  // pred: ^bb5
      llvm.br ^bb8(%19 : f32)
    ^bb8(%30: f32):  // 2 preds: ^bb6, ^bb7
      llvm.br ^bb9
    ^bb9:  // pred: ^bb8
      %31 = llvm.add %18, %1  : i32
      llvm.br ^bb4(%31, %30 : i32, f32)
    ^bb10:  // pred: ^bb4
      %32 = llvm.getelementptr %arg6[%15] : (!llvm.ptr<f32>, i32) -> !llvm.ptr<f32>
      %33 = llvm.load %32 : !llvm.ptr<f32>
      llvm.br ^bb11(%33 : f32)
    ^bb11(%34: f32):  // 2 preds: ^bb10, ^bb11
      %35 = llvm.fcmp "ogt" %34, %19 : f32
      %36 = llvm.select %35, %34, %19 : i1, f32
      %37 = llvm.bitcast %32 : !llvm.ptr<f32> to !llvm.ptr<i32>
      %38 = llvm.bitcast %34 : f32 to i32
      %39 = llvm.bitcast %36 : f32 to i32
      %40 = llvm.cmpxchg %37, %38, %39 acq_rel monotonic : !llvm.ptr<i32>, i32
      %41 = llvm.extractvalue %40[0] : !llvm.struct<(i32, i1)> 
      %42 = llvm.bitcast %41 : i32 to f32
      %43 = llvm.extractvalue %40[1] : !llvm.struct<(i32, i1)> 
      llvm.cond_br %43, ^bb12, ^bb11(%42 : f32)
    ^bb12:  // pred: ^bb11
      llvm.br ^bb13
    ^bb13:  // 2 preds: ^bb2, ^bb12
      llvm.br ^bb14
    ^bb14:  // 2 preds: ^bb1, ^bb13
      llvm.return
    }
  }
  gpu.module @main_kernel_1 attributes {gpu.binary = "P\EDU\BA\01\00\10\008\04\00\00\00\00\00\00\02\00\01\01@\00\00\00\F8\03\00\00\00\00\00\00\F7\03\00\00\00\00\00\00\07\00\01\00P\00\00\00\00\00\00\00\00\00\00\00\11 \00\00\00\00\00\00\00\00\00\00\00\00\00\00\E8\0B\00\00\00\00\00\00\A2\7FELF\02\01\013\07\00\01\00f\02\00\BE\00q\00\01\00!@\0B\07\001\00\80\08\07\00\F5\0E\00P\05P\00@\008\00\03\00@\00\0B\00\01\00\00.shstrtab\00.\08\00'ym\08\00\F0\1B_shndx\00.nv.info\00.text.main_kColReduction_r\0A\00\F5\00e__4_1_0___thin7\00\0F1\00\1Aoshared3\00\1A\9Fconstant06\00\17\FA\01debug_frame\00.rel\11\00!nv\14\00\11aB\00\0F'\01 \0F\87\00\14\0FO\01\B6o_paramV\01\1C\0F\01\00\07\8CZ\00\00\00\03\00\0A\00\01\00\11\EC\18\00,\09\00\01\00 )\01\18\00,\04\00\01\00\11G\18\00,\07\00\01\00f2\00\00\00\12\10`\00!\80\01\07\00v\00\FF\FF\FF\FF(\00\0C\00\00\01\00@\03\00\04|\08\00\D1\0F\0C\81\80\80(\00\08\FF\81\80(\08\0B\00\06(\00/0\00\01\00\03\13pX\00\10\04\9B\00R\04\14\00\00\00E\00\01\0B\00\00\13\00p/\08\00\05\00\00\00\9F\03\22\04#\0C\00\00\01\00'\04\12\0C\00\17\11\0C\00!7\04\E0\03\F3\08\015\00\00\04\0A\08\00\02\00\00\00`\01\10\00\03\19\10\00\04\17\0C\04\04U\08\00\00\F0!\10\00\10\01\18\01%\F0\11\10\00\01\01\00\F2\02\F0\11\00\03\1B\FF\00\04\1C\08\00P\00\00\00\B0\00\01\00#K\00\01\00s\02\02\08\10\0A/\22\9B\00\00\07\00\03\08\00\13\10\08\00\13\18\08\00\13 \08\00\13(\08\00\130\08\00\228\08|\00\13\00\08\00\13\08\08\00\13\10\08\00\13\18\08\00\13 \08\00\13(\08\00\130\08\00\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\A2\00\00\00\14,\00\00\00H\00\01\00\000\01/\05\00\01\00\FF\B8A\02z\01\00'\04\B1\0F\00\00\00\C4\0F\00\19y\02\00\01\00\10%\93\02Q\0E\00\19y\03\0F\00\F5\1A\00!\00\00\00$\0E\00$z\02\02\00X\00\00\03\02\8E\07\00\CA\1F\00\0Cz\00\02\00Y\00\00p`\F0\03\00\DA\0F\00M[\04\A0\80\03\00\EA\0F\005t\03\FF\BB\03\10\FF\C8\03P\E2\0F\00\02x>\02B\80\FF\00\0F\10\00r\B9z\04\00\00F\00\84\00\94\D0\0F\00%v\02\02\00Z`\00`\0F\00\86y\00\022\00@\04\19\10\0C0\009My\00`\00PGy\00\00\F0A\04\A6\FF\83\03\00\C0\0F\00\18y\00\01\00\0F\10\00\90\0F\01\00-\00W\01.\03\00\01\00\22@\00\01\00='\01\000\00\08\01\00\1F\0B@\00\04\13g)\00\1FV@\00\0C\13\13\14\04\0C\01\00\13\C0\15\00&\90\008\04#\04\00\8D\04\00\FE\04\12\00\01\00\1F\FAT\00\00\00\01\00\13P\95\00/p\00\80\00\0B\1F)'\00\03#\00\C0@\00\04X\06\04\E4\00*\04\00\01\00\1F`@\00\04\13\F01\00&\\\00@\00\1F\0A@\00\00!\18\01D\01\0D@\00\13P)\00*\D8\00\01\00\1B\08\08\00?\07\01\00\8E\07\00Q\00\00(\05\00\01\00&\10\00\80\00\17\048\00\04\18\00\13\C4\14\01\0C\84\01*8\05@\07\1F\00\C0\00\04\132@\00+\06\00\01\00\1A\07\D8\07\12\03\F8\05:\08\80\00\01\00\13\06\10\06\04(\0B\0C\01\00*\A8\00\08\00\04\F8\00\13\018\00\04\A8\00\0C\01\009H\03\00\08\00\088\00/\06\00\01\00\17\80\08\00\00\00\00\00\00\00\00"} {
    llvm.func @main_kColReduction_reduce__4_1_0___thin(%arg0: i32, %arg1: i32, %arg2: !llvm.ptr<f32>) attributes {disc.elimargs = [2 : index, 4 : index, 5 : index, 6 : index], gpu.kernel, nvvm.kernel} {
      %0 = llvm.mlir.constant(0xFF800000 : f32) : f32
      %1 = nvvm.read.ptx.sreg.ctaid.x : i32
      %2 = nvvm.read.ptx.sreg.tid.x : i32
      llvm.br ^bb1
    ^bb1:  // pred: ^bb0
      %3 = llvm.mul %1, %arg0  : i32
      %4 = llvm.add %2, %3  : i32
      %5 = llvm.icmp "ult" %4, %arg1 : i32
      llvm.cond_br %5, ^bb2, ^bb3
    ^bb2:  // pred: ^bb1
      %6 = llvm.getelementptr %arg2[%4] : (!llvm.ptr<f32>, i32) -> !llvm.ptr<f32>
      llvm.store %0, %6 : !llvm.ptr<f32>
      llvm.br ^bb3
    ^bb3:  // 2 preds: ^bb1, ^bb2
      llvm.return
    }
  }
  gpu.module @main_kernel_2 attributes {gpu.binary = "P\EDU\BA\01\00\10\00(\0C\00\00\00\00\00\00\02\00\01\01@\00\00\00\E8\0B\00\00\00\00\00\00\E3\0B\00\00\00\00\00\00\07\00\01\00P\00\00\00\00\00\00\00\00\00\00\00\11 \00\00\00\00\00\00\00\00\00\00\00\00\00\00(:\00\00\00\00\00\00\A2\7FELF\02\01\013\07\00\01\00f\02\00\BE\00q\00\01\00#\809\08\00\116\07\00\F5\0E\00P\05P\00@\008\00\03\00@\00\0C\00\01\00\00.shstrtab\00.\08\00'ym\08\00\F0\1B_shndx\00.nv.info\00.text.main_kColReduction_r\0A\00\F5\02e__4_1_0___thin_19\00\0F3\00\1Coshared5\00\19Orela\9D\00\1E?rel\D1\00!\9Fconstant08\00\19\B2debug_framey\00\09\11\00!nv\14\00\11aD\00\0F\98\01 \0F\89\00\16\0F\C2\01\EF\8F$____wg_2\00\16\00\0B\00/27\FA\01&o_param\01\02\1C\0F\01\00\0B\8C\\\00\00\00\03\00\0A\00\01\00\11\BF\18\00,\0B\00\01\00 \95\01\18\00,\09\00\01\00\11\D4\18\00,\04\00\01\00\11\F2\18\00,\07\00\01\00g2\00\00\00\12\10x\00\11.\06\00v\00\FF\FF\FF\FF(\00\0C\00\00\01\00@\03\00\04|\08\00\D1\0F\0C\81\80\80(\00\08\FF\81\80(\08\0B\00\06(\00/0\00\01\00\03!\F0-\07\00 \00\04\9B\00R\04\14\00\00\00E\00\22\044\DC\00\90\04/\08\00\06\00\00\00\15\0C\00\12#\0C\00\00\01\00'\04\12\0C\00\17\11\0C\00!7\04\18\05\F1\08\015\00\00\04\0A\08\00\03\00\00\00`\01(\00\03\19(\00\04\17\0C$\00u\06\00 \00\00\F0!\10\00u\05\00\1C\00\00\F0\11\10\009\04\00\18\10\009\03\00\14\10\009\02\00\10\10\009\01\00\08P\00\01\01\00\F2\0A\F0\11\00\03\1B\FF\00\04\1C\0C\00P\00\00\000,\00\000-\00\00\04\1E\84\01#K\00\01\00\B1\02\02\08\10\0A/\22\00\00\00\08\10\00#\00\08\08\00\13\10\08\00\13\18\08\00\13 \08\00\13(\08\00\130\08\00\228\08\84\00\13\00\08\00\13\08\08\00\13\10\08\00\13\18\08\00\13 \08\00\13(\08\00\130\08\00\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\A2\00\00\00\14,\00\00\00H\00\01\00\11\02h\01\0F\01\00\FF\B8@$v\01\FF\87\04\B2\FF\00\8E\07\00\C4\0F\00\19y\00\01\00\10%\93\02a\0E\00\19y\03\00\01\00\10!-\00\F0\04\0E\00$z\00\00\00]\00\00\03\02\8E\07\00\CA\1F\00\0C\10\00\C5^\00\00p`\F0\03\00\DA\0F\00M\A3\04\F0\0C\80\03\00\EA\0F\00\06{\04\00\00_\00\00\00\90 \00\00\22\0E\00\19x\05\FF\1F\1F\00\F1\00\14\01\00\00\E2\0F\00\B9z\04\00\00F\00\00\0B\05 \E2\0FP\00\10\FF0\00@pP\F4\03\10\00cEy\00\00 *P\00p\E2\0F\00\11r\05\05\0E\00\B2\FF@\8F\07\00\C6\0F\00\08s\04\FE\04\10\10\A0\00\F1\06\1E\00\10x\02\04\FE\FF\FF\0F\FF\E0\FF\07\00\CC\1F\00\05s\03\B4\02\00\C8\03\A3\00d\00\00$r\02\FF\FF\00\F0\00a\E4\1F\00$r\07\10\00\C0\03\0A\8E\07\00\C8/\00$z\07\07\80\00\10\FF\E0\00\81\C8\0F\00'r\06\03\07\F8\02 \8E\07\80\00@\19x\03\FF\19\03\10\05\C0\00q\E4\0F\00\12x\05\05\09\05!\C0\8E\90\00T'r\06\06\03`\00a\C8\0F\00$r\04`\00#\06\0A\10\00Tz\02\04\00_@\01\01\F0\006\02\00_@\010\10\0A\02\10\00\11\80\C0\00\80\E4\0F\00\10\08\06\06\01P\00\03\10\00\060\00\11\F2\10\010$x\02$\04A\00\05\0A\8E \00\A2$t\05\FF\00\00\80\FF\FF\00\90\00@\12x\00\02p\01\22\FF\C0 \00@\19x\07\FFA\01@\02\16\01\00\80\00*\10\18`\001\12\AA\06\80\01\22\FF30\00\EF\0Cx\00\02\7F\00\00\00p@\F4\03\00\C6\D0\00\01\16\03\D0\00\00\10\00@x\03\03 p\01\12\02@\00\22$x\81\04\22\07\02\90\00h\0Cz\00\03\00X\C0\00\00\98\05\030\01\96\D8\0F\00G\19\00\00 (@\02E$x\04\06@\00\01`\004\04\04@0\00\02@\01\10\04M\07\C7pb\F8\03\00\E4\0F\04\10x\08\04\D0\00g\04\10x\0A\04\02\E0\00B\0Cz\00\080\00\A5\FA\03\00\CE\0F\00$\C4\07\FF\80\00\00\B0\00V\CA\06\04\00X\D0\00c%\C6\06\06\00Z\80\00\02\A0\02\12\0A@\00 \F0\03 \00@\81\C9\0C\06@\00\B6\00\19\1E\0C\00\A2\00\00$\D4\09P\00\84\E2\0F\00\10x\0E\04\03\80\00\00\F0\009\DA\08\08`\000\D6\08\08`\00\16\09`\00\12\0E`\00 \F6\03\E0\00E\81\D9\08\08`\00j\E2\02\00$\84\0B\B0\009\8A\0A\0AP\000\86\0A\0AP\00\10\0B\10\00u\CC\0F\00\81\89\0A\0A@\00X\22\0F\00$\B4\F0\00i\1F\00$\BA\06\0E@\00\17\B6\F0\00\00\80\00&\B9\0E\E0\00\88b\01\00\10x\10\04\04\D0\00\08p\02p\E2\0F\00\0B\C8\00\0C\10\00\B2\00b\FC\03\00\C8O\00\08\C8\09\10\00\B1\02\00\03\00\E4/\04\0B\C2\00\0CN\07\10\80 \00r\0F\00\08\C2\05\0C\09Y\06\13\03\A0\01\17\10\D0\01\F4\07\00\0B\D2\00\05\08\00\00@\00\C0\FC\03\00\E4\8F\08\10x\0C\04\05\80\00\82\E4\0F\00\08\D2\09\08\05@\00\00\10\003\04\0B\D2\E9\06\04`\005\D2\05\08`\00\1B\E2\F0\01\11\E2\C0\04\15\0C\10\02\11\E2\00\02\15\10\10\01\00\D0\00E\82\00\05\0A\80\00 \0F\09\00\01\17\06\00\01\0C \02I\08\82\09\0A\90\00D\82\00\0A\0A\90\00\10\C60\02\17\060\02i\0E\00\08\82\05\0A\A0\00\0B@\02\06\10\01#\F0\03@\02\19\0C\A0\00F\B2\00\05\0E\A0\00\10\0A \01\18\07\A0\00\0B`\02I\08\B2\0B\0E\A0\00H\B2\00\0E\0E\A0\00\09p\02x\0E\00\08\B2\05\0E\0B\A0\00\08\80\02'\E2\0F@\01\12\F6 \00?\8A\0A\10\90\02\16\10$\90\02\16\0DP\00\00\D0\02:\BA\0C\0C\90\02 \0C\0C@\00\14\0D@\005\B9\0C\0C@\00\12b`\03\14\08\F0\00\01\F0\036\10\04\09\10\00d\00\0B\C2\00\05\06 \01\00\90\029\C2\07\06\00\01D\C2\00\06\06\00\01\02\90\02$\06\07\F0\00\18\E4\90\03\0F\90\02\00o\C8\8F\00\08\D2\07\80\02\0E\1F\07\80\02\06\08\E0\01\05\80\02\06\90\03\0F\80\02\024\0E\04\0A\E0\00\0F\80\02S\1B\0E\80\02\1C\10\80\02\19\0C\80\02?\10\04\0B\80\02\0B\1B\0C\80\02/\0C\0C\80\02\0B\1F\0C\80\02\0C\1B\10\80\02\1F\0E\80\02,\1F\10\80\02\1D\14\0C\F0\00\03\80\02\1F\0D\80\02\CC\14\0E\E0\00\0F\80\02\84\1F\0F\80\02\DC\14\10\F0\00\03\80\02\1F\11\80\02\CC\14\12\E0\00\0F\80\02\84\1F\13\80\02\DC\14\14\F0\00\03\80\02\1F\15\80\02\CC\14\16\E0\00\0F\80\02\84\1F\17\80\02\DC\14\18\F0\00\03\80\02\1F\19\80\02\CC\14\1A\E0\00\0F\80\02\84\1F\1B\80\02\DC\14\1C\F0\00\03\80\02\1F\1D\80\02\CC\14\1E\E0\00\0F\80\02\84\1F\1F\80\02\DC\14 \F0\00\03\80\02\1F!\80\02\CC\14\22\E0\00\0F\80\02\84\1F#\80\02\DC\14$\F0\00\03\80\02\1F%\80\02\CC\14&\E0\00\0F\80\02\84\1F'\80\02\DC\14(\F0\00\03\80\02\1F)\80\02\CC\14*\E0\00\0F\80\02\84\1F+\80\02\DC\14,\F0\00\03\80\02\1F-\80\02\CC\14.\E0\00\0F\80\02\84\1F/\80\02\DC\140\F0\00\03\80\02\1F1\80\02\CC\142\E0\00\0F\80\02\84\1F3\80\02\DC\144\F0\00\03\80\02\1F5\80\02\CC\146\E0\00\0F\80\02\84\1F7\80\02\DC\148\F0\00\03\80\02\1F9\80\02\CC\14:\E0\00\0F\80\02\84\1F;\80\02\DC\14<\F0\00\03\80\02\19=\10\009\12\04>\10\00?\04\04?\A0\02\C5?\C6\0F\01\90\02\1AW\E4\0F\04\81\C9`$/\A4\00\90\02\11\1F\12\90\02\16?\C6\0F\02\80\02+\1F\02\80\02\00\0D@$\1A\04\80\02*\06\12\80\02\07\10\01\01\80\02\07\80\03W$\01\00$\B4\D0\02\01\80\02\1A\0A\80&\1B\B6\C0\028\B9\0A\0A\80\02\00@\02\19\0E@\02\17\09\E0#\01p%\06\E0#\02@\02\15\0E`\01\1E\C8`%\06 \02\01P\02\19\04@\02(\C8\1F0\02\11\02@\00\14\82\C0\02\10\FA0\004\09\0B\82\B0\02\03@\00\14\82\D0\02 \80\060\00I\08\82\05\06@\00\14\B2@\02\010\004\0A\0B\B2 \02\12\F0@\00$\B2\07@\02\02@\003\B2\05\0A@\00\10\00\B0*\14A\B0*\030( \88s\F8-\12\00u(f\E8\0F\00\1D{\00\01\00b\EC\0F\00\84\A9\07,*\00 \00Q\22\0E\00\1Cx\1A\00P\00p\F0\F0\03P\00\13\0C0)1pD\F2\90*!\84\A9n\01\020\00qb\0E\00\0B\A2\00\07\80\00\A1\80\F6\03\00\E4\1F\08\0B\A2\00p%\02\C0\00T/\00\1C\A8\00P\00\14\01@)\11?@)@\F6\03\00\D6\00\01#\07\07R\00\10\06 \00\09\80\00\8F\C6\0F\00\88\A3\00\00\07\C0\00\09\22\B9\05\1C\00\12\08\A0++\84\B9\A0\00\22\B2\00!+`\80\F4\03\00\C4\1F\10\00(\04\05\A0\00#\B8\00p\00Ap\01\00\DA\90\01\15\05\90\00_\CA\0F\00\88\B3@\01\0B9M\19\00p\017$t\02\10\03R\0F\00\84y\05\D1)\00@\00\90&\0E\00%v\02\03\00`\0F/\04 \00\00\1D\00\03`\00vh\0E\00\81y\06\02\D0\02D\05\00\0Br\C0\00\12\F0\C0\00\14r\C0\00W\F2\03\00\D6/\B0\00 \80\04\F0\022\0Br\00`\02\11@\10\04(\0EF\10\02S\E6\0F\00\08r\80\02\00\01\00p\CC\0F\00\A9s\07\02\A0\02\C0\07\E1\1E\00\00\A4\0E\00\0Cr\00\07\10\00 pR@\00QO\00$r\06\E0*\14\07\90*@\09\00\00\90\E0+!\FF\83\F0\00\1BMp\02TGy\00\00\F0 \00f\C0\0F\00\18y\00\01\00\0F\10\00\90\0F\01\00-\11\01H1\0E\01\00\22@\00\01\00=\98\01\000\00\08\01\00\1F\0B@\00\04\13\D8)\00?\01\02\00@\00\0A\13\13\0B\04\0C\01\00\13\E0U\00\03\8F\03\01/\04\13\05w\02\00\01\00\22\18\00\01\00.k\01T\00\00\01\00#\88\04(\03\1F\00\80\00\0B/)\00'\00\02#\00\F8@\00\04 3\04\E4\00*\04\00\01\00\1Fb@\00\04*(\05\C0\00\13\03#/\0C@\00!\89\01D\01\0D@\00\13\D0@\00*\D8\00\01\00\1B\08\08\00?x\01\00V4\000\00\00\A8\15\03\03W/\04\80\00\17\048\00\04\18\00\133@\01\0C\84\01\13\B8@\00\17\881\01\0F\C0\00\01\132T\01\15\06R\00\03^\03\1A\08\A04\11\03$\00J\00\15\80\00\01\00\13\95\94\00*\03\00\01\00\0409/\00\04\80\00\0B\13\06\18\02\04h9\0D\08\01\1A\00\08\00\04\97\00\13\018\00\04\E8\00\0C\01\009\C8/\00\08\00\088\00\18\06\A0\00\0F\01\00\05\03\A9\00\80\08\00\00\00\00\00\00\00\00\00\00\00\00"} {
    llvm.mlir.global internal @__wg_main_kColReduction_reduce__4_1_0___thin_1_0() {addr_space = 3 : i32} : !llvm.array<256 x f32>
    llvm.func @__nv_fabsf(f32) -> f32
    llvm.func @main_kColReduction_reduce__4_1_0___thin_1(%arg0: i32, %arg1: !llvm.ptr<f32>, %arg2: i32, %arg3: i32, %arg4: i32, %arg5: i32, %arg6: !llvm.ptr<f32>) attributes {disc.elimargs = [1 : index, 3 : index, 5 : index, 6 : index, 7 : index, 8 : index, 9 : index, 13 : index, 15 : index, 16 : index, 17 : index], gpu.kernel, nvvm.kernel} {
      %0 = llvm.mlir.constant(32 : index) : i32
      %1 = llvm.mlir.constant(8 : index) : i32
      %2 = llvm.mlir.constant(64 : index) : i32
      %3 = llvm.mlir.constant(2 : index) : i32
      %4 = llvm.mlir.constant(0xFF800000 : f32) : f32
      %5 = llvm.mlir.constant(4 : index) : i32
      %6 = llvm.mlir.constant(1 : index) : i32
      %7 = llvm.mlir.constant(256 : index) : i32
      %8 = llvm.mlir.constant(0 : index) : i32
      %9 = llvm.mlir.addressof @__wg_main_kColReduction_reduce__4_1_0___thin_1_0 : !llvm.ptr<array<256 x f32>, 3>
      %10 = llvm.getelementptr %9[0, 0] : (!llvm.ptr<array<256 x f32>, 3>) -> !llvm.ptr<f32, 3>
      %11 = nvvm.read.ptx.sreg.ctaid.x : i32
      %12 = nvvm.read.ptx.sreg.tid.x : i32
      llvm.br ^bb1
    ^bb1:  // pred: ^bb0
      %13 = llvm.mul %11, %arg3  : i32
      %14 = llvm.add %12, %13  : i32
      %15 = llvm.icmp "ult" %14, %arg4 : i32
      llvm.cond_br %15, ^bb2, ^bb21
    ^bb2:  // pred: ^bb1
      %16 = llvm.srem %14, %7  : i32
      %17 = llvm.sdiv %14, %7  : i32
      %18 = llvm.udiv %17, %arg5  : i32
      %19 = llvm.urem %17, %arg5  : i32
      %20 = llvm.udiv %16, %0  : i32
      %21 = llvm.urem %16, %0  : i32
      %22 = llvm.mul %21, %1  : i32
      %23 = llvm.add %20, %22  : i32
      %24 = llvm.mul %19, %0  : i32
      %25 = llvm.add %21, %24  : i32
      %26 = llvm.icmp "ult" %25, %arg0 : i32
      llvm.cond_br %26, ^bb3, ^bb10(%4 : f32)
    ^bb3:  // pred: ^bb2
      %27 = llvm.mul %18, %1  : i32
      %28 = llvm.add %20, %27  : i32
      %29 = llvm.mul %28, %2  : i32
      llvm.br ^bb4(%8, %4 : i32, f32)
    ^bb4(%30: i32, %31: f32):  // 2 preds: ^bb3, ^bb9
      %32 = llvm.icmp "slt" %30, %2 : i32
      llvm.cond_br %32, ^bb5, ^bb10(%31 : f32)
    ^bb5:  // pred: ^bb4
      %33 = llvm.add %30, %29  : i32
      %34 = llvm.icmp "slt" %33, %arg2 : i32
      llvm.cond_br %34, ^bb6, ^bb7
    ^bb6:  // pred: ^bb5
      %35 = llvm.mul %33, %arg0  : i32
      %36 = llvm.add %35, %25  : i32
      %37 = llvm.getelementptr %arg1[%36] : (!llvm.ptr<f32>, i32) -> !llvm.ptr<f32>
      %38 = llvm.load %37 : !llvm.ptr<f32>
      %39 = llvm.call @__nv_fabsf(%38) : (f32) -> f32
      %40 = llvm.fcmp "ugt" %31, %39 : f32
      %41 = llvm.select %40, %31, %39 : i1, f32
      %42 = llvm.fcmp "uno" %39, %39 : f32
      %43 = llvm.select %42, %39, %41 : i1, f32
      llvm.br ^bb8(%43 : f32)
    ^bb7:  // pred: ^bb5
      llvm.br ^bb8(%31 : f32)
    ^bb8(%44: f32):  // 2 preds: ^bb6, ^bb7
      llvm.br ^bb9
    ^bb9:  // pred: ^bb8
      %45 = llvm.add %30, %6  : i32
      llvm.br ^bb4(%45, %44 : i32, f32)
    ^bb10(%46: f32):  // 2 preds: ^bb2, ^bb4
      llvm.br ^bb11(%46 : f32)
    ^bb11(%47: f32):  // pred: ^bb10
      llvm.br ^bb12
    ^bb12:  // pred: ^bb11
      %48 = llvm.getelementptr %10[%23] : (!llvm.ptr<f32, 3>, i32) -> !llvm.ptr<f32, 3>
      llvm.store %47, %48 : !llvm.ptr<f32, 3>
      nvvm.barrier0
      %49 = llvm.icmp "slt" %20, %5 : i32
      llvm.cond_br %49, ^bb13, ^bb14
    ^bb13:  // pred: ^bb12
      %50 = llvm.add %23, %5  : i32
      %51 = llvm.getelementptr %10[%23] : (!llvm.ptr<f32, 3>, i32) -> !llvm.ptr<f32, 3>
      %52 = llvm.load %51 : !llvm.ptr<f32, 3>
      %53 = llvm.getelementptr %10[%50] : (!llvm.ptr<f32, 3>, i32) -> !llvm.ptr<f32, 3>
      %54 = llvm.load %53 : !llvm.ptr<f32, 3>
      %55 = llvm.fcmp "ugt" %52, %54 : f32
      %56 = llvm.select %55, %52, %54 : i1, f32
      %57 = llvm.fcmp "uno" %54, %54 : f32
      %58 = llvm.select %57, %54, %56 : i1, f32
      %59 = llvm.getelementptr %10[%23] : (!llvm.ptr<f32, 3>, i32) -> !llvm.ptr<f32, 3>
      llvm.store %58, %59 : !llvm.ptr<f32, 3>
      llvm.br ^bb14
    ^bb14:  // 2 preds: ^bb12, ^bb13
      nvvm.barrier0
      %60 = llvm.icmp "slt" %20, %3 : i32
      llvm.cond_br %60, ^bb15, ^bb16
    ^bb15:  // pred: ^bb14
      %61 = llvm.add %23, %3  : i32
      %62 = llvm.getelementptr %10[%23] : (!llvm.ptr<f32, 3>, i32) -> !llvm.ptr<f32, 3>
      %63 = llvm.load %62 : !llvm.ptr<f32, 3>
      %64 = llvm.getelementptr %10[%61] : (!llvm.ptr<f32, 3>, i32) -> !llvm.ptr<f32, 3>
      %65 = llvm.load %64 : !llvm.ptr<f32, 3>
      %66 = llvm.fcmp "ugt" %63, %65 : f32
      %67 = llvm.select %66, %63, %65 : i1, f32
      %68 = llvm.fcmp "uno" %65, %65 : f32
      %69 = llvm.select %68, %65, %67 : i1, f32
      %70 = llvm.getelementptr %10[%23] : (!llvm.ptr<f32, 3>, i32) -> !llvm.ptr<f32, 3>
      llvm.store %69, %70 : !llvm.ptr<f32, 3>
      llvm.br ^bb16
    ^bb16:  // 2 preds: ^bb14, ^bb15
      nvvm.barrier0
      %71 = llvm.icmp "eq" %20, %8 : i32
      %72 = llvm.and %71, %26  : i1
      llvm.cond_br %72, ^bb17, ^bb20
    ^bb17:  // pred: ^bb16
      %73 = llvm.add %23, %6  : i32
      %74 = llvm.getelementptr %10[%23] : (!llvm.ptr<f32, 3>, i32) -> !llvm.ptr<f32, 3>
      %75 = llvm.load %74 : !llvm.ptr<f32, 3>
      %76 = llvm.getelementptr %10[%73] : (!llvm.ptr<f32, 3>, i32) -> !llvm.ptr<f32, 3>
      %77 = llvm.load %76 : !llvm.ptr<f32, 3>
      %78 = llvm.fcmp "ugt" %75, %77 : f32
      %79 = llvm.select %78, %75, %77 : i1, f32
      %80 = llvm.fcmp "uno" %77, %77 : f32
      %81 = llvm.select %80, %77, %79 : i1, f32
      %82 = llvm.getelementptr %arg6[%25] : (!llvm.ptr<f32>, i32) -> !llvm.ptr<f32>
      %83 = llvm.load %82 : !llvm.ptr<f32>
      llvm.br ^bb18(%83 : f32)
    ^bb18(%84: f32):  // 2 preds: ^bb17, ^bb18
      %85 = llvm.fcmp "ogt" %84, %81 : f32
      %86 = llvm.select %85, %84, %81 : i1, f32
      %87 = llvm.bitcast %82 : !llvm.ptr<f32> to !llvm.ptr<i32>
      %88 = llvm.bitcast %84 : f32 to i32
      %89 = llvm.bitcast %86 : f32 to i32
      %90 = llvm.cmpxchg %87, %88, %89 acq_rel monotonic : !llvm.ptr<i32>, i32
      %91 = llvm.extractvalue %90[0] : !llvm.struct<(i32, i1)> 
      %92 = llvm.bitcast %91 : i32 to f32
      %93 = llvm.extractvalue %90[1] : !llvm.struct<(i32, i1)> 
      llvm.cond_br %93, ^bb19, ^bb18(%92 : f32)
    ^bb19:  // pred: ^bb18
      llvm.br ^bb20
    ^bb20:  // 2 preds: ^bb16, ^bb19
      llvm.br ^bb21
    ^bb21:  // 2 preds: ^bb1, ^bb20
      llvm.return
    }
  }
}


// -----// IR Dump After DiscToLLVMPass (disc-to-llvm) //----- //
module attributes {gpu.container_module} {
  llvm.mlir.global internal constant @main_kernel_0_main_kColReduction_reduce__4_1_0___flat_1_kernel_name("main_kColReduction_reduce__4_1_0___flat_1\00") {addr_space = 0 : i32}
  llvm.mlir.global internal constant @main_kernel_0_blob_gpu.binary("P\EDU\BA\01\00\10\00\90\0A\00\00\00\00\00\00\02\00\01\01@\00\00\00P\0A\00\00\00\00\00\00P\0A\00\00\00\00\00\00\07\00\01\00P\00\00\00\00\00\00\00\00\00\00\00\11 \00\00\00\00\00\00\00\00\00\00\00\00\00\00\E8\1C\00\00\00\00\00\00\A2\7FELF\02\01\013\07\00\01\00f\02\00\BE\00q\00\01\00!@\1C\07\001\00\80\19\07\00\F5\0E\00P\05P\00@\008\00\03\00@\00\0B\00\01\00\00.shstrtab\00.\08\00'ym\08\00\F0\1B_shndx\00.nv.info\00.text.main_kColReduction_r\0A\00\F5\02e__4_1_0___flat_19\00\0F3\00\1Coshared5\00\19Orela\9D\00\1E?rel\D1\00!\9Fconstant08\00\19\B2debug_framey\00\09\11\00!nv\14\00\11aD\00\0F\98\01 \0F\89\00\16\0F\C2\01\FF(o_param\C9\01\1C\0F\01\00\0B\8C\\\00\00\00\03\00\0A\00\01\00 ]\01\18\00,\09\00\01\00\11\9C\18\00,\04\00\01\00\11\BA\18\00,\07\00\01\00f2\00\00\00\12\10`\00!\80\11\07\00v\00\FF\FF\FF\FF(\00\0C\00\00\01\00@\03\00\04|\08\00\D1\0F\0C\81\80\80(\00\08\FF\81\80(\08\0B\00\06(\00/0\00\01\00\03\13pX\00\10\04\9B\00R\04\14\00\00\00E\00\22\04 \AC\00\90\04/\08\00\05\00\00\00\16\0C\00\12#\0C\00\00\01\00'\04\12\0C\00\17\11\0C\00!7\04\C8\04\F1\08\015\00\00\04\0A\08\00\02\00\00\00`\01(\00\03\19(\00\04\17\0C$\00u\06\00 \00\00\F0!\10\00u\05\00\1C\00\00\F0\11\10\009\04\00\18\10\009\03\00\14\10\009\02\00\10\10\009\01\00\08P\00\01\01\00\F2\0A\F0\11\00\03\1B\FF\00\04\1C\0C\00P\00\00\00\10\02\00\00\E0\10\00\00\04\1E\D8\00#K\00\01\00\B1\02\02\08\10\0A/\22\00\00\00\08\10\00#\00\08\08\00\13\10\08\00\13\18\08\00\13 \08\00\13(\08\00\130\08\00\228\08\84\00\13\00\08\00\13\08\08\00\13\10\08\00\13\18\08\00\13 \08\00\13(\08\00\130\08\00\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\A2\00\00\00\14,\00\00\00H\00\01\00\00x\01/\05\00\01\00\FF\88@$v\01\FF?\04\B2\FF\00\8E\07\00\C4\0F\00\19y\00\01\00\10%c\02Q\0E\00\19y\03\0F\00 \00!-\00@\0E\00$z]\04\B0\00\00\03\02\8E\07\00\CA\1F\00\0C\10\00\C5^\00\00p`\F0\03\00\DA\0F\00Ms\04\F0\0C\80\03\00\EA\0F\00\06{\04\00\00_\00\00\00\90 \00\00\22\0E\00\19x\05\FF\1F\1F\00\D3\14\01\00\00\E2\0F\00$r\02\FF\FF\00\80\00 \E2\0FP\00\10\FF0\00\B1pP\F4\03\00\E4\0F\00\11r\05?\02\B2\FFH\8F\07\00\C6\0F\00\08s\04\BE\04\10\10\90\00\F1\07\1E\00\10x\03\04\FE\FF\FF\0F\FF\E0\FF\07\00\CC\1F\00\05s\03\00\B2\00\22\F0!\B0\00!r\07`\00\C0\03\0A\8E\07\00\C8\1F\00$z\07\07`\00\10\FF\C0\00\81\C8\0F\00'r\07\03\07\A8\02\02\80\00@\19x\02\FF\C3\00\10\05\A0\00\E1\E4\0F\00\12x\05\05\00\FE\FF\FF\FF\C0\8E\80\00T'r\07\07\02\B0\00\10\C8\C0\00\11\09`\00#\07\0A\10\000z\03\09`\00\11\02 \01\01\D0\006\03\00_ \010\10\0A\03\10\00\11\80\B0\00\80\E4\0F\00\10\08\07\07\01P\00\03\10\00\060\00\12\F20\00\1A\18 \001\12\AA\07 \01\22\FF3`\00;$r\03\80\00%\02\03\80\00`\E4\0F\00$x\03\14\04$\00\05 \000x\03\02\95\03\18\03\A0\00\1FX\C0\01\07W$x\02\07 \A0\01\81\B9z\04\00\00F\00\00s\06\00\C0\01Dt\00\FF\04 \00\11\C6P\00\10\02\95\060pb\F8\C0\01U\04\10x\06\02\C0\00\11\C8 \00\12\06 \00\C3\FA\03\00\CE\0F\00$\CA\05\02\00X\90\00\90\C8\0F\00%\C6\04\05\00Z\91\01\03\B0\005\DA\07\06 \00a\E2\0F\00\81\C9\0A#\06\C5\00\19\1E\0C\00\A6\00\00%\D6\06\070\00u\CA\0F\00\81\D9\0D\06 \00\86\E2\02\00\10x\08\02\02@\01j\04\10x\10\02\03p\01\12\08\90\00#\F0\03\10\00\12\10\10\00\11\F2\10\00Y\10x\0C\02\04@\004\0E\02\05\10\00\11\C40\00\12\0C0\00 \F4\03\80\00E$\8A\09\08\B0\00w\E4\0F\10$\9A\0B\10\10\00U\00%\86\08\09\B0\00\02\F0\02\12\0E@\00 \F6\03\F0\00E\81\89\05\08\C0\00v\22\11\00%\96\06\0B0\00 /\00\C0\00\14\06\80\00w\C6\0F\00$\AA\0B\0C \01F\10\81\99\0F\00\01\B4\22\03\00$t\04\FF\00\00\80\FF\C0\01\00\D0\00\14\07@\00\00\C0\015\BA\09\0E@\00W\C8\1F\00%\B6\A0\00g\CC\0F\00\81\B9\09\90\00@\01\00\0B\C8\FA\01\D2\80\FF\00\C2\FC\03\00\C8O\00\08\C8\04\10\000\02\00\03P\007%\A6\0A\B0\00\08P\01\01\10\02\F5\06\00\0B\D2\00\04\0D\00\00@\00`\FC\03\00\C6\8F\00\81\A9\0B\0A`\00b\E2\04\00\08\D2\04 \00!\00\000\00\06`\01\12\FA`\01&\CA\07P\01z\C8/\00%\C6\06\070\02*\0D\0C0\02\16\11\10\01y\E6\02\00%\D6\0C\0D\E0\006\D9\0D\0C\80\00t\00\00\0B\82\00\04\05\A0\00\84\E4\0F\09\10x\0A\02\080\01\8A\E4O\00\08\82\04\04\05\A0\00\18\0A@\02V\0B\92\00\04\0F@\00\01\80\02\14\09@\00\8A\C4\1F\00\08\92\04\04\0F@\00\1A\08p\028\0E\02\0A\E0\016\8A\07\0A\D0\00\11/\C0\01\17\0B \00+%\86\00\03&\89\05\F0\00!\A2\000\02\14\0C0\00\00\A0\00D\A2\00\04\0B\A0\00\8A\C8\8F\00\08\A2\04\04\0B\90\00\15\0E\D0\02\000\00D\B2\00\04\090\00\000\02&\9A\0B\E0\02\00\90\05D\B2\04\04\09@\00\00\00\02(\96\08\00\02\16\08\C0\01\12\F6P\00D\C2\00\04\11P\00x\E4\0F\08\81\99\0F\08\B0\03H$\AA\0B\0E\C0\01Z\08\C2\04\04\11\A0\00\06P\02\0Fp\02\00\00@\01\17\0D\A0\03\09p\02f\E2\0F\08$\BA\07 \02*\E4\1F\80\02.\22\01\80\02\05\10\01\12\FAp\00\1B\B6p\028\CA\09\10\A0\01'\81\B9p\02{&\03\00%\C6\08\09\A0\02\1A\0E\A0\02\16\13\00\01/&\01\A0\02\0B+\22\03\A0\02 O\08\A0\02\15\0E\E0\01\1F\1F\A0\02\0E\00 \01\1B\0F\10\05\17\100\01\00`\02\14\11\10\00*\CE/\80\02\1F\C8p\02\0F\09\00\03/\C8\8F\F0\02\01\18\0E\F0\02\0A\90\02/\0F\01\90\02\00\1C\10\90\02\05@\02\00 \03)\9A\09P\01'\08\B20\02\11\C6\90\02\06\80\01\0F\90\02\04\14\13P\000\E4\0F\08 \01\14\12\00\01\11\E2`\05\18\10`\00\0C\B0\02\00\A0\02\1E\13\E0\00\0F\A0\02\06\09\90\02\01p\018\10\02\13p\00\08\A0\02*\E2\1F\90\02-\E2\0F\B0\02\06 \01\0F\A0\02\06\1F\0E\A0\02\1C\1F\10\A0\02=\1F\14\A0\02\1C\1B\15\A0\02\1B\16\A0\02\1F\17\A0\02\A7\19\10\A0\02\02@\00\06\90\02\1D\E2\B0\02\0F\A0\02\02\14\18\00\01\0F\A0\02\1D\1F\C4\A0\02\0E\19\08\A0\02\01p\00?\10\02\19\A0\02U\10\E4\A0\02\16\07\B0\01\1F$\A0\02\11\1F\11\A0\02.\1F\1A\A0\02\1A5\06\02\1B0\00\11/\10\0A\1B\1C\A0\02\14\1D\10\00\11\CE\10\0A\0B\A0\02+\08\09\A0\02\1C\08\B0\07\19\1E\00\036\02\02\1F\10\00\0F\C0\02\11\1F\06\C0\02\1C\1C\0E\C0\02\19\07\C0\02,\0B\06`\05\14\07@\00+\C6\0F\A0\0A\0F\F0\07\0C\00\80\07\08\E0\07\10\E4\B0\02,\07\06\B0\02\1E\11`\03\0FP\05\13\11\C6\D0\0A\0F\A0\02\18\08\B0\0C\04\A0\02\0Bp\02\17\CAp\02\1B\E4\10\0B\10$\B0\0C)\0C\0D0\009\DA\0F\020\009\C9\0D\0C0\00:\D6\0E\0F\A0\02(\0F\0E\90\00g%v\02\03\00` \025y\06\02 \00*b!\C0\02*\C8O\B0\02\12\C8 \02\02\C0\01\15\F0 \02\03\B0\018\00\00\C8\10\02\1C\F0\10\02\02 \00\05\F0\09\01 \00\08\E0\09\02 \00\14\C2p\01\03 \00\15\C2`\01\02 \00\14\D2\A0\02\03 \00\15\D2\A0\02\10\00`\0C2\0Br\00\F0\01 @\F0p\017\0EFyp\0ES\E6\0F\00\08r\10\02\00\01\00\80\CC\0F\00\A9s\07\02\06\E0\0E\B0\E1\1E\00\00\A4\0E\00\0Cr\00\07\10\00 pR@\00RO\00$r\06\00\0F\00\E0\0C\80\D8\0F\00G\09\00\00\90i\14!\FF\83\C0\0E*My\D0\0ETGy\00\00\F0 \00f\C0\0F\00\18y\00\01\00\0F\10\00`\0F\01\00-\00\\\0F.\03\00\01\00\02q\01]\00\00\98\01\000\00\08\01\00\1F\0B@\00\04\13\D8)\00\1F\C9@\00\0C\13\13\E4\13\0C\01\00\13\A8U\00\11\90\06\00\02$\00#\04\00]\14\00\CE\14\12\00\01\00.k\01T\00\00\01\00\138\05\02/p\00\80\00\0B\1F)'\00\03#\00\A8@\00\04p\16\04\E4\00*\04\00\01\00\1Fb@\00\04\13\D8)\00&\A8\00@\00\1F\0A@\00\00!\89\01D\01\0D@\00\11\80\B6\12J\00\00\D8\00\01\00\1B\08\08\00?x\01\00\A6\17\000\00\00X\E5\02\03\E7\12\04\80\00\17\048\00\04\18\00\133@\01\0C\84\01\13h@\00\17\881\01\0F\C0\00\01\132T\01+\06\00\01\00\1A\08\F0\17\12\03\C8\15:\16\80\00\01\00\13\06\E0\15\04(\1C\0D\88\01\1A\00\08\00\04\C0\00\13\018\00\04\A8\00\0C\01\009\18\13\00\08\00\088\00/\06\00\01\00\17\80\08\00\00\00\00\00\00\00") {addr_space = 0 : i32}
  llvm.mlir.global internal constant @main_kernel_main_kColReduction_reduce__4_1_0___flat_kernel_name("main_kColReduction_reduce__4_1_0___flat\00") {addr_space = 0 : i32}
  llvm.mlir.global internal constant @main_kernel_blob_gpu.binary("P\EDU\BA\01\00\10\008\04\00\00\00\00\00\00\02\00\01\01@\00\00\00\F8\03\00\00\00\00\00\00\F7\03\00\00\00\00\00\00\07\00\01\00P\00\00\00\00\00\00\00\00\00\00\00\11 \00\00\00\00\00\00\00\00\00\00\00\00\00\00\E8\0B\00\00\00\00\00\00\A2\7FELF\02\01\013\07\00\01\00f\02\00\BE\00q\00\01\00!@\0B\07\001\00\80\08\07\00\F5\0E\00P\05P\00@\008\00\03\00@\00\0B\00\01\00\00.shstrtab\00.\08\00'ym\08\00\F0\1B_shndx\00.nv.info\00.text.main_kColReduction_r\0A\00\F5\00e__4_1_0___flat7\00\0F1\00\1Aoshared3\00\1A\9Fconstant06\00\17\FA\01debug_frame\00.rel\11\00!nv\14\00\11aB\00\0F'\01 \0F\87\00\14\0FO\01\B6o_paramV\01\1C\0F\01\00\07\8CZ\00\00\00\03\00\0A\00\01\00\11\EC\18\00,\09\00\01\00 )\01\18\00,\04\00\01\00\11G\18\00,\07\00\01\00f2\00\00\00\12\10`\00!\80\01\07\00v\00\FF\FF\FF\FF(\00\0C\00\00\01\00@\03\00\04|\08\00\D1\0F\0C\81\80\80(\00\08\FF\81\80(\08\0B\00\06(\00/0\00\01\00\03\13pX\00\10\04\9B\00R\04\14\00\00\00E\00\01\0B\00\00\13\00p/\08\00\05\00\00\00\9F\03\22\04#\0C\00\00\01\00'\04\12\0C\00\17\11\0C\00!7\04\E0\03\F3\08\015\00\00\04\0A\08\00\02\00\00\00`\01\10\00\03\19\10\00\04\17\0C\04\04U\08\00\00\F0!\10\00\10\01\18\01%\F0\11\10\00\01\01\00\F2\02\F0\11\00\03\1B\FF\00\04\1C\08\00P\00\00\00\B0\00\01\00#K\00\01\00s\02\02\08\10\0A/\22\9B\00\00\07\00\03\08\00\13\10\08\00\13\18\08\00\13 \08\00\13(\08\00\130\08\00\228\08|\00\13\00\08\00\13\08\08\00\13\10\08\00\13\18\08\00\13 \08\00\13(\08\00\130\08\00\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\A2\00\00\00\14,\00\00\00H\00\01\00\000\01/\05\00\01\00\FF\B8A\02z\01\00'\04\B1\0F\00\00\00\C4\0F\00\19y\02\00\01\00\10%\93\02Q\0E\00\19y\03\0F\00\F5\1A\00!\00\00\00$\0E\00$z\02\02\00X\00\00\03\02\8E\07\00\CA\1F\00\0Cz\00\02\00Y\00\00p`\F0\03\00\DA\0F\00M[\04\A0\80\03\00\EA\0F\005t\03\FF\BB\03\10\FF\C8\03P\E2\0F\00\02x>\02B\80\FF\00\0F\10\00r\B9z\04\00\00F\00\84\00\94\D0\0F\00%v\02\02\00Z`\00`\0F\00\86y\00\022\00@\04\19\10\0C0\009My\00`\00PGy\00\00\F0A\04\A6\FF\83\03\00\C0\0F\00\18y\00\01\00\0F\10\00\90\0F\01\00-\00W\01.\03\00\01\00\22@\00\01\00='\01\000\00\08\01\00\1F\0B@\00\04\13g)\00\1FV@\00\0C\13\13\14\04\0C\01\00\13\C0\15\00&\90\008\04#\04\00\8D\04\00\FE\04\12\00\01\00\1F\FAT\00\00\00\01\00\13P\95\00/p\00\80\00\0B\1F)'\00\03#\00\C0@\00\04X\06\04\E4\00*\04\00\01\00\1F`@\00\04\13\F01\00&\\\00@\00\1F\0A@\00\00!\18\01D\01\0D@\00\13P)\00*\D8\00\01\00\1B\08\08\00?\07\01\00\8E\07\00Q\00\00(\05\00\01\00&\10\00\80\00\17\048\00\04\18\00\13\C4\14\01\0C\84\01*8\05@\07\1F\00\C0\00\04\132@\00+\06\00\01\00\1A\07\D8\07\12\03\F8\05:\08\80\00\01\00\13\06\10\06\04(\0B\0C\01\00*\A8\00\08\00\04\F8\00\13\018\00\04\A8\00\0C\01\009H\03\00\08\00\088\00/\06\00\01\00\17\80\08\00\00\00\00\00\00\00\00") {addr_space = 0 : i32}
  llvm.mlir.global internal constant @ral_send_output___cpu___pvoid_i64_m2df32___void("ral_send_output___cpu___pvoid_i64_m2df32___void\00") {addr_space = 0 : i32}
  llvm.mlir.global internal constant @dealloc___gpu___pvoid_pvoid___void("dealloc___gpu___pvoid_pvoid___void\00") {addr_space = 0 : i32}
  llvm.mlir.global internal constant @inc_ref___gpu___pvoid_pvoid_m1df32_m1di64___m2df32("inc_ref___gpu___pvoid_pvoid_m1df32_m1di64___m2df32\00") {addr_space = 0 : i32}
  llvm.mlir.global internal constant @main_kernel_2_main_kColReduction_reduce__4_1_0___thin_1_kernel_name("main_kColReduction_reduce__4_1_0___thin_1\00") {addr_space = 0 : i32}
  llvm.mlir.global internal constant @main_kernel_2_blob_gpu.binary("P\EDU\BA\01\00\10\00(\0C\00\00\00\00\00\00\02\00\01\01@\00\00\00\E8\0B\00\00\00\00\00\00\E3\0B\00\00\00\00\00\00\07\00\01\00P\00\00\00\00\00\00\00\00\00\00\00\11 \00\00\00\00\00\00\00\00\00\00\00\00\00\00(:\00\00\00\00\00\00\A2\7FELF\02\01\013\07\00\01\00f\02\00\BE\00q\00\01\00#\809\08\00\116\07\00\F5\0E\00P\05P\00@\008\00\03\00@\00\0C\00\01\00\00.shstrtab\00.\08\00'ym\08\00\F0\1B_shndx\00.nv.info\00.text.main_kColReduction_r\0A\00\F5\02e__4_1_0___thin_19\00\0F3\00\1Coshared5\00\19Orela\9D\00\1E?rel\D1\00!\9Fconstant08\00\19\B2debug_framey\00\09\11\00!nv\14\00\11aD\00\0F\98\01 \0F\89\00\16\0F\C2\01\EF\8F$____wg_2\00\16\00\0B\00/27\FA\01&o_param\01\02\1C\0F\01\00\0B\8C\\\00\00\00\03\00\0A\00\01\00\11\BF\18\00,\0B\00\01\00 \95\01\18\00,\09\00\01\00\11\D4\18\00,\04\00\01\00\11\F2\18\00,\07\00\01\00g2\00\00\00\12\10x\00\11.\06\00v\00\FF\FF\FF\FF(\00\0C\00\00\01\00@\03\00\04|\08\00\D1\0F\0C\81\80\80(\00\08\FF\81\80(\08\0B\00\06(\00/0\00\01\00\03!\F0-\07\00 \00\04\9B\00R\04\14\00\00\00E\00\22\044\DC\00\90\04/\08\00\06\00\00\00\15\0C\00\12#\0C\00\00\01\00'\04\12\0C\00\17\11\0C\00!7\04\18\05\F1\08\015\00\00\04\0A\08\00\03\00\00\00`\01(\00\03\19(\00\04\17\0C$\00u\06\00 \00\00\F0!\10\00u\05\00\1C\00\00\F0\11\10\009\04\00\18\10\009\03\00\14\10\009\02\00\10\10\009\01\00\08P\00\01\01\00\F2\0A\F0\11\00\03\1B\FF\00\04\1C\0C\00P\00\00\000,\00\000-\00\00\04\1E\84\01#K\00\01\00\B1\02\02\08\10\0A/\22\00\00\00\08\10\00#\00\08\08\00\13\10\08\00\13\18\08\00\13 \08\00\13(\08\00\130\08\00\228\08\84\00\13\00\08\00\13\08\08\00\13\10\08\00\13\18\08\00\13 \08\00\13(\08\00\130\08\00\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\A2\00\00\00\14,\00\00\00H\00\01\00\11\02h\01\0F\01\00\FF\B8@$v\01\FF\87\04\B2\FF\00\8E\07\00\C4\0F\00\19y\00\01\00\10%\93\02a\0E\00\19y\03\00\01\00\10!-\00\F0\04\0E\00$z\00\00\00]\00\00\03\02\8E\07\00\CA\1F\00\0C\10\00\C5^\00\00p`\F0\03\00\DA\0F\00M\A3\04\F0\0C\80\03\00\EA\0F\00\06{\04\00\00_\00\00\00\90 \00\00\22\0E\00\19x\05\FF\1F\1F\00\F1\00\14\01\00\00\E2\0F\00\B9z\04\00\00F\00\00\0B\05 \E2\0FP\00\10\FF0\00@pP\F4\03\10\00cEy\00\00 *P\00p\E2\0F\00\11r\05\05\0E\00\B2\FF@\8F\07\00\C6\0F\00\08s\04\FE\04\10\10\A0\00\F1\06\1E\00\10x\02\04\FE\FF\FF\0F\FF\E0\FF\07\00\CC\1F\00\05s\03\B4\02\00\C8\03\A3\00d\00\00$r\02\FF\FF\00\F0\00a\E4\1F\00$r\07\10\00\C0\03\0A\8E\07\00\C8/\00$z\07\07\80\00\10\FF\E0\00\81\C8\0F\00'r\06\03\07\F8\02 \8E\07\80\00@\19x\03\FF\19\03\10\05\C0\00q\E4\0F\00\12x\05\05\09\05!\C0\8E\90\00T'r\06\06\03`\00a\C8\0F\00$r\04`\00#\06\0A\10\00Tz\02\04\00_@\01\01\F0\006\02\00_@\010\10\0A\02\10\00\11\80\C0\00\80\E4\0F\00\10\08\06\06\01P\00\03\10\00\060\00\11\F2\10\010$x\02$\04A\00\05\0A\8E \00\A2$t\05\FF\00\00\80\FF\FF\00\90\00@\12x\00\02p\01\22\FF\C0 \00@\19x\07\FFA\01@\02\16\01\00\80\00*\10\18`\001\12\AA\06\80\01\22\FF30\00\EF\0Cx\00\02\7F\00\00\00p@\F4\03\00\C6\D0\00\01\16\03\D0\00\00\10\00@x\03\03 p\01\12\02@\00\22$x\81\04\22\07\02\90\00h\0Cz\00\03\00X\C0\00\00\98\05\030\01\96\D8\0F\00G\19\00\00 (@\02E$x\04\06@\00\01`\004\04\04@0\00\02@\01\10\04M\07\C7pb\F8\03\00\E4\0F\04\10x\08\04\D0\00g\04\10x\0A\04\02\E0\00B\0Cz\00\080\00\A5\FA\03\00\CE\0F\00$\C4\07\FF\80\00\00\B0\00V\CA\06\04\00X\D0\00c%\C6\06\06\00Z\80\00\02\A0\02\12\0A@\00 \F0\03 \00@\81\C9\0C\06@\00\B6\00\19\1E\0C\00\A2\00\00$\D4\09P\00\84\E2\0F\00\10x\0E\04\03\80\00\00\F0\009\DA\08\08`\000\D6\08\08`\00\16\09`\00\12\0E`\00 \F6\03\E0\00E\81\D9\08\08`\00j\E2\02\00$\84\0B\B0\009\8A\0A\0AP\000\86\0A\0AP\00\10\0B\10\00u\CC\0F\00\81\89\0A\0A@\00X\22\0F\00$\B4\F0\00i\1F\00$\BA\06\0E@\00\17\B6\F0\00\00\80\00&\B9\0E\E0\00\88b\01\00\10x\10\04\04\D0\00\08p\02p\E2\0F\00\0B\C8\00\0C\10\00\B2\00b\FC\03\00\C8O\00\08\C8\09\10\00\B1\02\00\03\00\E4/\04\0B\C2\00\0CN\07\10\80 \00r\0F\00\08\C2\05\0C\09Y\06\13\03\A0\01\17\10\D0\01\F4\07\00\0B\D2\00\05\08\00\00@\00\C0\FC\03\00\E4\8F\08\10x\0C\04\05\80\00\82\E4\0F\00\08\D2\09\08\05@\00\00\10\003\04\0B\D2\E9\06\04`\005\D2\05\08`\00\1B\E2\F0\01\11\E2\C0\04\15\0C\10\02\11\E2\00\02\15\10\10\01\00\D0\00E\82\00\05\0A\80\00 \0F\09\00\01\17\06\00\01\0C \02I\08\82\09\0A\90\00D\82\00\0A\0A\90\00\10\C60\02\17\060\02i\0E\00\08\82\05\0A\A0\00\0B@\02\06\10\01#\F0\03@\02\19\0C\A0\00F\B2\00\05\0E\A0\00\10\0A \01\18\07\A0\00\0B`\02I\08\B2\0B\0E\A0\00H\B2\00\0E\0E\A0\00\09p\02x\0E\00\08\B2\05\0E\0B\A0\00\08\80\02'\E2\0F@\01\12\F6 \00?\8A\0A\10\90\02\16\10$\90\02\16\0DP\00\00\D0\02:\BA\0C\0C\90\02 \0C\0C@\00\14\0D@\005\B9\0C\0C@\00\12b`\03\14\08\F0\00\01\F0\036\10\04\09\10\00d\00\0B\C2\00\05\06 \01\00\90\029\C2\07\06\00\01D\C2\00\06\06\00\01\02\90\02$\06\07\F0\00\18\E4\90\03\0F\90\02\00o\C8\8F\00\08\D2\07\80\02\0E\1F\07\80\02\06\08\E0\01\05\80\02\06\90\03\0F\80\02\024\0E\04\0A\E0\00\0F\80\02S\1B\0E\80\02\1C\10\80\02\19\0C\80\02?\10\04\0B\80\02\0B\1B\0C\80\02/\0C\0C\80\02\0B\1F\0C\80\02\0C\1B\10\80\02\1F\0E\80\02,\1F\10\80\02\1D\14\0C\F0\00\03\80\02\1F\0D\80\02\CC\14\0E\E0\00\0F\80\02\84\1F\0F\80\02\DC\14\10\F0\00\03\80\02\1F\11\80\02\CC\14\12\E0\00\0F\80\02\84\1F\13\80\02\DC\14\14\F0\00\03\80\02\1F\15\80\02\CC\14\16\E0\00\0F\80\02\84\1F\17\80\02\DC\14\18\F0\00\03\80\02\1F\19\80\02\CC\14\1A\E0\00\0F\80\02\84\1F\1B\80\02\DC\14\1C\F0\00\03\80\02\1F\1D\80\02\CC\14\1E\E0\00\0F\80\02\84\1F\1F\80\02\DC\14 \F0\00\03\80\02\1F!\80\02\CC\14\22\E0\00\0F\80\02\84\1F#\80\02\DC\14$\F0\00\03\80\02\1F%\80\02\CC\14&\E0\00\0F\80\02\84\1F'\80\02\DC\14(\F0\00\03\80\02\1F)\80\02\CC\14*\E0\00\0F\80\02\84\1F+\80\02\DC\14,\F0\00\03\80\02\1F-\80\02\CC\14.\E0\00\0F\80\02\84\1F/\80\02\DC\140\F0\00\03\80\02\1F1\80\02\CC\142\E0\00\0F\80\02\84\1F3\80\02\DC\144\F0\00\03\80\02\1F5\80\02\CC\146\E0\00\0F\80\02\84\1F7\80\02\DC\148\F0\00\03\80\02\1F9\80\02\CC\14:\E0\00\0F\80\02\84\1F;\80\02\DC\14<\F0\00\03\80\02\19=\10\009\12\04>\10\00?\04\04?\A0\02\C5?\C6\0F\01\90\02\1AW\E4\0F\04\81\C9`$/\A4\00\90\02\11\1F\12\90\02\16?\C6\0F\02\80\02+\1F\02\80\02\00\0D@$\1A\04\80\02*\06\12\80\02\07\10\01\01\80\02\07\80\03W$\01\00$\B4\D0\02\01\80\02\1A\0A\80&\1B\B6\C0\028\B9\0A\0A\80\02\00@\02\19\0E@\02\17\09\E0#\01p%\06\E0#\02@\02\15\0E`\01\1E\C8`%\06 \02\01P\02\19\04@\02(\C8\1F0\02\11\02@\00\14\82\C0\02\10\FA0\004\09\0B\82\B0\02\03@\00\14\82\D0\02 \80\060\00I\08\82\05\06@\00\14\B2@\02\010\004\0A\0B\B2 \02\12\F0@\00$\B2\07@\02\02@\003\B2\05\0A@\00\10\00\B0*\14A\B0*\030( \88s\F8-\12\00u(f\E8\0F\00\1D{\00\01\00b\EC\0F\00\84\A9\07,*\00 \00Q\22\0E\00\1Cx\1A\00P\00p\F0\F0\03P\00\13\0C0)1pD\F2\90*!\84\A9n\01\020\00qb\0E\00\0B\A2\00\07\80\00\A1\80\F6\03\00\E4\1F\08\0B\A2\00p%\02\C0\00T/\00\1C\A8\00P\00\14\01@)\11?@)@\F6\03\00\D6\00\01#\07\07R\00\10\06 \00\09\80\00\8F\C6\0F\00\88\A3\00\00\07\C0\00\09\22\B9\05\1C\00\12\08\A0++\84\B9\A0\00\22\B2\00!+`\80\F4\03\00\C4\1F\10\00(\04\05\A0\00#\B8\00p\00Ap\01\00\DA\90\01\15\05\90\00_\CA\0F\00\88\B3@\01\0B9M\19\00p\017$t\02\10\03R\0F\00\84y\05\D1)\00@\00\90&\0E\00%v\02\03\00`\0F/\04 \00\00\1D\00\03`\00vh\0E\00\81y\06\02\D0\02D\05\00\0Br\C0\00\12\F0\C0\00\14r\C0\00W\F2\03\00\D6/\B0\00 \80\04\F0\022\0Br\00`\02\11@\10\04(\0EF\10\02S\E6\0F\00\08r\80\02\00\01\00p\CC\0F\00\A9s\07\02\A0\02\C0\07\E1\1E\00\00\A4\0E\00\0Cr\00\07\10\00 pR@\00QO\00$r\06\E0*\14\07\90*@\09\00\00\90\E0+!\FF\83\F0\00\1BMp\02TGy\00\00\F0 \00f\C0\0F\00\18y\00\01\00\0F\10\00\90\0F\01\00-\11\01H1\0E\01\00\22@\00\01\00=\98\01\000\00\08\01\00\1F\0B@\00\04\13\D8)\00?\01\02\00@\00\0A\13\13\0B\04\0C\01\00\13\E0U\00\03\8F\03\01/\04\13\05w\02\00\01\00\22\18\00\01\00.k\01T\00\00\01\00#\88\04(\03\1F\00\80\00\0B/)\00'\00\02#\00\F8@\00\04 3\04\E4\00*\04\00\01\00\1Fb@\00\04*(\05\C0\00\13\03#/\0C@\00!\89\01D\01\0D@\00\13\D0@\00*\D8\00\01\00\1B\08\08\00?x\01\00V4\000\00\00\A8\15\03\03W/\04\80\00\17\048\00\04\18\00\133@\01\0C\84\01\13\B8@\00\17\881\01\0F\C0\00\01\132T\01\15\06R\00\03^\03\1A\08\A04\11\03$\00J\00\15\80\00\01\00\13\95\94\00*\03\00\01\00\0409/\00\04\80\00\0B\13\06\18\02\04h9\0D\08\01\1A\00\08\00\04\97\00\13\018\00\04\E8\00\0C\01\009\C8/\00\08\00\088\00\18\06\A0\00\0F\01\00\05\03\A9\00\80\08\00\00\00\00\00\00\00\00\00\00\00\00") {addr_space = 0 : i32}
  llvm.mlir.global internal constant @ral_kernel_launch___gpu___pvoid_ppvoid_i64_pvoid_i64_i64_i64_i64_i64_i64_i32_pvoid_i32_ppvoid___void("ral_kernel_launch___gpu___pvoid_ppvoid_i64_pvoid_i64_i64_i64_i64_i64_i64_i32_pvoid_i32_ppvoid___void\00") {addr_space = 0 : i32}
  llvm.mlir.global internal constant @main_kernel_1_main_kColReduction_reduce__4_1_0___thin_kernel_name("main_kColReduction_reduce__4_1_0___thin\00") {addr_space = 0 : i32}
  llvm.mlir.global internal constant @main_kernel_1_blob_gpu.binary("P\EDU\BA\01\00\10\008\04\00\00\00\00\00\00\02\00\01\01@\00\00\00\F8\03\00\00\00\00\00\00\F7\03\00\00\00\00\00\00\07\00\01\00P\00\00\00\00\00\00\00\00\00\00\00\11 \00\00\00\00\00\00\00\00\00\00\00\00\00\00\E8\0B\00\00\00\00\00\00\A2\7FELF\02\01\013\07\00\01\00f\02\00\BE\00q\00\01\00!@\0B\07\001\00\80\08\07\00\F5\0E\00P\05P\00@\008\00\03\00@\00\0B\00\01\00\00.shstrtab\00.\08\00'ym\08\00\F0\1B_shndx\00.nv.info\00.text.main_kColReduction_r\0A\00\F5\00e__4_1_0___thin7\00\0F1\00\1Aoshared3\00\1A\9Fconstant06\00\17\FA\01debug_frame\00.rel\11\00!nv\14\00\11aB\00\0F'\01 \0F\87\00\14\0FO\01\B6o_paramV\01\1C\0F\01\00\07\8CZ\00\00\00\03\00\0A\00\01\00\11\EC\18\00,\09\00\01\00 )\01\18\00,\04\00\01\00\11G\18\00,\07\00\01\00f2\00\00\00\12\10`\00!\80\01\07\00v\00\FF\FF\FF\FF(\00\0C\00\00\01\00@\03\00\04|\08\00\D1\0F\0C\81\80\80(\00\08\FF\81\80(\08\0B\00\06(\00/0\00\01\00\03\13pX\00\10\04\9B\00R\04\14\00\00\00E\00\01\0B\00\00\13\00p/\08\00\05\00\00\00\9F\03\22\04#\0C\00\00\01\00'\04\12\0C\00\17\11\0C\00!7\04\E0\03\F3\08\015\00\00\04\0A\08\00\02\00\00\00`\01\10\00\03\19\10\00\04\17\0C\04\04U\08\00\00\F0!\10\00\10\01\18\01%\F0\11\10\00\01\01\00\F2\02\F0\11\00\03\1B\FF\00\04\1C\08\00P\00\00\00\B0\00\01\00#K\00\01\00s\02\02\08\10\0A/\22\9B\00\00\07\00\03\08\00\13\10\08\00\13\18\08\00\13 \08\00\13(\08\00\130\08\00\228\08|\00\13\00\08\00\13\08\08\00\13\10\08\00\13\18\08\00\13 \08\00\13(\08\00\130\08\00\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\A2\00\00\00\14,\00\00\00H\00\01\00\000\01/\05\00\01\00\FF\B8A\02z\01\00'\04\B1\0F\00\00\00\C4\0F\00\19y\02\00\01\00\10%\93\02Q\0E\00\19y\03\0F\00\F5\1A\00!\00\00\00$\0E\00$z\02\02\00X\00\00\03\02\8E\07\00\CA\1F\00\0Cz\00\02\00Y\00\00p`\F0\03\00\DA\0F\00M[\04\A0\80\03\00\EA\0F\005t\03\FF\BB\03\10\FF\C8\03P\E2\0F\00\02x>\02B\80\FF\00\0F\10\00r\B9z\04\00\00F\00\84\00\94\D0\0F\00%v\02\02\00Z`\00`\0F\00\86y\00\022\00@\04\19\10\0C0\009My\00`\00PGy\00\00\F0A\04\A6\FF\83\03\00\C0\0F\00\18y\00\01\00\0F\10\00\90\0F\01\00-\00W\01.\03\00\01\00\22@\00\01\00='\01\000\00\08\01\00\1F\0B@\00\04\13g)\00\1FV@\00\0C\13\13\14\04\0C\01\00\13\C0\15\00&\90\008\04#\04\00\8D\04\00\FE\04\12\00\01\00\1F\FAT\00\00\00\01\00\13P\95\00/p\00\80\00\0B\1F)'\00\03#\00\C0@\00\04X\06\04\E4\00*\04\00\01\00\1F`@\00\04\13\F01\00&\\\00@\00\1F\0A@\00\00!\18\01D\01\0D@\00\13P)\00*\D8\00\01\00\1B\08\08\00?\07\01\00\8E\07\00Q\00\00(\05\00\01\00&\10\00\80\00\17\048\00\04\18\00\13\C4\14\01\0C\84\01*8\05@\07\1F\00\C0\00\04\132@\00+\06\00\01\00\1A\07\D8\07\12\03\F8\05:\08\80\00\01\00\13\06\10\06\04(\0B\0C\01\00*\A8\00\08\00\04\F8\00\13\018\00\04\A8\00\0C\01\009H\03\00\08\00\088\00/\06\00\01\00\17\80\08\00\00\00\00\00\00\00\00") {addr_space = 0 : i32}
  llvm.mlir.global internal constant @alloc___gpu___pvoid_i64___pvoid("alloc___gpu___pvoid_i64___pvoid\00") {addr_space = 0 : i32}
  llvm.mlir.global internal constant @ral_recv_input___cpu___pvoid_i64___m3df32("ral_recv_input___cpu___pvoid_i64___m3df32\00") {addr_space = 0 : i32}
  llvm.func @disc_ral_call(!llvm.ptr<i8>, !llvm.ptr<i8>, !llvm.ptr<ptr<i8>>)
  llvm.func @main(%arg0: !llvm.ptr<i8>) attributes {tf.entry_function = {input_placements = "gpu", inputs = "input0", output_placements = "gpu", outputs = "output0"}} {
    %0 = llvm.mlir.constant(256 : index) : i64
    %1 = llvm.mlir.constant(32 : index) : i64
    %2 = llvm.mlir.constant(512 : index) : i64
    %3 = llvm.mlir.constant(0 : i32) : i32
    %4 = llvm.mlir.constant(0 : index) : i64
    %5 = llvm.mlir.constant(1 : index) : i64
    %6 = llvm.mlir.constant(2 : index) : i64
    %7 = llvm.mlir.constant(0 : i32) : i32
    %8 = llvm.mlir.constant(1 : i32) : i32
    %9 = llvm.alloca %8 x !llvm.struct<"", (ptr<i8>, i64, struct<(ptr<f32>, ptr<f32>, i64, array<3 x i64>, array<3 x i64>)>)> : (i32) -> !llvm.ptr<struct<"", (ptr<i8>, i64, struct<(ptr<f32>, ptr<f32>, i64, array<3 x i64>, array<3 x i64>)>)>>
    %10 = llvm.mlir.constant(3 : i32) : i32
    %11 = llvm.alloca %10 x !llvm.ptr<i8> : (i32) -> !llvm.ptr<ptr<i8>>
    %12 = llvm.mlir.constant(0 : i32) : i32
    %13 = llvm.getelementptr %9[%7, 0] : (!llvm.ptr<struct<"", (ptr<i8>, i64, struct<(ptr<f32>, ptr<f32>, i64, array<3 x i64>, array<3 x i64>)>)>>, i32) -> !llvm.ptr<ptr<i8>>
    llvm.store %arg0, %13 : !llvm.ptr<ptr<i8>>
    %14 = llvm.getelementptr %11[%12] : (!llvm.ptr<ptr<i8>>, i32) -> !llvm.ptr<ptr<i8>>
    %15 = llvm.bitcast %13 : !llvm.ptr<ptr<i8>> to !llvm.ptr<i8>
    llvm.store %15, %14 : !llvm.ptr<ptr<i8>>
    %16 = llvm.mlir.constant(1 : i32) : i32
    %17 = llvm.getelementptr %9[%7, 1] : (!llvm.ptr<struct<"", (ptr<i8>, i64, struct<(ptr<f32>, ptr<f32>, i64, array<3 x i64>, array<3 x i64>)>)>>, i32) -> !llvm.ptr<i64>
    llvm.store %4, %17 : !llvm.ptr<i64>
    %18 = llvm.getelementptr %11[%16] : (!llvm.ptr<ptr<i8>>, i32) -> !llvm.ptr<ptr<i8>>
    %19 = llvm.bitcast %17 : !llvm.ptr<i64> to !llvm.ptr<i8>
    llvm.store %19, %18 : !llvm.ptr<ptr<i8>>
    %20 = llvm.mlir.constant(2 : i32) : i32
    %21 = llvm.getelementptr %9[%7, 2] : (!llvm.ptr<struct<"", (ptr<i8>, i64, struct<(ptr<f32>, ptr<f32>, i64, array<3 x i64>, array<3 x i64>)>)>>, i32) -> !llvm.ptr<struct<(ptr<f32>, ptr<f32>, i64, array<3 x i64>, array<3 x i64>)>>
    %22 = llvm.getelementptr %11[%20] : (!llvm.ptr<ptr<i8>>, i32) -> !llvm.ptr<ptr<i8>>
    %23 = llvm.bitcast %21 : !llvm.ptr<struct<(ptr<f32>, ptr<f32>, i64, array<3 x i64>, array<3 x i64>)>> to !llvm.ptr<i8>
    llvm.store %23, %22 : !llvm.ptr<ptr<i8>>
    %24 = llvm.mlir.addressof @ral_recv_input___cpu___pvoid_i64___m3df32 : !llvm.ptr<array<42 x i8>>
    %25 = llvm.mlir.constant(0 : index) : i64
    %26 = llvm.getelementptr %24[%25, %25] : (!llvm.ptr<array<42 x i8>>, i64, i64) -> !llvm.ptr<i8>
    llvm.call @disc_ral_call(%arg0, %26, %11) : (!llvm.ptr<i8>, !llvm.ptr<i8>, !llvm.ptr<ptr<i8>>) -> ()
    %27 = llvm.load %21 : !llvm.ptr<struct<(ptr<f32>, ptr<f32>, i64, array<3 x i64>, array<3 x i64>)>>
    %28 = llvm.extractvalue %27[3, 2] : !llvm.struct<(ptr<f32>, ptr<f32>, i64, array<3 x i64>, array<3 x i64>)> 
    %29 = llvm.extractvalue %27[3, 1] : !llvm.struct<(ptr<f32>, ptr<f32>, i64, array<3 x i64>, array<3 x i64>)> 
    %30 = llvm.extractvalue %27[3, 0] : !llvm.struct<(ptr<f32>, ptr<f32>, i64, array<3 x i64>, array<3 x i64>)> 
    %31 = llvm.trunc %29 : i64 to i32
    %32 = llvm.trunc %28 : i64 to i32
    %33 = llvm.mul %31, %32  : i32
    %34 = llvm.sext %33 : i32 to i64
    %35 = llvm.mlir.constant(1 : index) : i64
    %36 = llvm.mlir.null : !llvm.ptr<f32>
    %37 = llvm.getelementptr %36[%34] : (!llvm.ptr<f32>, i64) -> !llvm.ptr<f32>
    %38 = llvm.ptrtoint %37 : !llvm.ptr<f32> to i64
    %39 = llvm.mlir.constant(0 : i32) : i32
    %40 = llvm.mlir.constant(1 : i32) : i32
    %41 = llvm.alloca %40 x !llvm.struct<".1", (ptr<i8>, i64, ptr<i8>)> : (i32) -> !llvm.ptr<struct<".1", (ptr<i8>, i64, ptr<i8>)>>
    %42 = llvm.mlir.constant(3 : i32) : i32
    %43 = llvm.alloca %42 x !llvm.ptr<i8> : (i32) -> !llvm.ptr<ptr<i8>>
    %44 = llvm.mlir.constant(0 : i32) : i32
    %45 = llvm.getelementptr %41[%39, 0] : (!llvm.ptr<struct<".1", (ptr<i8>, i64, ptr<i8>)>>, i32) -> !llvm.ptr<ptr<i8>>
    llvm.store %arg0, %45 : !llvm.ptr<ptr<i8>>
    %46 = llvm.getelementptr %43[%44] : (!llvm.ptr<ptr<i8>>, i32) -> !llvm.ptr<ptr<i8>>
    %47 = llvm.bitcast %45 : !llvm.ptr<ptr<i8>> to !llvm.ptr<i8>
    llvm.store %47, %46 : !llvm.ptr<ptr<i8>>
    %48 = llvm.mlir.constant(1 : i32) : i32
    %49 = llvm.getelementptr %41[%39, 1] : (!llvm.ptr<struct<".1", (ptr<i8>, i64, ptr<i8>)>>, i32) -> !llvm.ptr<i64>
    llvm.store %38, %49 : !llvm.ptr<i64>
    %50 = llvm.getelementptr %43[%48] : (!llvm.ptr<ptr<i8>>, i32) -> !llvm.ptr<ptr<i8>>
    %51 = llvm.bitcast %49 : !llvm.ptr<i64> to !llvm.ptr<i8>
    llvm.store %51, %50 : !llvm.ptr<ptr<i8>>
    %52 = llvm.mlir.constant(2 : i32) : i32
    %53 = llvm.getelementptr %41[%39, 2] : (!llvm.ptr<struct<".1", (ptr<i8>, i64, ptr<i8>)>>, i32) -> !llvm.ptr<ptr<i8>>
    %54 = llvm.getelementptr %43[%52] : (!llvm.ptr<ptr<i8>>, i32) -> !llvm.ptr<ptr<i8>>
    %55 = llvm.bitcast %53 : !llvm.ptr<ptr<i8>> to !llvm.ptr<i8>
    llvm.store %55, %54 : !llvm.ptr<ptr<i8>>
    %56 = llvm.mlir.addressof @alloc___gpu___pvoid_i64___pvoid : !llvm.ptr<array<32 x i8>>
    %57 = llvm.mlir.constant(0 : index) : i64
    %58 = llvm.getelementptr %56[%57, %57] : (!llvm.ptr<array<32 x i8>>, i64, i64) -> !llvm.ptr<i8>
    llvm.call @disc_ral_call(%arg0, %58, %43) : (!llvm.ptr<i8>, !llvm.ptr<i8>, !llvm.ptr<ptr<i8>>) -> ()
    %59 = llvm.load %53 : !llvm.ptr<ptr<i8>>
    %60 = llvm.mlir.undef : !llvm.struct<(ptr<f32>, ptr<f32>, i64, array<1 x i64>, array<1 x i64>)>
    %61 = llvm.bitcast %59 : !llvm.ptr<i8> to !llvm.ptr<f32>
    %62 = llvm.insertvalue %61, %60[0] : !llvm.struct<(ptr<f32>, ptr<f32>, i64, array<1 x i64>, array<1 x i64>)> 
    %63 = llvm.insertvalue %61, %62[1] : !llvm.struct<(ptr<f32>, ptr<f32>, i64, array<1 x i64>, array<1 x i64>)> 
    %64 = llvm.mlir.constant(0 : index) : i64
    %65 = llvm.insertvalue %64, %63[2] : !llvm.struct<(ptr<f32>, ptr<f32>, i64, array<1 x i64>, array<1 x i64>)> 
    %66 = llvm.mlir.constant(1 : index) : i64
    %67 = llvm.insertvalue %34, %65[3, 0] : !llvm.struct<(ptr<f32>, ptr<f32>, i64, array<1 x i64>, array<1 x i64>)> 
    %68 = llvm.insertvalue %66, %67[4, 0] : !llvm.struct<(ptr<f32>, ptr<f32>, i64, array<1 x i64>, array<1 x i64>)> 
    %69 = llvm.icmp "slt" %30, %34 : i64
    llvm.cond_br %69, ^bb1, ^bb2
  ^bb1:  // pred: ^bb0
    %70 = llvm.icmp "sle" %34, %4 : i64
    %71 = llvm.sub %4, %34  : i64
    %72 = llvm.sub %34, %5  : i64
    %73 = llvm.select %70, %71, %72 : i1, i64
    %74 = llvm.sdiv %73, %2  : i64
    %75 = llvm.sub %4, %74  : i64
    %76 = llvm.add %74, %5  : i64
    %77 = llvm.select %70, %75, %76 : i1, i64
    %78 = llvm.mlir.addressof @main_kernel_blob_gpu.binary : !llvm.ptr<array<1096 x i8>>
    %79 = llvm.mlir.constant(0 : index) : i64
    %80 = llvm.getelementptr %78[%79, %79] : (!llvm.ptr<array<1096 x i8>>, i64, i64) -> !llvm.ptr<i8>
    %81 = llvm.mlir.constant(1 : i32) : i32
    %82 = llvm.alloca %81 x !llvm.ptr<i8> : (i32) -> !llvm.ptr<ptr<i8>>
    %83 = llvm.mlir.constant(0 : i32) : i32
    %84 = llvm.getelementptr %82[%83] : (!llvm.ptr<ptr<i8>>, i32) -> !llvm.ptr<ptr<i8>>
    llvm.store %80, %84 : !llvm.ptr<ptr<i8>>
    %85 = llvm.mlir.constant(1 : i64) : i64
    %86 = llvm.mlir.addressof @main_kernel_main_kColReduction_reduce__4_1_0___flat_kernel_name : !llvm.ptr<array<40 x i8>>
    %87 = llvm.mlir.constant(0 : index) : i64
    %88 = llvm.getelementptr %86[%87, %87] : (!llvm.ptr<array<40 x i8>>, i64, i64) -> !llvm.ptr<i8>
    %89 = llvm.extractvalue %68[0] : !llvm.struct<(ptr<f32>, ptr<f32>, i64, array<1 x i64>, array<1 x i64>)> 
    %90 = llvm.extractvalue %68[1] : !llvm.struct<(ptr<f32>, ptr<f32>, i64, array<1 x i64>, array<1 x i64>)> 
    %91 = llvm.extractvalue %68[2] : !llvm.struct<(ptr<f32>, ptr<f32>, i64, array<1 x i64>, array<1 x i64>)> 
    %92 = llvm.extractvalue %68[3, 0] : !llvm.struct<(ptr<f32>, ptr<f32>, i64, array<1 x i64>, array<1 x i64>)> 
    %93 = llvm.extractvalue %68[4, 0] : !llvm.struct<(ptr<f32>, ptr<f32>, i64, array<1 x i64>, array<1 x i64>)> 
    %94 = llvm.mlir.constant(1 : i32) : i32
    %95 = llvm.alloca %94 x !llvm.struct<".9", (i64, i64, ptr<f32>)> : (i32) -> !llvm.ptr<struct<".9", (i64, i64, ptr<f32>)>>
    %96 = llvm.mlir.constant(3 : i32) : i32
    %97 = llvm.alloca %96 x !llvm.ptr<i8> : (i32) -> !llvm.ptr<ptr<i8>>
    %98 = llvm.mlir.constant(0 : i32) : i32
    %99 = llvm.mlir.constant(0 : i32) : i32
    %100 = llvm.getelementptr %95[%98, 0] : (!llvm.ptr<struct<".9", (i64, i64, ptr<f32>)>>, i32) -> !llvm.ptr<i64>
    llvm.store %2, %100 : !llvm.ptr<i64>
    %101 = llvm.getelementptr %97[%99] : (!llvm.ptr<ptr<i8>>, i32) -> !llvm.ptr<ptr<i8>>
    %102 = llvm.bitcast %100 : !llvm.ptr<i64> to !llvm.ptr<i8>
    llvm.store %102, %101 : !llvm.ptr<ptr<i8>>
    %103 = llvm.mlir.constant(1 : i32) : i32
    %104 = llvm.getelementptr %95[%98, 1] : (!llvm.ptr<struct<".9", (i64, i64, ptr<f32>)>>, i32) -> !llvm.ptr<i64>
    llvm.store %34, %104 : !llvm.ptr<i64>
    %105 = llvm.getelementptr %97[%103] : (!llvm.ptr<ptr<i8>>, i32) -> !llvm.ptr<ptr<i8>>
    %106 = llvm.bitcast %104 : !llvm.ptr<i64> to !llvm.ptr<i8>
    llvm.store %106, %105 : !llvm.ptr<ptr<i8>>
    %107 = llvm.mlir.constant(2 : i32) : i32
    %108 = llvm.getelementptr %95[%98, 2] : (!llvm.ptr<struct<".9", (i64, i64, ptr<f32>)>>, i32) -> !llvm.ptr<ptr<f32>>
    llvm.store %90, %108 : !llvm.ptr<ptr<f32>>
    %109 = llvm.getelementptr %97[%107] : (!llvm.ptr<ptr<i8>>, i32) -> !llvm.ptr<ptr<i8>>
    %110 = llvm.bitcast %108 : !llvm.ptr<ptr<f32>> to !llvm.ptr<i8>
    llvm.store %110, %109 : !llvm.ptr<ptr<i8>>
    %111 = llvm.mlir.constant(0 : i32) : i32
    %112 = llvm.mlir.constant(3 : i32) : i32
    %113 = llvm.inttoptr %111 : i32 to !llvm.ptr<i8>
    %114 = llvm.mlir.constant(0 : i32) : i32
    %115 = llvm.mlir.constant(1 : i32) : i32
    %116 = llvm.alloca %115 x !llvm.struct<".10", (ptr<i8>, ptr<ptr<i8>>, i64, ptr<i8>, i64, i64, i64, i64, i64, i64, i32, ptr<i8>, i32, ptr<ptr<i8>>)> : (i32) -> !llvm.ptr<struct<".10", (ptr<i8>, ptr<ptr<i8>>, i64, ptr<i8>, i64, i64, i64, i64, i64, i64, i32, ptr<i8>, i32, ptr<ptr<i8>>)>>
    %117 = llvm.mlir.constant(14 : i32) : i32
    %118 = llvm.alloca %117 x !llvm.ptr<i8> : (i32) -> !llvm.ptr<ptr<i8>>
    %119 = llvm.mlir.constant(0 : i32) : i32
    %120 = llvm.getelementptr %116[%114, 0] : (!llvm.ptr<struct<".10", (ptr<i8>, ptr<ptr<i8>>, i64, ptr<i8>, i64, i64, i64, i64, i64, i64, i32, ptr<i8>, i32, ptr<ptr<i8>>)>>, i32) -> !llvm.ptr<ptr<i8>>
    llvm.store %arg0, %120 : !llvm.ptr<ptr<i8>>
    %121 = llvm.getelementptr %118[%119] : (!llvm.ptr<ptr<i8>>, i32) -> !llvm.ptr<ptr<i8>>
    %122 = llvm.bitcast %120 : !llvm.ptr<ptr<i8>> to !llvm.ptr<i8>
    llvm.store %122, %121 : !llvm.ptr<ptr<i8>>
    %123 = llvm.mlir.constant(1 : i32) : i32
    %124 = llvm.getelementptr %116[%114, 1] : (!llvm.ptr<struct<".10", (ptr<i8>, ptr<ptr<i8>>, i64, ptr<i8>, i64, i64, i64, i64, i64, i64, i32, ptr<i8>, i32, ptr<ptr<i8>>)>>, i32) -> !llvm.ptr<ptr<ptr<i8>>>
    llvm.store %82, %124 : !llvm.ptr<ptr<ptr<i8>>>
    %125 = llvm.getelementptr %118[%123] : (!llvm.ptr<ptr<i8>>, i32) -> !llvm.ptr<ptr<i8>>
    %126 = llvm.bitcast %124 : !llvm.ptr<ptr<ptr<i8>>> to !llvm.ptr<i8>
    llvm.store %126, %125 : !llvm.ptr<ptr<i8>>
    %127 = llvm.mlir.constant(2 : i32) : i32
    %128 = llvm.getelementptr %116[%114, 2] : (!llvm.ptr<struct<".10", (ptr<i8>, ptr<ptr<i8>>, i64, ptr<i8>, i64, i64, i64, i64, i64, i64, i32, ptr<i8>, i32, ptr<ptr<i8>>)>>, i32) -> !llvm.ptr<i64>
    llvm.store %85, %128 : !llvm.ptr<i64>
    %129 = llvm.getelementptr %118[%127] : (!llvm.ptr<ptr<i8>>, i32) -> !llvm.ptr<ptr<i8>>
    %130 = llvm.bitcast %128 : !llvm.ptr<i64> to !llvm.ptr<i8>
    llvm.store %130, %129 : !llvm.ptr<ptr<i8>>
    %131 = llvm.mlir.constant(3 : i32) : i32
    %132 = llvm.getelementptr %116[%114, 3] : (!llvm.ptr<struct<".10", (ptr<i8>, ptr<ptr<i8>>, i64, ptr<i8>, i64, i64, i64, i64, i64, i64, i32, ptr<i8>, i32, ptr<ptr<i8>>)>>, i32) -> !llvm.ptr<ptr<i8>>
    llvm.store %88, %132 : !llvm.ptr<ptr<i8>>
    %133 = llvm.getelementptr %118[%131] : (!llvm.ptr<ptr<i8>>, i32) -> !llvm.ptr<ptr<i8>>
    %134 = llvm.bitcast %132 : !llvm.ptr<ptr<i8>> to !llvm.ptr<i8>
    llvm.store %134, %133 : !llvm.ptr<ptr<i8>>
    %135 = llvm.mlir.constant(4 : i32) : i32
    %136 = llvm.getelementptr %116[%114, 4] : (!llvm.ptr<struct<".10", (ptr<i8>, ptr<ptr<i8>>, i64, ptr<i8>, i64, i64, i64, i64, i64, i64, i32, ptr<i8>, i32, ptr<ptr<i8>>)>>, i32) -> !llvm.ptr<i64>
    llvm.store %77, %136 : !llvm.ptr<i64>
    %137 = llvm.getelementptr %118[%135] : (!llvm.ptr<ptr<i8>>, i32) -> !llvm.ptr<ptr<i8>>
    %138 = llvm.bitcast %136 : !llvm.ptr<i64> to !llvm.ptr<i8>
    llvm.store %138, %137 : !llvm.ptr<ptr<i8>>
    %139 = llvm.mlir.constant(5 : i32) : i32
    %140 = llvm.getelementptr %116[%114, 5] : (!llvm.ptr<struct<".10", (ptr<i8>, ptr<ptr<i8>>, i64, ptr<i8>, i64, i64, i64, i64, i64, i64, i32, ptr<i8>, i32, ptr<ptr<i8>>)>>, i32) -> !llvm.ptr<i64>
    llvm.store %5, %140 : !llvm.ptr<i64>
    %141 = llvm.getelementptr %118[%139] : (!llvm.ptr<ptr<i8>>, i32) -> !llvm.ptr<ptr<i8>>
    %142 = llvm.bitcast %140 : !llvm.ptr<i64> to !llvm.ptr<i8>
    llvm.store %142, %141 : !llvm.ptr<ptr<i8>>
    %143 = llvm.mlir.constant(6 : i32) : i32
    %144 = llvm.getelementptr %116[%114, 6] : (!llvm.ptr<struct<".10", (ptr<i8>, ptr<ptr<i8>>, i64, ptr<i8>, i64, i64, i64, i64, i64, i64, i32, ptr<i8>, i32, ptr<ptr<i8>>)>>, i32) -> !llvm.ptr<i64>
    llvm.store %5, %144 : !llvm.ptr<i64>
    %145 = llvm.getelementptr %118[%143] : (!llvm.ptr<ptr<i8>>, i32) -> !llvm.ptr<ptr<i8>>
    %146 = llvm.bitcast %144 : !llvm.ptr<i64> to !llvm.ptr<i8>
    llvm.store %146, %145 : !llvm.ptr<ptr<i8>>
    %147 = llvm.mlir.constant(7 : i32) : i32
    %148 = llvm.getelementptr %116[%114, 7] : (!llvm.ptr<struct<".10", (ptr<i8>, ptr<ptr<i8>>, i64, ptr<i8>, i64, i64, i64, i64, i64, i64, i32, ptr<i8>, i32, ptr<ptr<i8>>)>>, i32) -> !llvm.ptr<i64>
    llvm.store %2, %148 : !llvm.ptr<i64>
    %149 = llvm.getelementptr %118[%147] : (!llvm.ptr<ptr<i8>>, i32) -> !llvm.ptr<ptr<i8>>
    %150 = llvm.bitcast %148 : !llvm.ptr<i64> to !llvm.ptr<i8>
    llvm.store %150, %149 : !llvm.ptr<ptr<i8>>
    %151 = llvm.mlir.constant(8 : i32) : i32
    %152 = llvm.getelementptr %116[%114, 8] : (!llvm.ptr<struct<".10", (ptr<i8>, ptr<ptr<i8>>, i64, ptr<i8>, i64, i64, i64, i64, i64, i64, i32, ptr<i8>, i32, ptr<ptr<i8>>)>>, i32) -> !llvm.ptr<i64>
    llvm.store %5, %152 : !llvm.ptr<i64>
    %153 = llvm.getelementptr %118[%151] : (!llvm.ptr<ptr<i8>>, i32) -> !llvm.ptr<ptr<i8>>
    %154 = llvm.bitcast %152 : !llvm.ptr<i64> to !llvm.ptr<i8>
    llvm.store %154, %153 : !llvm.ptr<ptr<i8>>
    %155 = llvm.mlir.constant(9 : i32) : i32
    %156 = llvm.getelementptr %116[%114, 9] : (!llvm.ptr<struct<".10", (ptr<i8>, ptr<ptr<i8>>, i64, ptr<i8>, i64, i64, i64, i64, i64, i64, i32, ptr<i8>, i32, ptr<ptr<i8>>)>>, i32) -> !llvm.ptr<i64>
    llvm.store %5, %156 : !llvm.ptr<i64>
    %157 = llvm.getelementptr %118[%155] : (!llvm.ptr<ptr<i8>>, i32) -> !llvm.ptr<ptr<i8>>
    %158 = llvm.bitcast %156 : !llvm.ptr<i64> to !llvm.ptr<i8>
    llvm.store %158, %157 : !llvm.ptr<ptr<i8>>
    %159 = llvm.mlir.constant(10 : i32) : i32
    %160 = llvm.getelementptr %116[%114, 10] : (!llvm.ptr<struct<".10", (ptr<i8>, ptr<ptr<i8>>, i64, ptr<i8>, i64, i64, i64, i64, i64, i64, i32, ptr<i8>, i32, ptr<ptr<i8>>)>>, i32) -> !llvm.ptr<i32>
    llvm.store %111, %160 : !llvm.ptr<i32>
    %161 = llvm.getelementptr %118[%159] : (!llvm.ptr<ptr<i8>>, i32) -> !llvm.ptr<ptr<i8>>
    %162 = llvm.bitcast %160 : !llvm.ptr<i32> to !llvm.ptr<i8>
    llvm.store %162, %161 : !llvm.ptr<ptr<i8>>
    %163 = llvm.mlir.constant(11 : i32) : i32
    %164 = llvm.getelementptr %116[%114, 11] : (!llvm.ptr<struct<".10", (ptr<i8>, ptr<ptr<i8>>, i64, ptr<i8>, i64, i64, i64, i64, i64, i64, i32, ptr<i8>, i32, ptr<ptr<i8>>)>>, i32) -> !llvm.ptr<ptr<i8>>
    llvm.store %113, %164 : !llvm.ptr<ptr<i8>>
    %165 = llvm.getelementptr %118[%163] : (!llvm.ptr<ptr<i8>>, i32) -> !llvm.ptr<ptr<i8>>
    %166 = llvm.bitcast %164 : !llvm.ptr<ptr<i8>> to !llvm.ptr<i8>
    llvm.store %166, %165 : !llvm.ptr<ptr<i8>>
    %167 = llvm.mlir.constant(12 : i32) : i32
    %168 = llvm.getelementptr %116[%114, 12] : (!llvm.ptr<struct<".10", (ptr<i8>, ptr<ptr<i8>>, i64, ptr<i8>, i64, i64, i64, i64, i64, i64, i32, ptr<i8>, i32, ptr<ptr<i8>>)>>, i32) -> !llvm.ptr<i32>
    llvm.store %112, %168 : !llvm.ptr<i32>
    %169 = llvm.getelementptr %118[%167] : (!llvm.ptr<ptr<i8>>, i32) -> !llvm.ptr<ptr<i8>>
    %170 = llvm.bitcast %168 : !llvm.ptr<i32> to !llvm.ptr<i8>
    llvm.store %170, %169 : !llvm.ptr<ptr<i8>>
    %171 = llvm.mlir.constant(13 : i32) : i32
    %172 = llvm.getelementptr %116[%114, 13] : (!llvm.ptr<struct<".10", (ptr<i8>, ptr<ptr<i8>>, i64, ptr<i8>, i64, i64, i64, i64, i64, i64, i32, ptr<i8>, i32, ptr<ptr<i8>>)>>, i32) -> !llvm.ptr<ptr<ptr<i8>>>
    llvm.store %97, %172 : !llvm.ptr<ptr<ptr<i8>>>
    %173 = llvm.getelementptr %118[%171] : (!llvm.ptr<ptr<i8>>, i32) -> !llvm.ptr<ptr<i8>>
    %174 = llvm.bitcast %172 : !llvm.ptr<ptr<ptr<i8>>> to !llvm.ptr<i8>
    llvm.store %174, %173 : !llvm.ptr<ptr<i8>>
    %175 = llvm.mlir.addressof @ral_kernel_launch___gpu___pvoid_ppvoid_i64_pvoid_i64_i64_i64_i64_i64_i64_i32_pvoid_i32_ppvoid___void : !llvm.ptr<array<101 x i8>>
    %176 = llvm.mlir.constant(0 : index) : i64
    %177 = llvm.getelementptr %175[%176, %176] : (!llvm.ptr<array<101 x i8>>, i64, i64) -> !llvm.ptr<i8>
    llvm.call @disc_ral_call(%arg0, %177, %118) : (!llvm.ptr<i8>, !llvm.ptr<i8>, !llvm.ptr<ptr<i8>>) -> ()
    %178 = llvm.icmp "eq" %34, %4 : i64
    %179 = llvm.sub %34, %5  : i64
    %180 = llvm.udiv %179, %2  : i64
    %181 = llvm.add %180, %5  : i64
    %182 = llvm.select %178, %4, %181 : i1, i64
    %183 = llvm.icmp "eq" %30, %4 : i64
    %184 = llvm.sub %30, %5  : i64
    %185 = llvm.udiv %184, %1  : i64
    %186 = llvm.add %185, %5  : i64
    %187 = llvm.select %183, %4, %186 : i1, i64
    %188 = llvm.mul %182, %187  : i64
    %189 = llvm.mul %188, %2  : i64
    %190 = llvm.icmp "sle" %189, %4 : i64
    %191 = llvm.sub %4, %189  : i64
    %192 = llvm.sub %189, %5  : i64
    %193 = llvm.select %190, %191, %192 : i1, i64
    %194 = llvm.sdiv %193, %2  : i64
    %195 = llvm.sub %4, %194  : i64
    %196 = llvm.add %194, %5  : i64
    %197 = llvm.select %190, %195, %196 : i1, i64
    %198 = llvm.mlir.addressof @main_kernel_0_blob_gpu.binary : !llvm.ptr<array<2720 x i8>>
    %199 = llvm.mlir.constant(0 : index) : i64
    %200 = llvm.getelementptr %198[%199, %199] : (!llvm.ptr<array<2720 x i8>>, i64, i64) -> !llvm.ptr<i8>
    %201 = llvm.mlir.constant(1 : i32) : i32
    %202 = llvm.alloca %201 x !llvm.ptr<i8> : (i32) -> !llvm.ptr<ptr<i8>>
    %203 = llvm.mlir.constant(0 : i32) : i32
    %204 = llvm.getelementptr %202[%203] : (!llvm.ptr<ptr<i8>>, i32) -> !llvm.ptr<ptr<i8>>
    llvm.store %200, %204 : !llvm.ptr<ptr<i8>>
    %205 = llvm.mlir.constant(1 : i64) : i64
    %206 = llvm.mlir.addressof @main_kernel_0_main_kColReduction_reduce__4_1_0___flat_1_kernel_name : !llvm.ptr<array<42 x i8>>
    %207 = llvm.mlir.constant(0 : index) : i64
    %208 = llvm.getelementptr %206[%207, %207] : (!llvm.ptr<array<42 x i8>>, i64, i64) -> !llvm.ptr<i8>
    %209 = llvm.extractvalue %27[0] : !llvm.struct<(ptr<f32>, ptr<f32>, i64, array<3 x i64>, array<3 x i64>)> 
    %210 = llvm.extractvalue %27[1] : !llvm.struct<(ptr<f32>, ptr<f32>, i64, array<3 x i64>, array<3 x i64>)> 
    %211 = llvm.extractvalue %27[2] : !llvm.struct<(ptr<f32>, ptr<f32>, i64, array<3 x i64>, array<3 x i64>)> 
    %212 = llvm.extractvalue %27[3, 0] : !llvm.struct<(ptr<f32>, ptr<f32>, i64, array<3 x i64>, array<3 x i64>)> 
    %213 = llvm.extractvalue %27[3, 1] : !llvm.struct<(ptr<f32>, ptr<f32>, i64, array<3 x i64>, array<3 x i64>)> 
    %214 = llvm.extractvalue %27[3, 2] : !llvm.struct<(ptr<f32>, ptr<f32>, i64, array<3 x i64>, array<3 x i64>)> 
    %215 = llvm.extractvalue %27[4, 0] : !llvm.struct<(ptr<f32>, ptr<f32>, i64, array<3 x i64>, array<3 x i64>)> 
    %216 = llvm.extractvalue %27[4, 1] : !llvm.struct<(ptr<f32>, ptr<f32>, i64, array<3 x i64>, array<3 x i64>)> 
    %217 = llvm.extractvalue %27[4, 2] : !llvm.struct<(ptr<f32>, ptr<f32>, i64, array<3 x i64>, array<3 x i64>)> 
    %218 = llvm.extractvalue %68[0] : !llvm.struct<(ptr<f32>, ptr<f32>, i64, array<1 x i64>, array<1 x i64>)> 
    %219 = llvm.extractvalue %68[1] : !llvm.struct<(ptr<f32>, ptr<f32>, i64, array<1 x i64>, array<1 x i64>)> 
    %220 = llvm.extractvalue %68[2] : !llvm.struct<(ptr<f32>, ptr<f32>, i64, array<1 x i64>, array<1 x i64>)> 
    %221 = llvm.extractvalue %68[3, 0] : !llvm.struct<(ptr<f32>, ptr<f32>, i64, array<1 x i64>, array<1 x i64>)> 
    %222 = llvm.extractvalue %68[4, 0] : !llvm.struct<(ptr<f32>, ptr<f32>, i64, array<1 x i64>, array<1 x i64>)> 
    %223 = llvm.mlir.constant(1 : i32) : i32
    %224 = llvm.alloca %223 x !llvm.struct<".11", (i64, ptr<f32>, i64, i64, i64, i64, ptr<f32>)> : (i32) -> !llvm.ptr<struct<".11", (i64, ptr<f32>, i64, i64, i64, i64, ptr<f32>)>>
    %225 = llvm.mlir.constant(7 : i32) : i32
    %226 = llvm.alloca %225 x !llvm.ptr<i8> : (i32) -> !llvm.ptr<ptr<i8>>
    %227 = llvm.mlir.constant(0 : i32) : i32
    %228 = llvm.mlir.constant(0 : i32) : i32
    %229 = llvm.getelementptr %224[%227, 0] : (!llvm.ptr<struct<".11", (i64, ptr<f32>, i64, i64, i64, i64, ptr<f32>)>>, i32) -> !llvm.ptr<i64>
    llvm.store %34, %229 : !llvm.ptr<i64>
    %230 = llvm.getelementptr %226[%228] : (!llvm.ptr<ptr<i8>>, i32) -> !llvm.ptr<ptr<i8>>
    %231 = llvm.bitcast %229 : !llvm.ptr<i64> to !llvm.ptr<i8>
    llvm.store %231, %230 : !llvm.ptr<ptr<i8>>
    %232 = llvm.mlir.constant(1 : i32) : i32
    %233 = llvm.getelementptr %224[%227, 1] : (!llvm.ptr<struct<".11", (i64, ptr<f32>, i64, i64, i64, i64, ptr<f32>)>>, i32) -> !llvm.ptr<ptr<f32>>
    llvm.store %210, %233 : !llvm.ptr<ptr<f32>>
    %234 = llvm.getelementptr %226[%232] : (!llvm.ptr<ptr<i8>>, i32) -> !llvm.ptr<ptr<i8>>
    %235 = llvm.bitcast %233 : !llvm.ptr<ptr<f32>> to !llvm.ptr<i8>
    llvm.store %235, %234 : !llvm.ptr<ptr<i8>>
    %236 = llvm.mlir.constant(2 : i32) : i32
    %237 = llvm.getelementptr %224[%227, 2] : (!llvm.ptr<struct<".11", (i64, ptr<f32>, i64, i64, i64, i64, ptr<f32>)>>, i32) -> !llvm.ptr<i64>
    llvm.store %212, %237 : !llvm.ptr<i64>
    %238 = llvm.getelementptr %226[%236] : (!llvm.ptr<ptr<i8>>, i32) -> !llvm.ptr<ptr<i8>>
    %239 = llvm.bitcast %237 : !llvm.ptr<i64> to !llvm.ptr<i8>
    llvm.store %239, %238 : !llvm.ptr<ptr<i8>>
    %240 = llvm.mlir.constant(3 : i32) : i32
    %241 = llvm.getelementptr %224[%227, 3] : (!llvm.ptr<struct<".11", (i64, ptr<f32>, i64, i64, i64, i64, ptr<f32>)>>, i32) -> !llvm.ptr<i64>
    llvm.store %2, %241 : !llvm.ptr<i64>
    %242 = llvm.getelementptr %226[%240] : (!llvm.ptr<ptr<i8>>, i32) -> !llvm.ptr<ptr<i8>>
    %243 = llvm.bitcast %241 : !llvm.ptr<i64> to !llvm.ptr<i8>
    llvm.store %243, %242 : !llvm.ptr<ptr<i8>>
    %244 = llvm.mlir.constant(4 : i32) : i32
    %245 = llvm.getelementptr %224[%227, 4] : (!llvm.ptr<struct<".11", (i64, ptr<f32>, i64, i64, i64, i64, ptr<f32>)>>, i32) -> !llvm.ptr<i64>
    llvm.store %189, %245 : !llvm.ptr<i64>
    %246 = llvm.getelementptr %226[%244] : (!llvm.ptr<ptr<i8>>, i32) -> !llvm.ptr<ptr<i8>>
    %247 = llvm.bitcast %245 : !llvm.ptr<i64> to !llvm.ptr<i8>
    llvm.store %247, %246 : !llvm.ptr<ptr<i8>>
    %248 = llvm.mlir.constant(5 : i32) : i32
    %249 = llvm.getelementptr %224[%227, 5] : (!llvm.ptr<struct<".11", (i64, ptr<f32>, i64, i64, i64, i64, ptr<f32>)>>, i32) -> !llvm.ptr<i64>
    llvm.store %182, %249 : !llvm.ptr<i64>
    %250 = llvm.getelementptr %226[%248] : (!llvm.ptr<ptr<i8>>, i32) -> !llvm.ptr<ptr<i8>>
    %251 = llvm.bitcast %249 : !llvm.ptr<i64> to !llvm.ptr<i8>
    llvm.store %251, %250 : !llvm.ptr<ptr<i8>>
    %252 = llvm.mlir.constant(6 : i32) : i32
    %253 = llvm.getelementptr %224[%227, 6] : (!llvm.ptr<struct<".11", (i64, ptr<f32>, i64, i64, i64, i64, ptr<f32>)>>, i32) -> !llvm.ptr<ptr<f32>>
    llvm.store %219, %253 : !llvm.ptr<ptr<f32>>
    %254 = llvm.getelementptr %226[%252] : (!llvm.ptr<ptr<i8>>, i32) -> !llvm.ptr<ptr<i8>>
    %255 = llvm.bitcast %253 : !llvm.ptr<ptr<f32>> to !llvm.ptr<i8>
    llvm.store %255, %254 : !llvm.ptr<ptr<i8>>
    %256 = llvm.mlir.constant(0 : i32) : i32
    %257 = llvm.mlir.constant(7 : i32) : i32
    %258 = llvm.inttoptr %256 : i32 to !llvm.ptr<i8>
    %259 = llvm.mlir.constant(0 : i32) : i32
    %260 = llvm.mlir.constant(1 : i32) : i32
    %261 = llvm.alloca %260 x !llvm.struct<".12", (ptr<i8>, ptr<ptr<i8>>, i64, ptr<i8>, i64, i64, i64, i64, i64, i64, i32, ptr<i8>, i32, ptr<ptr<i8>>)> : (i32) -> !llvm.ptr<struct<".12", (ptr<i8>, ptr<ptr<i8>>, i64, ptr<i8>, i64, i64, i64, i64, i64, i64, i32, ptr<i8>, i32, ptr<ptr<i8>>)>>
    %262 = llvm.mlir.constant(14 : i32) : i32
    %263 = llvm.alloca %262 x !llvm.ptr<i8> : (i32) -> !llvm.ptr<ptr<i8>>
    %264 = llvm.mlir.constant(0 : i32) : i32
    %265 = llvm.getelementptr %261[%259, 0] : (!llvm.ptr<struct<".12", (ptr<i8>, ptr<ptr<i8>>, i64, ptr<i8>, i64, i64, i64, i64, i64, i64, i32, ptr<i8>, i32, ptr<ptr<i8>>)>>, i32) -> !llvm.ptr<ptr<i8>>
    llvm.store %arg0, %265 : !llvm.ptr<ptr<i8>>
    %266 = llvm.getelementptr %263[%264] : (!llvm.ptr<ptr<i8>>, i32) -> !llvm.ptr<ptr<i8>>
    %267 = llvm.bitcast %265 : !llvm.ptr<ptr<i8>> to !llvm.ptr<i8>
    llvm.store %267, %266 : !llvm.ptr<ptr<i8>>
    %268 = llvm.mlir.constant(1 : i32) : i32
    %269 = llvm.getelementptr %261[%259, 1] : (!llvm.ptr<struct<".12", (ptr<i8>, ptr<ptr<i8>>, i64, ptr<i8>, i64, i64, i64, i64, i64, i64, i32, ptr<i8>, i32, ptr<ptr<i8>>)>>, i32) -> !llvm.ptr<ptr<ptr<i8>>>
    llvm.store %202, %269 : !llvm.ptr<ptr<ptr<i8>>>
    %270 = llvm.getelementptr %263[%268] : (!llvm.ptr<ptr<i8>>, i32) -> !llvm.ptr<ptr<i8>>
    %271 = llvm.bitcast %269 : !llvm.ptr<ptr<ptr<i8>>> to !llvm.ptr<i8>
    llvm.store %271, %270 : !llvm.ptr<ptr<i8>>
    %272 = llvm.mlir.constant(2 : i32) : i32
    %273 = llvm.getelementptr %261[%259, 2] : (!llvm.ptr<struct<".12", (ptr<i8>, ptr<ptr<i8>>, i64, ptr<i8>, i64, i64, i64, i64, i64, i64, i32, ptr<i8>, i32, ptr<ptr<i8>>)>>, i32) -> !llvm.ptr<i64>
    llvm.store %205, %273 : !llvm.ptr<i64>
    %274 = llvm.getelementptr %263[%272] : (!llvm.ptr<ptr<i8>>, i32) -> !llvm.ptr<ptr<i8>>
    %275 = llvm.bitcast %273 : !llvm.ptr<i64> to !llvm.ptr<i8>
    llvm.store %275, %274 : !llvm.ptr<ptr<i8>>
    %276 = llvm.mlir.constant(3 : i32) : i32
    %277 = llvm.getelementptr %261[%259, 3] : (!llvm.ptr<struct<".12", (ptr<i8>, ptr<ptr<i8>>, i64, ptr<i8>, i64, i64, i64, i64, i64, i64, i32, ptr<i8>, i32, ptr<ptr<i8>>)>>, i32) -> !llvm.ptr<ptr<i8>>
    llvm.store %208, %277 : !llvm.ptr<ptr<i8>>
    %278 = llvm.getelementptr %263[%276] : (!llvm.ptr<ptr<i8>>, i32) -> !llvm.ptr<ptr<i8>>
    %279 = llvm.bitcast %277 : !llvm.ptr<ptr<i8>> to !llvm.ptr<i8>
    llvm.store %279, %278 : !llvm.ptr<ptr<i8>>
    %280 = llvm.mlir.constant(4 : i32) : i32
    %281 = llvm.getelementptr %261[%259, 4] : (!llvm.ptr<struct<".12", (ptr<i8>, ptr<ptr<i8>>, i64, ptr<i8>, i64, i64, i64, i64, i64, i64, i32, ptr<i8>, i32, ptr<ptr<i8>>)>>, i32) -> !llvm.ptr<i64>
    llvm.store %197, %281 : !llvm.ptr<i64>
    %282 = llvm.getelementptr %263[%280] : (!llvm.ptr<ptr<i8>>, i32) -> !llvm.ptr<ptr<i8>>
    %283 = llvm.bitcast %281 : !llvm.ptr<i64> to !llvm.ptr<i8>
    llvm.store %283, %282 : !llvm.ptr<ptr<i8>>
    %284 = llvm.mlir.constant(5 : i32) : i32
    %285 = llvm.getelementptr %261[%259, 5] : (!llvm.ptr<struct<".12", (ptr<i8>, ptr<ptr<i8>>, i64, ptr<i8>, i64, i64, i64, i64, i64, i64, i32, ptr<i8>, i32, ptr<ptr<i8>>)>>, i32) -> !llvm.ptr<i64>
    llvm.store %5, %285 : !llvm.ptr<i64>
    %286 = llvm.getelementptr %263[%284] : (!llvm.ptr<ptr<i8>>, i32) -> !llvm.ptr<ptr<i8>>
    %287 = llvm.bitcast %285 : !llvm.ptr<i64> to !llvm.ptr<i8>
    llvm.store %287, %286 : !llvm.ptr<ptr<i8>>
    %288 = llvm.mlir.constant(6 : i32) : i32
    %289 = llvm.getelementptr %261[%259, 6] : (!llvm.ptr<struct<".12", (ptr<i8>, ptr<ptr<i8>>, i64, ptr<i8>, i64, i64, i64, i64, i64, i64, i32, ptr<i8>, i32, ptr<ptr<i8>>)>>, i32) -> !llvm.ptr<i64>
    llvm.store %5, %289 : !llvm.ptr<i64>
    %290 = llvm.getelementptr %263[%288] : (!llvm.ptr<ptr<i8>>, i32) -> !llvm.ptr<ptr<i8>>
    %291 = llvm.bitcast %289 : !llvm.ptr<i64> to !llvm.ptr<i8>
    llvm.store %291, %290 : !llvm.ptr<ptr<i8>>
    %292 = llvm.mlir.constant(7 : i32) : i32
    %293 = llvm.getelementptr %261[%259, 7] : (!llvm.ptr<struct<".12", (ptr<i8>, ptr<ptr<i8>>, i64, ptr<i8>, i64, i64, i64, i64, i64, i64, i32, ptr<i8>, i32, ptr<ptr<i8>>)>>, i32) -> !llvm.ptr<i64>
    llvm.store %2, %293 : !llvm.ptr<i64>
    %294 = llvm.getelementptr %263[%292] : (!llvm.ptr<ptr<i8>>, i32) -> !llvm.ptr<ptr<i8>>
    %295 = llvm.bitcast %293 : !llvm.ptr<i64> to !llvm.ptr<i8>
    llvm.store %295, %294 : !llvm.ptr<ptr<i8>>
    %296 = llvm.mlir.constant(8 : i32) : i32
    %297 = llvm.getelementptr %261[%259, 8] : (!llvm.ptr<struct<".12", (ptr<i8>, ptr<ptr<i8>>, i64, ptr<i8>, i64, i64, i64, i64, i64, i64, i32, ptr<i8>, i32, ptr<ptr<i8>>)>>, i32) -> !llvm.ptr<i64>
    llvm.store %5, %297 : !llvm.ptr<i64>
    %298 = llvm.getelementptr %263[%296] : (!llvm.ptr<ptr<i8>>, i32) -> !llvm.ptr<ptr<i8>>
    %299 = llvm.bitcast %297 : !llvm.ptr<i64> to !llvm.ptr<i8>
    llvm.store %299, %298 : !llvm.ptr<ptr<i8>>
    %300 = llvm.mlir.constant(9 : i32) : i32
    %301 = llvm.getelementptr %261[%259, 9] : (!llvm.ptr<struct<".12", (ptr<i8>, ptr<ptr<i8>>, i64, ptr<i8>, i64, i64, i64, i64, i64, i64, i32, ptr<i8>, i32, ptr<ptr<i8>>)>>, i32) -> !llvm.ptr<i64>
    llvm.store %5, %301 : !llvm.ptr<i64>
    %302 = llvm.getelementptr %263[%300] : (!llvm.ptr<ptr<i8>>, i32) -> !llvm.ptr<ptr<i8>>
    %303 = llvm.bitcast %301 : !llvm.ptr<i64> to !llvm.ptr<i8>
    llvm.store %303, %302 : !llvm.ptr<ptr<i8>>
    %304 = llvm.mlir.constant(10 : i32) : i32
    %305 = llvm.getelementptr %261[%259, 10] : (!llvm.ptr<struct<".12", (ptr<i8>, ptr<ptr<i8>>, i64, ptr<i8>, i64, i64, i64, i64, i64, i64, i32, ptr<i8>, i32, ptr<ptr<i8>>)>>, i32) -> !llvm.ptr<i32>
    llvm.store %256, %305 : !llvm.ptr<i32>
    %306 = llvm.getelementptr %263[%304] : (!llvm.ptr<ptr<i8>>, i32) -> !llvm.ptr<ptr<i8>>
    %307 = llvm.bitcast %305 : !llvm.ptr<i32> to !llvm.ptr<i8>
    llvm.store %307, %306 : !llvm.ptr<ptr<i8>>
    %308 = llvm.mlir.constant(11 : i32) : i32
    %309 = llvm.getelementptr %261[%259, 11] : (!llvm.ptr<struct<".12", (ptr<i8>, ptr<ptr<i8>>, i64, ptr<i8>, i64, i64, i64, i64, i64, i64, i32, ptr<i8>, i32, ptr<ptr<i8>>)>>, i32) -> !llvm.ptr<ptr<i8>>
    llvm.store %258, %309 : !llvm.ptr<ptr<i8>>
    %310 = llvm.getelementptr %263[%308] : (!llvm.ptr<ptr<i8>>, i32) -> !llvm.ptr<ptr<i8>>
    %311 = llvm.bitcast %309 : !llvm.ptr<ptr<i8>> to !llvm.ptr<i8>
    llvm.store %311, %310 : !llvm.ptr<ptr<i8>>
    %312 = llvm.mlir.constant(12 : i32) : i32
    %313 = llvm.getelementptr %261[%259, 12] : (!llvm.ptr<struct<".12", (ptr<i8>, ptr<ptr<i8>>, i64, ptr<i8>, i64, i64, i64, i64, i64, i64, i32, ptr<i8>, i32, ptr<ptr<i8>>)>>, i32) -> !llvm.ptr<i32>
    llvm.store %257, %313 : !llvm.ptr<i32>
    %314 = llvm.getelementptr %263[%312] : (!llvm.ptr<ptr<i8>>, i32) -> !llvm.ptr<ptr<i8>>
    %315 = llvm.bitcast %313 : !llvm.ptr<i32> to !llvm.ptr<i8>
    llvm.store %315, %314 : !llvm.ptr<ptr<i8>>
    %316 = llvm.mlir.constant(13 : i32) : i32
    %317 = llvm.getelementptr %261[%259, 13] : (!llvm.ptr<struct<".12", (ptr<i8>, ptr<ptr<i8>>, i64, ptr<i8>, i64, i64, i64, i64, i64, i64, i32, ptr<i8>, i32, ptr<ptr<i8>>)>>, i32) -> !llvm.ptr<ptr<ptr<i8>>>
    llvm.store %226, %317 : !llvm.ptr<ptr<ptr<i8>>>
    %318 = llvm.getelementptr %263[%316] : (!llvm.ptr<ptr<i8>>, i32) -> !llvm.ptr<ptr<i8>>
    %319 = llvm.bitcast %317 : !llvm.ptr<ptr<ptr<i8>>> to !llvm.ptr<i8>
    llvm.store %319, %318 : !llvm.ptr<ptr<i8>>
    %320 = llvm.mlir.addressof @ral_kernel_launch___gpu___pvoid_ppvoid_i64_pvoid_i64_i64_i64_i64_i64_i64_i32_pvoid_i32_ppvoid___void : !llvm.ptr<array<101 x i8>>
    %321 = llvm.mlir.constant(0 : index) : i64
    %322 = llvm.getelementptr %320[%321, %321] : (!llvm.ptr<array<101 x i8>>, i64, i64) -> !llvm.ptr<i8>
    llvm.call @disc_ral_call(%arg0, %322, %263) : (!llvm.ptr<i8>, !llvm.ptr<i8>, !llvm.ptr<ptr<i8>>) -> ()
    llvm.br ^bb3
  ^bb2:  // pred: ^bb0
    %323 = llvm.icmp "sle" %34, %4 : i64
    %324 = llvm.sub %4, %34  : i64
    %325 = llvm.sub %34, %5  : i64
    %326 = llvm.select %323, %324, %325 : i1, i64
    %327 = llvm.sdiv %326, %0  : i64
    %328 = llvm.sub %4, %327  : i64
    %329 = llvm.add %327, %5  : i64
    %330 = llvm.select %323, %328, %329 : i1, i64
    %331 = llvm.mlir.addressof @main_kernel_1_blob_gpu.binary : !llvm.ptr<array<1096 x i8>>
    %332 = llvm.mlir.constant(0 : index) : i64
    %333 = llvm.getelementptr %331[%332, %332] : (!llvm.ptr<array<1096 x i8>>, i64, i64) -> !llvm.ptr<i8>
    %334 = llvm.mlir.constant(1 : i32) : i32
    %335 = llvm.alloca %334 x !llvm.ptr<i8> : (i32) -> !llvm.ptr<ptr<i8>>
    %336 = llvm.mlir.constant(0 : i32) : i32
    %337 = llvm.getelementptr %335[%336] : (!llvm.ptr<ptr<i8>>, i32) -> !llvm.ptr<ptr<i8>>
    llvm.store %333, %337 : !llvm.ptr<ptr<i8>>
    %338 = llvm.mlir.constant(1 : i64) : i64
    %339 = llvm.mlir.addressof @main_kernel_1_main_kColReduction_reduce__4_1_0___thin_kernel_name : !llvm.ptr<array<40 x i8>>
    %340 = llvm.mlir.constant(0 : index) : i64
    %341 = llvm.getelementptr %339[%340, %340] : (!llvm.ptr<array<40 x i8>>, i64, i64) -> !llvm.ptr<i8>
    %342 = llvm.extractvalue %68[0] : !llvm.struct<(ptr<f32>, ptr<f32>, i64, array<1 x i64>, array<1 x i64>)> 
    %343 = llvm.extractvalue %68[1] : !llvm.struct<(ptr<f32>, ptr<f32>, i64, array<1 x i64>, array<1 x i64>)> 
    %344 = llvm.extractvalue %68[2] : !llvm.struct<(ptr<f32>, ptr<f32>, i64, array<1 x i64>, array<1 x i64>)> 
    %345 = llvm.extractvalue %68[3, 0] : !llvm.struct<(ptr<f32>, ptr<f32>, i64, array<1 x i64>, array<1 x i64>)> 
    %346 = llvm.extractvalue %68[4, 0] : !llvm.struct<(ptr<f32>, ptr<f32>, i64, array<1 x i64>, array<1 x i64>)> 
    %347 = llvm.mlir.constant(1 : i32) : i32
    %348 = llvm.alloca %347 x !llvm.struct<".2", (i64, i64, ptr<f32>)> : (i32) -> !llvm.ptr<struct<".2", (i64, i64, ptr<f32>)>>
    %349 = llvm.mlir.constant(3 : i32) : i32
    %350 = llvm.alloca %349 x !llvm.ptr<i8> : (i32) -> !llvm.ptr<ptr<i8>>
    %351 = llvm.mlir.constant(0 : i32) : i32
    %352 = llvm.mlir.constant(0 : i32) : i32
    %353 = llvm.getelementptr %348[%351, 0] : (!llvm.ptr<struct<".2", (i64, i64, ptr<f32>)>>, i32) -> !llvm.ptr<i64>
    llvm.store %0, %353 : !llvm.ptr<i64>
    %354 = llvm.getelementptr %350[%352] : (!llvm.ptr<ptr<i8>>, i32) -> !llvm.ptr<ptr<i8>>
    %355 = llvm.bitcast %353 : !llvm.ptr<i64> to !llvm.ptr<i8>
    llvm.store %355, %354 : !llvm.ptr<ptr<i8>>
    %356 = llvm.mlir.constant(1 : i32) : i32
    %357 = llvm.getelementptr %348[%351, 1] : (!llvm.ptr<struct<".2", (i64, i64, ptr<f32>)>>, i32) -> !llvm.ptr<i64>
    llvm.store %34, %357 : !llvm.ptr<i64>
    %358 = llvm.getelementptr %350[%356] : (!llvm.ptr<ptr<i8>>, i32) -> !llvm.ptr<ptr<i8>>
    %359 = llvm.bitcast %357 : !llvm.ptr<i64> to !llvm.ptr<i8>
    llvm.store %359, %358 : !llvm.ptr<ptr<i8>>
    %360 = llvm.mlir.constant(2 : i32) : i32
    %361 = llvm.getelementptr %348[%351, 2] : (!llvm.ptr<struct<".2", (i64, i64, ptr<f32>)>>, i32) -> !llvm.ptr<ptr<f32>>
    llvm.store %343, %361 : !llvm.ptr<ptr<f32>>
    %362 = llvm.getelementptr %350[%360] : (!llvm.ptr<ptr<i8>>, i32) -> !llvm.ptr<ptr<i8>>
    %363 = llvm.bitcast %361 : !llvm.ptr<ptr<f32>> to !llvm.ptr<i8>
    llvm.store %363, %362 : !llvm.ptr<ptr<i8>>
    %364 = llvm.mlir.constant(0 : i32) : i32
    %365 = llvm.mlir.constant(3 : i32) : i32
    %366 = llvm.inttoptr %364 : i32 to !llvm.ptr<i8>
    %367 = llvm.mlir.constant(0 : i32) : i32
    %368 = llvm.mlir.constant(1 : i32) : i32
    %369 = llvm.alloca %368 x !llvm.struct<".3", (ptr<i8>, ptr<ptr<i8>>, i64, ptr<i8>, i64, i64, i64, i64, i64, i64, i32, ptr<i8>, i32, ptr<ptr<i8>>)> : (i32) -> !llvm.ptr<struct<".3", (ptr<i8>, ptr<ptr<i8>>, i64, ptr<i8>, i64, i64, i64, i64, i64, i64, i32, ptr<i8>, i32, ptr<ptr<i8>>)>>
    %370 = llvm.mlir.constant(14 : i32) : i32
    %371 = llvm.alloca %370 x !llvm.ptr<i8> : (i32) -> !llvm.ptr<ptr<i8>>
    %372 = llvm.mlir.constant(0 : i32) : i32
    %373 = llvm.getelementptr %369[%367, 0] : (!llvm.ptr<struct<".3", (ptr<i8>, ptr<ptr<i8>>, i64, ptr<i8>, i64, i64, i64, i64, i64, i64, i32, ptr<i8>, i32, ptr<ptr<i8>>)>>, i32) -> !llvm.ptr<ptr<i8>>
    llvm.store %arg0, %373 : !llvm.ptr<ptr<i8>>
    %374 = llvm.getelementptr %371[%372] : (!llvm.ptr<ptr<i8>>, i32) -> !llvm.ptr<ptr<i8>>
    %375 = llvm.bitcast %373 : !llvm.ptr<ptr<i8>> to !llvm.ptr<i8>
    llvm.store %375, %374 : !llvm.ptr<ptr<i8>>
    %376 = llvm.mlir.constant(1 : i32) : i32
    %377 = llvm.getelementptr %369[%367, 1] : (!llvm.ptr<struct<".3", (ptr<i8>, ptr<ptr<i8>>, i64, ptr<i8>, i64, i64, i64, i64, i64, i64, i32, ptr<i8>, i32, ptr<ptr<i8>>)>>, i32) -> !llvm.ptr<ptr<ptr<i8>>>
    llvm.store %335, %377 : !llvm.ptr<ptr<ptr<i8>>>
    %378 = llvm.getelementptr %371[%376] : (!llvm.ptr<ptr<i8>>, i32) -> !llvm.ptr<ptr<i8>>
    %379 = llvm.bitcast %377 : !llvm.ptr<ptr<ptr<i8>>> to !llvm.ptr<i8>
    llvm.store %379, %378 : !llvm.ptr<ptr<i8>>
    %380 = llvm.mlir.constant(2 : i32) : i32
    %381 = llvm.getelementptr %369[%367, 2] : (!llvm.ptr<struct<".3", (ptr<i8>, ptr<ptr<i8>>, i64, ptr<i8>, i64, i64, i64, i64, i64, i64, i32, ptr<i8>, i32, ptr<ptr<i8>>)>>, i32) -> !llvm.ptr<i64>
    llvm.store %338, %381 : !llvm.ptr<i64>
    %382 = llvm.getelementptr %371[%380] : (!llvm.ptr<ptr<i8>>, i32) -> !llvm.ptr<ptr<i8>>
    %383 = llvm.bitcast %381 : !llvm.ptr<i64> to !llvm.ptr<i8>
    llvm.store %383, %382 : !llvm.ptr<ptr<i8>>
    %384 = llvm.mlir.constant(3 : i32) : i32
    %385 = llvm.getelementptr %369[%367, 3] : (!llvm.ptr<struct<".3", (ptr<i8>, ptr<ptr<i8>>, i64, ptr<i8>, i64, i64, i64, i64, i64, i64, i32, ptr<i8>, i32, ptr<ptr<i8>>)>>, i32) -> !llvm.ptr<ptr<i8>>
    llvm.store %341, %385 : !llvm.ptr<ptr<i8>>
    %386 = llvm.getelementptr %371[%384] : (!llvm.ptr<ptr<i8>>, i32) -> !llvm.ptr<ptr<i8>>
    %387 = llvm.bitcast %385 : !llvm.ptr<ptr<i8>> to !llvm.ptr<i8>
    llvm.store %387, %386 : !llvm.ptr<ptr<i8>>
    %388 = llvm.mlir.constant(4 : i32) : i32
    %389 = llvm.getelementptr %369[%367, 4] : (!llvm.ptr<struct<".3", (ptr<i8>, ptr<ptr<i8>>, i64, ptr<i8>, i64, i64, i64, i64, i64, i64, i32, ptr<i8>, i32, ptr<ptr<i8>>)>>, i32) -> !llvm.ptr<i64>
    llvm.store %330, %389 : !llvm.ptr<i64>
    %390 = llvm.getelementptr %371[%388] : (!llvm.ptr<ptr<i8>>, i32) -> !llvm.ptr<ptr<i8>>
    %391 = llvm.bitcast %389 : !llvm.ptr<i64> to !llvm.ptr<i8>
    llvm.store %391, %390 : !llvm.ptr<ptr<i8>>
    %392 = llvm.mlir.constant(5 : i32) : i32
    %393 = llvm.getelementptr %369[%367, 5] : (!llvm.ptr<struct<".3", (ptr<i8>, ptr<ptr<i8>>, i64, ptr<i8>, i64, i64, i64, i64, i64, i64, i32, ptr<i8>, i32, ptr<ptr<i8>>)>>, i32) -> !llvm.ptr<i64>
    llvm.store %5, %393 : !llvm.ptr<i64>
    %394 = llvm.getelementptr %371[%392] : (!llvm.ptr<ptr<i8>>, i32) -> !llvm.ptr<ptr<i8>>
    %395 = llvm.bitcast %393 : !llvm.ptr<i64> to !llvm.ptr<i8>
    llvm.store %395, %394 : !llvm.ptr<ptr<i8>>
    %396 = llvm.mlir.constant(6 : i32) : i32
    %397 = llvm.getelementptr %369[%367, 6] : (!llvm.ptr<struct<".3", (ptr<i8>, ptr<ptr<i8>>, i64, ptr<i8>, i64, i64, i64, i64, i64, i64, i32, ptr<i8>, i32, ptr<ptr<i8>>)>>, i32) -> !llvm.ptr<i64>
    llvm.store %5, %397 : !llvm.ptr<i64>
    %398 = llvm.getelementptr %371[%396] : (!llvm.ptr<ptr<i8>>, i32) -> !llvm.ptr<ptr<i8>>
    %399 = llvm.bitcast %397 : !llvm.ptr<i64> to !llvm.ptr<i8>
    llvm.store %399, %398 : !llvm.ptr<ptr<i8>>
    %400 = llvm.mlir.constant(7 : i32) : i32
    %401 = llvm.getelementptr %369[%367, 7] : (!llvm.ptr<struct<".3", (ptr<i8>, ptr<ptr<i8>>, i64, ptr<i8>, i64, i64, i64, i64, i64, i64, i32, ptr<i8>, i32, ptr<ptr<i8>>)>>, i32) -> !llvm.ptr<i64>
    llvm.store %0, %401 : !llvm.ptr<i64>
    %402 = llvm.getelementptr %371[%400] : (!llvm.ptr<ptr<i8>>, i32) -> !llvm.ptr<ptr<i8>>
    %403 = llvm.bitcast %401 : !llvm.ptr<i64> to !llvm.ptr<i8>
    llvm.store %403, %402 : !llvm.ptr<ptr<i8>>
    %404 = llvm.mlir.constant(8 : i32) : i32
    %405 = llvm.getelementptr %369[%367, 8] : (!llvm.ptr<struct<".3", (ptr<i8>, ptr<ptr<i8>>, i64, ptr<i8>, i64, i64, i64, i64, i64, i64, i32, ptr<i8>, i32, ptr<ptr<i8>>)>>, i32) -> !llvm.ptr<i64>
    llvm.store %5, %405 : !llvm.ptr<i64>
    %406 = llvm.getelementptr %371[%404] : (!llvm.ptr<ptr<i8>>, i32) -> !llvm.ptr<ptr<i8>>
    %407 = llvm.bitcast %405 : !llvm.ptr<i64> to !llvm.ptr<i8>
    llvm.store %407, %406 : !llvm.ptr<ptr<i8>>
    %408 = llvm.mlir.constant(9 : i32) : i32
    %409 = llvm.getelementptr %369[%367, 9] : (!llvm.ptr<struct<".3", (ptr<i8>, ptr<ptr<i8>>, i64, ptr<i8>, i64, i64, i64, i64, i64, i64, i32, ptr<i8>, i32, ptr<ptr<i8>>)>>, i32) -> !llvm.ptr<i64>
    llvm.store %5, %409 : !llvm.ptr<i64>
    %410 = llvm.getelementptr %371[%408] : (!llvm.ptr<ptr<i8>>, i32) -> !llvm.ptr<ptr<i8>>
    %411 = llvm.bitcast %409 : !llvm.ptr<i64> to !llvm.ptr<i8>
    llvm.store %411, %410 : !llvm.ptr<ptr<i8>>
    %412 = llvm.mlir.constant(10 : i32) : i32
    %413 = llvm.getelementptr %369[%367, 10] : (!llvm.ptr<struct<".3", (ptr<i8>, ptr<ptr<i8>>, i64, ptr<i8>, i64, i64, i64, i64, i64, i64, i32, ptr<i8>, i32, ptr<ptr<i8>>)>>, i32) -> !llvm.ptr<i32>
    llvm.store %364, %413 : !llvm.ptr<i32>
    %414 = llvm.getelementptr %371[%412] : (!llvm.ptr<ptr<i8>>, i32) -> !llvm.ptr<ptr<i8>>
    %415 = llvm.bitcast %413 : !llvm.ptr<i32> to !llvm.ptr<i8>
    llvm.store %415, %414 : !llvm.ptr<ptr<i8>>
    %416 = llvm.mlir.constant(11 : i32) : i32
    %417 = llvm.getelementptr %369[%367, 11] : (!llvm.ptr<struct<".3", (ptr<i8>, ptr<ptr<i8>>, i64, ptr<i8>, i64, i64, i64, i64, i64, i64, i32, ptr<i8>, i32, ptr<ptr<i8>>)>>, i32) -> !llvm.ptr<ptr<i8>>
    llvm.store %366, %417 : !llvm.ptr<ptr<i8>>
    %418 = llvm.getelementptr %371[%416] : (!llvm.ptr<ptr<i8>>, i32) -> !llvm.ptr<ptr<i8>>
    %419 = llvm.bitcast %417 : !llvm.ptr<ptr<i8>> to !llvm.ptr<i8>
    llvm.store %419, %418 : !llvm.ptr<ptr<i8>>
    %420 = llvm.mlir.constant(12 : i32) : i32
    %421 = llvm.getelementptr %369[%367, 12] : (!llvm.ptr<struct<".3", (ptr<i8>, ptr<ptr<i8>>, i64, ptr<i8>, i64, i64, i64, i64, i64, i64, i32, ptr<i8>, i32, ptr<ptr<i8>>)>>, i32) -> !llvm.ptr<i32>
    llvm.store %365, %421 : !llvm.ptr<i32>
    %422 = llvm.getelementptr %371[%420] : (!llvm.ptr<ptr<i8>>, i32) -> !llvm.ptr<ptr<i8>>
    %423 = llvm.bitcast %421 : !llvm.ptr<i32> to !llvm.ptr<i8>
    llvm.store %423, %422 : !llvm.ptr<ptr<i8>>
    %424 = llvm.mlir.constant(13 : i32) : i32
    %425 = llvm.getelementptr %369[%367, 13] : (!llvm.ptr<struct<".3", (ptr<i8>, ptr<ptr<i8>>, i64, ptr<i8>, i64, i64, i64, i64, i64, i64, i32, ptr<i8>, i32, ptr<ptr<i8>>)>>, i32) -> !llvm.ptr<ptr<ptr<i8>>>
    llvm.store %350, %425 : !llvm.ptr<ptr<ptr<i8>>>
    %426 = llvm.getelementptr %371[%424] : (!llvm.ptr<ptr<i8>>, i32) -> !llvm.ptr<ptr<i8>>
    %427 = llvm.bitcast %425 : !llvm.ptr<ptr<ptr<i8>>> to !llvm.ptr<i8>
    llvm.store %427, %426 : !llvm.ptr<ptr<i8>>
    %428 = llvm.mlir.addressof @ral_kernel_launch___gpu___pvoid_ppvoid_i64_pvoid_i64_i64_i64_i64_i64_i64_i32_pvoid_i32_ppvoid___void : !llvm.ptr<array<101 x i8>>
    %429 = llvm.mlir.constant(0 : index) : i64
    %430 = llvm.getelementptr %428[%429, %429] : (!llvm.ptr<array<101 x i8>>, i64, i64) -> !llvm.ptr<i8>
    llvm.call @disc_ral_call(%arg0, %430, %371) : (!llvm.ptr<i8>, !llvm.ptr<i8>, !llvm.ptr<ptr<i8>>) -> ()
    %431 = llvm.icmp "eq" %34, %4 : i64
    %432 = llvm.sub %34, %5  : i64
    %433 = llvm.udiv %432, %1  : i64
    %434 = llvm.add %433, %5  : i64
    %435 = llvm.select %431, %4, %434 : i1, i64
    %436 = llvm.icmp "eq" %30, %4 : i64
    %437 = llvm.sub %30, %5  : i64
    %438 = llvm.udiv %437, %2  : i64
    %439 = llvm.add %438, %5  : i64
    %440 = llvm.select %436, %4, %439 : i1, i64
    %441 = llvm.mul %435, %440  : i64
    %442 = llvm.mul %441, %0  : i64
    %443 = llvm.icmp "sle" %442, %4 : i64
    %444 = llvm.sub %4, %442  : i64
    %445 = llvm.sub %442, %5  : i64
    %446 = llvm.select %443, %444, %445 : i1, i64
    %447 = llvm.sdiv %446, %0  : i64
    %448 = llvm.sub %4, %447  : i64
    %449 = llvm.add %447, %5  : i64
    %450 = llvm.select %443, %448, %449 : i1, i64
    %451 = llvm.mlir.addressof @main_kernel_2_blob_gpu.binary : !llvm.ptr<array<3128 x i8>>
    %452 = llvm.mlir.constant(0 : index) : i64
    %453 = llvm.getelementptr %451[%452, %452] : (!llvm.ptr<array<3128 x i8>>, i64, i64) -> !llvm.ptr<i8>
    %454 = llvm.mlir.constant(1 : i32) : i32
    %455 = llvm.alloca %454 x !llvm.ptr<i8> : (i32) -> !llvm.ptr<ptr<i8>>
    %456 = llvm.mlir.constant(0 : i32) : i32
    %457 = llvm.getelementptr %455[%456] : (!llvm.ptr<ptr<i8>>, i32) -> !llvm.ptr<ptr<i8>>
    llvm.store %453, %457 : !llvm.ptr<ptr<i8>>
    %458 = llvm.mlir.constant(1 : i64) : i64
    %459 = llvm.mlir.addressof @main_kernel_2_main_kColReduction_reduce__4_1_0___thin_1_kernel_name : !llvm.ptr<array<42 x i8>>
    %460 = llvm.mlir.constant(0 : index) : i64
    %461 = llvm.getelementptr %459[%460, %460] : (!llvm.ptr<array<42 x i8>>, i64, i64) -> !llvm.ptr<i8>
    %462 = llvm.extractvalue %27[0] : !llvm.struct<(ptr<f32>, ptr<f32>, i64, array<3 x i64>, array<3 x i64>)> 
    %463 = llvm.extractvalue %27[1] : !llvm.struct<(ptr<f32>, ptr<f32>, i64, array<3 x i64>, array<3 x i64>)> 
    %464 = llvm.extractvalue %27[2] : !llvm.struct<(ptr<f32>, ptr<f32>, i64, array<3 x i64>, array<3 x i64>)> 
    %465 = llvm.extractvalue %27[3, 0] : !llvm.struct<(ptr<f32>, ptr<f32>, i64, array<3 x i64>, array<3 x i64>)> 
    %466 = llvm.extractvalue %27[3, 1] : !llvm.struct<(ptr<f32>, ptr<f32>, i64, array<3 x i64>, array<3 x i64>)> 
    %467 = llvm.extractvalue %27[3, 2] : !llvm.struct<(ptr<f32>, ptr<f32>, i64, array<3 x i64>, array<3 x i64>)> 
    %468 = llvm.extractvalue %27[4, 0] : !llvm.struct<(ptr<f32>, ptr<f32>, i64, array<3 x i64>, array<3 x i64>)> 
    %469 = llvm.extractvalue %27[4, 1] : !llvm.struct<(ptr<f32>, ptr<f32>, i64, array<3 x i64>, array<3 x i64>)> 
    %470 = llvm.extractvalue %27[4, 2] : !llvm.struct<(ptr<f32>, ptr<f32>, i64, array<3 x i64>, array<3 x i64>)> 
    %471 = llvm.extractvalue %68[0] : !llvm.struct<(ptr<f32>, ptr<f32>, i64, array<1 x i64>, array<1 x i64>)> 
    %472 = llvm.extractvalue %68[1] : !llvm.struct<(ptr<f32>, ptr<f32>, i64, array<1 x i64>, array<1 x i64>)> 
    %473 = llvm.extractvalue %68[2] : !llvm.struct<(ptr<f32>, ptr<f32>, i64, array<1 x i64>, array<1 x i64>)> 
    %474 = llvm.extractvalue %68[3, 0] : !llvm.struct<(ptr<f32>, ptr<f32>, i64, array<1 x i64>, array<1 x i64>)> 
    %475 = llvm.extractvalue %68[4, 0] : !llvm.struct<(ptr<f32>, ptr<f32>, i64, array<1 x i64>, array<1 x i64>)> 
    %476 = llvm.mlir.constant(1 : i32) : i32
    %477 = llvm.alloca %476 x !llvm.struct<".4", (i64, ptr<f32>, i64, i64, i64, i64, ptr<f32>)> : (i32) -> !llvm.ptr<struct<".4", (i64, ptr<f32>, i64, i64, i64, i64, ptr<f32>)>>
    %478 = llvm.mlir.constant(7 : i32) : i32
    %479 = llvm.alloca %478 x !llvm.ptr<i8> : (i32) -> !llvm.ptr<ptr<i8>>
    %480 = llvm.mlir.constant(0 : i32) : i32
    %481 = llvm.mlir.constant(0 : i32) : i32
    %482 = llvm.getelementptr %477[%480, 0] : (!llvm.ptr<struct<".4", (i64, ptr<f32>, i64, i64, i64, i64, ptr<f32>)>>, i32) -> !llvm.ptr<i64>
    llvm.store %34, %482 : !llvm.ptr<i64>
    %483 = llvm.getelementptr %479[%481] : (!llvm.ptr<ptr<i8>>, i32) -> !llvm.ptr<ptr<i8>>
    %484 = llvm.bitcast %482 : !llvm.ptr<i64> to !llvm.ptr<i8>
    llvm.store %484, %483 : !llvm.ptr<ptr<i8>>
    %485 = llvm.mlir.constant(1 : i32) : i32
    %486 = llvm.getelementptr %477[%480, 1] : (!llvm.ptr<struct<".4", (i64, ptr<f32>, i64, i64, i64, i64, ptr<f32>)>>, i32) -> !llvm.ptr<ptr<f32>>
    llvm.store %463, %486 : !llvm.ptr<ptr<f32>>
    %487 = llvm.getelementptr %479[%485] : (!llvm.ptr<ptr<i8>>, i32) -> !llvm.ptr<ptr<i8>>
    %488 = llvm.bitcast %486 : !llvm.ptr<ptr<f32>> to !llvm.ptr<i8>
    llvm.store %488, %487 : !llvm.ptr<ptr<i8>>
    %489 = llvm.mlir.constant(2 : i32) : i32
    %490 = llvm.getelementptr %477[%480, 2] : (!llvm.ptr<struct<".4", (i64, ptr<f32>, i64, i64, i64, i64, ptr<f32>)>>, i32) -> !llvm.ptr<i64>
    llvm.store %465, %490 : !llvm.ptr<i64>
    %491 = llvm.getelementptr %479[%489] : (!llvm.ptr<ptr<i8>>, i32) -> !llvm.ptr<ptr<i8>>
    %492 = llvm.bitcast %490 : !llvm.ptr<i64> to !llvm.ptr<i8>
    llvm.store %492, %491 : !llvm.ptr<ptr<i8>>
    %493 = llvm.mlir.constant(3 : i32) : i32
    %494 = llvm.getelementptr %477[%480, 3] : (!llvm.ptr<struct<".4", (i64, ptr<f32>, i64, i64, i64, i64, ptr<f32>)>>, i32) -> !llvm.ptr<i64>
    llvm.store %0, %494 : !llvm.ptr<i64>
    %495 = llvm.getelementptr %479[%493] : (!llvm.ptr<ptr<i8>>, i32) -> !llvm.ptr<ptr<i8>>
    %496 = llvm.bitcast %494 : !llvm.ptr<i64> to !llvm.ptr<i8>
    llvm.store %496, %495 : !llvm.ptr<ptr<i8>>
    %497 = llvm.mlir.constant(4 : i32) : i32
    %498 = llvm.getelementptr %477[%480, 4] : (!llvm.ptr<struct<".4", (i64, ptr<f32>, i64, i64, i64, i64, ptr<f32>)>>, i32) -> !llvm.ptr<i64>
    llvm.store %442, %498 : !llvm.ptr<i64>
    %499 = llvm.getelementptr %479[%497] : (!llvm.ptr<ptr<i8>>, i32) -> !llvm.ptr<ptr<i8>>
    %500 = llvm.bitcast %498 : !llvm.ptr<i64> to !llvm.ptr<i8>
    llvm.store %500, %499 : !llvm.ptr<ptr<i8>>
    %501 = llvm.mlir.constant(5 : i32) : i32
    %502 = llvm.getelementptr %477[%480, 5] : (!llvm.ptr<struct<".4", (i64, ptr<f32>, i64, i64, i64, i64, ptr<f32>)>>, i32) -> !llvm.ptr<i64>
    llvm.store %435, %502 : !llvm.ptr<i64>
    %503 = llvm.getelementptr %479[%501] : (!llvm.ptr<ptr<i8>>, i32) -> !llvm.ptr<ptr<i8>>
    %504 = llvm.bitcast %502 : !llvm.ptr<i64> to !llvm.ptr<i8>
    llvm.store %504, %503 : !llvm.ptr<ptr<i8>>
    %505 = llvm.mlir.constant(6 : i32) : i32
    %506 = llvm.getelementptr %477[%480, 6] : (!llvm.ptr<struct<".4", (i64, ptr<f32>, i64, i64, i64, i64, ptr<f32>)>>, i32) -> !llvm.ptr<ptr<f32>>
    llvm.store %472, %506 : !llvm.ptr<ptr<f32>>
    %507 = llvm.getelementptr %479[%505] : (!llvm.ptr<ptr<i8>>, i32) -> !llvm.ptr<ptr<i8>>
    %508 = llvm.bitcast %506 : !llvm.ptr<ptr<f32>> to !llvm.ptr<i8>
    llvm.store %508, %507 : !llvm.ptr<ptr<i8>>
    %509 = llvm.mlir.constant(0 : i32) : i32
    %510 = llvm.mlir.constant(7 : i32) : i32
    %511 = llvm.inttoptr %509 : i32 to !llvm.ptr<i8>
    %512 = llvm.mlir.constant(0 : i32) : i32
    %513 = llvm.mlir.constant(1 : i32) : i32
    %514 = llvm.alloca %513 x !llvm.struct<".5", (ptr<i8>, ptr<ptr<i8>>, i64, ptr<i8>, i64, i64, i64, i64, i64, i64, i32, ptr<i8>, i32, ptr<ptr<i8>>)> : (i32) -> !llvm.ptr<struct<".5", (ptr<i8>, ptr<ptr<i8>>, i64, ptr<i8>, i64, i64, i64, i64, i64, i64, i32, ptr<i8>, i32, ptr<ptr<i8>>)>>
    %515 = llvm.mlir.constant(14 : i32) : i32
    %516 = llvm.alloca %515 x !llvm.ptr<i8> : (i32) -> !llvm.ptr<ptr<i8>>
    %517 = llvm.mlir.constant(0 : i32) : i32
    %518 = llvm.getelementptr %514[%512, 0] : (!llvm.ptr<struct<".5", (ptr<i8>, ptr<ptr<i8>>, i64, ptr<i8>, i64, i64, i64, i64, i64, i64, i32, ptr<i8>, i32, ptr<ptr<i8>>)>>, i32) -> !llvm.ptr<ptr<i8>>
    llvm.store %arg0, %518 : !llvm.ptr<ptr<i8>>
    %519 = llvm.getelementptr %516[%517] : (!llvm.ptr<ptr<i8>>, i32) -> !llvm.ptr<ptr<i8>>
    %520 = llvm.bitcast %518 : !llvm.ptr<ptr<i8>> to !llvm.ptr<i8>
    llvm.store %520, %519 : !llvm.ptr<ptr<i8>>
    %521 = llvm.mlir.constant(1 : i32) : i32
    %522 = llvm.getelementptr %514[%512, 1] : (!llvm.ptr<struct<".5", (ptr<i8>, ptr<ptr<i8>>, i64, ptr<i8>, i64, i64, i64, i64, i64, i64, i32, ptr<i8>, i32, ptr<ptr<i8>>)>>, i32) -> !llvm.ptr<ptr<ptr<i8>>>
    llvm.store %455, %522 : !llvm.ptr<ptr<ptr<i8>>>
    %523 = llvm.getelementptr %516[%521] : (!llvm.ptr<ptr<i8>>, i32) -> !llvm.ptr<ptr<i8>>
    %524 = llvm.bitcast %522 : !llvm.ptr<ptr<ptr<i8>>> to !llvm.ptr<i8>
    llvm.store %524, %523 : !llvm.ptr<ptr<i8>>
    %525 = llvm.mlir.constant(2 : i32) : i32
    %526 = llvm.getelementptr %514[%512, 2] : (!llvm.ptr<struct<".5", (ptr<i8>, ptr<ptr<i8>>, i64, ptr<i8>, i64, i64, i64, i64, i64, i64, i32, ptr<i8>, i32, ptr<ptr<i8>>)>>, i32) -> !llvm.ptr<i64>
    llvm.store %458, %526 : !llvm.ptr<i64>
    %527 = llvm.getelementptr %516[%525] : (!llvm.ptr<ptr<i8>>, i32) -> !llvm.ptr<ptr<i8>>
    %528 = llvm.bitcast %526 : !llvm.ptr<i64> to !llvm.ptr<i8>
    llvm.store %528, %527 : !llvm.ptr<ptr<i8>>
    %529 = llvm.mlir.constant(3 : i32) : i32
    %530 = llvm.getelementptr %514[%512, 3] : (!llvm.ptr<struct<".5", (ptr<i8>, ptr<ptr<i8>>, i64, ptr<i8>, i64, i64, i64, i64, i64, i64, i32, ptr<i8>, i32, ptr<ptr<i8>>)>>, i32) -> !llvm.ptr<ptr<i8>>
    llvm.store %461, %530 : !llvm.ptr<ptr<i8>>
    %531 = llvm.getelementptr %516[%529] : (!llvm.ptr<ptr<i8>>, i32) -> !llvm.ptr<ptr<i8>>
    %532 = llvm.bitcast %530 : !llvm.ptr<ptr<i8>> to !llvm.ptr<i8>
    llvm.store %532, %531 : !llvm.ptr<ptr<i8>>
    %533 = llvm.mlir.constant(4 : i32) : i32
    %534 = llvm.getelementptr %514[%512, 4] : (!llvm.ptr<struct<".5", (ptr<i8>, ptr<ptr<i8>>, i64, ptr<i8>, i64, i64, i64, i64, i64, i64, i32, ptr<i8>, i32, ptr<ptr<i8>>)>>, i32) -> !llvm.ptr<i64>
    llvm.store %450, %534 : !llvm.ptr<i64>
    %535 = llvm.getelementptr %516[%533] : (!llvm.ptr<ptr<i8>>, i32) -> !llvm.ptr<ptr<i8>>
    %536 = llvm.bitcast %534 : !llvm.ptr<i64> to !llvm.ptr<i8>
    llvm.store %536, %535 : !llvm.ptr<ptr<i8>>
    %537 = llvm.mlir.constant(5 : i32) : i32
    %538 = llvm.getelementptr %514[%512, 5] : (!llvm.ptr<struct<".5", (ptr<i8>, ptr<ptr<i8>>, i64, ptr<i8>, i64, i64, i64, i64, i64, i64, i32, ptr<i8>, i32, ptr<ptr<i8>>)>>, i32) -> !llvm.ptr<i64>
    llvm.store %5, %538 : !llvm.ptr<i64>
    %539 = llvm.getelementptr %516[%537] : (!llvm.ptr<ptr<i8>>, i32) -> !llvm.ptr<ptr<i8>>
    %540 = llvm.bitcast %538 : !llvm.ptr<i64> to !llvm.ptr<i8>
    llvm.store %540, %539 : !llvm.ptr<ptr<i8>>
    %541 = llvm.mlir.constant(6 : i32) : i32
    %542 = llvm.getelementptr %514[%512, 6] : (!llvm.ptr<struct<".5", (ptr<i8>, ptr<ptr<i8>>, i64, ptr<i8>, i64, i64, i64, i64, i64, i64, i32, ptr<i8>, i32, ptr<ptr<i8>>)>>, i32) -> !llvm.ptr<i64>
    llvm.store %5, %542 : !llvm.ptr<i64>
    %543 = llvm.getelementptr %516[%541] : (!llvm.ptr<ptr<i8>>, i32) -> !llvm.ptr<ptr<i8>>
    %544 = llvm.bitcast %542 : !llvm.ptr<i64> to !llvm.ptr<i8>
    llvm.store %544, %543 : !llvm.ptr<ptr<i8>>
    %545 = llvm.mlir.constant(7 : i32) : i32
    %546 = llvm.getelementptr %514[%512, 7] : (!llvm.ptr<struct<".5", (ptr<i8>, ptr<ptr<i8>>, i64, ptr<i8>, i64, i64, i64, i64, i64, i64, i32, ptr<i8>, i32, ptr<ptr<i8>>)>>, i32) -> !llvm.ptr<i64>
    llvm.store %0, %546 : !llvm.ptr<i64>
    %547 = llvm.getelementptr %516[%545] : (!llvm.ptr<ptr<i8>>, i32) -> !llvm.ptr<ptr<i8>>
    %548 = llvm.bitcast %546 : !llvm.ptr<i64> to !llvm.ptr<i8>
    llvm.store %548, %547 : !llvm.ptr<ptr<i8>>
    %549 = llvm.mlir.constant(8 : i32) : i32
    %550 = llvm.getelementptr %514[%512, 8] : (!llvm.ptr<struct<".5", (ptr<i8>, ptr<ptr<i8>>, i64, ptr<i8>, i64, i64, i64, i64, i64, i64, i32, ptr<i8>, i32, ptr<ptr<i8>>)>>, i32) -> !llvm.ptr<i64>
    llvm.store %5, %550 : !llvm.ptr<i64>
    %551 = llvm.getelementptr %516[%549] : (!llvm.ptr<ptr<i8>>, i32) -> !llvm.ptr<ptr<i8>>
    %552 = llvm.bitcast %550 : !llvm.ptr<i64> to !llvm.ptr<i8>
    llvm.store %552, %551 : !llvm.ptr<ptr<i8>>
    %553 = llvm.mlir.constant(9 : i32) : i32
    %554 = llvm.getelementptr %514[%512, 9] : (!llvm.ptr<struct<".5", (ptr<i8>, ptr<ptr<i8>>, i64, ptr<i8>, i64, i64, i64, i64, i64, i64, i32, ptr<i8>, i32, ptr<ptr<i8>>)>>, i32) -> !llvm.ptr<i64>
    llvm.store %5, %554 : !llvm.ptr<i64>
    %555 = llvm.getelementptr %516[%553] : (!llvm.ptr<ptr<i8>>, i32) -> !llvm.ptr<ptr<i8>>
    %556 = llvm.bitcast %554 : !llvm.ptr<i64> to !llvm.ptr<i8>
    llvm.store %556, %555 : !llvm.ptr<ptr<i8>>
    %557 = llvm.mlir.constant(10 : i32) : i32
    %558 = llvm.getelementptr %514[%512, 10] : (!llvm.ptr<struct<".5", (ptr<i8>, ptr<ptr<i8>>, i64, ptr<i8>, i64, i64, i64, i64, i64, i64, i32, ptr<i8>, i32, ptr<ptr<i8>>)>>, i32) -> !llvm.ptr<i32>
    llvm.store %509, %558 : !llvm.ptr<i32>
    %559 = llvm.getelementptr %516[%557] : (!llvm.ptr<ptr<i8>>, i32) -> !llvm.ptr<ptr<i8>>
    %560 = llvm.bitcast %558 : !llvm.ptr<i32> to !llvm.ptr<i8>
    llvm.store %560, %559 : !llvm.ptr<ptr<i8>>
    %561 = llvm.mlir.constant(11 : i32) : i32
    %562 = llvm.getelementptr %514[%512, 11] : (!llvm.ptr<struct<".5", (ptr<i8>, ptr<ptr<i8>>, i64, ptr<i8>, i64, i64, i64, i64, i64, i64, i32, ptr<i8>, i32, ptr<ptr<i8>>)>>, i32) -> !llvm.ptr<ptr<i8>>
    llvm.store %511, %562 : !llvm.ptr<ptr<i8>>
    %563 = llvm.getelementptr %516[%561] : (!llvm.ptr<ptr<i8>>, i32) -> !llvm.ptr<ptr<i8>>
    %564 = llvm.bitcast %562 : !llvm.ptr<ptr<i8>> to !llvm.ptr<i8>
    llvm.store %564, %563 : !llvm.ptr<ptr<i8>>
    %565 = llvm.mlir.constant(12 : i32) : i32
    %566 = llvm.getelementptr %514[%512, 12] : (!llvm.ptr<struct<".5", (ptr<i8>, ptr<ptr<i8>>, i64, ptr<i8>, i64, i64, i64, i64, i64, i64, i32, ptr<i8>, i32, ptr<ptr<i8>>)>>, i32) -> !llvm.ptr<i32>
    llvm.store %510, %566 : !llvm.ptr<i32>
    %567 = llvm.getelementptr %516[%565] : (!llvm.ptr<ptr<i8>>, i32) -> !llvm.ptr<ptr<i8>>
    %568 = llvm.bitcast %566 : !llvm.ptr<i32> to !llvm.ptr<i8>
    llvm.store %568, %567 : !llvm.ptr<ptr<i8>>
    %569 = llvm.mlir.constant(13 : i32) : i32
    %570 = llvm.getelementptr %514[%512, 13] : (!llvm.ptr<struct<".5", (ptr<i8>, ptr<ptr<i8>>, i64, ptr<i8>, i64, i64, i64, i64, i64, i64, i32, ptr<i8>, i32, ptr<ptr<i8>>)>>, i32) -> !llvm.ptr<ptr<ptr<i8>>>
    llvm.store %479, %570 : !llvm.ptr<ptr<ptr<i8>>>
    %571 = llvm.getelementptr %516[%569] : (!llvm.ptr<ptr<i8>>, i32) -> !llvm.ptr<ptr<i8>>
    %572 = llvm.bitcast %570 : !llvm.ptr<ptr<ptr<i8>>> to !llvm.ptr<i8>
    llvm.store %572, %571 : !llvm.ptr<ptr<i8>>
    %573 = llvm.mlir.addressof @ral_kernel_launch___gpu___pvoid_ppvoid_i64_pvoid_i64_i64_i64_i64_i64_i64_i32_pvoid_i32_ppvoid___void : !llvm.ptr<array<101 x i8>>
    %574 = llvm.mlir.constant(0 : index) : i64
    %575 = llvm.getelementptr %573[%574, %574] : (!llvm.ptr<array<101 x i8>>, i64, i64) -> !llvm.ptr<i8>
    llvm.call @disc_ral_call(%arg0, %575, %516) : (!llvm.ptr<i8>, !llvm.ptr<i8>, !llvm.ptr<ptr<i8>>) -> ()
    llvm.br ^bb3
  ^bb3:  // 2 preds: ^bb1, ^bb2
    %576 = llvm.inttoptr %3 : i32 to !llvm.ptr<i8>
    %577 = llvm.mlir.constant(2 : index) : i64
    %578 = llvm.mlir.constant(1 : index) : i64
    %579 = llvm.mlir.null : !llvm.ptr<i64>
    %580 = llvm.getelementptr %579[%577] : (!llvm.ptr<i64>, i64) -> !llvm.ptr<i64>
    %581 = llvm.ptrtoint %580 : !llvm.ptr<i64> to i64
    %582 = llvm.alloca %581 x i64 : (i64) -> !llvm.ptr<i64>
    %583 = llvm.mlir.undef : !llvm.struct<(ptr<i64>, ptr<i64>, i64, array<1 x i64>, array<1 x i64>)>
    %584 = llvm.insertvalue %582, %583[0] : !llvm.struct<(ptr<i64>, ptr<i64>, i64, array<1 x i64>, array<1 x i64>)> 
    %585 = llvm.insertvalue %582, %584[1] : !llvm.struct<(ptr<i64>, ptr<i64>, i64, array<1 x i64>, array<1 x i64>)> 
    %586 = llvm.mlir.constant(0 : index) : i64
    %587 = llvm.insertvalue %586, %585[2] : !llvm.struct<(ptr<i64>, ptr<i64>, i64, array<1 x i64>, array<1 x i64>)> 
    %588 = llvm.insertvalue %577, %587[3, 0] : !llvm.struct<(ptr<i64>, ptr<i64>, i64, array<1 x i64>, array<1 x i64>)> 
    %589 = llvm.insertvalue %578, %588[4, 0] : !llvm.struct<(ptr<i64>, ptr<i64>, i64, array<1 x i64>, array<1 x i64>)> 
    %590 = llvm.extractvalue %589[1] : !llvm.struct<(ptr<i64>, ptr<i64>, i64, array<1 x i64>, array<1 x i64>)> 
    %591 = llvm.getelementptr %590[%4] : (!llvm.ptr<i64>, i64) -> !llvm.ptr<i64>
    llvm.store %29, %591 : !llvm.ptr<i64>
    %592 = llvm.extractvalue %589[1] : !llvm.struct<(ptr<i64>, ptr<i64>, i64, array<1 x i64>, array<1 x i64>)> 
    %593 = llvm.getelementptr %592[%5] : (!llvm.ptr<i64>, i64) -> !llvm.ptr<i64>
    llvm.store %28, %593 : !llvm.ptr<i64>
    %594 = llvm.mlir.constant(0 : i32) : i32
    %595 = llvm.mlir.constant(1 : i32) : i32
    %596 = llvm.extractvalue %68[0] : !llvm.struct<(ptr<f32>, ptr<f32>, i64, array<1 x i64>, array<1 x i64>)> 
    %597 = llvm.extractvalue %68[1] : !llvm.struct<(ptr<f32>, ptr<f32>, i64, array<1 x i64>, array<1 x i64>)> 
    %598 = llvm.extractvalue %68[2] : !llvm.struct<(ptr<f32>, ptr<f32>, i64, array<1 x i64>, array<1 x i64>)> 
    %599 = llvm.extractvalue %68[3, 0] : !llvm.struct<(ptr<f32>, ptr<f32>, i64, array<1 x i64>, array<1 x i64>)> 
    %600 = llvm.extractvalue %68[4, 0] : !llvm.struct<(ptr<f32>, ptr<f32>, i64, array<1 x i64>, array<1 x i64>)> 
    %601 = llvm.extractvalue %589[0] : !llvm.struct<(ptr<i64>, ptr<i64>, i64, array<1 x i64>, array<1 x i64>)> 
    %602 = llvm.extractvalue %589[1] : !llvm.struct<(ptr<i64>, ptr<i64>, i64, array<1 x i64>, array<1 x i64>)> 
    %603 = llvm.extractvalue %589[2] : !llvm.struct<(ptr<i64>, ptr<i64>, i64, array<1 x i64>, array<1 x i64>)> 
    %604 = llvm.extractvalue %589[3, 0] : !llvm.struct<(ptr<i64>, ptr<i64>, i64, array<1 x i64>, array<1 x i64>)> 
    %605 = llvm.extractvalue %589[4, 0] : !llvm.struct<(ptr<i64>, ptr<i64>, i64, array<1 x i64>, array<1 x i64>)> 
    %606 = llvm.alloca %595 x !llvm.struct<".6", (ptr<i8>, ptr<i8>, ptr<f32>, ptr<f32>, i64, i64, i64, ptr<i64>, ptr<i64>, i64, i64, i64, struct<(ptr<f32>, ptr<f32>, i64, array<2 x i64>, array<2 x i64>)>)> : (i32) -> !llvm.ptr<struct<".6", (ptr<i8>, ptr<i8>, ptr<f32>, ptr<f32>, i64, i64, i64, ptr<i64>, ptr<i64>, i64, i64, i64, struct<(ptr<f32>, ptr<f32>, i64, array<2 x i64>, array<2 x i64>)>)>>
    %607 = llvm.mlir.constant(13 : i32) : i32
    %608 = llvm.alloca %607 x !llvm.ptr<i8> : (i32) -> !llvm.ptr<ptr<i8>>
    %609 = llvm.mlir.constant(0 : i32) : i32
    %610 = llvm.getelementptr %606[%594, 0] : (!llvm.ptr<struct<".6", (ptr<i8>, ptr<i8>, ptr<f32>, ptr<f32>, i64, i64, i64, ptr<i64>, ptr<i64>, i64, i64, i64, struct<(ptr<f32>, ptr<f32>, i64, array<2 x i64>, array<2 x i64>)>)>>, i32) -> !llvm.ptr<ptr<i8>>
    llvm.store %arg0, %610 : !llvm.ptr<ptr<i8>>
    %611 = llvm.getelementptr %608[%609] : (!llvm.ptr<ptr<i8>>, i32) -> !llvm.ptr<ptr<i8>>
    %612 = llvm.bitcast %610 : !llvm.ptr<ptr<i8>> to !llvm.ptr<i8>
    llvm.store %612, %611 : !llvm.ptr<ptr<i8>>
    %613 = llvm.mlir.constant(1 : i32) : i32
    %614 = llvm.getelementptr %606[%594, 1] : (!llvm.ptr<struct<".6", (ptr<i8>, ptr<i8>, ptr<f32>, ptr<f32>, i64, i64, i64, ptr<i64>, ptr<i64>, i64, i64, i64, struct<(ptr<f32>, ptr<f32>, i64, array<2 x i64>, array<2 x i64>)>)>>, i32) -> !llvm.ptr<ptr<i8>>
    llvm.store %576, %614 : !llvm.ptr<ptr<i8>>
    %615 = llvm.getelementptr %608[%613] : (!llvm.ptr<ptr<i8>>, i32) -> !llvm.ptr<ptr<i8>>
    %616 = llvm.bitcast %614 : !llvm.ptr<ptr<i8>> to !llvm.ptr<i8>
    llvm.store %616, %615 : !llvm.ptr<ptr<i8>>
    %617 = llvm.mlir.constant(2 : i32) : i32
    %618 = llvm.getelementptr %606[%594, 2] : (!llvm.ptr<struct<".6", (ptr<i8>, ptr<i8>, ptr<f32>, ptr<f32>, i64, i64, i64, ptr<i64>, ptr<i64>, i64, i64, i64, struct<(ptr<f32>, ptr<f32>, i64, array<2 x i64>, array<2 x i64>)>)>>, i32) -> !llvm.ptr<ptr<f32>>
    llvm.store %596, %618 : !llvm.ptr<ptr<f32>>
    %619 = llvm.getelementptr %608[%617] : (!llvm.ptr<ptr<i8>>, i32) -> !llvm.ptr<ptr<i8>>
    %620 = llvm.bitcast %618 : !llvm.ptr<ptr<f32>> to !llvm.ptr<i8>
    llvm.store %620, %619 : !llvm.ptr<ptr<i8>>
    %621 = llvm.mlir.constant(3 : i32) : i32
    %622 = llvm.getelementptr %606[%594, 3] : (!llvm.ptr<struct<".6", (ptr<i8>, ptr<i8>, ptr<f32>, ptr<f32>, i64, i64, i64, ptr<i64>, ptr<i64>, i64, i64, i64, struct<(ptr<f32>, ptr<f32>, i64, array<2 x i64>, array<2 x i64>)>)>>, i32) -> !llvm.ptr<ptr<f32>>
    llvm.store %597, %622 : !llvm.ptr<ptr<f32>>
    %623 = llvm.getelementptr %608[%621] : (!llvm.ptr<ptr<i8>>, i32) -> !llvm.ptr<ptr<i8>>
    %624 = llvm.bitcast %622 : !llvm.ptr<ptr<f32>> to !llvm.ptr<i8>
    llvm.store %624, %623 : !llvm.ptr<ptr<i8>>
    %625 = llvm.mlir.constant(4 : i32) : i32
    %626 = llvm.getelementptr %606[%594, 4] : (!llvm.ptr<struct<".6", (ptr<i8>, ptr<i8>, ptr<f32>, ptr<f32>, i64, i64, i64, ptr<i64>, ptr<i64>, i64, i64, i64, struct<(ptr<f32>, ptr<f32>, i64, array<2 x i64>, array<2 x i64>)>)>>, i32) -> !llvm.ptr<i64>
    llvm.store %598, %626 : !llvm.ptr<i64>
    %627 = llvm.getelementptr %608[%625] : (!llvm.ptr<ptr<i8>>, i32) -> !llvm.ptr<ptr<i8>>
    %628 = llvm.bitcast %626 : !llvm.ptr<i64> to !llvm.ptr<i8>
    llvm.store %628, %627 : !llvm.ptr<ptr<i8>>
    %629 = llvm.mlir.constant(5 : i32) : i32
    %630 = llvm.getelementptr %606[%594, 5] : (!llvm.ptr<struct<".6", (ptr<i8>, ptr<i8>, ptr<f32>, ptr<f32>, i64, i64, i64, ptr<i64>, ptr<i64>, i64, i64, i64, struct<(ptr<f32>, ptr<f32>, i64, array<2 x i64>, array<2 x i64>)>)>>, i32) -> !llvm.ptr<i64>
    llvm.store %599, %630 : !llvm.ptr<i64>
    %631 = llvm.getelementptr %608[%629] : (!llvm.ptr<ptr<i8>>, i32) -> !llvm.ptr<ptr<i8>>
    %632 = llvm.bitcast %630 : !llvm.ptr<i64> to !llvm.ptr<i8>
    llvm.store %632, %631 : !llvm.ptr<ptr<i8>>
    %633 = llvm.mlir.constant(6 : i32) : i32
    %634 = llvm.getelementptr %606[%594, 6] : (!llvm.ptr<struct<".6", (ptr<i8>, ptr<i8>, ptr<f32>, ptr<f32>, i64, i64, i64, ptr<i64>, ptr<i64>, i64, i64, i64, struct<(ptr<f32>, ptr<f32>, i64, array<2 x i64>, array<2 x i64>)>)>>, i32) -> !llvm.ptr<i64>
    llvm.store %600, %634 : !llvm.ptr<i64>
    %635 = llvm.getelementptr %608[%633] : (!llvm.ptr<ptr<i8>>, i32) -> !llvm.ptr<ptr<i8>>
    %636 = llvm.bitcast %634 : !llvm.ptr<i64> to !llvm.ptr<i8>
    llvm.store %636, %635 : !llvm.ptr<ptr<i8>>
    %637 = llvm.mlir.constant(7 : i32) : i32
    %638 = llvm.getelementptr %606[%594, 7] : (!llvm.ptr<struct<".6", (ptr<i8>, ptr<i8>, ptr<f32>, ptr<f32>, i64, i64, i64, ptr<i64>, ptr<i64>, i64, i64, i64, struct<(ptr<f32>, ptr<f32>, i64, array<2 x i64>, array<2 x i64>)>)>>, i32) -> !llvm.ptr<ptr<i64>>
    llvm.store %601, %638 : !llvm.ptr<ptr<i64>>
    %639 = llvm.getelementptr %608[%637] : (!llvm.ptr<ptr<i8>>, i32) -> !llvm.ptr<ptr<i8>>
    %640 = llvm.bitcast %638 : !llvm.ptr<ptr<i64>> to !llvm.ptr<i8>
    llvm.store %640, %639 : !llvm.ptr<ptr<i8>>
    %641 = llvm.mlir.constant(8 : i32) : i32
    %642 = llvm.getelementptr %606[%594, 8] : (!llvm.ptr<struct<".6", (ptr<i8>, ptr<i8>, ptr<f32>, ptr<f32>, i64, i64, i64, ptr<i64>, ptr<i64>, i64, i64, i64, struct<(ptr<f32>, ptr<f32>, i64, array<2 x i64>, array<2 x i64>)>)>>, i32) -> !llvm.ptr<ptr<i64>>
    llvm.store %602, %642 : !llvm.ptr<ptr<i64>>
    %643 = llvm.getelementptr %608[%641] : (!llvm.ptr<ptr<i8>>, i32) -> !llvm.ptr<ptr<i8>>
    %644 = llvm.bitcast %642 : !llvm.ptr<ptr<i64>> to !llvm.ptr<i8>
    llvm.store %644, %643 : !llvm.ptr<ptr<i8>>
    %645 = llvm.mlir.constant(9 : i32) : i32
    %646 = llvm.getelementptr %606[%594, 9] : (!llvm.ptr<struct<".6", (ptr<i8>, ptr<i8>, ptr<f32>, ptr<f32>, i64, i64, i64, ptr<i64>, ptr<i64>, i64, i64, i64, struct<(ptr<f32>, ptr<f32>, i64, array<2 x i64>, array<2 x i64>)>)>>, i32) -> !llvm.ptr<i64>
    llvm.store %603, %646 : !llvm.ptr<i64>
    %647 = llvm.getelementptr %608[%645] : (!llvm.ptr<ptr<i8>>, i32) -> !llvm.ptr<ptr<i8>>
    %648 = llvm.bitcast %646 : !llvm.ptr<i64> to !llvm.ptr<i8>
    llvm.store %648, %647 : !llvm.ptr<ptr<i8>>
    %649 = llvm.mlir.constant(10 : i32) : i32
    %650 = llvm.getelementptr %606[%594, 10] : (!llvm.ptr<struct<".6", (ptr<i8>, ptr<i8>, ptr<f32>, ptr<f32>, i64, i64, i64, ptr<i64>, ptr<i64>, i64, i64, i64, struct<(ptr<f32>, ptr<f32>, i64, array<2 x i64>, array<2 x i64>)>)>>, i32) -> !llvm.ptr<i64>
    llvm.store %604, %650 : !llvm.ptr<i64>
    %651 = llvm.getelementptr %608[%649] : (!llvm.ptr<ptr<i8>>, i32) -> !llvm.ptr<ptr<i8>>
    %652 = llvm.bitcast %650 : !llvm.ptr<i64> to !llvm.ptr<i8>
    llvm.store %652, %651 : !llvm.ptr<ptr<i8>>
    %653 = llvm.mlir.constant(11 : i32) : i32
    %654 = llvm.getelementptr %606[%594, 11] : (!llvm.ptr<struct<".6", (ptr<i8>, ptr<i8>, ptr<f32>, ptr<f32>, i64, i64, i64, ptr<i64>, ptr<i64>, i64, i64, i64, struct<(ptr<f32>, ptr<f32>, i64, array<2 x i64>, array<2 x i64>)>)>>, i32) -> !llvm.ptr<i64>
    llvm.store %605, %654 : !llvm.ptr<i64>
    %655 = llvm.getelementptr %608[%653] : (!llvm.ptr<ptr<i8>>, i32) -> !llvm.ptr<ptr<i8>>
    %656 = llvm.bitcast %654 : !llvm.ptr<i64> to !llvm.ptr<i8>
    llvm.store %656, %655 : !llvm.ptr<ptr<i8>>
    %657 = llvm.mlir.constant(12 : i32) : i32
    %658 = llvm.getelementptr %606[%594, 12] : (!llvm.ptr<struct<".6", (ptr<i8>, ptr<i8>, ptr<f32>, ptr<f32>, i64, i64, i64, ptr<i64>, ptr<i64>, i64, i64, i64, struct<(ptr<f32>, ptr<f32>, i64, array<2 x i64>, array<2 x i64>)>)>>, i32) -> !llvm.ptr<struct<(ptr<f32>, ptr<f32>, i64, array<2 x i64>, array<2 x i64>)>>
    %659 = llvm.getelementptr %608[%657] : (!llvm.ptr<ptr<i8>>, i32) -> !llvm.ptr<ptr<i8>>
    %660 = llvm.bitcast %658 : !llvm.ptr<struct<(ptr<f32>, ptr<f32>, i64, array<2 x i64>, array<2 x i64>)>> to !llvm.ptr<i8>
    llvm.store %660, %659 : !llvm.ptr<ptr<i8>>
    %661 = llvm.mlir.addressof @inc_ref___gpu___pvoid_pvoid_m1df32_m1di64___m2df32 : !llvm.ptr<array<51 x i8>>
    %662 = llvm.mlir.constant(0 : index) : i64
    %663 = llvm.getelementptr %661[%662, %662] : (!llvm.ptr<array<51 x i8>>, i64, i64) -> !llvm.ptr<i8>
    llvm.call @disc_ral_call(%arg0, %663, %608) : (!llvm.ptr<i8>, !llvm.ptr<i8>, !llvm.ptr<ptr<i8>>) -> ()
    %664 = llvm.load %658 : !llvm.ptr<struct<(ptr<f32>, ptr<f32>, i64, array<2 x i64>, array<2 x i64>)>>
    %665 = llvm.mlir.undef : !llvm.struct<(ptr<f32>, ptr<f32>, i64, array<2 x i64>, array<2 x i64>)>
    %666 = llvm.extractvalue %664[0] : !llvm.struct<(ptr<f32>, ptr<f32>, i64, array<2 x i64>, array<2 x i64>)> 
    %667 = llvm.extractvalue %664[1] : !llvm.struct<(ptr<f32>, ptr<f32>, i64, array<2 x i64>, array<2 x i64>)> 
    %668 = llvm.insertvalue %666, %665[0] : !llvm.struct<(ptr<f32>, ptr<f32>, i64, array<2 x i64>, array<2 x i64>)> 
    %669 = llvm.insertvalue %667, %668[1] : !llvm.struct<(ptr<f32>, ptr<f32>, i64, array<2 x i64>, array<2 x i64>)> 
    %670 = llvm.mlir.constant(0 : index) : i64
    %671 = llvm.insertvalue %670, %669[2] : !llvm.struct<(ptr<f32>, ptr<f32>, i64, array<2 x i64>, array<2 x i64>)> 
    %672 = llvm.insertvalue %29, %671[3, 0] : !llvm.struct<(ptr<f32>, ptr<f32>, i64, array<2 x i64>, array<2 x i64>)> 
    %673 = llvm.insertvalue %28, %672[4, 0] : !llvm.struct<(ptr<f32>, ptr<f32>, i64, array<2 x i64>, array<2 x i64>)> 
    %674 = llvm.insertvalue %28, %673[3, 1] : !llvm.struct<(ptr<f32>, ptr<f32>, i64, array<2 x i64>, array<2 x i64>)> 
    %675 = llvm.mlir.constant(1 : index) : i64
    %676 = llvm.insertvalue %675, %674[4, 1] : !llvm.struct<(ptr<f32>, ptr<f32>, i64, array<2 x i64>, array<2 x i64>)> 
    %677 = llvm.extractvalue %68[0] : !llvm.struct<(ptr<f32>, ptr<f32>, i64, array<1 x i64>, array<1 x i64>)> 
    %678 = llvm.bitcast %677 : !llvm.ptr<f32> to !llvm.ptr<i8>
    %679 = llvm.mlir.constant(0 : i32) : i32
    %680 = llvm.mlir.constant(1 : i32) : i32
    %681 = llvm.alloca %680 x !llvm.struct<".7", (ptr<i8>, ptr<i8>)> : (i32) -> !llvm.ptr<struct<".7", (ptr<i8>, ptr<i8>)>>
    %682 = llvm.mlir.constant(2 : i32) : i32
    %683 = llvm.alloca %682 x !llvm.ptr<i8> : (i32) -> !llvm.ptr<ptr<i8>>
    %684 = llvm.mlir.constant(0 : i32) : i32
    %685 = llvm.getelementptr %681[%679, 0] : (!llvm.ptr<struct<".7", (ptr<i8>, ptr<i8>)>>, i32) -> !llvm.ptr<ptr<i8>>
    llvm.store %arg0, %685 : !llvm.ptr<ptr<i8>>
    %686 = llvm.getelementptr %683[%684] : (!llvm.ptr<ptr<i8>>, i32) -> !llvm.ptr<ptr<i8>>
    %687 = llvm.bitcast %685 : !llvm.ptr<ptr<i8>> to !llvm.ptr<i8>
    llvm.store %687, %686 : !llvm.ptr<ptr<i8>>
    %688 = llvm.mlir.constant(1 : i32) : i32
    %689 = llvm.getelementptr %681[%679, 1] : (!llvm.ptr<struct<".7", (ptr<i8>, ptr<i8>)>>, i32) -> !llvm.ptr<ptr<i8>>
    llvm.store %678, %689 : !llvm.ptr<ptr<i8>>
    %690 = llvm.getelementptr %683[%688] : (!llvm.ptr<ptr<i8>>, i32) -> !llvm.ptr<ptr<i8>>
    %691 = llvm.bitcast %689 : !llvm.ptr<ptr<i8>> to !llvm.ptr<i8>
    llvm.store %691, %690 : !llvm.ptr<ptr<i8>>
    %692 = llvm.mlir.addressof @dealloc___gpu___pvoid_pvoid___void : !llvm.ptr<array<35 x i8>>
    %693 = llvm.mlir.constant(0 : index) : i64
    %694 = llvm.getelementptr %692[%693, %693] : (!llvm.ptr<array<35 x i8>>, i64, i64) -> !llvm.ptr<i8>
    llvm.call @disc_ral_call(%arg0, %694, %683) : (!llvm.ptr<i8>, !llvm.ptr<i8>, !llvm.ptr<ptr<i8>>) -> ()
    %695 = llvm.mlir.constant(0 : i32) : i32
    %696 = llvm.mlir.constant(1 : i32) : i32
    %697 = llvm.extractvalue %676[0] : !llvm.struct<(ptr<f32>, ptr<f32>, i64, array<2 x i64>, array<2 x i64>)> 
    %698 = llvm.extractvalue %676[1] : !llvm.struct<(ptr<f32>, ptr<f32>, i64, array<2 x i64>, array<2 x i64>)> 
    %699 = llvm.extractvalue %676[2] : !llvm.struct<(ptr<f32>, ptr<f32>, i64, array<2 x i64>, array<2 x i64>)> 
    %700 = llvm.extractvalue %676[3, 0] : !llvm.struct<(ptr<f32>, ptr<f32>, i64, array<2 x i64>, array<2 x i64>)> 
    %701 = llvm.extractvalue %676[3, 1] : !llvm.struct<(ptr<f32>, ptr<f32>, i64, array<2 x i64>, array<2 x i64>)> 
    %702 = llvm.extractvalue %676[4, 0] : !llvm.struct<(ptr<f32>, ptr<f32>, i64, array<2 x i64>, array<2 x i64>)> 
    %703 = llvm.extractvalue %676[4, 1] : !llvm.struct<(ptr<f32>, ptr<f32>, i64, array<2 x i64>, array<2 x i64>)> 
    %704 = llvm.alloca %696 x !llvm.struct<".8", (ptr<i8>, i64, ptr<f32>, ptr<f32>, i64, i64, i64, i64, i64)> : (i32) -> !llvm.ptr<struct<".8", (ptr<i8>, i64, ptr<f32>, ptr<f32>, i64, i64, i64, i64, i64)>>
    %705 = llvm.mlir.constant(9 : i32) : i32
    %706 = llvm.alloca %705 x !llvm.ptr<i8> : (i32) -> !llvm.ptr<ptr<i8>>
    %707 = llvm.mlir.constant(0 : i32) : i32
    %708 = llvm.getelementptr %704[%695, 0] : (!llvm.ptr<struct<".8", (ptr<i8>, i64, ptr<f32>, ptr<f32>, i64, i64, i64, i64, i64)>>, i32) -> !llvm.ptr<ptr<i8>>
    llvm.store %arg0, %708 : !llvm.ptr<ptr<i8>>
    %709 = llvm.getelementptr %706[%707] : (!llvm.ptr<ptr<i8>>, i32) -> !llvm.ptr<ptr<i8>>
    %710 = llvm.bitcast %708 : !llvm.ptr<ptr<i8>> to !llvm.ptr<i8>
    llvm.store %710, %709 : !llvm.ptr<ptr<i8>>
    %711 = llvm.mlir.constant(1 : i32) : i32
    %712 = llvm.getelementptr %704[%695, 1] : (!llvm.ptr<struct<".8", (ptr<i8>, i64, ptr<f32>, ptr<f32>, i64, i64, i64, i64, i64)>>, i32) -> !llvm.ptr<i64>
    llvm.store %4, %712 : !llvm.ptr<i64>
    %713 = llvm.getelementptr %706[%711] : (!llvm.ptr<ptr<i8>>, i32) -> !llvm.ptr<ptr<i8>>
    %714 = llvm.bitcast %712 : !llvm.ptr<i64> to !llvm.ptr<i8>
    llvm.store %714, %713 : !llvm.ptr<ptr<i8>>
    %715 = llvm.mlir.constant(2 : i32) : i32
    %716 = llvm.getelementptr %704[%695, 2] : (!llvm.ptr<struct<".8", (ptr<i8>, i64, ptr<f32>, ptr<f32>, i64, i64, i64, i64, i64)>>, i32) -> !llvm.ptr<ptr<f32>>
    llvm.store %697, %716 : !llvm.ptr<ptr<f32>>
    %717 = llvm.getelementptr %706[%715] : (!llvm.ptr<ptr<i8>>, i32) -> !llvm.ptr<ptr<i8>>
    %718 = llvm.bitcast %716 : !llvm.ptr<ptr<f32>> to !llvm.ptr<i8>
    llvm.store %718, %717 : !llvm.ptr<ptr<i8>>
    %719 = llvm.mlir.constant(3 : i32) : i32
    %720 = llvm.getelementptr %704[%695, 3] : (!llvm.ptr<struct<".8", (ptr<i8>, i64, ptr<f32>, ptr<f32>, i64, i64, i64, i64, i64)>>, i32) -> !llvm.ptr<ptr<f32>>
    llvm.store %698, %720 : !llvm.ptr<ptr<f32>>
    %721 = llvm.getelementptr %706[%719] : (!llvm.ptr<ptr<i8>>, i32) -> !llvm.ptr<ptr<i8>>
    %722 = llvm.bitcast %720 : !llvm.ptr<ptr<f32>> to !llvm.ptr<i8>
    llvm.store %722, %721 : !llvm.ptr<ptr<i8>>
    %723 = llvm.mlir.constant(4 : i32) : i32
    %724 = llvm.getelementptr %704[%695, 4] : (!llvm.ptr<struct<".8", (ptr<i8>, i64, ptr<f32>, ptr<f32>, i64, i64, i64, i64, i64)>>, i32) -> !llvm.ptr<i64>
    llvm.store %699, %724 : !llvm.ptr<i64>
    %725 = llvm.getelementptr %706[%723] : (!llvm.ptr<ptr<i8>>, i32) -> !llvm.ptr<ptr<i8>>
    %726 = llvm.bitcast %724 : !llvm.ptr<i64> to !llvm.ptr<i8>
    llvm.store %726, %725 : !llvm.ptr<ptr<i8>>
    %727 = llvm.mlir.constant(5 : i32) : i32
    %728 = llvm.getelementptr %704[%695, 5] : (!llvm.ptr<struct<".8", (ptr<i8>, i64, ptr<f32>, ptr<f32>, i64, i64, i64, i64, i64)>>, i32) -> !llvm.ptr<i64>
    llvm.store %700, %728 : !llvm.ptr<i64>
    %729 = llvm.getelementptr %706[%727] : (!llvm.ptr<ptr<i8>>, i32) -> !llvm.ptr<ptr<i8>>
    %730 = llvm.bitcast %728 : !llvm.ptr<i64> to !llvm.ptr<i8>
    llvm.store %730, %729 : !llvm.ptr<ptr<i8>>
    %731 = llvm.mlir.constant(6 : i32) : i32
    %732 = llvm.getelementptr %704[%695, 6] : (!llvm.ptr<struct<".8", (ptr<i8>, i64, ptr<f32>, ptr<f32>, i64, i64, i64, i64, i64)>>, i32) -> !llvm.ptr<i64>
    llvm.store %701, %732 : !llvm.ptr<i64>
    %733 = llvm.getelementptr %706[%731] : (!llvm.ptr<ptr<i8>>, i32) -> !llvm.ptr<ptr<i8>>
    %734 = llvm.bitcast %732 : !llvm.ptr<i64> to !llvm.ptr<i8>
    llvm.store %734, %733 : !llvm.ptr<ptr<i8>>
    %735 = llvm.mlir.constant(7 : i32) : i32
    %736 = llvm.getelementptr %704[%695, 7] : (!llvm.ptr<struct<".8", (ptr<i8>, i64, ptr<f32>, ptr<f32>, i64, i64, i64, i64, i64)>>, i32) -> !llvm.ptr<i64>
    llvm.store %702, %736 : !llvm.ptr<i64>
    %737 = llvm.getelementptr %706[%735] : (!llvm.ptr<ptr<i8>>, i32) -> !llvm.ptr<ptr<i8>>
    %738 = llvm.bitcast %736 : !llvm.ptr<i64> to !llvm.ptr<i8>
    llvm.store %738, %737 : !llvm.ptr<ptr<i8>>
    %739 = llvm.mlir.constant(8 : i32) : i32
    %740 = llvm.getelementptr %704[%695, 8] : (!llvm.ptr<struct<".8", (ptr<i8>, i64, ptr<f32>, ptr<f32>, i64, i64, i64, i64, i64)>>, i32) -> !llvm.ptr<i64>
    llvm.store %703, %740 : !llvm.ptr<i64>
    %741 = llvm.getelementptr %706[%739] : (!llvm.ptr<ptr<i8>>, i32) -> !llvm.ptr<ptr<i8>>
    %742 = llvm.bitcast %740 : !llvm.ptr<i64> to !llvm.ptr<i8>
    llvm.store %742, %741 : !llvm.ptr<ptr<i8>>
    %743 = llvm.mlir.addressof @ral_send_output___cpu___pvoid_i64_m2df32___void : !llvm.ptr<array<48 x i8>>
    %744 = llvm.mlir.constant(0 : index) : i64
    %745 = llvm.getelementptr %743[%744, %744] : (!llvm.ptr<array<48 x i8>>, i64, i64) -> !llvm.ptr<i8>
    llvm.call @disc_ral_call(%arg0, %745, %706) : (!llvm.ptr<i8>, !llvm.ptr<i8>, !llvm.ptr<ptr<i8>>) -> ()
    llvm.return
  }
}


===-------------------------------------------------------------------------===
                         ... Execution time report ...
===-------------------------------------------------------------------------===
  Total Execution Time: 0.8074 seconds

  ----Wall Time----  ----Name----
    0.0004 (  0.0%)  Inliner
    0.0000 (  0.0%)    (A) CallGraph
    0.0001 (  0.0%)  'func.func' Pipeline
    0.0001 (  0.0%)    Canonicalizer
    0.0004 (  0.0%)  'func.func' Pipeline
    0.0000 (  0.0%)    MhloDecompositionRewriterPass
    0.0000 (  0.0%)    RemoveShapeConstraintsPass
    0.0001 (  0.0%)    Canonicalizer
    0.0000 (  0.0%)    CSE
    0.0000 (  0.0%)      (A) DominanceInfo
    0.0001 (  0.0%)    Canonicalizer
    0.0000 (  0.0%)    DiscTranformWeightDataLayoutForWeightOnlyQuantPass
    0.0001 (  0.0%)    Canonicalizer
    0.0000 (  0.0%)    DiscCustomCallRewriterPass
    0.0000 (  0.0%)    DiscConvertFakeQuantOpPass
    0.0000 (  0.0%)    DiscLowerGpuQuantizeAndDequantizePass
    0.0000 (  0.0%)    ConvertShapeToStandardPass
    0.0014 (  0.2%)  DiscShapeOptimizationPass
    0.0015 (  0.2%)  'builtin.func' Pipeline
    0.0000 (  0.0%)    CSE
    0.0000 (  0.0%)      (A) DominanceInfo
    0.0014 (  0.2%)    CSE
    0.0000 (  0.0%)      (A) DominanceInfo
    0.0004 (  0.0%)  'func.func' Pipeline
    0.0000 (  0.0%)    ConvertTensorToStandardPass
    0.0000 (  0.0%)    ConvertHloToStandardPass
    0.0001 (  0.0%)    Canonicalizer
    0.0000 (  0.0%)    CSE
    0.0000 (  0.0%)      (A) DominanceInfo
    0.0001 (  0.0%)    Canonicalizer
    0.0000 (  0.0%)    DiscAlgebraicSimplifierPass
    0.0000 (  0.0%)    SplitLargeOpsPass
    0.0000 (  0.0%)    DotRewriterPass
    0.0011 (  0.1%)  DiscShapeOptimizationPass
    0.0001 (  0.0%)  'func.func' Pipeline
    0.0001 (  0.0%)    DiscDotMergePass
    0.0011 (  0.1%)  DiscShapeOptimizationPass
    0.0007 (  0.1%)  'func.func' Pipeline
    0.0006 (  0.1%)    HloCanonicalizeReductionPass
    0.0030 (  0.4%)  DiscShapeOptimizationPass
    0.0007 (  0.1%)  DiscMarkShapeCalculationPass
    0.0010 (  0.1%)  PlaceOpsPass
    0.0004 (  0.0%)  'func.func' Pipeline
    0.0002 (  0.0%)    Canonicalizer
    0.0000 (  0.0%)    CSE
    0.0000 (  0.0%)      (A) DominanceInfo
    0.0001 (  0.0%)    Canonicalizer
    0.0000 (  0.0%)    ElementTypeConverterPass
    0.0019 (  0.2%)  DiscShapeOptimizationPass
    0.0002 (  0.0%)  'func.func' Pipeline
    0.0001 (  0.0%)    ReductionRewriterPass
    0.0000 (  0.0%)    ConvRewriterPass
    0.0000 (  0.0%)    ConvRewriterPass
    0.0000 (  0.0%)    QuantizedDotRewriterPass
    0.0024 (  0.3%)  DiscShapeOptimizationPass
    0.0018 (  0.2%)  'func.func' Pipeline
    0.0002 (  0.0%)    Canonicalizer
    0.0000 (  0.0%)    CSE
    0.0000 (  0.0%)      (A) DominanceInfo
    0.0001 (  0.0%)    Canonicalizer
    0.0014 (  0.2%)    TransposeSimplifierPass
    0.0000 (  0.0%)    GpuConvPaddingLegalizationPass
    0.0018 (  0.2%)  DiscShapeOptimizationPass
    0.0001 (  0.0%)  'func.func' Pipeline
    0.0001 (  0.0%)    DiscAlgebraicSimplifierPass
    0.0020 (  0.3%)  DiscShapeOptimizationPass
    0.0012 (  0.1%)  'func.func' Pipeline
    0.0009 (  0.1%)    Canonicalizer
    0.0000 (  0.0%)    CSE
    0.0000 (  0.0%)      (A) DominanceInfo
    0.0003 (  0.0%)    Canonicalizer
    0.0011 (  0.1%)  FuncBufferize
    0.0013 (  0.2%)  DiscHloLegalizeToLhloPass
    0.0027 (  0.3%)  HloLegalizeToLhloPass
    0.0016 (  0.2%)  'func.func' Pipeline
    0.0016 (  0.2%)    Canonicalizer
    0.0002 (  0.0%)  DiscLhloRewriterPass
    0.0057 (  0.7%)  'func.func' Pipeline
    0.0002 (  0.0%)    Canonicalizer
    0.0001 (  0.0%)    ConvertShapeToStandardPass
    0.0002 (  0.0%)    Canonicalizer
    0.0019 (  0.2%)    CSE
    0.0000 (  0.0%)      (A) DominanceInfo
    0.0003 (  0.0%)    Canonicalizer
    0.0018 (  0.2%)    LegalizeToTensorOpPass
    0.0011 (  0.1%)    Canonicalizer
    0.0001 (  0.0%)    StdBufferizePass
    0.0001 (  0.0%)  ArithBufferize
    0.0044 (  0.5%)  'func.func' Pipeline
    0.0010 (  0.1%)    TensorBufferize
    0.0001 (  0.0%)    FinalizingBufferize
    0.0011 (  0.1%)    Canonicalizer
    0.0009 (  0.1%)    CSE
    0.0000 (  0.0%)      (A) DominanceInfo
    0.0002 (  0.0%)    Canonicalizer
    0.0011 (  0.1%)    DiscMemrefCanonicalizer
    0.0013 (  0.2%)  DiscAssignMemorySpacePass
    0.0085 (  1.1%)  'func.func' Pipeline
    0.0001 (  0.0%)    DiscDuplicateComputationForFusionPass
    0.0010 (  0.1%)    PromoteBuffersToStack
    0.0001 (  0.0%)    DiscMemRefLoadStoreSimplifierPass
    0.0014 (  0.2%)    DiscFusionPass
    0.0000 (  0.0%)    DiscFuseSplatConstPass
    0.0016 (  0.2%)    DiscSpecializeFusionWithSpeculationPass
    0.0015 (  0.2%)    Canonicalizer
    0.0001 (  0.0%)    CSE
    0.0000 (  0.0%)      (A) DominanceInfo
    0.0002 (  0.0%)    Canonicalizer
    0.0002 (  0.0%)    Canonicalizer
    0.0000 (  0.0%)    CSE
    0.0000 (  0.0%)      (A) DominanceInfo
    0.0002 (  0.0%)    Canonicalizer
    0.0019 (  0.2%)    BufferDeallocation
    0.0001 (  0.0%)    DiscBufferDeallocationPass
    0.0034 (  0.4%)  RalInjectExecutionContextPass
    0.0024 (  0.3%)  'func.func' Pipeline
    0.0024 (  0.3%)    DiscLowerToLibraryCallPass
    0.0002 (  0.0%)  DiscConstToRALPass
    0.0576 (  7.1%)  'func.func' Pipeline
    0.0001 (  0.0%)    DiscMemRefLoadStoreSimplifierPass
    0.0033 (  0.4%)    DiscLhloLegalizeRootsToParallelLoopsPass
    0.0030 (  0.4%)    ExpandOps
    0.0003 (  0.0%)    UnhandledAtomicRMWConverterPass
    0.0034 (  0.4%)    InputInlineFusionPass
    0.0002 (  0.0%)    ForLoopUnrollInterleave
    0.0031 (  0.4%)    ArithExpandOps
    0.0056 (  0.7%)    DiscBF16ExpansionPass
    0.0004 (  0.1%)    FoldMemRefAliasOps
    0.0050 (  0.6%)    DiscFlattenMemrefAccessPass
    0.0043 (  0.5%)    Canonicalizer
    0.0032 (  0.4%)    CSE
    0.0000 (  0.0%)      (A) DominanceInfo
    0.0005 (  0.1%)    Canonicalizer
    0.0005 (  0.1%)    DiscMemRefCSEPass
    0.0036 (  0.4%)    ConvertShapeToStandardPass
    0.0051 (  0.6%)    Canonicalizer
    0.0002 (  0.0%)    CSE
    0.0000 (  0.0%)      (A) DominanceInfo
    0.0005 (  0.1%)    Canonicalizer
    0.0028 (  0.3%)    ParallelLoopCollapsing
    0.0034 (  0.4%)    SCFParallelLoopTiling
    0.0039 (  0.5%)    GpuMapParallelLoopsPass
    0.0051 (  0.6%)    ConvertParallelLoopToGpu
    0.0003 (  0.0%)  'func' Pipeline
    0.0003 (  0.0%)    CSE
    0.0000 (  0.0%)      (A) DominanceInfo
    0.0062 (  0.8%)  GpuLaunchSinkIndexComputations
    0.0056 (  0.7%)  GpuKernelOutlining
    0.0058 (  0.7%)  AssignKernelNamePass
    0.0029 (  0.4%)  'func.func' Pipeline
    0.0029 (  0.4%)    LhloFusionInlinerPass
    0.0005 (  0.1%)  DiscCompIntensFusionToCUDASourcePass
    0.0056 (  0.7%)  ReviseGpuKernelOutliningPass
    0.5358 ( 66.4%)  'gpu.module' Pipeline
    0.0005 (  0.1%)    LoopInvariantCodeMotion
    0.0036 (  0.4%)    'gpu.func' Pipeline
    0.0035 (  0.4%)      SideEffectLoopInvariantCodeMotionPass
    0.0002 (  0.0%)    LoopInvariantCodeMotion
    0.0029 (  0.4%)    CSE
    0.0000 (  0.0%)      (A) DominanceInfo
    0.0035 (  0.4%)    SCFToControlFlow
    0.0037 (  0.5%)    ConvertAffineToStandard
    0.0029 (  0.4%)    StripDebugInfo
    0.0086 (  1.1%)    DiscLowerGpuOpsToNVVMOpsPass
    0.0036 (  0.4%)    'llvm.func' Pipeline
    0.0036 (  0.4%)      LLVMInsertValueSimplifierPass
    0.0046 (  0.6%)    FunctionDeadArgumentEliminationPass
    0.5015 ( 62.1%)    GpuKernelToBlobPass
    0.0004 (  0.0%)  DiscGPUSourceToLibPass
    0.0025 (  0.3%)  'func.func' Pipeline
    0.0019 (  0.2%)    Canonicalizer
    0.0001 (  0.0%)    CSE
    0.0000 (  0.0%)      (A) DominanceInfo
    0.0002 (  0.0%)    Canonicalizer
    0.0000 (  0.0%)    RemoveDeadBufferPass
    0.0002 (  0.0%)    LinalgLowerToLoops
    0.0203 (  2.5%)  SCFToControlFlow
    0.0021 (  0.3%)  'func.func' Pipeline
    0.0002 (  0.0%)    ExpandStridedMetadata
    0.0015 (  0.2%)    Canonicalizer
    0.0001 (  0.0%)    CSE
    0.0000 (  0.0%)      (A) DominanceInfo
    0.0003 (  0.0%)    Canonicalizer
    0.0215 (  2.7%)  ConvertAffineToStandard
    0.0219 (  2.7%)  StripDebugInfo
    0.0206 (  2.5%)  DiscStripShapeConstraintOpsPass
    0.0363 (  4.5%)  DiscToLLVMPass
    0.0050 (  0.6%)  Rest
    0.8074 (100.0%)  Total
[DISC] LowerHLOToLLVM takes: 8.084980e-01 s.
before optimize llvm module:
; ModuleID = 'LLVMDialectModule'
source_filename = "LLVMDialectModule"

%0 = type { ptr, i64, { ptr, ptr, i64, [3 x i64], [3 x i64] } }
%.1 = type { ptr, i64, ptr }
%.9 = type { i64, i64, ptr }
%.10 = type { ptr, ptr, i64, ptr, i64, i64, i64, i64, i64, i64, i32, ptr, i32, ptr }
%.11 = type { i64, ptr, i64, i64, i64, i64, ptr }
%.12 = type { ptr, ptr, i64, ptr, i64, i64, i64, i64, i64, i64, i32, ptr, i32, ptr }
%.2 = type { i64, i64, ptr }
%.3 = type { ptr, ptr, i64, ptr, i64, i64, i64, i64, i64, i64, i32, ptr, i32, ptr }
%.4 = type { i64, ptr, i64, i64, i64, i64, ptr }
%.5 = type { ptr, ptr, i64, ptr, i64, i64, i64, i64, i64, i64, i32, ptr, i32, ptr }
%.6 = type { ptr, ptr, ptr, ptr, i64, i64, i64, ptr, ptr, i64, i64, i64, { ptr, ptr, i64, [2 x i64], [2 x i64] } }
%.7 = type { ptr, ptr }
%.8 = type { ptr, i64, ptr, ptr, i64, i64, i64, i64, i64 }

@main_kernel_0_main_kColReduction_reduce__4_1_0___flat_1_kernel_name = internal constant [42 x i8] c"main_kColReduction_reduce__4_1_0___flat_1\00"
@main_kernel_0_blob_gpu.binary = internal constant [2720 x i8] c"P\EDU\BA\01\00\10\00\90\0A\00\00\00\00\00\00\02\00\01\01@\00\00\00P\0A\00\00\00\00\00\00P\0A\00\00\00\00\00\00\07\00\01\00P\00\00\00\00\00\00\00\00\00\00\00\11 \00\00\00\00\00\00\00\00\00\00\00\00\00\00\E8\1C\00\00\00\00\00\00\A2\7FELF\02\01\013\07\00\01\00f\02\00\BE\00q\00\01\00!@\1C\07\001\00\80\19\07\00\F5\0E\00P\05P\00@\008\00\03\00@\00\0B\00\01\00\00.shstrtab\00.\08\00'ym\08\00\F0\1B_shndx\00.nv.info\00.text.main_kColReduction_r\0A\00\F5\02e__4_1_0___flat_19\00\0F3\00\1Coshared5\00\19Orela\9D\00\1E?rel\D1\00!\9Fconstant08\00\19\B2debug_framey\00\09\11\00!nv\14\00\11aD\00\0F\98\01 \0F\89\00\16\0F\C2\01\FF(o_param\C9\01\1C\0F\01\00\0B\8C\\\00\00\00\03\00\0A\00\01\00 ]\01\18\00,\09\00\01\00\11\9C\18\00,\04\00\01\00\11\BA\18\00,\07\00\01\00f2\00\00\00\12\10`\00!\80\11\07\00v\00\FF\FF\FF\FF(\00\0C\00\00\01\00@\03\00\04|\08\00\D1\0F\0C\81\80\80(\00\08\FF\81\80(\08\0B\00\06(\00/0\00\01\00\03\13pX\00\10\04\9B\00R\04\14\00\00\00E\00\22\04 \AC\00\90\04/\08\00\05\00\00\00\16\0C\00\12#\0C\00\00\01\00'\04\12\0C\00\17\11\0C\00!7\04\C8\04\F1\08\015\00\00\04\0A\08\00\02\00\00\00`\01(\00\03\19(\00\04\17\0C$\00u\06\00 \00\00\F0!\10\00u\05\00\1C\00\00\F0\11\10\009\04\00\18\10\009\03\00\14\10\009\02\00\10\10\009\01\00\08P\00\01\01\00\F2\0A\F0\11\00\03\1B\FF\00\04\1C\0C\00P\00\00\00\10\02\00\00\E0\10\00\00\04\1E\D8\00#K\00\01\00\B1\02\02\08\10\0A/\22\00\00\00\08\10\00#\00\08\08\00\13\10\08\00\13\18\08\00\13 \08\00\13(\08\00\130\08\00\228\08\84\00\13\00\08\00\13\08\08\00\13\10\08\00\13\18\08\00\13 \08\00\13(\08\00\130\08\00\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\A2\00\00\00\14,\00\00\00H\00\01\00\00x\01/\05\00\01\00\FF\88@$v\01\FF?\04\B2\FF\00\8E\07\00\C4\0F\00\19y\00\01\00\10%c\02Q\0E\00\19y\03\0F\00 \00!-\00@\0E\00$z]\04\B0\00\00\03\02\8E\07\00\CA\1F\00\0C\10\00\C5^\00\00p`\F0\03\00\DA\0F\00Ms\04\F0\0C\80\03\00\EA\0F\00\06{\04\00\00_\00\00\00\90 \00\00\22\0E\00\19x\05\FF\1F\1F\00\D3\14\01\00\00\E2\0F\00$r\02\FF\FF\00\80\00 \E2\0FP\00\10\FF0\00\B1pP\F4\03\00\E4\0F\00\11r\05?\02\B2\FFH\8F\07\00\C6\0F\00\08s\04\BE\04\10\10\90\00\F1\07\1E\00\10x\03\04\FE\FF\FF\0F\FF\E0\FF\07\00\CC\1F\00\05s\03\00\B2\00\22\F0!\B0\00!r\07`\00\C0\03\0A\8E\07\00\C8\1F\00$z\07\07`\00\10\FF\C0\00\81\C8\0F\00'r\07\03\07\A8\02\02\80\00@\19x\02\FF\C3\00\10\05\A0\00\E1\E4\0F\00\12x\05\05\00\FE\FF\FF\FF\C0\8E\80\00T'r\07\07\02\B0\00\10\C8\C0\00\11\09`\00#\07\0A\10\000z\03\09`\00\11\02 \01\01\D0\006\03\00_ \010\10\0A\03\10\00\11\80\B0\00\80\E4\0F\00\10\08\07\07\01P\00\03\10\00\060\00\12\F20\00\1A\18 \001\12\AA\07 \01\22\FF3`\00;$r\03\80\00%\02\03\80\00`\E4\0F\00$x\03\14\04$\00\05 \000x\03\02\95\03\18\03\A0\00\1FX\C0\01\07W$x\02\07 \A0\01\81\B9z\04\00\00F\00\00s\06\00\C0\01Dt\00\FF\04 \00\11\C6P\00\10\02\95\060pb\F8\C0\01U\04\10x\06\02\C0\00\11\C8 \00\12\06 \00\C3\FA\03\00\CE\0F\00$\CA\05\02\00X\90\00\90\C8\0F\00%\C6\04\05\00Z\91\01\03\B0\005\DA\07\06 \00a\E2\0F\00\81\C9\0A#\06\C5\00\19\1E\0C\00\A6\00\00%\D6\06\070\00u\CA\0F\00\81\D9\0D\06 \00\86\E2\02\00\10x\08\02\02@\01j\04\10x\10\02\03p\01\12\08\90\00#\F0\03\10\00\12\10\10\00\11\F2\10\00Y\10x\0C\02\04@\004\0E\02\05\10\00\11\C40\00\12\0C0\00 \F4\03\80\00E$\8A\09\08\B0\00w\E4\0F\10$\9A\0B\10\10\00U\00%\86\08\09\B0\00\02\F0\02\12\0E@\00 \F6\03\F0\00E\81\89\05\08\C0\00v\22\11\00%\96\06\0B0\00 /\00\C0\00\14\06\80\00w\C6\0F\00$\AA\0B\0C \01F\10\81\99\0F\00\01\B4\22\03\00$t\04\FF\00\00\80\FF\C0\01\00\D0\00\14\07@\00\00\C0\015\BA\09\0E@\00W\C8\1F\00%\B6\A0\00g\CC\0F\00\81\B9\09\90\00@\01\00\0B\C8\FA\01\D2\80\FF\00\C2\FC\03\00\C8O\00\08\C8\04\10\000\02\00\03P\007%\A6\0A\B0\00\08P\01\01\10\02\F5\06\00\0B\D2\00\04\0D\00\00@\00`\FC\03\00\C6\8F\00\81\A9\0B\0A`\00b\E2\04\00\08\D2\04 \00!\00\000\00\06`\01\12\FA`\01&\CA\07P\01z\C8/\00%\C6\06\070\02*\0D\0C0\02\16\11\10\01y\E6\02\00%\D6\0C\0D\E0\006\D9\0D\0C\80\00t\00\00\0B\82\00\04\05\A0\00\84\E4\0F\09\10x\0A\02\080\01\8A\E4O\00\08\82\04\04\05\A0\00\18\0A@\02V\0B\92\00\04\0F@\00\01\80\02\14\09@\00\8A\C4\1F\00\08\92\04\04\0F@\00\1A\08p\028\0E\02\0A\E0\016\8A\07\0A\D0\00\11/\C0\01\17\0B \00+%\86\00\03&\89\05\F0\00!\A2\000\02\14\0C0\00\00\A0\00D\A2\00\04\0B\A0\00\8A\C8\8F\00\08\A2\04\04\0B\90\00\15\0E\D0\02\000\00D\B2\00\04\090\00\000\02&\9A\0B\E0\02\00\90\05D\B2\04\04\09@\00\00\00\02(\96\08\00\02\16\08\C0\01\12\F6P\00D\C2\00\04\11P\00x\E4\0F\08\81\99\0F\08\B0\03H$\AA\0B\0E\C0\01Z\08\C2\04\04\11\A0\00\06P\02\0Fp\02\00\00@\01\17\0D\A0\03\09p\02f\E2\0F\08$\BA\07 \02*\E4\1F\80\02.\22\01\80\02\05\10\01\12\FAp\00\1B\B6p\028\CA\09\10\A0\01'\81\B9p\02{&\03\00%\C6\08\09\A0\02\1A\0E\A0\02\16\13\00\01/&\01\A0\02\0B+\22\03\A0\02 O\08\A0\02\15\0E\E0\01\1F\1F\A0\02\0E\00 \01\1B\0F\10\05\17\100\01\00`\02\14\11\10\00*\CE/\80\02\1F\C8p\02\0F\09\00\03/\C8\8F\F0\02\01\18\0E\F0\02\0A\90\02/\0F\01\90\02\00\1C\10\90\02\05@\02\00 \03)\9A\09P\01'\08\B20\02\11\C6\90\02\06\80\01\0F\90\02\04\14\13P\000\E4\0F\08 \01\14\12\00\01\11\E2`\05\18\10`\00\0C\B0\02\00\A0\02\1E\13\E0\00\0F\A0\02\06\09\90\02\01p\018\10\02\13p\00\08\A0\02*\E2\1F\90\02-\E2\0F\B0\02\06 \01\0F\A0\02\06\1F\0E\A0\02\1C\1F\10\A0\02=\1F\14\A0\02\1C\1B\15\A0\02\1B\16\A0\02\1F\17\A0\02\A7\19\10\A0\02\02@\00\06\90\02\1D\E2\B0\02\0F\A0\02\02\14\18\00\01\0F\A0\02\1D\1F\C4\A0\02\0E\19\08\A0\02\01p\00?\10\02\19\A0\02U\10\E4\A0\02\16\07\B0\01\1F$\A0\02\11\1F\11\A0\02.\1F\1A\A0\02\1A5\06\02\1B0\00\11/\10\0A\1B\1C\A0\02\14\1D\10\00\11\CE\10\0A\0B\A0\02+\08\09\A0\02\1C\08\B0\07\19\1E\00\036\02\02\1F\10\00\0F\C0\02\11\1F\06\C0\02\1C\1C\0E\C0\02\19\07\C0\02,\0B\06`\05\14\07@\00+\C6\0F\A0\0A\0F\F0\07\0C\00\80\07\08\E0\07\10\E4\B0\02,\07\06\B0\02\1E\11`\03\0FP\05\13\11\C6\D0\0A\0F\A0\02\18\08\B0\0C\04\A0\02\0Bp\02\17\CAp\02\1B\E4\10\0B\10$\B0\0C)\0C\0D0\009\DA\0F\020\009\C9\0D\0C0\00:\D6\0E\0F\A0\02(\0F\0E\90\00g%v\02\03\00` \025y\06\02 \00*b!\C0\02*\C8O\B0\02\12\C8 \02\02\C0\01\15\F0 \02\03\B0\018\00\00\C8\10\02\1C\F0\10\02\02 \00\05\F0\09\01 \00\08\E0\09\02 \00\14\C2p\01\03 \00\15\C2`\01\02 \00\14\D2\A0\02\03 \00\15\D2\A0\02\10\00`\0C2\0Br\00\F0\01 @\F0p\017\0EFyp\0ES\E6\0F\00\08r\10\02\00\01\00\80\CC\0F\00\A9s\07\02\06\E0\0E\B0\E1\1E\00\00\A4\0E\00\0Cr\00\07\10\00 pR@\00RO\00$r\06\00\0F\00\E0\0C\80\D8\0F\00G\09\00\00\90i\14!\FF\83\C0\0E*My\D0\0ETGy\00\00\F0 \00f\C0\0F\00\18y\00\01\00\0F\10\00`\0F\01\00-\00\\\0F.\03\00\01\00\02q\01]\00\00\98\01\000\00\08\01\00\1F\0B@\00\04\13\D8)\00\1F\C9@\00\0C\13\13\E4\13\0C\01\00\13\A8U\00\11\90\06\00\02$\00#\04\00]\14\00\CE\14\12\00\01\00.k\01T\00\00\01\00\138\05\02/p\00\80\00\0B\1F)'\00\03#\00\A8@\00\04p\16\04\E4\00*\04\00\01\00\1Fb@\00\04\13\D8)\00&\A8\00@\00\1F\0A@\00\00!\89\01D\01\0D@\00\11\80\B6\12J\00\00\D8\00\01\00\1B\08\08\00?x\01\00\A6\17\000\00\00X\E5\02\03\E7\12\04\80\00\17\048\00\04\18\00\133@\01\0C\84\01\13h@\00\17\881\01\0F\C0\00\01\132T\01+\06\00\01\00\1A\08\F0\17\12\03\C8\15:\16\80\00\01\00\13\06\E0\15\04(\1C\0D\88\01\1A\00\08\00\04\C0\00\13\018\00\04\A8\00\0C\01\009\18\13\00\08\00\088\00/\06\00\01\00\17\80\08\00\00\00\00\00\00\00"
@main_kernel_main_kColReduction_reduce__4_1_0___flat_kernel_name = internal constant [40 x i8] c"main_kColReduction_reduce__4_1_0___flat\00"
@main_kernel_blob_gpu.binary = internal constant [1096 x i8] c"P\EDU\BA\01\00\10\008\04\00\00\00\00\00\00\02\00\01\01@\00\00\00\F8\03\00\00\00\00\00\00\F7\03\00\00\00\00\00\00\07\00\01\00P\00\00\00\00\00\00\00\00\00\00\00\11 \00\00\00\00\00\00\00\00\00\00\00\00\00\00\E8\0B\00\00\00\00\00\00\A2\7FELF\02\01\013\07\00\01\00f\02\00\BE\00q\00\01\00!@\0B\07\001\00\80\08\07\00\F5\0E\00P\05P\00@\008\00\03\00@\00\0B\00\01\00\00.shstrtab\00.\08\00'ym\08\00\F0\1B_shndx\00.nv.info\00.text.main_kColReduction_r\0A\00\F5\00e__4_1_0___flat7\00\0F1\00\1Aoshared3\00\1A\9Fconstant06\00\17\FA\01debug_frame\00.rel\11\00!nv\14\00\11aB\00\0F'\01 \0F\87\00\14\0FO\01\B6o_paramV\01\1C\0F\01\00\07\8CZ\00\00\00\03\00\0A\00\01\00\11\EC\18\00,\09\00\01\00 )\01\18\00,\04\00\01\00\11G\18\00,\07\00\01\00f2\00\00\00\12\10`\00!\80\01\07\00v\00\FF\FF\FF\FF(\00\0C\00\00\01\00@\03\00\04|\08\00\D1\0F\0C\81\80\80(\00\08\FF\81\80(\08\0B\00\06(\00/0\00\01\00\03\13pX\00\10\04\9B\00R\04\14\00\00\00E\00\01\0B\00\00\13\00p/\08\00\05\00\00\00\9F\03\22\04#\0C\00\00\01\00'\04\12\0C\00\17\11\0C\00!7\04\E0\03\F3\08\015\00\00\04\0A\08\00\02\00\00\00`\01\10\00\03\19\10\00\04\17\0C\04\04U\08\00\00\F0!\10\00\10\01\18\01%\F0\11\10\00\01\01\00\F2\02\F0\11\00\03\1B\FF\00\04\1C\08\00P\00\00\00\B0\00\01\00#K\00\01\00s\02\02\08\10\0A/\22\9B\00\00\07\00\03\08\00\13\10\08\00\13\18\08\00\13 \08\00\13(\08\00\130\08\00\228\08|\00\13\00\08\00\13\08\08\00\13\10\08\00\13\18\08\00\13 \08\00\13(\08\00\130\08\00\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\A2\00\00\00\14,\00\00\00H\00\01\00\000\01/\05\00\01\00\FF\B8A\02z\01\00'\04\B1\0F\00\00\00\C4\0F\00\19y\02\00\01\00\10%\93\02Q\0E\00\19y\03\0F\00\F5\1A\00!\00\00\00$\0E\00$z\02\02\00X\00\00\03\02\8E\07\00\CA\1F\00\0Cz\00\02\00Y\00\00p`\F0\03\00\DA\0F\00M[\04\A0\80\03\00\EA\0F\005t\03\FF\BB\03\10\FF\C8\03P\E2\0F\00\02x>\02B\80\FF\00\0F\10\00r\B9z\04\00\00F\00\84\00\94\D0\0F\00%v\02\02\00Z`\00`\0F\00\86y\00\022\00@\04\19\10\0C0\009My\00`\00PGy\00\00\F0A\04\A6\FF\83\03\00\C0\0F\00\18y\00\01\00\0F\10\00\90\0F\01\00-\00W\01.\03\00\01\00\22@\00\01\00='\01\000\00\08\01\00\1F\0B@\00\04\13g)\00\1FV@\00\0C\13\13\14\04\0C\01\00\13\C0\15\00&\90\008\04#\04\00\8D\04\00\FE\04\12\00\01\00\1F\FAT\00\00\00\01\00\13P\95\00/p\00\80\00\0B\1F)'\00\03#\00\C0@\00\04X\06\04\E4\00*\04\00\01\00\1F`@\00\04\13\F01\00&\\\00@\00\1F\0A@\00\00!\18\01D\01\0D@\00\13P)\00*\D8\00\01\00\1B\08\08\00?\07\01\00\8E\07\00Q\00\00(\05\00\01\00&\10\00\80\00\17\048\00\04\18\00\13\C4\14\01\0C\84\01*8\05@\07\1F\00\C0\00\04\132@\00+\06\00\01\00\1A\07\D8\07\12\03\F8\05:\08\80\00\01\00\13\06\10\06\04(\0B\0C\01\00*\A8\00\08\00\04\F8\00\13\018\00\04\A8\00\0C\01\009H\03\00\08\00\088\00/\06\00\01\00\17\80\08\00\00\00\00\00\00\00\00"
@ral_send_output___cpu___pvoid_i64_m2df32___void = internal constant [48 x i8] c"ral_send_output___cpu___pvoid_i64_m2df32___void\00"
@dealloc___gpu___pvoid_pvoid___void = internal constant [35 x i8] c"dealloc___gpu___pvoid_pvoid___void\00"
@inc_ref___gpu___pvoid_pvoid_m1df32_m1di64___m2df32 = internal constant [51 x i8] c"inc_ref___gpu___pvoid_pvoid_m1df32_m1di64___m2df32\00"
@main_kernel_2_main_kColReduction_reduce__4_1_0___thin_1_kernel_name = internal constant [42 x i8] c"main_kColReduction_reduce__4_1_0___thin_1\00"
@main_kernel_2_blob_gpu.binary = internal constant [3128 x i8] c"P\EDU\BA\01\00\10\00(\0C\00\00\00\00\00\00\02\00\01\01@\00\00\00\E8\0B\00\00\00\00\00\00\E3\0B\00\00\00\00\00\00\07\00\01\00P\00\00\00\00\00\00\00\00\00\00\00\11 \00\00\00\00\00\00\00\00\00\00\00\00\00\00(:\00\00\00\00\00\00\A2\7FELF\02\01\013\07\00\01\00f\02\00\BE\00q\00\01\00#\809\08\00\116\07\00\F5\0E\00P\05P\00@\008\00\03\00@\00\0C\00\01\00\00.shstrtab\00.\08\00'ym\08\00\F0\1B_shndx\00.nv.info\00.text.main_kColReduction_r\0A\00\F5\02e__4_1_0___thin_19\00\0F3\00\1Coshared5\00\19Orela\9D\00\1E?rel\D1\00!\9Fconstant08\00\19\B2debug_framey\00\09\11\00!nv\14\00\11aD\00\0F\98\01 \0F\89\00\16\0F\C2\01\EF\8F$____wg_2\00\16\00\0B\00/27\FA\01&o_param\01\02\1C\0F\01\00\0B\8C\\\00\00\00\03\00\0A\00\01\00\11\BF\18\00,\0B\00\01\00 \95\01\18\00,\09\00\01\00\11\D4\18\00,\04\00\01\00\11\F2\18\00,\07\00\01\00g2\00\00\00\12\10x\00\11.\06\00v\00\FF\FF\FF\FF(\00\0C\00\00\01\00@\03\00\04|\08\00\D1\0F\0C\81\80\80(\00\08\FF\81\80(\08\0B\00\06(\00/0\00\01\00\03!\F0-\07\00 \00\04\9B\00R\04\14\00\00\00E\00\22\044\DC\00\90\04/\08\00\06\00\00\00\15\0C\00\12#\0C\00\00\01\00'\04\12\0C\00\17\11\0C\00!7\04\18\05\F1\08\015\00\00\04\0A\08\00\03\00\00\00`\01(\00\03\19(\00\04\17\0C$\00u\06\00 \00\00\F0!\10\00u\05\00\1C\00\00\F0\11\10\009\04\00\18\10\009\03\00\14\10\009\02\00\10\10\009\01\00\08P\00\01\01\00\F2\0A\F0\11\00\03\1B\FF\00\04\1C\0C\00P\00\00\000,\00\000-\00\00\04\1E\84\01#K\00\01\00\B1\02\02\08\10\0A/\22\00\00\00\08\10\00#\00\08\08\00\13\10\08\00\13\18\08\00\13 \08\00\13(\08\00\130\08\00\228\08\84\00\13\00\08\00\13\08\08\00\13\10\08\00\13\18\08\00\13 \08\00\13(\08\00\130\08\00\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\A2\00\00\00\14,\00\00\00H\00\01\00\11\02h\01\0F\01\00\FF\B8@$v\01\FF\87\04\B2\FF\00\8E\07\00\C4\0F\00\19y\00\01\00\10%\93\02a\0E\00\19y\03\00\01\00\10!-\00\F0\04\0E\00$z\00\00\00]\00\00\03\02\8E\07\00\CA\1F\00\0C\10\00\C5^\00\00p`\F0\03\00\DA\0F\00M\A3\04\F0\0C\80\03\00\EA\0F\00\06{\04\00\00_\00\00\00\90 \00\00\22\0E\00\19x\05\FF\1F\1F\00\F1\00\14\01\00\00\E2\0F\00\B9z\04\00\00F\00\00\0B\05 \E2\0FP\00\10\FF0\00@pP\F4\03\10\00cEy\00\00 *P\00p\E2\0F\00\11r\05\05\0E\00\B2\FF@\8F\07\00\C6\0F\00\08s\04\FE\04\10\10\A0\00\F1\06\1E\00\10x\02\04\FE\FF\FF\0F\FF\E0\FF\07\00\CC\1F\00\05s\03\B4\02\00\C8\03\A3\00d\00\00$r\02\FF\FF\00\F0\00a\E4\1F\00$r\07\10\00\C0\03\0A\8E\07\00\C8/\00$z\07\07\80\00\10\FF\E0\00\81\C8\0F\00'r\06\03\07\F8\02 \8E\07\80\00@\19x\03\FF\19\03\10\05\C0\00q\E4\0F\00\12x\05\05\09\05!\C0\8E\90\00T'r\06\06\03`\00a\C8\0F\00$r\04`\00#\06\0A\10\00Tz\02\04\00_@\01\01\F0\006\02\00_@\010\10\0A\02\10\00\11\80\C0\00\80\E4\0F\00\10\08\06\06\01P\00\03\10\00\060\00\11\F2\10\010$x\02$\04A\00\05\0A\8E \00\A2$t\05\FF\00\00\80\FF\FF\00\90\00@\12x\00\02p\01\22\FF\C0 \00@\19x\07\FFA\01@\02\16\01\00\80\00*\10\18`\001\12\AA\06\80\01\22\FF30\00\EF\0Cx\00\02\7F\00\00\00p@\F4\03\00\C6\D0\00\01\16\03\D0\00\00\10\00@x\03\03 p\01\12\02@\00\22$x\81\04\22\07\02\90\00h\0Cz\00\03\00X\C0\00\00\98\05\030\01\96\D8\0F\00G\19\00\00 (@\02E$x\04\06@\00\01`\004\04\04@0\00\02@\01\10\04M\07\C7pb\F8\03\00\E4\0F\04\10x\08\04\D0\00g\04\10x\0A\04\02\E0\00B\0Cz\00\080\00\A5\FA\03\00\CE\0F\00$\C4\07\FF\80\00\00\B0\00V\CA\06\04\00X\D0\00c%\C6\06\06\00Z\80\00\02\A0\02\12\0A@\00 \F0\03 \00@\81\C9\0C\06@\00\B6\00\19\1E\0C\00\A2\00\00$\D4\09P\00\84\E2\0F\00\10x\0E\04\03\80\00\00\F0\009\DA\08\08`\000\D6\08\08`\00\16\09`\00\12\0E`\00 \F6\03\E0\00E\81\D9\08\08`\00j\E2\02\00$\84\0B\B0\009\8A\0A\0AP\000\86\0A\0AP\00\10\0B\10\00u\CC\0F\00\81\89\0A\0A@\00X\22\0F\00$\B4\F0\00i\1F\00$\BA\06\0E@\00\17\B6\F0\00\00\80\00&\B9\0E\E0\00\88b\01\00\10x\10\04\04\D0\00\08p\02p\E2\0F\00\0B\C8\00\0C\10\00\B2\00b\FC\03\00\C8O\00\08\C8\09\10\00\B1\02\00\03\00\E4/\04\0B\C2\00\0CN\07\10\80 \00r\0F\00\08\C2\05\0C\09Y\06\13\03\A0\01\17\10\D0\01\F4\07\00\0B\D2\00\05\08\00\00@\00\C0\FC\03\00\E4\8F\08\10x\0C\04\05\80\00\82\E4\0F\00\08\D2\09\08\05@\00\00\10\003\04\0B\D2\E9\06\04`\005\D2\05\08`\00\1B\E2\F0\01\11\E2\C0\04\15\0C\10\02\11\E2\00\02\15\10\10\01\00\D0\00E\82\00\05\0A\80\00 \0F\09\00\01\17\06\00\01\0C \02I\08\82\09\0A\90\00D\82\00\0A\0A\90\00\10\C60\02\17\060\02i\0E\00\08\82\05\0A\A0\00\0B@\02\06\10\01#\F0\03@\02\19\0C\A0\00F\B2\00\05\0E\A0\00\10\0A \01\18\07\A0\00\0B`\02I\08\B2\0B\0E\A0\00H\B2\00\0E\0E\A0\00\09p\02x\0E\00\08\B2\05\0E\0B\A0\00\08\80\02'\E2\0F@\01\12\F6 \00?\8A\0A\10\90\02\16\10$\90\02\16\0DP\00\00\D0\02:\BA\0C\0C\90\02 \0C\0C@\00\14\0D@\005\B9\0C\0C@\00\12b`\03\14\08\F0\00\01\F0\036\10\04\09\10\00d\00\0B\C2\00\05\06 \01\00\90\029\C2\07\06\00\01D\C2\00\06\06\00\01\02\90\02$\06\07\F0\00\18\E4\90\03\0F\90\02\00o\C8\8F\00\08\D2\07\80\02\0E\1F\07\80\02\06\08\E0\01\05\80\02\06\90\03\0F\80\02\024\0E\04\0A\E0\00\0F\80\02S\1B\0E\80\02\1C\10\80\02\19\0C\80\02?\10\04\0B\80\02\0B\1B\0C\80\02/\0C\0C\80\02\0B\1F\0C\80\02\0C\1B\10\80\02\1F\0E\80\02,\1F\10\80\02\1D\14\0C\F0\00\03\80\02\1F\0D\80\02\CC\14\0E\E0\00\0F\80\02\84\1F\0F\80\02\DC\14\10\F0\00\03\80\02\1F\11\80\02\CC\14\12\E0\00\0F\80\02\84\1F\13\80\02\DC\14\14\F0\00\03\80\02\1F\15\80\02\CC\14\16\E0\00\0F\80\02\84\1F\17\80\02\DC\14\18\F0\00\03\80\02\1F\19\80\02\CC\14\1A\E0\00\0F\80\02\84\1F\1B\80\02\DC\14\1C\F0\00\03\80\02\1F\1D\80\02\CC\14\1E\E0\00\0F\80\02\84\1F\1F\80\02\DC\14 \F0\00\03\80\02\1F!\80\02\CC\14\22\E0\00\0F\80\02\84\1F#\80\02\DC\14$\F0\00\03\80\02\1F%\80\02\CC\14&\E0\00\0F\80\02\84\1F'\80\02\DC\14(\F0\00\03\80\02\1F)\80\02\CC\14*\E0\00\0F\80\02\84\1F+\80\02\DC\14,\F0\00\03\80\02\1F-\80\02\CC\14.\E0\00\0F\80\02\84\1F/\80\02\DC\140\F0\00\03\80\02\1F1\80\02\CC\142\E0\00\0F\80\02\84\1F3\80\02\DC\144\F0\00\03\80\02\1F5\80\02\CC\146\E0\00\0F\80\02\84\1F7\80\02\DC\148\F0\00\03\80\02\1F9\80\02\CC\14:\E0\00\0F\80\02\84\1F;\80\02\DC\14<\F0\00\03\80\02\19=\10\009\12\04>\10\00?\04\04?\A0\02\C5?\C6\0F\01\90\02\1AW\E4\0F\04\81\C9`$/\A4\00\90\02\11\1F\12\90\02\16?\C6\0F\02\80\02+\1F\02\80\02\00\0D@$\1A\04\80\02*\06\12\80\02\07\10\01\01\80\02\07\80\03W$\01\00$\B4\D0\02\01\80\02\1A\0A\80&\1B\B6\C0\028\B9\0A\0A\80\02\00@\02\19\0E@\02\17\09\E0#\01p%\06\E0#\02@\02\15\0E`\01\1E\C8`%\06 \02\01P\02\19\04@\02(\C8\1F0\02\11\02@\00\14\82\C0\02\10\FA0\004\09\0B\82\B0\02\03@\00\14\82\D0\02 \80\060\00I\08\82\05\06@\00\14\B2@\02\010\004\0A\0B\B2 \02\12\F0@\00$\B2\07@\02\02@\003\B2\05\0A@\00\10\00\B0*\14A\B0*\030( \88s\F8-\12\00u(f\E8\0F\00\1D{\00\01\00b\EC\0F\00\84\A9\07,*\00 \00Q\22\0E\00\1Cx\1A\00P\00p\F0\F0\03P\00\13\0C0)1pD\F2\90*!\84\A9n\01\020\00qb\0E\00\0B\A2\00\07\80\00\A1\80\F6\03\00\E4\1F\08\0B\A2\00p%\02\C0\00T/\00\1C\A8\00P\00\14\01@)\11?@)@\F6\03\00\D6\00\01#\07\07R\00\10\06 \00\09\80\00\8F\C6\0F\00\88\A3\00\00\07\C0\00\09\22\B9\05\1C\00\12\08\A0++\84\B9\A0\00\22\B2\00!+`\80\F4\03\00\C4\1F\10\00(\04\05\A0\00#\B8\00p\00Ap\01\00\DA\90\01\15\05\90\00_\CA\0F\00\88\B3@\01\0B9M\19\00p\017$t\02\10\03R\0F\00\84y\05\D1)\00@\00\90&\0E\00%v\02\03\00`\0F/\04 \00\00\1D\00\03`\00vh\0E\00\81y\06\02\D0\02D\05\00\0Br\C0\00\12\F0\C0\00\14r\C0\00W\F2\03\00\D6/\B0\00 \80\04\F0\022\0Br\00`\02\11@\10\04(\0EF\10\02S\E6\0F\00\08r\80\02\00\01\00p\CC\0F\00\A9s\07\02\A0\02\C0\07\E1\1E\00\00\A4\0E\00\0Cr\00\07\10\00 pR@\00QO\00$r\06\E0*\14\07\90*@\09\00\00\90\E0+!\FF\83\F0\00\1BMp\02TGy\00\00\F0 \00f\C0\0F\00\18y\00\01\00\0F\10\00\90\0F\01\00-\11\01H1\0E\01\00\22@\00\01\00=\98\01\000\00\08\01\00\1F\0B@\00\04\13\D8)\00?\01\02\00@\00\0A\13\13\0B\04\0C\01\00\13\E0U\00\03\8F\03\01/\04\13\05w\02\00\01\00\22\18\00\01\00.k\01T\00\00\01\00#\88\04(\03\1F\00\80\00\0B/)\00'\00\02#\00\F8@\00\04 3\04\E4\00*\04\00\01\00\1Fb@\00\04*(\05\C0\00\13\03#/\0C@\00!\89\01D\01\0D@\00\13\D0@\00*\D8\00\01\00\1B\08\08\00?x\01\00V4\000\00\00\A8\15\03\03W/\04\80\00\17\048\00\04\18\00\133@\01\0C\84\01\13\B8@\00\17\881\01\0F\C0\00\01\132T\01\15\06R\00\03^\03\1A\08\A04\11\03$\00J\00\15\80\00\01\00\13\95\94\00*\03\00\01\00\0409/\00\04\80\00\0B\13\06\18\02\04h9\0D\08\01\1A\00\08\00\04\97\00\13\018\00\04\E8\00\0C\01\009\C8/\00\08\00\088\00\18\06\A0\00\0F\01\00\05\03\A9\00\80\08\00\00\00\00\00\00\00\00\00\00\00\00"
@ral_kernel_launch___gpu___pvoid_ppvoid_i64_pvoid_i64_i64_i64_i64_i64_i64_i32_pvoid_i32_ppvoid___void = internal constant [101 x i8] c"ral_kernel_launch___gpu___pvoid_ppvoid_i64_pvoid_i64_i64_i64_i64_i64_i64_i32_pvoid_i32_ppvoid___void\00"
@main_kernel_1_main_kColReduction_reduce__4_1_0___thin_kernel_name = internal constant [40 x i8] c"main_kColReduction_reduce__4_1_0___thin\00"
@main_kernel_1_blob_gpu.binary = internal constant [1096 x i8] c"P\EDU\BA\01\00\10\008\04\00\00\00\00\00\00\02\00\01\01@\00\00\00\F8\03\00\00\00\00\00\00\F7\03\00\00\00\00\00\00\07\00\01\00P\00\00\00\00\00\00\00\00\00\00\00\11 \00\00\00\00\00\00\00\00\00\00\00\00\00\00\E8\0B\00\00\00\00\00\00\A2\7FELF\02\01\013\07\00\01\00f\02\00\BE\00q\00\01\00!@\0B\07\001\00\80\08\07\00\F5\0E\00P\05P\00@\008\00\03\00@\00\0B\00\01\00\00.shstrtab\00.\08\00'ym\08\00\F0\1B_shndx\00.nv.info\00.text.main_kColReduction_r\0A\00\F5\00e__4_1_0___thin7\00\0F1\00\1Aoshared3\00\1A\9Fconstant06\00\17\FA\01debug_frame\00.rel\11\00!nv\14\00\11aB\00\0F'\01 \0F\87\00\14\0FO\01\B6o_paramV\01\1C\0F\01\00\07\8CZ\00\00\00\03\00\0A\00\01\00\11\EC\18\00,\09\00\01\00 )\01\18\00,\04\00\01\00\11G\18\00,\07\00\01\00f2\00\00\00\12\10`\00!\80\01\07\00v\00\FF\FF\FF\FF(\00\0C\00\00\01\00@\03\00\04|\08\00\D1\0F\0C\81\80\80(\00\08\FF\81\80(\08\0B\00\06(\00/0\00\01\00\03\13pX\00\10\04\9B\00R\04\14\00\00\00E\00\01\0B\00\00\13\00p/\08\00\05\00\00\00\9F\03\22\04#\0C\00\00\01\00'\04\12\0C\00\17\11\0C\00!7\04\E0\03\F3\08\015\00\00\04\0A\08\00\02\00\00\00`\01\10\00\03\19\10\00\04\17\0C\04\04U\08\00\00\F0!\10\00\10\01\18\01%\F0\11\10\00\01\01\00\F2\02\F0\11\00\03\1B\FF\00\04\1C\08\00P\00\00\00\B0\00\01\00#K\00\01\00s\02\02\08\10\0A/\22\9B\00\00\07\00\03\08\00\13\10\08\00\13\18\08\00\13 \08\00\13(\08\00\130\08\00\228\08|\00\13\00\08\00\13\08\08\00\13\10\08\00\13\18\08\00\13 \08\00\13(\08\00\130\08\00\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\A2\00\00\00\14,\00\00\00H\00\01\00\000\01/\05\00\01\00\FF\B8A\02z\01\00'\04\B1\0F\00\00\00\C4\0F\00\19y\02\00\01\00\10%\93\02Q\0E\00\19y\03\0F\00\F5\1A\00!\00\00\00$\0E\00$z\02\02\00X\00\00\03\02\8E\07\00\CA\1F\00\0Cz\00\02\00Y\00\00p`\F0\03\00\DA\0F\00M[\04\A0\80\03\00\EA\0F\005t\03\FF\BB\03\10\FF\C8\03P\E2\0F\00\02x>\02B\80\FF\00\0F\10\00r\B9z\04\00\00F\00\84\00\94\D0\0F\00%v\02\02\00Z`\00`\0F\00\86y\00\022\00@\04\19\10\0C0\009My\00`\00PGy\00\00\F0A\04\A6\FF\83\03\00\C0\0F\00\18y\00\01\00\0F\10\00\90\0F\01\00-\00W\01.\03\00\01\00\22@\00\01\00='\01\000\00\08\01\00\1F\0B@\00\04\13g)\00\1FV@\00\0C\13\13\14\04\0C\01\00\13\C0\15\00&\90\008\04#\04\00\8D\04\00\FE\04\12\00\01\00\1F\FAT\00\00\00\01\00\13P\95\00/p\00\80\00\0B\1F)'\00\03#\00\C0@\00\04X\06\04\E4\00*\04\00\01\00\1F`@\00\04\13\F01\00&\\\00@\00\1F\0A@\00\00!\18\01D\01\0D@\00\13P)\00*\D8\00\01\00\1B\08\08\00?\07\01\00\8E\07\00Q\00\00(\05\00\01\00&\10\00\80\00\17\048\00\04\18\00\13\C4\14\01\0C\84\01*8\05@\07\1F\00\C0\00\04\132@\00+\06\00\01\00\1A\07\D8\07\12\03\F8\05:\08\80\00\01\00\13\06\10\06\04(\0B\0C\01\00*\A8\00\08\00\04\F8\00\13\018\00\04\A8\00\0C\01\009H\03\00\08\00\088\00/\06\00\01\00\17\80\08\00\00\00\00\00\00\00\00"
@alloc___gpu___pvoid_i64___pvoid = internal constant [32 x i8] c"alloc___gpu___pvoid_i64___pvoid\00"
@ral_recv_input___cpu___pvoid_i64___m3df32 = internal constant [42 x i8] c"ral_recv_input___cpu___pvoid_i64___m3df32\00"

declare ptr @malloc(i64)

declare void @free(ptr)

declare void @disc_ral_call(ptr, ptr, ptr)

define void @main(ptr %0) {
  %2 = alloca %0, align 8
  %3 = alloca ptr, i32 3, align 8
  %4 = getelementptr %0, ptr %2, i32 0, i32 0
  store ptr %0, ptr %4, align 8
  %5 = getelementptr ptr, ptr %3, i32 0
  store ptr %4, ptr %5, align 8
  %6 = getelementptr %0, ptr %2, i32 0, i32 1
  store i64 0, ptr %6, align 4
  %7 = getelementptr ptr, ptr %3, i32 1
  store ptr %6, ptr %7, align 8
  %8 = getelementptr %0, ptr %2, i32 0, i32 2
  %9 = getelementptr ptr, ptr %3, i32 2
  store ptr %8, ptr %9, align 8
  call void @disc_ral_call(ptr %0, ptr @ral_recv_input___cpu___pvoid_i64___m3df32, ptr %3)
  %10 = load { ptr, ptr, i64, [3 x i64], [3 x i64] }, ptr %8, align 8
  %11 = extractvalue { ptr, ptr, i64, [3 x i64], [3 x i64] } %10, 3, 2
  %12 = extractvalue { ptr, ptr, i64, [3 x i64], [3 x i64] } %10, 3, 1
  %13 = extractvalue { ptr, ptr, i64, [3 x i64], [3 x i64] } %10, 3, 0
  %14 = trunc i64 %12 to i32
  %15 = trunc i64 %11 to i32
  %16 = mul i32 %14, %15
  %17 = sext i32 %16 to i64
  %18 = getelementptr float, ptr null, i64 %17
  %19 = ptrtoint ptr %18 to i64
  %20 = alloca %.1, align 8
  %21 = alloca ptr, i32 3, align 8
  %22 = getelementptr %.1, ptr %20, i32 0, i32 0
  store ptr %0, ptr %22, align 8
  %23 = getelementptr ptr, ptr %21, i32 0
  store ptr %22, ptr %23, align 8
  %24 = getelementptr %.1, ptr %20, i32 0, i32 1
  store i64 %19, ptr %24, align 4
  %25 = getelementptr ptr, ptr %21, i32 1
  store ptr %24, ptr %25, align 8
  %26 = getelementptr %.1, ptr %20, i32 0, i32 2
  %27 = getelementptr ptr, ptr %21, i32 2
  store ptr %26, ptr %27, align 8
  call void @disc_ral_call(ptr %0, ptr @alloc___gpu___pvoid_i64___pvoid, ptr %21)
  %28 = load ptr, ptr %26, align 8
  %29 = insertvalue { ptr, ptr, i64, [1 x i64], [1 x i64] } undef, ptr %28, 0
  %30 = insertvalue { ptr, ptr, i64, [1 x i64], [1 x i64] } %29, ptr %28, 1
  %31 = insertvalue { ptr, ptr, i64, [1 x i64], [1 x i64] } %30, i64 0, 2
  %32 = insertvalue { ptr, ptr, i64, [1 x i64], [1 x i64] } %31, i64 %17, 3, 0
  %33 = insertvalue { ptr, ptr, i64, [1 x i64], [1 x i64] } %32, i64 1, 4, 0
  %34 = icmp slt i64 %13, %17
  br i1 %34, label %35, label %171

35:                                               ; preds = %1
  %36 = icmp sle i64 %17, 0
  %37 = sub i64 0, %17
  %38 = sub i64 %17, 1
  %39 = select i1 %36, i64 %37, i64 %38
  %40 = sdiv i64 %39, 512
  %41 = sub i64 0, %40
  %42 = add i64 %40, 1
  %43 = select i1 %36, i64 %41, i64 %42
  %44 = alloca ptr, align 8
  %45 = getelementptr ptr, ptr %44, i32 0
  store ptr @main_kernel_blob_gpu.binary, ptr %45, align 8
  %46 = extractvalue { ptr, ptr, i64, [1 x i64], [1 x i64] } %33, 0
  %47 = extractvalue { ptr, ptr, i64, [1 x i64], [1 x i64] } %33, 1
  %48 = extractvalue { ptr, ptr, i64, [1 x i64], [1 x i64] } %33, 2
  %49 = extractvalue { ptr, ptr, i64, [1 x i64], [1 x i64] } %33, 3, 0
  %50 = extractvalue { ptr, ptr, i64, [1 x i64], [1 x i64] } %33, 4, 0
  %51 = alloca %.9, align 8
  %52 = alloca ptr, i32 3, align 8
  %53 = getelementptr %.9, ptr %51, i32 0, i32 0
  store i64 512, ptr %53, align 4
  %54 = getelementptr ptr, ptr %52, i32 0
  store ptr %53, ptr %54, align 8
  %55 = getelementptr %.9, ptr %51, i32 0, i32 1
  store i64 %17, ptr %55, align 4
  %56 = getelementptr ptr, ptr %52, i32 1
  store ptr %55, ptr %56, align 8
  %57 = getelementptr %.9, ptr %51, i32 0, i32 2
  store ptr %47, ptr %57, align 8
  %58 = getelementptr ptr, ptr %52, i32 2
  store ptr %57, ptr %58, align 8
  %59 = alloca %.10, align 8
  %60 = alloca ptr, i32 14, align 8
  %61 = getelementptr %.10, ptr %59, i32 0, i32 0
  store ptr %0, ptr %61, align 8
  %62 = getelementptr ptr, ptr %60, i32 0
  store ptr %61, ptr %62, align 8
  %63 = getelementptr %.10, ptr %59, i32 0, i32 1
  store ptr %44, ptr %63, align 8
  %64 = getelementptr ptr, ptr %60, i32 1
  store ptr %63, ptr %64, align 8
  %65 = getelementptr %.10, ptr %59, i32 0, i32 2
  store i64 1, ptr %65, align 4
  %66 = getelementptr ptr, ptr %60, i32 2
  store ptr %65, ptr %66, align 8
  %67 = getelementptr %.10, ptr %59, i32 0, i32 3
  store ptr @main_kernel_main_kColReduction_reduce__4_1_0___flat_kernel_name, ptr %67, align 8
  %68 = getelementptr ptr, ptr %60, i32 3
  store ptr %67, ptr %68, align 8
  %69 = getelementptr %.10, ptr %59, i32 0, i32 4
  store i64 %43, ptr %69, align 4
  %70 = getelementptr ptr, ptr %60, i32 4
  store ptr %69, ptr %70, align 8
  %71 = getelementptr %.10, ptr %59, i32 0, i32 5
  store i64 1, ptr %71, align 4
  %72 = getelementptr ptr, ptr %60, i32 5
  store ptr %71, ptr %72, align 8
  %73 = getelementptr %.10, ptr %59, i32 0, i32 6
  store i64 1, ptr %73, align 4
  %74 = getelementptr ptr, ptr %60, i32 6
  store ptr %73, ptr %74, align 8
  %75 = getelementptr %.10, ptr %59, i32 0, i32 7
  store i64 512, ptr %75, align 4
  %76 = getelementptr ptr, ptr %60, i32 7
  store ptr %75, ptr %76, align 8
  %77 = getelementptr %.10, ptr %59, i32 0, i32 8
  store i64 1, ptr %77, align 4
  %78 = getelementptr ptr, ptr %60, i32 8
  store ptr %77, ptr %78, align 8
  %79 = getelementptr %.10, ptr %59, i32 0, i32 9
  store i64 1, ptr %79, align 4
  %80 = getelementptr ptr, ptr %60, i32 9
  store ptr %79, ptr %80, align 8
  %81 = getelementptr %.10, ptr %59, i32 0, i32 10
  store i32 0, ptr %81, align 4
  %82 = getelementptr ptr, ptr %60, i32 10
  store ptr %81, ptr %82, align 8
  %83 = getelementptr %.10, ptr %59, i32 0, i32 11
  store ptr null, ptr %83, align 8
  %84 = getelementptr ptr, ptr %60, i32 11
  store ptr %83, ptr %84, align 8
  %85 = getelementptr %.10, ptr %59, i32 0, i32 12
  store i32 3, ptr %85, align 4
  %86 = getelementptr ptr, ptr %60, i32 12
  store ptr %85, ptr %86, align 8
  %87 = getelementptr %.10, ptr %59, i32 0, i32 13
  store ptr %52, ptr %87, align 8
  %88 = getelementptr ptr, ptr %60, i32 13
  store ptr %87, ptr %88, align 8
  call void @disc_ral_call(ptr %0, ptr @ral_kernel_launch___gpu___pvoid_ppvoid_i64_pvoid_i64_i64_i64_i64_i64_i64_i32_pvoid_i32_ppvoid___void, ptr %60)
  %89 = icmp eq i64 %17, 0
  %90 = sub i64 %17, 1
  %91 = udiv i64 %90, 512
  %92 = add i64 %91, 1
  %93 = select i1 %89, i64 0, i64 %92
  %94 = icmp eq i64 %13, 0
  %95 = sub i64 %13, 1
  %96 = udiv i64 %95, 32
  %97 = add i64 %96, 1
  %98 = select i1 %94, i64 0, i64 %97
  %99 = mul i64 %93, %98
  %100 = mul i64 %99, 512
  %101 = icmp sle i64 %100, 0
  %102 = sub i64 0, %100
  %103 = sub i64 %100, 1
  %104 = select i1 %101, i64 %102, i64 %103
  %105 = sdiv i64 %104, 512
  %106 = sub i64 0, %105
  %107 = add i64 %105, 1
  %108 = select i1 %101, i64 %106, i64 %107
  %109 = alloca ptr, align 8
  %110 = getelementptr ptr, ptr %109, i32 0
  store ptr @main_kernel_0_blob_gpu.binary, ptr %110, align 8
  %111 = extractvalue { ptr, ptr, i64, [3 x i64], [3 x i64] } %10, 0
  %112 = extractvalue { ptr, ptr, i64, [3 x i64], [3 x i64] } %10, 1
  %113 = extractvalue { ptr, ptr, i64, [3 x i64], [3 x i64] } %10, 2
  %114 = extractvalue { ptr, ptr, i64, [3 x i64], [3 x i64] } %10, 3, 0
  %115 = extractvalue { ptr, ptr, i64, [3 x i64], [3 x i64] } %10, 3, 1
  %116 = extractvalue { ptr, ptr, i64, [3 x i64], [3 x i64] } %10, 3, 2
  %117 = extractvalue { ptr, ptr, i64, [3 x i64], [3 x i64] } %10, 4, 0
  %118 = extractvalue { ptr, ptr, i64, [3 x i64], [3 x i64] } %10, 4, 1
  %119 = extractvalue { ptr, ptr, i64, [3 x i64], [3 x i64] } %10, 4, 2
  %120 = extractvalue { ptr, ptr, i64, [1 x i64], [1 x i64] } %33, 0
  %121 = extractvalue { ptr, ptr, i64, [1 x i64], [1 x i64] } %33, 1
  %122 = extractvalue { ptr, ptr, i64, [1 x i64], [1 x i64] } %33, 2
  %123 = extractvalue { ptr, ptr, i64, [1 x i64], [1 x i64] } %33, 3, 0
  %124 = extractvalue { ptr, ptr, i64, [1 x i64], [1 x i64] } %33, 4, 0
  %125 = alloca %.11, align 8
  %126 = alloca ptr, i32 7, align 8
  %127 = getelementptr %.11, ptr %125, i32 0, i32 0
  store i64 %17, ptr %127, align 4
  %128 = getelementptr ptr, ptr %126, i32 0
  store ptr %127, ptr %128, align 8
  %129 = getelementptr %.11, ptr %125, i32 0, i32 1
  store ptr %112, ptr %129, align 8
  %130 = getelementptr ptr, ptr %126, i32 1
  store ptr %129, ptr %130, align 8
  %131 = getelementptr %.11, ptr %125, i32 0, i32 2
  store i64 %114, ptr %131, align 4
  %132 = getelementptr ptr, ptr %126, i32 2
  store ptr %131, ptr %132, align 8
  %133 = getelementptr %.11, ptr %125, i32 0, i32 3
  store i64 512, ptr %133, align 4
  %134 = getelementptr ptr, ptr %126, i32 3
  store ptr %133, ptr %134, align 8
  %135 = getelementptr %.11, ptr %125, i32 0, i32 4
  store i64 %100, ptr %135, align 4
  %136 = getelementptr ptr, ptr %126, i32 4
  store ptr %135, ptr %136, align 8
  %137 = getelementptr %.11, ptr %125, i32 0, i32 5
  store i64 %93, ptr %137, align 4
  %138 = getelementptr ptr, ptr %126, i32 5
  store ptr %137, ptr %138, align 8
  %139 = getelementptr %.11, ptr %125, i32 0, i32 6
  store ptr %121, ptr %139, align 8
  %140 = getelementptr ptr, ptr %126, i32 6
  store ptr %139, ptr %140, align 8
  %141 = alloca %.12, align 8
  %142 = alloca ptr, i32 14, align 8
  %143 = getelementptr %.12, ptr %141, i32 0, i32 0
  store ptr %0, ptr %143, align 8
  %144 = getelementptr ptr, ptr %142, i32 0
  store ptr %143, ptr %144, align 8
  %145 = getelementptr %.12, ptr %141, i32 0, i32 1
  store ptr %109, ptr %145, align 8
  %146 = getelementptr ptr, ptr %142, i32 1
  store ptr %145, ptr %146, align 8
  %147 = getelementptr %.12, ptr %141, i32 0, i32 2
  store i64 1, ptr %147, align 4
  %148 = getelementptr ptr, ptr %142, i32 2
  store ptr %147, ptr %148, align 8
  %149 = getelementptr %.12, ptr %141, i32 0, i32 3
  store ptr @main_kernel_0_main_kColReduction_reduce__4_1_0___flat_1_kernel_name, ptr %149, align 8
  %150 = getelementptr ptr, ptr %142, i32 3
  store ptr %149, ptr %150, align 8
  %151 = getelementptr %.12, ptr %141, i32 0, i32 4
  store i64 %108, ptr %151, align 4
  %152 = getelementptr ptr, ptr %142, i32 4
  store ptr %151, ptr %152, align 8
  %153 = getelementptr %.12, ptr %141, i32 0, i32 5
  store i64 1, ptr %153, align 4
  %154 = getelementptr ptr, ptr %142, i32 5
  store ptr %153, ptr %154, align 8
  %155 = getelementptr %.12, ptr %141, i32 0, i32 6
  store i64 1, ptr %155, align 4
  %156 = getelementptr ptr, ptr %142, i32 6
  store ptr %155, ptr %156, align 8
  %157 = getelementptr %.12, ptr %141, i32 0, i32 7
  store i64 512, ptr %157, align 4
  %158 = getelementptr ptr, ptr %142, i32 7
  store ptr %157, ptr %158, align 8
  %159 = getelementptr %.12, ptr %141, i32 0, i32 8
  store i64 1, ptr %159, align 4
  %160 = getelementptr ptr, ptr %142, i32 8
  store ptr %159, ptr %160, align 8
  %161 = getelementptr %.12, ptr %141, i32 0, i32 9
  store i64 1, ptr %161, align 4
  %162 = getelementptr ptr, ptr %142, i32 9
  store ptr %161, ptr %162, align 8
  %163 = getelementptr %.12, ptr %141, i32 0, i32 10
  store i32 0, ptr %163, align 4
  %164 = getelementptr ptr, ptr %142, i32 10
  store ptr %163, ptr %164, align 8
  %165 = getelementptr %.12, ptr %141, i32 0, i32 11
  store ptr null, ptr %165, align 8
  %166 = getelementptr ptr, ptr %142, i32 11
  store ptr %165, ptr %166, align 8
  %167 = getelementptr %.12, ptr %141, i32 0, i32 12
  store i32 7, ptr %167, align 4
  %168 = getelementptr ptr, ptr %142, i32 12
  store ptr %167, ptr %168, align 8
  %169 = getelementptr %.12, ptr %141, i32 0, i32 13
  store ptr %126, ptr %169, align 8
  %170 = getelementptr ptr, ptr %142, i32 13
  store ptr %169, ptr %170, align 8
  call void @disc_ral_call(ptr %0, ptr @ral_kernel_launch___gpu___pvoid_ppvoid_i64_pvoid_i64_i64_i64_i64_i64_i64_i32_pvoid_i32_ppvoid___void, ptr %142)
  br label %307

171:                                              ; preds = %1
  %172 = icmp sle i64 %17, 0
  %173 = sub i64 0, %17
  %174 = sub i64 %17, 1
  %175 = select i1 %172, i64 %173, i64 %174
  %176 = sdiv i64 %175, 256
  %177 = sub i64 0, %176
  %178 = add i64 %176, 1
  %179 = select i1 %172, i64 %177, i64 %178
  %180 = alloca ptr, align 8
  %181 = getelementptr ptr, ptr %180, i32 0
  store ptr @main_kernel_1_blob_gpu.binary, ptr %181, align 8
  %182 = extractvalue { ptr, ptr, i64, [1 x i64], [1 x i64] } %33, 0
  %183 = extractvalue { ptr, ptr, i64, [1 x i64], [1 x i64] } %33, 1
  %184 = extractvalue { ptr, ptr, i64, [1 x i64], [1 x i64] } %33, 2
  %185 = extractvalue { ptr, ptr, i64, [1 x i64], [1 x i64] } %33, 3, 0
  %186 = extractvalue { ptr, ptr, i64, [1 x i64], [1 x i64] } %33, 4, 0
  %187 = alloca %.2, align 8
  %188 = alloca ptr, i32 3, align 8
  %189 = getelementptr %.2, ptr %187, i32 0, i32 0
  store i64 256, ptr %189, align 4
  %190 = getelementptr ptr, ptr %188, i32 0
  store ptr %189, ptr %190, align 8
  %191 = getelementptr %.2, ptr %187, i32 0, i32 1
  store i64 %17, ptr %191, align 4
  %192 = getelementptr ptr, ptr %188, i32 1
  store ptr %191, ptr %192, align 8
  %193 = getelementptr %.2, ptr %187, i32 0, i32 2
  store ptr %183, ptr %193, align 8
  %194 = getelementptr ptr, ptr %188, i32 2
  store ptr %193, ptr %194, align 8
  %195 = alloca %.3, align 8
  %196 = alloca ptr, i32 14, align 8
  %197 = getelementptr %.3, ptr %195, i32 0, i32 0
  store ptr %0, ptr %197, align 8
  %198 = getelementptr ptr, ptr %196, i32 0
  store ptr %197, ptr %198, align 8
  %199 = getelementptr %.3, ptr %195, i32 0, i32 1
  store ptr %180, ptr %199, align 8
  %200 = getelementptr ptr, ptr %196, i32 1
  store ptr %199, ptr %200, align 8
  %201 = getelementptr %.3, ptr %195, i32 0, i32 2
  store i64 1, ptr %201, align 4
  %202 = getelementptr ptr, ptr %196, i32 2
  store ptr %201, ptr %202, align 8
  %203 = getelementptr %.3, ptr %195, i32 0, i32 3
  store ptr @main_kernel_1_main_kColReduction_reduce__4_1_0___thin_kernel_name, ptr %203, align 8
  %204 = getelementptr ptr, ptr %196, i32 3
  store ptr %203, ptr %204, align 8
  %205 = getelementptr %.3, ptr %195, i32 0, i32 4
  store i64 %179, ptr %205, align 4
  %206 = getelementptr ptr, ptr %196, i32 4
  store ptr %205, ptr %206, align 8
  %207 = getelementptr %.3, ptr %195, i32 0, i32 5
  store i64 1, ptr %207, align 4
  %208 = getelementptr ptr, ptr %196, i32 5
  store ptr %207, ptr %208, align 8
  %209 = getelementptr %.3, ptr %195, i32 0, i32 6
  store i64 1, ptr %209, align 4
  %210 = getelementptr ptr, ptr %196, i32 6
  store ptr %209, ptr %210, align 8
  %211 = getelementptr %.3, ptr %195, i32 0, i32 7
  store i64 256, ptr %211, align 4
  %212 = getelementptr ptr, ptr %196, i32 7
  store ptr %211, ptr %212, align 8
  %213 = getelementptr %.3, ptr %195, i32 0, i32 8
  store i64 1, ptr %213, align 4
  %214 = getelementptr ptr, ptr %196, i32 8
  store ptr %213, ptr %214, align 8
  %215 = getelementptr %.3, ptr %195, i32 0, i32 9
  store i64 1, ptr %215, align 4
  %216 = getelementptr ptr, ptr %196, i32 9
  store ptr %215, ptr %216, align 8
  %217 = getelementptr %.3, ptr %195, i32 0, i32 10
  store i32 0, ptr %217, align 4
  %218 = getelementptr ptr, ptr %196, i32 10
  store ptr %217, ptr %218, align 8
  %219 = getelementptr %.3, ptr %195, i32 0, i32 11
  store ptr null, ptr %219, align 8
  %220 = getelementptr ptr, ptr %196, i32 11
  store ptr %219, ptr %220, align 8
  %221 = getelementptr %.3, ptr %195, i32 0, i32 12
  store i32 3, ptr %221, align 4
  %222 = getelementptr ptr, ptr %196, i32 12
  store ptr %221, ptr %222, align 8
  %223 = getelementptr %.3, ptr %195, i32 0, i32 13
  store ptr %188, ptr %223, align 8
  %224 = getelementptr ptr, ptr %196, i32 13
  store ptr %223, ptr %224, align 8
  call void @disc_ral_call(ptr %0, ptr @ral_kernel_launch___gpu___pvoid_ppvoid_i64_pvoid_i64_i64_i64_i64_i64_i64_i32_pvoid_i32_ppvoid___void, ptr %196)
  %225 = icmp eq i64 %17, 0
  %226 = sub i64 %17, 1
  %227 = udiv i64 %226, 32
  %228 = add i64 %227, 1
  %229 = select i1 %225, i64 0, i64 %228
  %230 = icmp eq i64 %13, 0
  %231 = sub i64 %13, 1
  %232 = udiv i64 %231, 512
  %233 = add i64 %232, 1
  %234 = select i1 %230, i64 0, i64 %233
  %235 = mul i64 %229, %234
  %236 = mul i64 %235, 256
  %237 = icmp sle i64 %236, 0
  %238 = sub i64 0, %236
  %239 = sub i64 %236, 1
  %240 = select i1 %237, i64 %238, i64 %239
  %241 = sdiv i64 %240, 256
  %242 = sub i64 0, %241
  %243 = add i64 %241, 1
  %244 = select i1 %237, i64 %242, i64 %243
  %245 = alloca ptr, align 8
  %246 = getelementptr ptr, ptr %245, i32 0
  store ptr @main_kernel_2_blob_gpu.binary, ptr %246, align 8
  %247 = extractvalue { ptr, ptr, i64, [3 x i64], [3 x i64] } %10, 0
  %248 = extractvalue { ptr, ptr, i64, [3 x i64], [3 x i64] } %10, 1
  %249 = extractvalue { ptr, ptr, i64, [3 x i64], [3 x i64] } %10, 2
  %250 = extractvalue { ptr, ptr, i64, [3 x i64], [3 x i64] } %10, 3, 0
  %251 = extractvalue { ptr, ptr, i64, [3 x i64], [3 x i64] } %10, 3, 1
  %252 = extractvalue { ptr, ptr, i64, [3 x i64], [3 x i64] } %10, 3, 2
  %253 = extractvalue { ptr, ptr, i64, [3 x i64], [3 x i64] } %10, 4, 0
  %254 = extractvalue { ptr, ptr, i64, [3 x i64], [3 x i64] } %10, 4, 1
  %255 = extractvalue { ptr, ptr, i64, [3 x i64], [3 x i64] } %10, 4, 2
  %256 = extractvalue { ptr, ptr, i64, [1 x i64], [1 x i64] } %33, 0
  %257 = extractvalue { ptr, ptr, i64, [1 x i64], [1 x i64] } %33, 1
  %258 = extractvalue { ptr, ptr, i64, [1 x i64], [1 x i64] } %33, 2
  %259 = extractvalue { ptr, ptr, i64, [1 x i64], [1 x i64] } %33, 3, 0
  %260 = extractvalue { ptr, ptr, i64, [1 x i64], [1 x i64] } %33, 4, 0
  %261 = alloca %.4, align 8
  %262 = alloca ptr, i32 7, align 8
  %263 = getelementptr %.4, ptr %261, i32 0, i32 0
  store i64 %17, ptr %263, align 4
  %264 = getelementptr ptr, ptr %262, i32 0
  store ptr %263, ptr %264, align 8
  %265 = getelementptr %.4, ptr %261, i32 0, i32 1
  store ptr %248, ptr %265, align 8
  %266 = getelementptr ptr, ptr %262, i32 1
  store ptr %265, ptr %266, align 8
  %267 = getelementptr %.4, ptr %261, i32 0, i32 2
  store i64 %250, ptr %267, align 4
  %268 = getelementptr ptr, ptr %262, i32 2
  store ptr %267, ptr %268, align 8
  %269 = getelementptr %.4, ptr %261, i32 0, i32 3
  store i64 256, ptr %269, align 4
  %270 = getelementptr ptr, ptr %262, i32 3
  store ptr %269, ptr %270, align 8
  %271 = getelementptr %.4, ptr %261, i32 0, i32 4
  store i64 %236, ptr %271, align 4
  %272 = getelementptr ptr, ptr %262, i32 4
  store ptr %271, ptr %272, align 8
  %273 = getelementptr %.4, ptr %261, i32 0, i32 5
  store i64 %229, ptr %273, align 4
  %274 = getelementptr ptr, ptr %262, i32 5
  store ptr %273, ptr %274, align 8
  %275 = getelementptr %.4, ptr %261, i32 0, i32 6
  store ptr %257, ptr %275, align 8
  %276 = getelementptr ptr, ptr %262, i32 6
  store ptr %275, ptr %276, align 8
  %277 = alloca %.5, align 8
  %278 = alloca ptr, i32 14, align 8
  %279 = getelementptr %.5, ptr %277, i32 0, i32 0
  store ptr %0, ptr %279, align 8
  %280 = getelementptr ptr, ptr %278, i32 0
  store ptr %279, ptr %280, align 8
  %281 = getelementptr %.5, ptr %277, i32 0, i32 1
  store ptr %245, ptr %281, align 8
  %282 = getelementptr ptr, ptr %278, i32 1
  store ptr %281, ptr %282, align 8
  %283 = getelementptr %.5, ptr %277, i32 0, i32 2
  store i64 1, ptr %283, align 4
  %284 = getelementptr ptr, ptr %278, i32 2
  store ptr %283, ptr %284, align 8
  %285 = getelementptr %.5, ptr %277, i32 0, i32 3
  store ptr @main_kernel_2_main_kColReduction_reduce__4_1_0___thin_1_kernel_name, ptr %285, align 8
  %286 = getelementptr ptr, ptr %278, i32 3
  store ptr %285, ptr %286, align 8
  %287 = getelementptr %.5, ptr %277, i32 0, i32 4
  store i64 %244, ptr %287, align 4
  %288 = getelementptr ptr, ptr %278, i32 4
  store ptr %287, ptr %288, align 8
  %289 = getelementptr %.5, ptr %277, i32 0, i32 5
  store i64 1, ptr %289, align 4
  %290 = getelementptr ptr, ptr %278, i32 5
  store ptr %289, ptr %290, align 8
  %291 = getelementptr %.5, ptr %277, i32 0, i32 6
  store i64 1, ptr %291, align 4
  %292 = getelementptr ptr, ptr %278, i32 6
  store ptr %291, ptr %292, align 8
  %293 = getelementptr %.5, ptr %277, i32 0, i32 7
  store i64 256, ptr %293, align 4
  %294 = getelementptr ptr, ptr %278, i32 7
  store ptr %293, ptr %294, align 8
  %295 = getelementptr %.5, ptr %277, i32 0, i32 8
  store i64 1, ptr %295, align 4
  %296 = getelementptr ptr, ptr %278, i32 8
  store ptr %295, ptr %296, align 8
  %297 = getelementptr %.5, ptr %277, i32 0, i32 9
  store i64 1, ptr %297, align 4
  %298 = getelementptr ptr, ptr %278, i32 9
  store ptr %297, ptr %298, align 8
  %299 = getelementptr %.5, ptr %277, i32 0, i32 10
  store i32 0, ptr %299, align 4
  %300 = getelementptr ptr, ptr %278, i32 10
  store ptr %299, ptr %300, align 8
  %301 = getelementptr %.5, ptr %277, i32 0, i32 11
  store ptr null, ptr %301, align 8
  %302 = getelementptr ptr, ptr %278, i32 11
  store ptr %301, ptr %302, align 8
  %303 = getelementptr %.5, ptr %277, i32 0, i32 12
  store i32 7, ptr %303, align 4
  %304 = getelementptr ptr, ptr %278, i32 12
  store ptr %303, ptr %304, align 8
  %305 = getelementptr %.5, ptr %277, i32 0, i32 13
  store ptr %262, ptr %305, align 8
  %306 = getelementptr ptr, ptr %278, i32 13
  store ptr %305, ptr %306, align 8
  call void @disc_ral_call(ptr %0, ptr @ral_kernel_launch___gpu___pvoid_ppvoid_i64_pvoid_i64_i64_i64_i64_i64_i64_i32_pvoid_i32_ppvoid___void, ptr %278)
  br label %307

307:                                              ; preds = %35, %171
  %308 = alloca i64, i64 ptrtoint (ptr getelementptr (i64, ptr null, i64 2) to i64), align 8
  %309 = insertvalue { ptr, ptr, i64, [1 x i64], [1 x i64] } undef, ptr %308, 0
  %310 = insertvalue { ptr, ptr, i64, [1 x i64], [1 x i64] } %309, ptr %308, 1
  %311 = insertvalue { ptr, ptr, i64, [1 x i64], [1 x i64] } %310, i64 0, 2
  %312 = insertvalue { ptr, ptr, i64, [1 x i64], [1 x i64] } %311, i64 2, 3, 0
  %313 = insertvalue { ptr, ptr, i64, [1 x i64], [1 x i64] } %312, i64 1, 4, 0
  %314 = extractvalue { ptr, ptr, i64, [1 x i64], [1 x i64] } %313, 1
  %315 = getelementptr i64, ptr %314, i64 0
  store i64 %12, ptr %315, align 4
  %316 = extractvalue { ptr, ptr, i64, [1 x i64], [1 x i64] } %313, 1
  %317 = getelementptr i64, ptr %316, i64 1
  store i64 %11, ptr %317, align 4
  %318 = extractvalue { ptr, ptr, i64, [1 x i64], [1 x i64] } %33, 0
  %319 = extractvalue { ptr, ptr, i64, [1 x i64], [1 x i64] } %33, 1
  %320 = extractvalue { ptr, ptr, i64, [1 x i64], [1 x i64] } %33, 2
  %321 = extractvalue { ptr, ptr, i64, [1 x i64], [1 x i64] } %33, 3, 0
  %322 = extractvalue { ptr, ptr, i64, [1 x i64], [1 x i64] } %33, 4, 0
  %323 = extractvalue { ptr, ptr, i64, [1 x i64], [1 x i64] } %313, 0
  %324 = extractvalue { ptr, ptr, i64, [1 x i64], [1 x i64] } %313, 1
  %325 = extractvalue { ptr, ptr, i64, [1 x i64], [1 x i64] } %313, 2
  %326 = extractvalue { ptr, ptr, i64, [1 x i64], [1 x i64] } %313, 3, 0
  %327 = extractvalue { ptr, ptr, i64, [1 x i64], [1 x i64] } %313, 4, 0
  %328 = alloca %.6, align 8
  %329 = alloca ptr, i32 13, align 8
  %330 = getelementptr %.6, ptr %328, i32 0, i32 0
  store ptr %0, ptr %330, align 8
  %331 = getelementptr ptr, ptr %329, i32 0
  store ptr %330, ptr %331, align 8
  %332 = getelementptr %.6, ptr %328, i32 0, i32 1
  store ptr null, ptr %332, align 8
  %333 = getelementptr ptr, ptr %329, i32 1
  store ptr %332, ptr %333, align 8
  %334 = getelementptr %.6, ptr %328, i32 0, i32 2
  store ptr %318, ptr %334, align 8
  %335 = getelementptr ptr, ptr %329, i32 2
  store ptr %334, ptr %335, align 8
  %336 = getelementptr %.6, ptr %328, i32 0, i32 3
  store ptr %319, ptr %336, align 8
  %337 = getelementptr ptr, ptr %329, i32 3
  store ptr %336, ptr %337, align 8
  %338 = getelementptr %.6, ptr %328, i32 0, i32 4
  store i64 %320, ptr %338, align 4
  %339 = getelementptr ptr, ptr %329, i32 4
  store ptr %338, ptr %339, align 8
  %340 = getelementptr %.6, ptr %328, i32 0, i32 5
  store i64 %321, ptr %340, align 4
  %341 = getelementptr ptr, ptr %329, i32 5
  store ptr %340, ptr %341, align 8
  %342 = getelementptr %.6, ptr %328, i32 0, i32 6
  store i64 %322, ptr %342, align 4
  %343 = getelementptr ptr, ptr %329, i32 6
  store ptr %342, ptr %343, align 8
  %344 = getelementptr %.6, ptr %328, i32 0, i32 7
  store ptr %323, ptr %344, align 8
  %345 = getelementptr ptr, ptr %329, i32 7
  store ptr %344, ptr %345, align 8
  %346 = getelementptr %.6, ptr %328, i32 0, i32 8
  store ptr %324, ptr %346, align 8
  %347 = getelementptr ptr, ptr %329, i32 8
  store ptr %346, ptr %347, align 8
  %348 = getelementptr %.6, ptr %328, i32 0, i32 9
  store i64 %325, ptr %348, align 4
  %349 = getelementptr ptr, ptr %329, i32 9
  store ptr %348, ptr %349, align 8
  %350 = getelementptr %.6, ptr %328, i32 0, i32 10
  store i64 %326, ptr %350, align 4
  %351 = getelementptr ptr, ptr %329, i32 10
  store ptr %350, ptr %351, align 8
  %352 = getelementptr %.6, ptr %328, i32 0, i32 11
  store i64 %327, ptr %352, align 4
  %353 = getelementptr ptr, ptr %329, i32 11
  store ptr %352, ptr %353, align 8
  %354 = getelementptr %.6, ptr %328, i32 0, i32 12
  %355 = getelementptr ptr, ptr %329, i32 12
  store ptr %354, ptr %355, align 8
  call void @disc_ral_call(ptr %0, ptr @inc_ref___gpu___pvoid_pvoid_m1df32_m1di64___m2df32, ptr %329)
  %356 = load { ptr, ptr, i64, [2 x i64], [2 x i64] }, ptr %354, align 8
  %357 = extractvalue { ptr, ptr, i64, [2 x i64], [2 x i64] } %356, 0
  %358 = extractvalue { ptr, ptr, i64, [2 x i64], [2 x i64] } %356, 1
  %359 = insertvalue { ptr, ptr, i64, [2 x i64], [2 x i64] } undef, ptr %357, 0
  %360 = insertvalue { ptr, ptr, i64, [2 x i64], [2 x i64] } %359, ptr %358, 1
  %361 = insertvalue { ptr, ptr, i64, [2 x i64], [2 x i64] } %360, i64 0, 2
  %362 = insertvalue { ptr, ptr, i64, [2 x i64], [2 x i64] } %361, i64 %12, 3, 0
  %363 = insertvalue { ptr, ptr, i64, [2 x i64], [2 x i64] } %362, i64 %11, 4, 0
  %364 = insertvalue { ptr, ptr, i64, [2 x i64], [2 x i64] } %363, i64 %11, 3, 1
  %365 = insertvalue { ptr, ptr, i64, [2 x i64], [2 x i64] } %364, i64 1, 4, 1
  %366 = extractvalue { ptr, ptr, i64, [1 x i64], [1 x i64] } %33, 0
  %367 = alloca %.7, align 8
  %368 = alloca ptr, i32 2, align 8
  %369 = getelementptr %.7, ptr %367, i32 0, i32 0
  store ptr %0, ptr %369, align 8
  %370 = getelementptr ptr, ptr %368, i32 0
  store ptr %369, ptr %370, align 8
  %371 = getelementptr %.7, ptr %367, i32 0, i32 1
  store ptr %366, ptr %371, align 8
  %372 = getelementptr ptr, ptr %368, i32 1
  store ptr %371, ptr %372, align 8
  call void @disc_ral_call(ptr %0, ptr @dealloc___gpu___pvoid_pvoid___void, ptr %368)
  %373 = extractvalue { ptr, ptr, i64, [2 x i64], [2 x i64] } %365, 0
  %374 = extractvalue { ptr, ptr, i64, [2 x i64], [2 x i64] } %365, 1
  %375 = extractvalue { ptr, ptr, i64, [2 x i64], [2 x i64] } %365, 2
  %376 = extractvalue { ptr, ptr, i64, [2 x i64], [2 x i64] } %365, 3, 0
  %377 = extractvalue { ptr, ptr, i64, [2 x i64], [2 x i64] } %365, 3, 1
  %378 = extractvalue { ptr, ptr, i64, [2 x i64], [2 x i64] } %365, 4, 0
  %379 = extractvalue { ptr, ptr, i64, [2 x i64], [2 x i64] } %365, 4, 1
  %380 = alloca %.8, align 8
  %381 = alloca ptr, i32 9, align 8
  %382 = getelementptr %.8, ptr %380, i32 0, i32 0
  store ptr %0, ptr %382, align 8
  %383 = getelementptr ptr, ptr %381, i32 0
  store ptr %382, ptr %383, align 8
  %384 = getelementptr %.8, ptr %380, i32 0, i32 1
  store i64 0, ptr %384, align 4
  %385 = getelementptr ptr, ptr %381, i32 1
  store ptr %384, ptr %385, align 8
  %386 = getelementptr %.8, ptr %380, i32 0, i32 2
  store ptr %373, ptr %386, align 8
  %387 = getelementptr ptr, ptr %381, i32 2
  store ptr %386, ptr %387, align 8
  %388 = getelementptr %.8, ptr %380, i32 0, i32 3
  store ptr %374, ptr %388, align 8
  %389 = getelementptr ptr, ptr %381, i32 3
  store ptr %388, ptr %389, align 8
  %390 = getelementptr %.8, ptr %380, i32 0, i32 4
  store i64 %375, ptr %390, align 4
  %391 = getelementptr ptr, ptr %381, i32 4
  store ptr %390, ptr %391, align 8
  %392 = getelementptr %.8, ptr %380, i32 0, i32 5
  store i64 %376, ptr %392, align 4
  %393 = getelementptr ptr, ptr %381, i32 5
  store ptr %392, ptr %393, align 8
  %394 = getelementptr %.8, ptr %380, i32 0, i32 6
  store i64 %377, ptr %394, align 4
  %395 = getelementptr ptr, ptr %381, i32 6
  store ptr %394, ptr %395, align 8
  %396 = getelementptr %.8, ptr %380, i32 0, i32 7
  store i64 %378, ptr %396, align 4
  %397 = getelementptr ptr, ptr %381, i32 7
  store ptr %396, ptr %397, align 8
  %398 = getelementptr %.8, ptr %380, i32 0, i32 8
  store i64 %379, ptr %398, align 4
  %399 = getelementptr ptr, ptr %381, i32 8
  store ptr %398, ptr %399, align 8
  call void @disc_ral_call(ptr %0, ptr @ral_send_output___cpu___pvoid_i64_m2df32___void, ptr %381)
  ret void
}

host default target triple: x86_64-unknown-linux-gnu
host cpu name: icelake-server
host cpu features: -avx512pf,-tsxldtrk,+cx16,+sahf,-tbm,+avx512ifma,+sha,+crc32,-fma4,+vpclmulqdq,+prfchw,+bmi2,-cldemote,+fsgsbase,-avx512bf16,-amx-tile,-raoint,-uintr,+gfni,+popcnt,-ptwrite,+aes,+avx512bitalg,-movdiri,-widekl,+xsaves,-avx512er,-avxvnni,-avx512fp16,+avx512vnni,-amx-bf16,-avxvnniint8,+avx512vpopcntdq,-pconfig,+clwb,-cmpccxadd,+avx512f,+xsavec,-clzero,-pku,-amx-fp16,+mmx,-lwp,+rdpid,-xop,+rdseed,-waitpkg,-prefetchi,-kl,-movdir64b,-sse4a,+avx512bw,-avxneconvert,+clflushopt,+xsave,+avx512vbmi2,+64bit,+avx512vl,-serialize,-hreset,+invpcid,+avx512cd,+avx,+vaes,-amx-int8,+cx8,+fma,-rtm,+bmi,-enqcmd,+rdrnd,-mwaitx,+sse4.1,+sse4.2,+avx2,+fxsr,+wbnoinvd,+sse,+lzcnt,+pclmul,-rdpru,-avxifma,+f16c,+ssse3,-sgx,-prefetchwt1,+cmov,+avx512vbmi,-shstk,+movbe,-avx512vp2intersect,+xsaveopt,+avx512dq,+sse2,+adx,+sse3
after optimize llvm module:
; ModuleID = 'LLVMDialectModule'
source_filename = "LLVMDialectModule"
target datalayout = "e-m:e-p270:32:32-p271:32:32-p272:64:64-i64:64-f80:128-n8:16:32:64-S128"
target triple = "x86_64-unknown-linux-gnu"

%0 = type { ptr, i64, { ptr, ptr, i64, [3 x i64], [3 x i64] } }
%.1 = type { ptr, i64, ptr }
%.9 = type { i64, i64, ptr }
%.10 = type { ptr, ptr, i64, ptr, i64, i64, i64, i64, i64, i64, i32, ptr, i32, ptr }
%.11 = type { i64, ptr, i64, i64, i64, i64, ptr }
%.12 = type { ptr, ptr, i64, ptr, i64, i64, i64, i64, i64, i64, i32, ptr, i32, ptr }
%.2 = type { i64, i64, ptr }
%.3 = type { ptr, ptr, i64, ptr, i64, i64, i64, i64, i64, i64, i32, ptr, i32, ptr }
%.4 = type { i64, ptr, i64, i64, i64, i64, ptr }
%.5 = type { ptr, ptr, i64, ptr, i64, i64, i64, i64, i64, i64, i32, ptr, i32, ptr }
%.6 = type { ptr, ptr, ptr, ptr, i64, i64, i64, ptr, ptr, i64, i64, i64, { ptr, ptr, i64, [2 x i64], [2 x i64] } }
%.7 = type { ptr, ptr }
%.8 = type { ptr, i64, ptr, ptr, i64, i64, i64, i64, i64 }

@main_kernel_0_main_kColReduction_reduce__4_1_0___flat_1_kernel_name = internal constant [42 x i8] c"main_kColReduction_reduce__4_1_0___flat_1\00"
@main_kernel_0_blob_gpu.binary = internal constant [2720 x i8] c"P\EDU\BA\01\00\10\00\90\0A\00\00\00\00\00\00\02\00\01\01@\00\00\00P\0A\00\00\00\00\00\00P\0A\00\00\00\00\00\00\07\00\01\00P\00\00\00\00\00\00\00\00\00\00\00\11 \00\00\00\00\00\00\00\00\00\00\00\00\00\00\E8\1C\00\00\00\00\00\00\A2\7FELF\02\01\013\07\00\01\00f\02\00\BE\00q\00\01\00!@\1C\07\001\00\80\19\07\00\F5\0E\00P\05P\00@\008\00\03\00@\00\0B\00\01\00\00.shstrtab\00.\08\00'ym\08\00\F0\1B_shndx\00.nv.info\00.text.main_kColReduction_r\0A\00\F5\02e__4_1_0___flat_19\00\0F3\00\1Coshared5\00\19Orela\9D\00\1E?rel\D1\00!\9Fconstant08\00\19\B2debug_framey\00\09\11\00!nv\14\00\11aD\00\0F\98\01 \0F\89\00\16\0F\C2\01\FF(o_param\C9\01\1C\0F\01\00\0B\8C\\\00\00\00\03\00\0A\00\01\00 ]\01\18\00,\09\00\01\00\11\9C\18\00,\04\00\01\00\11\BA\18\00,\07\00\01\00f2\00\00\00\12\10`\00!\80\11\07\00v\00\FF\FF\FF\FF(\00\0C\00\00\01\00@\03\00\04|\08\00\D1\0F\0C\81\80\80(\00\08\FF\81\80(\08\0B\00\06(\00/0\00\01\00\03\13pX\00\10\04\9B\00R\04\14\00\00\00E\00\22\04 \AC\00\90\04/\08\00\05\00\00\00\16\0C\00\12#\0C\00\00\01\00'\04\12\0C\00\17\11\0C\00!7\04\C8\04\F1\08\015\00\00\04\0A\08\00\02\00\00\00`\01(\00\03\19(\00\04\17\0C$\00u\06\00 \00\00\F0!\10\00u\05\00\1C\00\00\F0\11\10\009\04\00\18\10\009\03\00\14\10\009\02\00\10\10\009\01\00\08P\00\01\01\00\F2\0A\F0\11\00\03\1B\FF\00\04\1C\0C\00P\00\00\00\10\02\00\00\E0\10\00\00\04\1E\D8\00#K\00\01\00\B1\02\02\08\10\0A/\22\00\00\00\08\10\00#\00\08\08\00\13\10\08\00\13\18\08\00\13 \08\00\13(\08\00\130\08\00\228\08\84\00\13\00\08\00\13\08\08\00\13\10\08\00\13\18\08\00\13 \08\00\13(\08\00\130\08\00\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\A2\00\00\00\14,\00\00\00H\00\01\00\00x\01/\05\00\01\00\FF\88@$v\01\FF?\04\B2\FF\00\8E\07\00\C4\0F\00\19y\00\01\00\10%c\02Q\0E\00\19y\03\0F\00 \00!-\00@\0E\00$z]\04\B0\00\00\03\02\8E\07\00\CA\1F\00\0C\10\00\C5^\00\00p`\F0\03\00\DA\0F\00Ms\04\F0\0C\80\03\00\EA\0F\00\06{\04\00\00_\00\00\00\90 \00\00\22\0E\00\19x\05\FF\1F\1F\00\D3\14\01\00\00\E2\0F\00$r\02\FF\FF\00\80\00 \E2\0FP\00\10\FF0\00\B1pP\F4\03\00\E4\0F\00\11r\05?\02\B2\FFH\8F\07\00\C6\0F\00\08s\04\BE\04\10\10\90\00\F1\07\1E\00\10x\03\04\FE\FF\FF\0F\FF\E0\FF\07\00\CC\1F\00\05s\03\00\B2\00\22\F0!\B0\00!r\07`\00\C0\03\0A\8E\07\00\C8\1F\00$z\07\07`\00\10\FF\C0\00\81\C8\0F\00'r\07\03\07\A8\02\02\80\00@\19x\02\FF\C3\00\10\05\A0\00\E1\E4\0F\00\12x\05\05\00\FE\FF\FF\FF\C0\8E\80\00T'r\07\07\02\B0\00\10\C8\C0\00\11\09`\00#\07\0A\10\000z\03\09`\00\11\02 \01\01\D0\006\03\00_ \010\10\0A\03\10\00\11\80\B0\00\80\E4\0F\00\10\08\07\07\01P\00\03\10\00\060\00\12\F20\00\1A\18 \001\12\AA\07 \01\22\FF3`\00;$r\03\80\00%\02\03\80\00`\E4\0F\00$x\03\14\04$\00\05 \000x\03\02\95\03\18\03\A0\00\1FX\C0\01\07W$x\02\07 \A0\01\81\B9z\04\00\00F\00\00s\06\00\C0\01Dt\00\FF\04 \00\11\C6P\00\10\02\95\060pb\F8\C0\01U\04\10x\06\02\C0\00\11\C8 \00\12\06 \00\C3\FA\03\00\CE\0F\00$\CA\05\02\00X\90\00\90\C8\0F\00%\C6\04\05\00Z\91\01\03\B0\005\DA\07\06 \00a\E2\0F\00\81\C9\0A#\06\C5\00\19\1E\0C\00\A6\00\00%\D6\06\070\00u\CA\0F\00\81\D9\0D\06 \00\86\E2\02\00\10x\08\02\02@\01j\04\10x\10\02\03p\01\12\08\90\00#\F0\03\10\00\12\10\10\00\11\F2\10\00Y\10x\0C\02\04@\004\0E\02\05\10\00\11\C40\00\12\0C0\00 \F4\03\80\00E$\8A\09\08\B0\00w\E4\0F\10$\9A\0B\10\10\00U\00%\86\08\09\B0\00\02\F0\02\12\0E@\00 \F6\03\F0\00E\81\89\05\08\C0\00v\22\11\00%\96\06\0B0\00 /\00\C0\00\14\06\80\00w\C6\0F\00$\AA\0B\0C \01F\10\81\99\0F\00\01\B4\22\03\00$t\04\FF\00\00\80\FF\C0\01\00\D0\00\14\07@\00\00\C0\015\BA\09\0E@\00W\C8\1F\00%\B6\A0\00g\CC\0F\00\81\B9\09\90\00@\01\00\0B\C8\FA\01\D2\80\FF\00\C2\FC\03\00\C8O\00\08\C8\04\10\000\02\00\03P\007%\A6\0A\B0\00\08P\01\01\10\02\F5\06\00\0B\D2\00\04\0D\00\00@\00`\FC\03\00\C6\8F\00\81\A9\0B\0A`\00b\E2\04\00\08\D2\04 \00!\00\000\00\06`\01\12\FA`\01&\CA\07P\01z\C8/\00%\C6\06\070\02*\0D\0C0\02\16\11\10\01y\E6\02\00%\D6\0C\0D\E0\006\D9\0D\0C\80\00t\00\00\0B\82\00\04\05\A0\00\84\E4\0F\09\10x\0A\02\080\01\8A\E4O\00\08\82\04\04\05\A0\00\18\0A@\02V\0B\92\00\04\0F@\00\01\80\02\14\09@\00\8A\C4\1F\00\08\92\04\04\0F@\00\1A\08p\028\0E\02\0A\E0\016\8A\07\0A\D0\00\11/\C0\01\17\0B \00+%\86\00\03&\89\05\F0\00!\A2\000\02\14\0C0\00\00\A0\00D\A2\00\04\0B\A0\00\8A\C8\8F\00\08\A2\04\04\0B\90\00\15\0E\D0\02\000\00D\B2\00\04\090\00\000\02&\9A\0B\E0\02\00\90\05D\B2\04\04\09@\00\00\00\02(\96\08\00\02\16\08\C0\01\12\F6P\00D\C2\00\04\11P\00x\E4\0F\08\81\99\0F\08\B0\03H$\AA\0B\0E\C0\01Z\08\C2\04\04\11\A0\00\06P\02\0Fp\02\00\00@\01\17\0D\A0\03\09p\02f\E2\0F\08$\BA\07 \02*\E4\1F\80\02.\22\01\80\02\05\10\01\12\FAp\00\1B\B6p\028\CA\09\10\A0\01'\81\B9p\02{&\03\00%\C6\08\09\A0\02\1A\0E\A0\02\16\13\00\01/&\01\A0\02\0B+\22\03\A0\02 O\08\A0\02\15\0E\E0\01\1F\1F\A0\02\0E\00 \01\1B\0F\10\05\17\100\01\00`\02\14\11\10\00*\CE/\80\02\1F\C8p\02\0F\09\00\03/\C8\8F\F0\02\01\18\0E\F0\02\0A\90\02/\0F\01\90\02\00\1C\10\90\02\05@\02\00 \03)\9A\09P\01'\08\B20\02\11\C6\90\02\06\80\01\0F\90\02\04\14\13P\000\E4\0F\08 \01\14\12\00\01\11\E2`\05\18\10`\00\0C\B0\02\00\A0\02\1E\13\E0\00\0F\A0\02\06\09\90\02\01p\018\10\02\13p\00\08\A0\02*\E2\1F\90\02-\E2\0F\B0\02\06 \01\0F\A0\02\06\1F\0E\A0\02\1C\1F\10\A0\02=\1F\14\A0\02\1C\1B\15\A0\02\1B\16\A0\02\1F\17\A0\02\A7\19\10\A0\02\02@\00\06\90\02\1D\E2\B0\02\0F\A0\02\02\14\18\00\01\0F\A0\02\1D\1F\C4\A0\02\0E\19\08\A0\02\01p\00?\10\02\19\A0\02U\10\E4\A0\02\16\07\B0\01\1F$\A0\02\11\1F\11\A0\02.\1F\1A\A0\02\1A5\06\02\1B0\00\11/\10\0A\1B\1C\A0\02\14\1D\10\00\11\CE\10\0A\0B\A0\02+\08\09\A0\02\1C\08\B0\07\19\1E\00\036\02\02\1F\10\00\0F\C0\02\11\1F\06\C0\02\1C\1C\0E\C0\02\19\07\C0\02,\0B\06`\05\14\07@\00+\C6\0F\A0\0A\0F\F0\07\0C\00\80\07\08\E0\07\10\E4\B0\02,\07\06\B0\02\1E\11`\03\0FP\05\13\11\C6\D0\0A\0F\A0\02\18\08\B0\0C\04\A0\02\0Bp\02\17\CAp\02\1B\E4\10\0B\10$\B0\0C)\0C\0D0\009\DA\0F\020\009\C9\0D\0C0\00:\D6\0E\0F\A0\02(\0F\0E\90\00g%v\02\03\00` \025y\06\02 \00*b!\C0\02*\C8O\B0\02\12\C8 \02\02\C0\01\15\F0 \02\03\B0\018\00\00\C8\10\02\1C\F0\10\02\02 \00\05\F0\09\01 \00\08\E0\09\02 \00\14\C2p\01\03 \00\15\C2`\01\02 \00\14\D2\A0\02\03 \00\15\D2\A0\02\10\00`\0C2\0Br\00\F0\01 @\F0p\017\0EFyp\0ES\E6\0F\00\08r\10\02\00\01\00\80\CC\0F\00\A9s\07\02\06\E0\0E\B0\E1\1E\00\00\A4\0E\00\0Cr\00\07\10\00 pR@\00RO\00$r\06\00\0F\00\E0\0C\80\D8\0F\00G\09\00\00\90i\14!\FF\83\C0\0E*My\D0\0ETGy\00\00\F0 \00f\C0\0F\00\18y\00\01\00\0F\10\00`\0F\01\00-\00\\\0F.\03\00\01\00\02q\01]\00\00\98\01\000\00\08\01\00\1F\0B@\00\04\13\D8)\00\1F\C9@\00\0C\13\13\E4\13\0C\01\00\13\A8U\00\11\90\06\00\02$\00#\04\00]\14\00\CE\14\12\00\01\00.k\01T\00\00\01\00\138\05\02/p\00\80\00\0B\1F)'\00\03#\00\A8@\00\04p\16\04\E4\00*\04\00\01\00\1Fb@\00\04\13\D8)\00&\A8\00@\00\1F\0A@\00\00!\89\01D\01\0D@\00\11\80\B6\12J\00\00\D8\00\01\00\1B\08\08\00?x\01\00\A6\17\000\00\00X\E5\02\03\E7\12\04\80\00\17\048\00\04\18\00\133@\01\0C\84\01\13h@\00\17\881\01\0F\C0\00\01\132T\01+\06\00\01\00\1A\08\F0\17\12\03\C8\15:\16\80\00\01\00\13\06\E0\15\04(\1C\0D\88\01\1A\00\08\00\04\C0\00\13\018\00\04\A8\00\0C\01\009\18\13\00\08\00\088\00/\06\00\01\00\17\80\08\00\00\00\00\00\00\00"
@main_kernel_main_kColReduction_reduce__4_1_0___flat_kernel_name = internal constant [40 x i8] c"main_kColReduction_reduce__4_1_0___flat\00"
@main_kernel_blob_gpu.binary = internal constant [1096 x i8] c"P\EDU\BA\01\00\10\008\04\00\00\00\00\00\00\02\00\01\01@\00\00\00\F8\03\00\00\00\00\00\00\F7\03\00\00\00\00\00\00\07\00\01\00P\00\00\00\00\00\00\00\00\00\00\00\11 \00\00\00\00\00\00\00\00\00\00\00\00\00\00\E8\0B\00\00\00\00\00\00\A2\7FELF\02\01\013\07\00\01\00f\02\00\BE\00q\00\01\00!@\0B\07\001\00\80\08\07\00\F5\0E\00P\05P\00@\008\00\03\00@\00\0B\00\01\00\00.shstrtab\00.\08\00'ym\08\00\F0\1B_shndx\00.nv.info\00.text.main_kColReduction_r\0A\00\F5\00e__4_1_0___flat7\00\0F1\00\1Aoshared3\00\1A\9Fconstant06\00\17\FA\01debug_frame\00.rel\11\00!nv\14\00\11aB\00\0F'\01 \0F\87\00\14\0FO\01\B6o_paramV\01\1C\0F\01\00\07\8CZ\00\00\00\03\00\0A\00\01\00\11\EC\18\00,\09\00\01\00 )\01\18\00,\04\00\01\00\11G\18\00,\07\00\01\00f2\00\00\00\12\10`\00!\80\01\07\00v\00\FF\FF\FF\FF(\00\0C\00\00\01\00@\03\00\04|\08\00\D1\0F\0C\81\80\80(\00\08\FF\81\80(\08\0B\00\06(\00/0\00\01\00\03\13pX\00\10\04\9B\00R\04\14\00\00\00E\00\01\0B\00\00\13\00p/\08\00\05\00\00\00\9F\03\22\04#\0C\00\00\01\00'\04\12\0C\00\17\11\0C\00!7\04\E0\03\F3\08\015\00\00\04\0A\08\00\02\00\00\00`\01\10\00\03\19\10\00\04\17\0C\04\04U\08\00\00\F0!\10\00\10\01\18\01%\F0\11\10\00\01\01\00\F2\02\F0\11\00\03\1B\FF\00\04\1C\08\00P\00\00\00\B0\00\01\00#K\00\01\00s\02\02\08\10\0A/\22\9B\00\00\07\00\03\08\00\13\10\08\00\13\18\08\00\13 \08\00\13(\08\00\130\08\00\228\08|\00\13\00\08\00\13\08\08\00\13\10\08\00\13\18\08\00\13 \08\00\13(\08\00\130\08\00\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\A2\00\00\00\14,\00\00\00H\00\01\00\000\01/\05\00\01\00\FF\B8A\02z\01\00'\04\B1\0F\00\00\00\C4\0F\00\19y\02\00\01\00\10%\93\02Q\0E\00\19y\03\0F\00\F5\1A\00!\00\00\00$\0E\00$z\02\02\00X\00\00\03\02\8E\07\00\CA\1F\00\0Cz\00\02\00Y\00\00p`\F0\03\00\DA\0F\00M[\04\A0\80\03\00\EA\0F\005t\03\FF\BB\03\10\FF\C8\03P\E2\0F\00\02x>\02B\80\FF\00\0F\10\00r\B9z\04\00\00F\00\84\00\94\D0\0F\00%v\02\02\00Z`\00`\0F\00\86y\00\022\00@\04\19\10\0C0\009My\00`\00PGy\00\00\F0A\04\A6\FF\83\03\00\C0\0F\00\18y\00\01\00\0F\10\00\90\0F\01\00-\00W\01.\03\00\01\00\22@\00\01\00='\01\000\00\08\01\00\1F\0B@\00\04\13g)\00\1FV@\00\0C\13\13\14\04\0C\01\00\13\C0\15\00&\90\008\04#\04\00\8D\04\00\FE\04\12\00\01\00\1F\FAT\00\00\00\01\00\13P\95\00/p\00\80\00\0B\1F)'\00\03#\00\C0@\00\04X\06\04\E4\00*\04\00\01\00\1F`@\00\04\13\F01\00&\\\00@\00\1F\0A@\00\00!\18\01D\01\0D@\00\13P)\00*\D8\00\01\00\1B\08\08\00?\07\01\00\8E\07\00Q\00\00(\05\00\01\00&\10\00\80\00\17\048\00\04\18\00\13\C4\14\01\0C\84\01*8\05@\07\1F\00\C0\00\04\132@\00+\06\00\01\00\1A\07\D8\07\12\03\F8\05:\08\80\00\01\00\13\06\10\06\04(\0B\0C\01\00*\A8\00\08\00\04\F8\00\13\018\00\04\A8\00\0C\01\009H\03\00\08\00\088\00/\06\00\01\00\17\80\08\00\00\00\00\00\00\00\00"
@ral_send_output___cpu___pvoid_i64_m2df32___void = internal constant [48 x i8] c"ral_send_output___cpu___pvoid_i64_m2df32___void\00"
@dealloc___gpu___pvoid_pvoid___void = internal constant [35 x i8] c"dealloc___gpu___pvoid_pvoid___void\00"
@inc_ref___gpu___pvoid_pvoid_m1df32_m1di64___m2df32 = internal constant [51 x i8] c"inc_ref___gpu___pvoid_pvoid_m1df32_m1di64___m2df32\00"
@main_kernel_2_main_kColReduction_reduce__4_1_0___thin_1_kernel_name = internal constant [42 x i8] c"main_kColReduction_reduce__4_1_0___thin_1\00"
@main_kernel_2_blob_gpu.binary = internal constant [3128 x i8] c"P\EDU\BA\01\00\10\00(\0C\00\00\00\00\00\00\02\00\01\01@\00\00\00\E8\0B\00\00\00\00\00\00\E3\0B\00\00\00\00\00\00\07\00\01\00P\00\00\00\00\00\00\00\00\00\00\00\11 \00\00\00\00\00\00\00\00\00\00\00\00\00\00(:\00\00\00\00\00\00\A2\7FELF\02\01\013\07\00\01\00f\02\00\BE\00q\00\01\00#\809\08\00\116\07\00\F5\0E\00P\05P\00@\008\00\03\00@\00\0C\00\01\00\00.shstrtab\00.\08\00'ym\08\00\F0\1B_shndx\00.nv.info\00.text.main_kColReduction_r\0A\00\F5\02e__4_1_0___thin_19\00\0F3\00\1Coshared5\00\19Orela\9D\00\1E?rel\D1\00!\9Fconstant08\00\19\B2debug_framey\00\09\11\00!nv\14\00\11aD\00\0F\98\01 \0F\89\00\16\0F\C2\01\EF\8F$____wg_2\00\16\00\0B\00/27\FA\01&o_param\01\02\1C\0F\01\00\0B\8C\\\00\00\00\03\00\0A\00\01\00\11\BF\18\00,\0B\00\01\00 \95\01\18\00,\09\00\01\00\11\D4\18\00,\04\00\01\00\11\F2\18\00,\07\00\01\00g2\00\00\00\12\10x\00\11.\06\00v\00\FF\FF\FF\FF(\00\0C\00\00\01\00@\03\00\04|\08\00\D1\0F\0C\81\80\80(\00\08\FF\81\80(\08\0B\00\06(\00/0\00\01\00\03!\F0-\07\00 \00\04\9B\00R\04\14\00\00\00E\00\22\044\DC\00\90\04/\08\00\06\00\00\00\15\0C\00\12#\0C\00\00\01\00'\04\12\0C\00\17\11\0C\00!7\04\18\05\F1\08\015\00\00\04\0A\08\00\03\00\00\00`\01(\00\03\19(\00\04\17\0C$\00u\06\00 \00\00\F0!\10\00u\05\00\1C\00\00\F0\11\10\009\04\00\18\10\009\03\00\14\10\009\02\00\10\10\009\01\00\08P\00\01\01\00\F2\0A\F0\11\00\03\1B\FF\00\04\1C\0C\00P\00\00\000,\00\000-\00\00\04\1E\84\01#K\00\01\00\B1\02\02\08\10\0A/\22\00\00\00\08\10\00#\00\08\08\00\13\10\08\00\13\18\08\00\13 \08\00\13(\08\00\130\08\00\228\08\84\00\13\00\08\00\13\08\08\00\13\10\08\00\13\18\08\00\13 \08\00\13(\08\00\130\08\00\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\A2\00\00\00\14,\00\00\00H\00\01\00\11\02h\01\0F\01\00\FF\B8@$v\01\FF\87\04\B2\FF\00\8E\07\00\C4\0F\00\19y\00\01\00\10%\93\02a\0E\00\19y\03\00\01\00\10!-\00\F0\04\0E\00$z\00\00\00]\00\00\03\02\8E\07\00\CA\1F\00\0C\10\00\C5^\00\00p`\F0\03\00\DA\0F\00M\A3\04\F0\0C\80\03\00\EA\0F\00\06{\04\00\00_\00\00\00\90 \00\00\22\0E\00\19x\05\FF\1F\1F\00\F1\00\14\01\00\00\E2\0F\00\B9z\04\00\00F\00\00\0B\05 \E2\0FP\00\10\FF0\00@pP\F4\03\10\00cEy\00\00 *P\00p\E2\0F\00\11r\05\05\0E\00\B2\FF@\8F\07\00\C6\0F\00\08s\04\FE\04\10\10\A0\00\F1\06\1E\00\10x\02\04\FE\FF\FF\0F\FF\E0\FF\07\00\CC\1F\00\05s\03\B4\02\00\C8\03\A3\00d\00\00$r\02\FF\FF\00\F0\00a\E4\1F\00$r\07\10\00\C0\03\0A\8E\07\00\C8/\00$z\07\07\80\00\10\FF\E0\00\81\C8\0F\00'r\06\03\07\F8\02 \8E\07\80\00@\19x\03\FF\19\03\10\05\C0\00q\E4\0F\00\12x\05\05\09\05!\C0\8E\90\00T'r\06\06\03`\00a\C8\0F\00$r\04`\00#\06\0A\10\00Tz\02\04\00_@\01\01\F0\006\02\00_@\010\10\0A\02\10\00\11\80\C0\00\80\E4\0F\00\10\08\06\06\01P\00\03\10\00\060\00\11\F2\10\010$x\02$\04A\00\05\0A\8E \00\A2$t\05\FF\00\00\80\FF\FF\00\90\00@\12x\00\02p\01\22\FF\C0 \00@\19x\07\FFA\01@\02\16\01\00\80\00*\10\18`\001\12\AA\06\80\01\22\FF30\00\EF\0Cx\00\02\7F\00\00\00p@\F4\03\00\C6\D0\00\01\16\03\D0\00\00\10\00@x\03\03 p\01\12\02@\00\22$x\81\04\22\07\02\90\00h\0Cz\00\03\00X\C0\00\00\98\05\030\01\96\D8\0F\00G\19\00\00 (@\02E$x\04\06@\00\01`\004\04\04@0\00\02@\01\10\04M\07\C7pb\F8\03\00\E4\0F\04\10x\08\04\D0\00g\04\10x\0A\04\02\E0\00B\0Cz\00\080\00\A5\FA\03\00\CE\0F\00$\C4\07\FF\80\00\00\B0\00V\CA\06\04\00X\D0\00c%\C6\06\06\00Z\80\00\02\A0\02\12\0A@\00 \F0\03 \00@\81\C9\0C\06@\00\B6\00\19\1E\0C\00\A2\00\00$\D4\09P\00\84\E2\0F\00\10x\0E\04\03\80\00\00\F0\009\DA\08\08`\000\D6\08\08`\00\16\09`\00\12\0E`\00 \F6\03\E0\00E\81\D9\08\08`\00j\E2\02\00$\84\0B\B0\009\8A\0A\0AP\000\86\0A\0AP\00\10\0B\10\00u\CC\0F\00\81\89\0A\0A@\00X\22\0F\00$\B4\F0\00i\1F\00$\BA\06\0E@\00\17\B6\F0\00\00\80\00&\B9\0E\E0\00\88b\01\00\10x\10\04\04\D0\00\08p\02p\E2\0F\00\0B\C8\00\0C\10\00\B2\00b\FC\03\00\C8O\00\08\C8\09\10\00\B1\02\00\03\00\E4/\04\0B\C2\00\0CN\07\10\80 \00r\0F\00\08\C2\05\0C\09Y\06\13\03\A0\01\17\10\D0\01\F4\07\00\0B\D2\00\05\08\00\00@\00\C0\FC\03\00\E4\8F\08\10x\0C\04\05\80\00\82\E4\0F\00\08\D2\09\08\05@\00\00\10\003\04\0B\D2\E9\06\04`\005\D2\05\08`\00\1B\E2\F0\01\11\E2\C0\04\15\0C\10\02\11\E2\00\02\15\10\10\01\00\D0\00E\82\00\05\0A\80\00 \0F\09\00\01\17\06\00\01\0C \02I\08\82\09\0A\90\00D\82\00\0A\0A\90\00\10\C60\02\17\060\02i\0E\00\08\82\05\0A\A0\00\0B@\02\06\10\01#\F0\03@\02\19\0C\A0\00F\B2\00\05\0E\A0\00\10\0A \01\18\07\A0\00\0B`\02I\08\B2\0B\0E\A0\00H\B2\00\0E\0E\A0\00\09p\02x\0E\00\08\B2\05\0E\0B\A0\00\08\80\02'\E2\0F@\01\12\F6 \00?\8A\0A\10\90\02\16\10$\90\02\16\0DP\00\00\D0\02:\BA\0C\0C\90\02 \0C\0C@\00\14\0D@\005\B9\0C\0C@\00\12b`\03\14\08\F0\00\01\F0\036\10\04\09\10\00d\00\0B\C2\00\05\06 \01\00\90\029\C2\07\06\00\01D\C2\00\06\06\00\01\02\90\02$\06\07\F0\00\18\E4\90\03\0F\90\02\00o\C8\8F\00\08\D2\07\80\02\0E\1F\07\80\02\06\08\E0\01\05\80\02\06\90\03\0F\80\02\024\0E\04\0A\E0\00\0F\80\02S\1B\0E\80\02\1C\10\80\02\19\0C\80\02?\10\04\0B\80\02\0B\1B\0C\80\02/\0C\0C\80\02\0B\1F\0C\80\02\0C\1B\10\80\02\1F\0E\80\02,\1F\10\80\02\1D\14\0C\F0\00\03\80\02\1F\0D\80\02\CC\14\0E\E0\00\0F\80\02\84\1F\0F\80\02\DC\14\10\F0\00\03\80\02\1F\11\80\02\CC\14\12\E0\00\0F\80\02\84\1F\13\80\02\DC\14\14\F0\00\03\80\02\1F\15\80\02\CC\14\16\E0\00\0F\80\02\84\1F\17\80\02\DC\14\18\F0\00\03\80\02\1F\19\80\02\CC\14\1A\E0\00\0F\80\02\84\1F\1B\80\02\DC\14\1C\F0\00\03\80\02\1F\1D\80\02\CC\14\1E\E0\00\0F\80\02\84\1F\1F\80\02\DC\14 \F0\00\03\80\02\1F!\80\02\CC\14\22\E0\00\0F\80\02\84\1F#\80\02\DC\14$\F0\00\03\80\02\1F%\80\02\CC\14&\E0\00\0F\80\02\84\1F'\80\02\DC\14(\F0\00\03\80\02\1F)\80\02\CC\14*\E0\00\0F\80\02\84\1F+\80\02\DC\14,\F0\00\03\80\02\1F-\80\02\CC\14.\E0\00\0F\80\02\84\1F/\80\02\DC\140\F0\00\03\80\02\1F1\80\02\CC\142\E0\00\0F\80\02\84\1F3\80\02\DC\144\F0\00\03\80\02\1F5\80\02\CC\146\E0\00\0F\80\02\84\1F7\80\02\DC\148\F0\00\03\80\02\1F9\80\02\CC\14:\E0\00\0F\80\02\84\1F;\80\02\DC\14<\F0\00\03\80\02\19=\10\009\12\04>\10\00?\04\04?\A0\02\C5?\C6\0F\01\90\02\1AW\E4\0F\04\81\C9`$/\A4\00\90\02\11\1F\12\90\02\16?\C6\0F\02\80\02+\1F\02\80\02\00\0D@$\1A\04\80\02*\06\12\80\02\07\10\01\01\80\02\07\80\03W$\01\00$\B4\D0\02\01\80\02\1A\0A\80&\1B\B6\C0\028\B9\0A\0A\80\02\00@\02\19\0E@\02\17\09\E0#\01p%\06\E0#\02@\02\15\0E`\01\1E\C8`%\06 \02\01P\02\19\04@\02(\C8\1F0\02\11\02@\00\14\82\C0\02\10\FA0\004\09\0B\82\B0\02\03@\00\14\82\D0\02 \80\060\00I\08\82\05\06@\00\14\B2@\02\010\004\0A\0B\B2 \02\12\F0@\00$\B2\07@\02\02@\003\B2\05\0A@\00\10\00\B0*\14A\B0*\030( \88s\F8-\12\00u(f\E8\0F\00\1D{\00\01\00b\EC\0F\00\84\A9\07,*\00 \00Q\22\0E\00\1Cx\1A\00P\00p\F0\F0\03P\00\13\0C0)1pD\F2\90*!\84\A9n\01\020\00qb\0E\00\0B\A2\00\07\80\00\A1\80\F6\03\00\E4\1F\08\0B\A2\00p%\02\C0\00T/\00\1C\A8\00P\00\14\01@)\11?@)@\F6\03\00\D6\00\01#\07\07R\00\10\06 \00\09\80\00\8F\C6\0F\00\88\A3\00\00\07\C0\00\09\22\B9\05\1C\00\12\08\A0++\84\B9\A0\00\22\B2\00!+`\80\F4\03\00\C4\1F\10\00(\04\05\A0\00#\B8\00p\00Ap\01\00\DA\90\01\15\05\90\00_\CA\0F\00\88\B3@\01\0B9M\19\00p\017$t\02\10\03R\0F\00\84y\05\D1)\00@\00\90&\0E\00%v\02\03\00`\0F/\04 \00\00\1D\00\03`\00vh\0E\00\81y\06\02\D0\02D\05\00\0Br\C0\00\12\F0\C0\00\14r\C0\00W\F2\03\00\D6/\B0\00 \80\04\F0\022\0Br\00`\02\11@\10\04(\0EF\10\02S\E6\0F\00\08r\80\02\00\01\00p\CC\0F\00\A9s\07\02\A0\02\C0\07\E1\1E\00\00\A4\0E\00\0Cr\00\07\10\00 pR@\00QO\00$r\06\E0*\14\07\90*@\09\00\00\90\E0+!\FF\83\F0\00\1BMp\02TGy\00\00\F0 \00f\C0\0F\00\18y\00\01\00\0F\10\00\90\0F\01\00-\11\01H1\0E\01\00\22@\00\01\00=\98\01\000\00\08\01\00\1F\0B@\00\04\13\D8)\00?\01\02\00@\00\0A\13\13\0B\04\0C\01\00\13\E0U\00\03\8F\03\01/\04\13\05w\02\00\01\00\22\18\00\01\00.k\01T\00\00\01\00#\88\04(\03\1F\00\80\00\0B/)\00'\00\02#\00\F8@\00\04 3\04\E4\00*\04\00\01\00\1Fb@\00\04*(\05\C0\00\13\03#/\0C@\00!\89\01D\01\0D@\00\13\D0@\00*\D8\00\01\00\1B\08\08\00?x\01\00V4\000\00\00\A8\15\03\03W/\04\80\00\17\048\00\04\18\00\133@\01\0C\84\01\13\B8@\00\17\881\01\0F\C0\00\01\132T\01\15\06R\00\03^\03\1A\08\A04\11\03$\00J\00\15\80\00\01\00\13\95\94\00*\03\00\01\00\0409/\00\04\80\00\0B\13\06\18\02\04h9\0D\08\01\1A\00\08\00\04\97\00\13\018\00\04\E8\00\0C\01\009\C8/\00\08\00\088\00\18\06\A0\00\0F\01\00\05\03\A9\00\80\08\00\00\00\00\00\00\00\00\00\00\00\00"
@ral_kernel_launch___gpu___pvoid_ppvoid_i64_pvoid_i64_i64_i64_i64_i64_i64_i32_pvoid_i32_ppvoid___void = internal constant [101 x i8] c"ral_kernel_launch___gpu___pvoid_ppvoid_i64_pvoid_i64_i64_i64_i64_i64_i64_i32_pvoid_i32_ppvoid___void\00"
@main_kernel_1_main_kColReduction_reduce__4_1_0___thin_kernel_name = internal constant [40 x i8] c"main_kColReduction_reduce__4_1_0___thin\00"
@main_kernel_1_blob_gpu.binary = internal constant [1096 x i8] c"P\EDU\BA\01\00\10\008\04\00\00\00\00\00\00\02\00\01\01@\00\00\00\F8\03\00\00\00\00\00\00\F7\03\00\00\00\00\00\00\07\00\01\00P\00\00\00\00\00\00\00\00\00\00\00\11 \00\00\00\00\00\00\00\00\00\00\00\00\00\00\E8\0B\00\00\00\00\00\00\A2\7FELF\02\01\013\07\00\01\00f\02\00\BE\00q\00\01\00!@\0B\07\001\00\80\08\07\00\F5\0E\00P\05P\00@\008\00\03\00@\00\0B\00\01\00\00.shstrtab\00.\08\00'ym\08\00\F0\1B_shndx\00.nv.info\00.text.main_kColReduction_r\0A\00\F5\00e__4_1_0___thin7\00\0F1\00\1Aoshared3\00\1A\9Fconstant06\00\17\FA\01debug_frame\00.rel\11\00!nv\14\00\11aB\00\0F'\01 \0F\87\00\14\0FO\01\B6o_paramV\01\1C\0F\01\00\07\8CZ\00\00\00\03\00\0A\00\01\00\11\EC\18\00,\09\00\01\00 )\01\18\00,\04\00\01\00\11G\18\00,\07\00\01\00f2\00\00\00\12\10`\00!\80\01\07\00v\00\FF\FF\FF\FF(\00\0C\00\00\01\00@\03\00\04|\08\00\D1\0F\0C\81\80\80(\00\08\FF\81\80(\08\0B\00\06(\00/0\00\01\00\03\13pX\00\10\04\9B\00R\04\14\00\00\00E\00\01\0B\00\00\13\00p/\08\00\05\00\00\00\9F\03\22\04#\0C\00\00\01\00'\04\12\0C\00\17\11\0C\00!7\04\E0\03\F3\08\015\00\00\04\0A\08\00\02\00\00\00`\01\10\00\03\19\10\00\04\17\0C\04\04U\08\00\00\F0!\10\00\10\01\18\01%\F0\11\10\00\01\01\00\F2\02\F0\11\00\03\1B\FF\00\04\1C\08\00P\00\00\00\B0\00\01\00#K\00\01\00s\02\02\08\10\0A/\22\9B\00\00\07\00\03\08\00\13\10\08\00\13\18\08\00\13 \08\00\13(\08\00\130\08\00\228\08|\00\13\00\08\00\13\08\08\00\13\10\08\00\13\18\08\00\13 \08\00\13(\08\00\130\08\00\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\13\02@\00\A2\00\00\00\14,\00\00\00H\00\01\00\000\01/\05\00\01\00\FF\B8A\02z\01\00'\04\B1\0F\00\00\00\C4\0F\00\19y\02\00\01\00\10%\93\02Q\0E\00\19y\03\0F\00\F5\1A\00!\00\00\00$\0E\00$z\02\02\00X\00\00\03\02\8E\07\00\CA\1F\00\0Cz\00\02\00Y\00\00p`\F0\03\00\DA\0F\00M[\04\A0\80\03\00\EA\0F\005t\03\FF\BB\03\10\FF\C8\03P\E2\0F\00\02x>\02B\80\FF\00\0F\10\00r\B9z\04\00\00F\00\84\00\94\D0\0F\00%v\02\02\00Z`\00`\0F\00\86y\00\022\00@\04\19\10\0C0\009My\00`\00PGy\00\00\F0A\04\A6\FF\83\03\00\C0\0F\00\18y\00\01\00\0F\10\00\90\0F\01\00-\00W\01.\03\00\01\00\22@\00\01\00='\01\000\00\08\01\00\1F\0B@\00\04\13g)\00\1FV@\00\0C\13\13\14\04\0C\01\00\13\C0\15\00&\90\008\04#\04\00\8D\04\00\FE\04\12\00\01\00\1F\FAT\00\00\00\01\00\13P\95\00/p\00\80\00\0B\1F)'\00\03#\00\C0@\00\04X\06\04\E4\00*\04\00\01\00\1F`@\00\04\13\F01\00&\\\00@\00\1F\0A@\00\00!\18\01D\01\0D@\00\13P)\00*\D8\00\01\00\1B\08\08\00?\07\01\00\8E\07\00Q\00\00(\05\00\01\00&\10\00\80\00\17\048\00\04\18\00\13\C4\14\01\0C\84\01*8\05@\07\1F\00\C0\00\04\132@\00+\06\00\01\00\1A\07\D8\07\12\03\F8\05:\08\80\00\01\00\13\06\10\06\04(\0B\0C\01\00*\A8\00\08\00\04\F8\00\13\018\00\04\A8\00\0C\01\009H\03\00\08\00\088\00/\06\00\01\00\17\80\08\00\00\00\00\00\00\00\00"
@alloc___gpu___pvoid_i64___pvoid = internal constant [32 x i8] c"alloc___gpu___pvoid_i64___pvoid\00"
@ral_recv_input___cpu___pvoid_i64___m3df32 = internal constant [42 x i8] c"ral_recv_input___cpu___pvoid_i64___m3df32\00"

define void @disc_ral_call(ptr nocapture readonly %0, ptr %1, ptr %2) local_unnamed_addr {
entry:
  %3 = load ptr, ptr %0, align 8
  %4 = getelementptr ptr, ptr %0, i64 1
  %5 = load ptr, ptr %4, align 8
  %6 = load ptr, ptr %2, align 8
  store ptr %3, ptr %6, align 8
  tail call void %5(ptr %3, ptr %1, ptr nonnull %2)
  ret void
}

define void @main(ptr %0) local_unnamed_addr {
  %2 = alloca %0, align 8
  %3 = alloca [3 x ptr], align 8
  store ptr %2, ptr %3, align 8
  %4 = getelementptr inbounds %0, ptr %2, i64 0, i32 1
  store i64 0, ptr %4, align 8
  %5 = getelementptr inbounds ptr, ptr %3, i64 1
  store ptr %4, ptr %5, align 8
  %6 = getelementptr inbounds %0, ptr %2, i64 0, i32 2
  %7 = getelementptr inbounds ptr, ptr %3, i64 2
  store ptr %6, ptr %7, align 8
  %8 = load ptr, ptr %0, align 8
  %9 = getelementptr ptr, ptr %0, i64 1
  %10 = load ptr, ptr %9, align 8
  store ptr %8, ptr %2, align 8
  call void %10(ptr %8, ptr nonnull @ral_recv_input___cpu___pvoid_i64___m3df32, ptr nonnull %3)
  %.fca.1.gep = getelementptr inbounds %0, ptr %2, i64 0, i32 2, i32 1
  %.fca.1.load = load ptr, ptr %.fca.1.gep, align 8
  %.fca.3.0.gep = getelementptr inbounds %0, ptr %2, i64 0, i32 2, i32 3
  %.fca.3.0.load = load i64, ptr %.fca.3.0.gep, align 8
  %.fca.3.1.gep = getelementptr inbounds %0, ptr %2, i64 0, i32 2, i32 3, i64 1
  %.fca.3.1.load = load i64, ptr %.fca.3.1.gep, align 8
  %.fca.3.2.gep = getelementptr inbounds %0, ptr %2, i64 0, i32 2, i32 3, i64 2
  %.fca.3.2.load = load i64, ptr %.fca.3.2.gep, align 8
  %11 = shl i64 %.fca.3.1.load, 32
  %sext = mul i64 %11, %.fca.3.2.load
  %12 = ashr exact i64 %sext, 32
  %.idx = ashr exact i64 %sext, 30
  %13 = alloca %.1, align 8
  %14 = alloca [3 x ptr], align 8
  store ptr %13, ptr %14, align 8
  %15 = getelementptr inbounds %.1, ptr %13, i64 0, i32 1
  store i64 %.idx, ptr %15, align 8
  %16 = getelementptr inbounds ptr, ptr %14, i64 1
  store ptr %15, ptr %16, align 8
  %17 = getelementptr inbounds %.1, ptr %13, i64 0, i32 2
  %18 = getelementptr inbounds ptr, ptr %14, i64 2
  store ptr %17, ptr %18, align 8
  %19 = load ptr, ptr %0, align 8
  %20 = load ptr, ptr %9, align 8
  store ptr %19, ptr %13, align 8
  call void %20(ptr %19, ptr nonnull @alloc___gpu___pvoid_i64___pvoid, ptr nonnull %14)
  %21 = load ptr, ptr %17, align 8
  %22 = icmp slt i64 %.fca.3.0.load, %12
  %23 = icmp slt i64 %sext, 4294967296
  %24 = sub nsw i64 0, %12
  %25 = add nsw i64 %12, -1
  %26 = select i1 %23, i64 %24, i64 %25
  %27 = alloca ptr, align 8
  %28 = alloca [3 x ptr], align 8
  %29 = getelementptr inbounds ptr, ptr %28, i64 1
  %30 = getelementptr inbounds ptr, ptr %28, i64 2
  %31 = alloca [14 x ptr], align 8
  %32 = getelementptr inbounds ptr, ptr %31, i64 1
  br i1 %22, label %33, label %132

33:                                               ; preds = %1
  %34 = sdiv i64 %26, 512
  %35 = sub nsw i64 0, %34
  %36 = add nsw i64 %34, 1
  %37 = select i1 %23, i64 %35, i64 %36
  store ptr @main_kernel_blob_gpu.binary, ptr %27, align 8
  %38 = alloca %.9, align 8
  store i64 512, ptr %38, align 8
  store ptr %38, ptr %28, align 8
  %39 = getelementptr inbounds %.9, ptr %38, i64 0, i32 1
  store i64 %12, ptr %39, align 8
  store ptr %39, ptr %29, align 8
  %40 = getelementptr inbounds %.9, ptr %38, i64 0, i32 2
  store ptr %21, ptr %40, align 8
  store ptr %40, ptr %30, align 8
  %41 = alloca %.10, align 8
  store ptr %41, ptr %31, align 8
  %42 = getelementptr inbounds %.10, ptr %41, i64 0, i32 1
  store ptr %27, ptr %42, align 8
  store ptr %42, ptr %32, align 8
  %43 = getelementptr inbounds %.10, ptr %41, i64 0, i32 2
  store i64 1, ptr %43, align 8
  %44 = getelementptr inbounds ptr, ptr %31, i64 2
  store ptr %43, ptr %44, align 8
  %45 = getelementptr inbounds %.10, ptr %41, i64 0, i32 3
  store ptr @main_kernel_main_kColReduction_reduce__4_1_0___flat_kernel_name, ptr %45, align 8
  %46 = getelementptr inbounds ptr, ptr %31, i64 3
  store ptr %45, ptr %46, align 8
  %47 = getelementptr inbounds %.10, ptr %41, i64 0, i32 4
  store i64 %37, ptr %47, align 8
  %48 = getelementptr inbounds ptr, ptr %31, i64 4
  store ptr %47, ptr %48, align 8
  %49 = getelementptr inbounds %.10, ptr %41, i64 0, i32 5
  store i64 1, ptr %49, align 8
  %50 = getelementptr inbounds ptr, ptr %31, i64 5
  store ptr %49, ptr %50, align 8
  %51 = getelementptr inbounds %.10, ptr %41, i64 0, i32 6
  store i64 1, ptr %51, align 8
  %52 = getelementptr inbounds ptr, ptr %31, i64 6
  store ptr %51, ptr %52, align 8
  %53 = getelementptr inbounds %.10, ptr %41, i64 0, i32 7
  store i64 512, ptr %53, align 8
  %54 = getelementptr inbounds ptr, ptr %31, i64 7
  store ptr %53, ptr %54, align 8
  %55 = getelementptr inbounds %.10, ptr %41, i64 0, i32 8
  store i64 1, ptr %55, align 8
  %56 = getelementptr inbounds ptr, ptr %31, i64 8
  store ptr %55, ptr %56, align 8
  %57 = getelementptr inbounds %.10, ptr %41, i64 0, i32 9
  store i64 1, ptr %57, align 8
  %58 = getelementptr inbounds ptr, ptr %31, i64 9
  store ptr %57, ptr %58, align 8
  %59 = getelementptr inbounds %.10, ptr %41, i64 0, i32 10
  store i32 0, ptr %59, align 8
  %60 = getelementptr inbounds ptr, ptr %31, i64 10
  store ptr %59, ptr %60, align 8
  %61 = getelementptr inbounds %.10, ptr %41, i64 0, i32 11
  store ptr null, ptr %61, align 8
  %62 = getelementptr inbounds ptr, ptr %31, i64 11
  store ptr %61, ptr %62, align 8
  %63 = getelementptr inbounds %.10, ptr %41, i64 0, i32 12
  store i32 3, ptr %63, align 8
  %64 = getelementptr inbounds ptr, ptr %31, i64 12
  store ptr %63, ptr %64, align 8
  %65 = getelementptr inbounds %.10, ptr %41, i64 0, i32 13
  store ptr %28, ptr %65, align 8
  %66 = getelementptr inbounds ptr, ptr %31, i64 13
  store ptr %65, ptr %66, align 8
  %67 = load ptr, ptr %0, align 8
  %68 = load ptr, ptr %9, align 8
  store ptr %67, ptr %41, align 8
  call void %68(ptr %67, ptr nonnull @ral_kernel_launch___gpu___pvoid_ppvoid_i64_pvoid_i64_i64_i64_i64_i64_i64_i32_pvoid_i32_ppvoid___void, ptr nonnull %31)
  %69 = icmp eq i64 %sext, 0
  %70 = lshr i64 %25, 9
  %71 = add nuw nsw i64 %70, 1
  %72 = select i1 %69, i64 0, i64 %71
  %73 = icmp eq i64 %.fca.3.0.load, 0
  %74 = shl i64 %.fca.3.0.load, 4
  %75 = add i64 %74, -16
  %76 = and i64 %75, -512
  %.op15 = add i64 %76, 512
  %77 = select i1 %73, i64 0, i64 %.op15
  %78 = mul i64 %77, %72
  %79 = icmp slt i64 %78, 1
  %80 = sub i64 0, %78
  %81 = add i64 %78, -1
  %82 = select i1 %79, i64 %80, i64 %81
  %83 = sdiv i64 %82, 512
  %84 = sub nsw i64 0, %83
  %85 = add nsw i64 %83, 1
  %86 = select i1 %79, i64 %84, i64 %85
  %87 = alloca ptr, align 8
  store ptr @main_kernel_0_blob_gpu.binary, ptr %87, align 8
  %88 = alloca %.11, align 8
  %89 = alloca [7 x ptr], align 8
  store i64 %12, ptr %88, align 8
  store ptr %88, ptr %89, align 8
  %90 = getelementptr inbounds %.11, ptr %88, i64 0, i32 1
  store ptr %.fca.1.load, ptr %90, align 8
  %91 = getelementptr inbounds ptr, ptr %89, i64 1
  store ptr %90, ptr %91, align 8
  %92 = getelementptr inbounds %.11, ptr %88, i64 0, i32 2
  store i64 %.fca.3.0.load, ptr %92, align 8
  %93 = getelementptr inbounds ptr, ptr %89, i64 2
  store ptr %92, ptr %93, align 8
  %94 = getelementptr inbounds %.11, ptr %88, i64 0, i32 3
  store i64 512, ptr %94, align 8
  %95 = getelementptr inbounds ptr, ptr %89, i64 3
  store ptr %94, ptr %95, align 8
  %96 = getelementptr inbounds %.11, ptr %88, i64 0, i32 4
  store i64 %78, ptr %96, align 8
  %97 = getelementptr inbounds ptr, ptr %89, i64 4
  store ptr %96, ptr %97, align 8
  %98 = getelementptr inbounds %.11, ptr %88, i64 0, i32 5
  store i64 %72, ptr %98, align 8
  %99 = getelementptr inbounds ptr, ptr %89, i64 5
  store ptr %98, ptr %99, align 8
  %100 = getelementptr inbounds %.11, ptr %88, i64 0, i32 6
  store ptr %21, ptr %100, align 8
  %101 = getelementptr inbounds ptr, ptr %89, i64 6
  store ptr %100, ptr %101, align 8
  %102 = alloca %.12, align 8
  %103 = alloca [14 x ptr], align 8
  store ptr %102, ptr %103, align 8
  %104 = getelementptr inbounds %.12, ptr %102, i64 0, i32 1
  store ptr %87, ptr %104, align 8
  %105 = getelementptr inbounds ptr, ptr %103, i64 1
  store ptr %104, ptr %105, align 8
  %106 = getelementptr inbounds %.12, ptr %102, i64 0, i32 2
  store i64 1, ptr %106, align 8
  %107 = getelementptr inbounds ptr, ptr %103, i64 2
  store ptr %106, ptr %107, align 8
  %108 = getelementptr inbounds %.12, ptr %102, i64 0, i32 3
  store ptr @main_kernel_0_main_kColReduction_reduce__4_1_0___flat_1_kernel_name, ptr %108, align 8
  %109 = getelementptr inbounds ptr, ptr %103, i64 3
  store ptr %108, ptr %109, align 8
  %110 = getelementptr inbounds %.12, ptr %102, i64 0, i32 4
  store i64 %86, ptr %110, align 8
  %111 = getelementptr inbounds ptr, ptr %103, i64 4
  store ptr %110, ptr %111, align 8
  %112 = getelementptr inbounds %.12, ptr %102, i64 0, i32 5
  store i64 1, ptr %112, align 8
  %113 = getelementptr inbounds ptr, ptr %103, i64 5
  store ptr %112, ptr %113, align 8
  %114 = getelementptr inbounds %.12, ptr %102, i64 0, i32 6
  store i64 1, ptr %114, align 8
  %115 = getelementptr inbounds ptr, ptr %103, i64 6
  store ptr %114, ptr %115, align 8
  %116 = getelementptr inbounds %.12, ptr %102, i64 0, i32 7
  store i64 512, ptr %116, align 8
  %117 = getelementptr inbounds ptr, ptr %103, i64 7
  store ptr %116, ptr %117, align 8
  %118 = getelementptr inbounds %.12, ptr %102, i64 0, i32 8
  store i64 1, ptr %118, align 8
  %119 = getelementptr inbounds ptr, ptr %103, i64 8
  store ptr %118, ptr %119, align 8
  %120 = getelementptr inbounds %.12, ptr %102, i64 0, i32 9
  store i64 1, ptr %120, align 8
  %121 = getelementptr inbounds ptr, ptr %103, i64 9
  store ptr %120, ptr %121, align 8
  %122 = getelementptr inbounds %.12, ptr %102, i64 0, i32 10
  store i32 0, ptr %122, align 8
  %123 = getelementptr inbounds ptr, ptr %103, i64 10
  store ptr %122, ptr %123, align 8
  %124 = getelementptr inbounds %.12, ptr %102, i64 0, i32 11
  store ptr null, ptr %124, align 8
  %125 = getelementptr inbounds ptr, ptr %103, i64 11
  store ptr %124, ptr %125, align 8
  %126 = getelementptr inbounds %.12, ptr %102, i64 0, i32 12
  store i32 7, ptr %126, align 8
  %127 = getelementptr inbounds ptr, ptr %103, i64 12
  store ptr %126, ptr %127, align 8
  %128 = getelementptr inbounds %.12, ptr %102, i64 0, i32 13
  store ptr %89, ptr %128, align 8
  %129 = getelementptr inbounds ptr, ptr %103, i64 13
  store ptr %128, ptr %129, align 8
  %130 = load ptr, ptr %0, align 8
  %131 = load ptr, ptr %9, align 8
  store ptr %130, ptr %102, align 8
  call void %131(ptr %130, ptr nonnull @ral_kernel_launch___gpu___pvoid_ppvoid_i64_pvoid_i64_i64_i64_i64_i64_i64_i32_pvoid_i32_ppvoid___void, ptr nonnull %103)
  br label %231

132:                                              ; preds = %1
  %133 = sdiv i64 %26, 256
  %134 = sub nsw i64 0, %133
  %135 = add nsw i64 %133, 1
  %136 = select i1 %23, i64 %134, i64 %135
  store ptr @main_kernel_1_blob_gpu.binary, ptr %27, align 8
  %137 = alloca %.2, align 8
  store i64 256, ptr %137, align 8
  store ptr %137, ptr %28, align 8
  %138 = getelementptr inbounds %.2, ptr %137, i64 0, i32 1
  store i64 %12, ptr %138, align 8
  store ptr %138, ptr %29, align 8
  %139 = getelementptr inbounds %.2, ptr %137, i64 0, i32 2
  store ptr %21, ptr %139, align 8
  store ptr %139, ptr %30, align 8
  %140 = alloca %.3, align 8
  store ptr %140, ptr %31, align 8
  %141 = getelementptr inbounds %.3, ptr %140, i64 0, i32 1
  store ptr %27, ptr %141, align 8
  store ptr %141, ptr %32, align 8
  %142 = getelementptr inbounds %.3, ptr %140, i64 0, i32 2
  store i64 1, ptr %142, align 8
  %143 = getelementptr inbounds ptr, ptr %31, i64 2
  store ptr %142, ptr %143, align 8
  %144 = getelementptr inbounds %.3, ptr %140, i64 0, i32 3
  store ptr @main_kernel_1_main_kColReduction_reduce__4_1_0___thin_kernel_name, ptr %144, align 8
  %145 = getelementptr inbounds ptr, ptr %31, i64 3
  store ptr %144, ptr %145, align 8
  %146 = getelementptr inbounds %.3, ptr %140, i64 0, i32 4
  store i64 %136, ptr %146, align 8
  %147 = getelementptr inbounds ptr, ptr %31, i64 4
  store ptr %146, ptr %147, align 8
  %148 = getelementptr inbounds %.3, ptr %140, i64 0, i32 5
  store i64 1, ptr %148, align 8
  %149 = getelementptr inbounds ptr, ptr %31, i64 5
  store ptr %148, ptr %149, align 8
  %150 = getelementptr inbounds %.3, ptr %140, i64 0, i32 6
  store i64 1, ptr %150, align 8
  %151 = getelementptr inbounds ptr, ptr %31, i64 6
  store ptr %150, ptr %151, align 8
  %152 = getelementptr inbounds %.3, ptr %140, i64 0, i32 7
  store i64 256, ptr %152, align 8
  %153 = getelementptr inbounds ptr, ptr %31, i64 7
  store ptr %152, ptr %153, align 8
  %154 = getelementptr inbounds %.3, ptr %140, i64 0, i32 8
  store i64 1, ptr %154, align 8
  %155 = getelementptr inbounds ptr, ptr %31, i64 8
  store ptr %154, ptr %155, align 8
  %156 = getelementptr inbounds %.3, ptr %140, i64 0, i32 9
  store i64 1, ptr %156, align 8
  %157 = getelementptr inbounds ptr, ptr %31, i64 9
  store ptr %156, ptr %157, align 8
  %158 = getelementptr inbounds %.3, ptr %140, i64 0, i32 10
  store i32 0, ptr %158, align 8
  %159 = getelementptr inbounds ptr, ptr %31, i64 10
  store ptr %158, ptr %159, align 8
  %160 = getelementptr inbounds %.3, ptr %140, i64 0, i32 11
  store ptr null, ptr %160, align 8
  %161 = getelementptr inbounds ptr, ptr %31, i64 11
  store ptr %160, ptr %161, align 8
  %162 = getelementptr inbounds %.3, ptr %140, i64 0, i32 12
  store i32 3, ptr %162, align 8
  %163 = getelementptr inbounds ptr, ptr %31, i64 12
  store ptr %162, ptr %163, align 8
  %164 = getelementptr inbounds %.3, ptr %140, i64 0, i32 13
  store ptr %28, ptr %164, align 8
  %165 = getelementptr inbounds ptr, ptr %31, i64 13
  store ptr %164, ptr %165, align 8
  %166 = load ptr, ptr %0, align 8
  %167 = load ptr, ptr %9, align 8
  store ptr %166, ptr %140, align 8
  call void %167(ptr %166, ptr nonnull @ral_kernel_launch___gpu___pvoid_ppvoid_i64_pvoid_i64_i64_i64_i64_i64_i64_i32_pvoid_i32_ppvoid___void, ptr nonnull %31)
  %168 = icmp eq i64 %sext, 0
  %169 = lshr i64 %25, 5
  %170 = add nuw nsw i64 %169, 1
  %171 = select i1 %168, i64 0, i64 %170
  %172 = icmp eq i64 %.fca.3.0.load, 0
  %173 = add nsw i64 %.fca.3.0.load, -1
  %174 = lshr i64 %173, 1
  %175 = and i64 %174, 9223372036854775552
  %.op = add nuw i64 %175, 256
  %176 = select i1 %172, i64 0, i64 %.op
  %177 = mul i64 %176, %171
  %178 = icmp slt i64 %177, 1
  %179 = sub i64 0, %177
  %180 = add i64 %177, -1
  %181 = select i1 %178, i64 %179, i64 %180
  %182 = sdiv i64 %181, 256
  %183 = sub nsw i64 0, %182
  %184 = add nsw i64 %182, 1
  %185 = select i1 %178, i64 %183, i64 %184
  %186 = alloca ptr, align 8
  store ptr @main_kernel_2_blob_gpu.binary, ptr %186, align 8
  %187 = alloca %.4, align 8
  %188 = alloca [7 x ptr], align 8
  store i64 %12, ptr %187, align 8
  store ptr %187, ptr %188, align 8
  %189 = getelementptr inbounds %.4, ptr %187, i64 0, i32 1
  store ptr %.fca.1.load, ptr %189, align 8
  %190 = getelementptr inbounds ptr, ptr %188, i64 1
  store ptr %189, ptr %190, align 8
  %191 = getelementptr inbounds %.4, ptr %187, i64 0, i32 2
  store i64 %.fca.3.0.load, ptr %191, align 8
  %192 = getelementptr inbounds ptr, ptr %188, i64 2
  store ptr %191, ptr %192, align 8
  %193 = getelementptr inbounds %.4, ptr %187, i64 0, i32 3
  store i64 256, ptr %193, align 8
  %194 = getelementptr inbounds ptr, ptr %188, i64 3
  store ptr %193, ptr %194, align 8
  %195 = getelementptr inbounds %.4, ptr %187, i64 0, i32 4
  store i64 %177, ptr %195, align 8
  %196 = getelementptr inbounds ptr, ptr %188, i64 4
  store ptr %195, ptr %196, align 8
  %197 = getelementptr inbounds %.4, ptr %187, i64 0, i32 5
  store i64 %171, ptr %197, align 8
  %198 = getelementptr inbounds ptr, ptr %188, i64 5
  store ptr %197, ptr %198, align 8
  %199 = getelementptr inbounds %.4, ptr %187, i64 0, i32 6
  store ptr %21, ptr %199, align 8
  %200 = getelementptr inbounds ptr, ptr %188, i64 6
  store ptr %199, ptr %200, align 8
  %201 = alloca %.5, align 8
  %202 = alloca [14 x ptr], align 8
  store ptr %201, ptr %202, align 8
  %203 = getelementptr inbounds %.5, ptr %201, i64 0, i32 1
  store ptr %186, ptr %203, align 8
  %204 = getelementptr inbounds ptr, ptr %202, i64 1
  store ptr %203, ptr %204, align 8
  %205 = getelementptr inbounds %.5, ptr %201, i64 0, i32 2
  store i64 1, ptr %205, align 8
  %206 = getelementptr inbounds ptr, ptr %202, i64 2
  store ptr %205, ptr %206, align 8
  %207 = getelementptr inbounds %.5, ptr %201, i64 0, i32 3
  store ptr @main_kernel_2_main_kColReduction_reduce__4_1_0___thin_1_kernel_name, ptr %207, align 8
  %208 = getelementptr inbounds ptr, ptr %202, i64 3
  store ptr %207, ptr %208, align 8
  %209 = getelementptr inbounds %.5, ptr %201, i64 0, i32 4
  store i64 %185, ptr %209, align 8
  %210 = getelementptr inbounds ptr, ptr %202, i64 4
  store ptr %209, ptr %210, align 8
  %211 = getelementptr inbounds %.5, ptr %201, i64 0, i32 5
  store i64 1, ptr %211, align 8
  %212 = getelementptr inbounds ptr, ptr %202, i64 5
  store ptr %211, ptr %212, align 8
  %213 = getelementptr inbounds %.5, ptr %201, i64 0, i32 6
  store i64 1, ptr %213, align 8
  %214 = getelementptr inbounds ptr, ptr %202, i64 6
  store ptr %213, ptr %214, align 8
  %215 = getelementptr inbounds %.5, ptr %201, i64 0, i32 7
  store i64 256, ptr %215, align 8
  %216 = getelementptr inbounds ptr, ptr %202, i64 7
  store ptr %215, ptr %216, align 8
  %217 = getelementptr inbounds %.5, ptr %201, i64 0, i32 8
  store i64 1, ptr %217, align 8
  %218 = getelementptr inbounds ptr, ptr %202, i64 8
  store ptr %217, ptr %218, align 8
  %219 = getelementptr inbounds %.5, ptr %201, i64 0, i32 9
  store i64 1, ptr %219, align 8
  %220 = getelementptr inbounds ptr, ptr %202, i64 9
  store ptr %219, ptr %220, align 8
  %221 = getelementptr inbounds %.5, ptr %201, i64 0, i32 10
  store i32 0, ptr %221, align 8
  %222 = getelementptr inbounds ptr, ptr %202, i64 10
  store ptr %221, ptr %222, align 8
  %223 = getelementptr inbounds %.5, ptr %201, i64 0, i32 11
  store ptr null, ptr %223, align 8
  %224 = getelementptr inbounds ptr, ptr %202, i64 11
  store ptr %223, ptr %224, align 8
  %225 = getelementptr inbounds %.5, ptr %201, i64 0, i32 12
  store i32 7, ptr %225, align 8
  %226 = getelementptr inbounds ptr, ptr %202, i64 12
  store ptr %225, ptr %226, align 8
  %227 = getelementptr inbounds %.5, ptr %201, i64 0, i32 13
  store ptr %188, ptr %227, align 8
  %228 = getelementptr inbounds ptr, ptr %202, i64 13
  store ptr %227, ptr %228, align 8
  %229 = load ptr, ptr %0, align 8
  %230 = load ptr, ptr %9, align 8
  store ptr %229, ptr %201, align 8
  call void %230(ptr %229, ptr nonnull @ral_kernel_launch___gpu___pvoid_ppvoid_i64_pvoid_i64_i64_i64_i64_i64_i64_i32_pvoid_i32_ppvoid___void, ptr nonnull %202)
  br label %231

231:                                              ; preds = %33, %132
  %232 = alloca [16 x i64], align 8
  store i64 %.fca.3.1.load, ptr %232, align 8
  %233 = getelementptr inbounds i64, ptr %232, i64 1
  store i64 %.fca.3.2.load, ptr %233, align 8
  %234 = alloca %.6, align 8
  %235 = alloca [13 x ptr], align 8
  store ptr %234, ptr %235, align 8
  %236 = getelementptr inbounds %.6, ptr %234, i64 0, i32 1
  store ptr null, ptr %236, align 8
  %237 = getelementptr inbounds ptr, ptr %235, i64 1
  store ptr %236, ptr %237, align 8
  %238 = getelementptr inbounds %.6, ptr %234, i64 0, i32 2
  store ptr %21, ptr %238, align 8
  %239 = getelementptr inbounds ptr, ptr %235, i64 2
  store ptr %238, ptr %239, align 8
  %240 = getelementptr inbounds %.6, ptr %234, i64 0, i32 3
  store ptr %21, ptr %240, align 8
  %241 = getelementptr inbounds ptr, ptr %235, i64 3
  store ptr %240, ptr %241, align 8
  %242 = getelementptr inbounds %.6, ptr %234, i64 0, i32 4
  store i64 0, ptr %242, align 8
  %243 = getelementptr inbounds ptr, ptr %235, i64 4
  store ptr %242, ptr %243, align 8
  %244 = getelementptr inbounds %.6, ptr %234, i64 0, i32 5
  store i64 %12, ptr %244, align 8
  %245 = getelementptr inbounds ptr, ptr %235, i64 5
  store ptr %244, ptr %245, align 8
  %246 = getelementptr inbounds %.6, ptr %234, i64 0, i32 6
  store i64 1, ptr %246, align 8
  %247 = getelementptr inbounds ptr, ptr %235, i64 6
  store ptr %246, ptr %247, align 8
  %248 = getelementptr inbounds %.6, ptr %234, i64 0, i32 7
  store ptr %232, ptr %248, align 8
  %249 = getelementptr inbounds ptr, ptr %235, i64 7
  store ptr %248, ptr %249, align 8
  %250 = getelementptr inbounds %.6, ptr %234, i64 0, i32 8
  store ptr %232, ptr %250, align 8
  %251 = getelementptr inbounds ptr, ptr %235, i64 8
  store ptr %250, ptr %251, align 8
  %252 = getelementptr inbounds %.6, ptr %234, i64 0, i32 9
  store i64 0, ptr %252, align 8
  %253 = getelementptr inbounds ptr, ptr %235, i64 9
  store ptr %252, ptr %253, align 8
  %254 = getelementptr inbounds %.6, ptr %234, i64 0, i32 10
  store i64 2, ptr %254, align 8
  %255 = getelementptr inbounds ptr, ptr %235, i64 10
  store ptr %254, ptr %255, align 8
  %256 = getelementptr inbounds %.6, ptr %234, i64 0, i32 11
  store i64 1, ptr %256, align 8
  %257 = getelementptr inbounds ptr, ptr %235, i64 11
  store ptr %256, ptr %257, align 8
  %258 = getelementptr inbounds %.6, ptr %234, i64 0, i32 12
  %259 = getelementptr inbounds ptr, ptr %235, i64 12
  store ptr %258, ptr %259, align 8
  %260 = load ptr, ptr %0, align 8
  %261 = load ptr, ptr %9, align 8
  store ptr %260, ptr %234, align 8
  call void %261(ptr %260, ptr nonnull @inc_ref___gpu___pvoid_pvoid_m1df32_m1di64___m2df32, ptr nonnull %235)
  %.unpack = load ptr, ptr %258, align 8
  %.elt1 = getelementptr inbounds %.6, ptr %234, i64 0, i32 12, i32 1
  %.unpack2 = load ptr, ptr %.elt1, align 8
  %262 = alloca %.7, align 8
  %263 = alloca [2 x ptr], align 8
  store ptr %262, ptr %263, align 8
  %264 = getelementptr inbounds %.7, ptr %262, i64 0, i32 1
  store ptr %21, ptr %264, align 8
  %265 = getelementptr inbounds ptr, ptr %263, i64 1
  store ptr %264, ptr %265, align 8
  %266 = load ptr, ptr %0, align 8
  %267 = load ptr, ptr %9, align 8
  store ptr %266, ptr %262, align 8
  call void %267(ptr %266, ptr nonnull @dealloc___gpu___pvoid_pvoid___void, ptr nonnull %263)
  %268 = alloca %.8, align 8
  %269 = alloca [9 x ptr], align 8
  store ptr %268, ptr %269, align 8
  %270 = getelementptr inbounds %.8, ptr %268, i64 0, i32 1
  store i64 0, ptr %270, align 8
  %271 = getelementptr inbounds ptr, ptr %269, i64 1
  store ptr %270, ptr %271, align 8
  %272 = getelementptr inbounds %.8, ptr %268, i64 0, i32 2
  store ptr %.unpack, ptr %272, align 8
  %273 = getelementptr inbounds ptr, ptr %269, i64 2
  store ptr %272, ptr %273, align 8
  %274 = getelementptr inbounds %.8, ptr %268, i64 0, i32 3
  store ptr %.unpack2, ptr %274, align 8
  %275 = getelementptr inbounds ptr, ptr %269, i64 3
  store ptr %274, ptr %275, align 8
  %276 = getelementptr inbounds %.8, ptr %268, i64 0, i32 4
  store i64 0, ptr %276, align 8
  %277 = getelementptr inbounds ptr, ptr %269, i64 4
  store ptr %276, ptr %277, align 8
  %278 = getelementptr inbounds %.8, ptr %268, i64 0, i32 5
  store i64 %.fca.3.1.load, ptr %278, align 8
  %279 = getelementptr inbounds ptr, ptr %269, i64 5
  store ptr %278, ptr %279, align 8
  %280 = getelementptr inbounds %.8, ptr %268, i64 0, i32 6
  store i64 %.fca.3.2.load, ptr %280, align 8
  %281 = getelementptr inbounds ptr, ptr %269, i64 6
  store ptr %280, ptr %281, align 8
  %282 = getelementptr inbounds %.8, ptr %268, i64 0, i32 7
  store i64 %.fca.3.2.load, ptr %282, align 8
  %283 = getelementptr inbounds ptr, ptr %269, i64 7
  store ptr %282, ptr %283, align 8
  %284 = getelementptr inbounds %.8, ptr %268, i64 0, i32 8
  store i64 1, ptr %284, align 8
  %285 = getelementptr inbounds ptr, ptr %269, i64 8
  store ptr %284, ptr %285, align 8
  %286 = load ptr, ptr %0, align 8
  %287 = load ptr, ptr %9, align 8
  store ptr %286, ptr %268, align 8
  call void %287(ptr %286, ptr nonnull @ral_send_output___cpu___pvoid_i64_m2df32___void, ptr nonnull %269)
  ret void
}

[DISC] LowerLLVMToBinary takes: 2.527400e-02 s.
object file to shared library command: gcc --shared -o /root/.cache/bazel/_bazel_root/54ece412abe75fa85ab728a2c061e33c/execroot/org_disc_compiler/_tmp/b247c1654ecfc81b23b002417f617beaColReduceFullyDynamicShape3DF32_1_0.so /root/.cache/bazel/_bazel_root/54ece412abe75fa85ab728a2c061e33c/execroot/org_disc_compiler/_tmp/b247c1654ecfc81b23b002417f617beaColReduceFullyDynamicShape3DF32_1_0.so.o
save shared lib file to : /root/.cache/bazel/_bazel_root/54ece412abe75fa85ab728a2c061e33c/execroot/org_disc_compiler/_tmp/b247c1654ecfc81b23b002417f617beaColReduceFullyDynamicShape3DF32_1_0.so
[DISC] BinaryStrToSharedLibrary takes: 6.423800e-02 s.
[DISC] LowerHLOToSharedLibrary takes: 1.697026e+00 s.

============ END ============

2023-07-03 08:37:04.236034: I mlir/disc/tests/mlir_test.cc:275] ret: 0

2023-07-03 08:37:04.236092: I mlir/disc/tests/mlir_test.cc:241] run compiled program

2023-07-03 08:37:05.951610: I external/org_tensorflow/tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-07-03 08:37:05.955916: I mlir/disc/tests/mlir_test.cc:932] --- MLIR Execution uses: 4.309 ms
2023-07-03 08:37:05.955995: I mlir/disc/tests/mlir_test.cc:183] out buffer = 0x7fa7db232000
2023-07-03 08:37:05.956002: I mlir/disc/tests/mlir_test.cc:184] out shape:
2023-07-03 08:37:05.956005: I mlir/disc/tests/mlir_test.cc:186]   dim #0: 10
2023-07-03 08:37:05.956007: I mlir/disc/tests/mlir_test.cc:186]   dim #1: 10
2023-07-03 08:37:05.956056: I mlir/disc/tests/mlir_test.cc:244] run golden tf

2023-07-03 08:37:05.956065: I mlir/disc/tests/mlir_test.cc:257] program_path: external/org_tensorflow/tensorflow/compiler/mlir/tf-mlir-translate

2023-07-03 08:37:06.048246: I mlir/disc/tests/mlir_test.cc:269] Executed: external/org_tensorflow/tensorflow/compiler/mlir/tf-mlir-translate -mlir-to-graphdef /root/.cache/bazel/_bazel_root/54ece412abe75fa85ab728a2c061e33c/execroot/org_disc_compiler/_tmp/b247c1654ecfc81b23b002417f617bea/tempfile-adf59ec6ac82-b79f4076-1052541-5ff91121403af -o /root/.cache/bazel/_bazel_root/54ece412abe75fa85ab728a2c061e33c/execroot/org_disc_compiler/_tmp/b247c1654ecfc81b23b002417f617beaColReduceFullyDynamicShape3DF32_1_0.pbtxt 
2023-07-03 08:37:06.048258: I mlir/disc/tests/mlir_test.cc:270] external/org_tensorflow/tensorflow/compiler/mlir/tf-mlir-translate: 0
2023-07-03 08:37:06.048261: I mlir/disc/tests/mlir_test.cc:271] -- stdout:

============ END ============

2023-07-03 08:37:06.048264: I mlir/disc/tests/mlir_test.cc:273] -- stderr:

============ END ============

2023-07-03 08:37:06.048266: I mlir/disc/tests/mlir_test.cc:275] ret: 0

2023-07-03 08:37:06.048274: I mlir/disc/tests/mlir_test.cc:391] graphdef_path: /root/.cache/bazel/_bazel_root/54ece412abe75fa85ab728a2c061e33c/execroot/org_disc_compiler/_tmp/b247c1654ecfc81b23b002417f617beaColReduceFullyDynamicShape3DF32_1_0.pbtxt
2023-07-03 08:37:06.049666: I external/org_tensorflow/tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: SSE3 SSE4.1 SSE4.2 AVX AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-07-03 08:37:08.330516: I external/org_tensorflow/tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-07-03 08:37:08.334663: I external/org_tensorflow/tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-07-03 08:37:08.338742: I external/org_tensorflow/tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-07-03 08:37:08.342333: I external/org_tensorflow/tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-07-03 08:37:08.346129: I external/org_tensorflow/tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-07-03 08:37:08.350031: I external/org_tensorflow/tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-07-03 08:37:08.354018: I external/org_tensorflow/tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-07-03 08:37:08.357634: I external/org_tensorflow/tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-07-03 08:37:08.579134: I external/org_tensorflow/tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-07-03 08:37:08.583430: I external/org_tensorflow/tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-07-03 08:37:08.587400: I external/org_tensorflow/tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-07-03 08:37:08.592484: I external/org_tensorflow/tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-07-03 08:37:08.596404: I external/org_tensorflow/tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-07-03 08:37:08.600399: I external/org_tensorflow/tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-07-03 08:37:08.604389: I external/org_tensorflow/tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-07-03 08:37:08.608531: I external/org_tensorflow/tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-07-03 08:37:08.612382: I external/org_tensorflow/tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-07-03 08:37:08.616515: I external/org_tensorflow/tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-07-03 08:37:08.619797: I external/org_tensorflow/tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-07-03 08:37:08.623140: I external/org_tensorflow/tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-07-03 08:37:08.626251: I external/org_tensorflow/tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-07-03 08:37:08.629576: I external/org_tensorflow/tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-07-03 08:37:08.632707: I external/org_tensorflow/tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-07-03 08:37:08.636111: I external/org_tensorflow/tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-07-03 08:37:11.783809: I external/org_tensorflow/tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-07-03 08:37:11.786437: I external/org_tensorflow/tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-07-03 08:37:11.788963: I external/org_tensorflow/tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-07-03 08:37:11.791370: I external/org_tensorflow/tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-07-03 08:37:11.793774: I external/org_tensorflow/tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-07-03 08:37:11.796159: I external/org_tensorflow/tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-07-03 08:37:11.798551: I external/org_tensorflow/tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-07-03 08:37:11.800925: I external/org_tensorflow/tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-07-03 08:37:11.803334: I external/org_tensorflow/tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-07-03 08:37:11.805694: I external/org_tensorflow/tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-07-03 08:37:11.808037: I external/org_tensorflow/tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-07-03 08:37:11.810370: I external/org_tensorflow/tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-07-03 08:37:11.812696: I external/org_tensorflow/tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-07-03 08:37:11.815025: I external/org_tensorflow/tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-07-03 08:37:11.817346: I external/org_tensorflow/tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-07-03 08:37:11.819681: I external/org_tensorflow/tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-07-03 08:37:11.822023: I external/org_tensorflow/tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-07-03 08:37:11.824329: I external/org_tensorflow/tensorflow/core/common_runtime/gpu/gpu_device.cc:1635] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 79147 MB memory:  -> device: 0, name: NVIDIA A100-SXM4-80GB, pci bus id: 0000:00:08.0, compute capability: 8.0
2023-07-03 08:37:11.825310: I external/org_tensorflow/tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-07-03 08:37:11.827635: I external/org_tensorflow/tensorflow/core/common_runtime/gpu/gpu_device.cc:1635] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 79149 MB memory:  -> device: 1, name: NVIDIA A100-SXM4-80GB, pci bus id: 0000:00:09.0, compute capability: 8.0
2023-07-03 08:37:11.828876: I external/org_tensorflow/tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-07-03 08:37:11.831165: I external/org_tensorflow/tensorflow/core/common_runtime/gpu/gpu_device.cc:1635] Created device /job:localhost/replica:0/task:0/device:GPU:2 with 79149 MB memory:  -> device: 2, name: NVIDIA A100-SXM4-80GB, pci bus id: 0000:00:0a.0, compute capability: 8.0
2023-07-03 08:37:11.832165: I external/org_tensorflow/tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-07-03 08:37:11.834449: I external/org_tensorflow/tensorflow/core/common_runtime/gpu/gpu_device.cc:1635] Created device /job:localhost/replica:0/task:0/device:GPU:3 with 79149 MB memory:  -> device: 3, name: NVIDIA A100-SXM4-80GB, pci bus id: 0000:00:0b.0, compute capability: 8.0
2023-07-03 08:37:11.835414: I external/org_tensorflow/tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-07-03 08:37:11.837704: I external/org_tensorflow/tensorflow/core/common_runtime/gpu/gpu_device.cc:1635] Created device /job:localhost/replica:0/task:0/device:GPU:4 with 79149 MB memory:  -> device: 4, name: NVIDIA A100-SXM4-80GB, pci bus id: 0000:00:0c.0, compute capability: 8.0
2023-07-03 08:37:11.838666: I external/org_tensorflow/tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-07-03 08:37:11.840937: I external/org_tensorflow/tensorflow/core/common_runtime/gpu/gpu_device.cc:1635] Created device /job:localhost/replica:0/task:0/device:GPU:5 with 79149 MB memory:  -> device: 5, name: NVIDIA A100-SXM4-80GB, pci bus id: 0000:00:0d.0, compute capability: 8.0
2023-07-03 08:37:11.841927: I external/org_tensorflow/tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-07-03 08:37:11.844205: I external/org_tensorflow/tensorflow/core/common_runtime/gpu/gpu_device.cc:1635] Created device /job:localhost/replica:0/task:0/device:GPU:6 with 79149 MB memory:  -> device: 6, name: NVIDIA A100-SXM4-80GB, pci bus id: 0000:00:0e.0, compute capability: 8.0
2023-07-03 08:37:11.845151: I external/org_tensorflow/tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-07-03 08:37:11.847434: I external/org_tensorflow/tensorflow/core/common_runtime/gpu/gpu_device.cc:1635] Created device /job:localhost/replica:0/task:0/device:GPU:7 with 79149 MB memory:  -> device: 7, name: NVIDIA A100-SXM4-80GB, pci bus id: 0000:00:0f.0, compute capability: 8.0
2023-07-03 08:37:11.890796: I mlir/disc/tests/mlir_test.cc:467] --- TF Execution uses: 25.064 ms
2023-07-03 08:37:11.890816: I mlir/disc/tests/mlir_test.cc:474] 	output shape #0: [10,10]
2023-07-03 08:37:11.891509: I mlir/disc/tests/mlir_test.cc:467] --- TF Execution uses: 0.686 ms
2023-07-03 08:37:11.891515: I mlir/disc/tests/mlir_test.cc:474] 	output shape #0: [10,10]
2023-07-03 08:37:11.891908: I mlir/disc/tests/mlir_test.cc:467] --- TF Execution uses: 0.388 ms
2023-07-03 08:37:11.891912: I mlir/disc/tests/mlir_test.cc:474] 	output shape #0: [10,10]
2023-07-03 08:37:11.892179: I mlir/disc/tests/mlir_test.cc:467] --- TF Execution uses: 0.262 ms
2023-07-03 08:37:11.892184: I mlir/disc/tests/mlir_test.cc:474] 	output shape #0: [10,10]
2023-07-03 08:37:11.892484: I mlir/disc/tests/mlir_test.cc:467] --- TF Execution uses: 0.295 ms
2023-07-03 08:37:11.892488: I mlir/disc/tests/mlir_test.cc:474] 	output shape #0: [10,10]
2023-07-03 08:37:11.892496: I mlir/disc/tests/mlir_test.cc:484] processing output 0
2023-07-03 08:37:11.893979: I ./mlir/disc/tests/mlir_feature_test.h:37] Unset env setting:
[       OK ] TFMaxOpTest.ColReduceFullyDynamicShape3DF32_1 (9698 ms)
[----------] 1 test from TFMaxOpTest (9698 ms total)

[----------] Global test environment tear-down
[==========] 1 test from 1 test suite ran. (9698 ms total)
[  PASSED  ] 1 test.
Generating '/tmp/nsys-report-8910.qdstrm'
[1/8] [0%                          ] report89.nsys-rep[1/8] [0%                          ] report89.nsys-rep[1/8] [==20%                       ] report89.nsys-rep[1/8] [========41%                 ] report89.nsys-rep[1/8] [=========43%                ] report89.nsys-rep[1/8] [=========46%                ] report89.nsys-rep[1/8] [===========51%              ] report89.nsys-rep[1/8] [============56%             ] report89.nsys-rep[1/8] [=============58%            ] report89.nsys-rep[1/8] [=============59%            ] report89.nsys-rep[1/8] [==============61%           ] report89.nsys-rep[1/8] [==============63%           ] report89.nsys-rep[1/8] [==============64%           ] report89.nsys-rep[1/8] [===============67%          ] report89.nsys-rep[1/8] [================70%         ] report89.nsys-rep[1/8] [================71%         ] report89.nsys-rep[1/8] [=================72%        ] report89.nsys-rep[1/8] [==================78%       ] report89.nsys-rep[1/8] [====================83%     ] report89.nsys-rep[1/8] [====================85%     ] report89.nsys-rep[1/8] [=====================87%    ] report89.nsys-rep[1/8] [=====================89%    ] report89.nsys-rep[1/8] [======================90%   ] report89.nsys-rep[1/8] [======================91%   ] report89.nsys-rep[1/8] [=======================93%  ] report89.nsys-rep[1/8] [=======================94%  ] report89.nsys-rep[1/8] [=======================96%  ] report89.nsys-rep[1/8] [========================97% ] report89.nsys-rep[1/8] [========================98% ] report89.nsys-rep[1/8] [========================99% ] report89.nsys-rep[1/8] [========================100%] report89.nsys-rep[1/8] [0%                          ] report89.nsys-rep[1/8] [5%                          ] report89.nsys-rep[1/8] [0%                          ] report89.nsys-rep[1/8] [6%                          ] report89.nsys-rep[1/8] [10%                         ] report89.nsys-rep[1/8] [14%                         ] report89.nsys-rep[1/8] [==18%                       ] report89.nsys-rep[1/8] [==21%                       ] report89.nsys-rep[1/8] [====25%                     ] report89.nsys-rep[1/8] [=====29%                    ] report89.nsys-rep[1/8] [======33%                   ] report89.nsys-rep[1/8] [=======37%                  ] report89.nsys-rep[1/8] [========41%                 ] report89.nsys-rep[1/8] [=========44%                ] report89.nsys-rep[1/8] [==========48%               ] report89.nsys-rep[1/8] [===========52%              ] report89.nsys-rep[1/8] [============56%             ] report89.nsys-rep[1/8] [=============60%            ] report89.nsys-rep[1/8] [==============62%           ] report89.nsys-rep[1/8] [===============66%          ] report89.nsys-rep[1/8] [================69%         ] report89.nsys-rep[1/8] [=================73%        ] report89.nsys-rep[1/8] [==================77%       ] report89.nsys-rep[1/8] [===================81%      ] report89.nsys-rep[1/8] [====================85%     ] report89.nsys-rep[1/8] [=====================89%    ] report89.nsys-rep[1/8] [======================92%   ] report89.nsys-rep[1/8] [=======================96%  ] report89.nsys-rep[1/8] [========================99% ] report89.nsys-rep[1/8] [========================100%] report89.nsys-rep[1/8] [========================100%] report89.nsys-rep
[2/8] [0%                          ] report89.sqlite[2/8] [1%                          ] report89.sqlite[2/8] [2%                          ] report89.sqlite[2/8] [3%                          ] report89.sqlite[2/8] [4%                          ] report89.sqlite[2/8] [5%                          ] report89.sqlite[2/8] [6%                          ] report89.sqlite[2/8] [7%                          ] report89.sqlite[2/8] [8%                          ] report89.sqlite[2/8] [9%                          ] report89.sqlite[2/8] [10%                         ] report89.sqlite[2/8] [11%                         ] report89.sqlite[2/8] [12%                         ] report89.sqlite[2/8] [13%                         ] report89.sqlite[2/8] [14%                         ] report89.sqlite[2/8] [=15%                        ] report89.sqlite[2/8] [=16%                        ] report89.sqlite[2/8] [=17%                        ] report89.sqlite[2/8] [==18%                       ] report89.sqlite[2/8] [==19%                       ] report89.sqlite[2/8] [==20%                       ] report89.sqlite[2/8] [==21%                       ] report89.sqlite[2/8] [===22%                      ] report89.sqlite[2/8] [===23%                      ] report89.sqlite[2/8] [===24%                      ] report89.sqlite[2/8] [====25%                     ] report89.sqlite[2/8] [====26%                     ] report89.sqlite[2/8] [====27%                     ] report89.sqlite[2/8] [====28%                     ] report89.sqlite[2/8] [=====29%                    ] report89.sqlite[2/8] [=====30%                    ] report89.sqlite[2/8] [=====31%                    ] report89.sqlite[2/8] [=====32%                    ] report89.sqlite[2/8] [======33%                   ] report89.sqlite[2/8] [======34%                   ] report89.sqlite[2/8] [======35%                   ] report89.sqlite[2/8] [=======36%                  ] report89.sqlite[2/8] [=======37%                  ] report89.sqlite[2/8] [=======38%                  ] report89.sqlite[2/8] [=======39%                  ] report89.sqlite[2/8] [========40%                 ] report89.sqlite[2/8] [========41%                 ] report89.sqlite[2/8] [========42%                 ] report89.sqlite[2/8] [=========43%                ] report89.sqlite[2/8] [=========44%                ] report89.sqlite[2/8] [=========45%                ] report89.sqlite[2/8] [=========46%                ] report89.sqlite[2/8] [==========47%               ] report89.sqlite[2/8] [==========48%               ] report89.sqlite[2/8] [==========49%               ] report89.sqlite[2/8] [===========50%              ] report89.sqlite[2/8] [===========51%              ] report89.sqlite[2/8] [===========52%              ] report89.sqlite[2/8] [===========53%              ] report89.sqlite[2/8] [============54%             ] report89.sqlite[2/8] [============55%             ] report89.sqlite[2/8] [============56%             ] report89.sqlite[2/8] [============57%             ] report89.sqlite[2/8] [=============58%            ] report89.sqlite[2/8] [=============59%            ] report89.sqlite[2/8] [=============60%            ] report89.sqlite[2/8] [==============61%           ] report89.sqlite[2/8] [==============62%           ] report89.sqlite[2/8] [==============63%           ] report89.sqlite[2/8] [==============64%           ] report89.sqlite[2/8] [===============65%          ] report89.sqlite[2/8] [===============66%          ] report89.sqlite[2/8] [===============67%          ] report89.sqlite[2/8] [================68%         ] report89.sqlite[2/8] [================69%         ] report89.sqlite[2/8] [================70%         ] report89.sqlite[2/8] [================71%         ] report89.sqlite[2/8] [=================72%        ] report89.sqlite[2/8] [=================73%        ] report89.sqlite[2/8] [=================74%        ] report89.sqlite[2/8] [==================75%       ] report89.sqlite[2/8] [==================76%       ] report89.sqlite[2/8] [==================77%       ] report89.sqlite[2/8] [==================78%       ] report89.sqlite[2/8] [===================79%      ] report89.sqlite[2/8] [===================80%      ] report89.sqlite[2/8] [===================81%      ] report89.sqlite[2/8] [===================82%      ] report89.sqlite[2/8] [====================83%     ] report89.sqlite[2/8] [====================84%     ] report89.sqlite[2/8] [====================85%     ] report89.sqlite[2/8] [=====================86%    ] report89.sqlite[2/8] [=====================87%    ] report89.sqlite[2/8] [=====================88%    ] report89.sqlite[2/8] [=====================89%    ] report89.sqlite[2/8] [======================90%   ] report89.sqlite[2/8] [======================91%   ] report89.sqlite[2/8] [======================92%   ] report89.sqlite[2/8] [=======================93%  ] report89.sqlite[2/8] [=======================94%  ] report89.sqlite[2/8] [=======================95%  ] report89.sqlite[2/8] [=======================96%  ] report89.sqlite[2/8] [========================97% ] report89.sqlite[2/8] [========================98% ] report89.sqlite[2/8] [========================99% ] report89.sqlite[2/8] [========================100%] report89.sqlite[2/8] [========================100%] report89.sqlite
[3/8] Executing 'nvtx_sum' stats report

 Time (%)  Total Time (ns)  Instances  Avg (ns)  Med (ns)  Min (ns)  Max (ns)  StdDev (ns)   Style                   Range                 
 --------  ---------------  ---------  --------  --------  --------  --------  -----------  -------  --------------------------------------
     78.9           404171          5   80834.2   91148.0     30572    119513      36446.6  PushPop  TSL:#edge_name=edge_7__arg_input0_0_0#
     21.1           108146          5   21629.2   19792.0     14753     29228       5614.5  PushPop  TSL:#edge_name=edge_8_tf.Max#         

[4/8] Executing 'osrt_sum' stats report

 Time (%)  Total Time (ns)  Num Calls   Avg (ns)    Med (ns)    Min (ns)  Max (ns)   StdDev (ns)           Name         
 --------  ---------------  ---------  ----------  -----------  --------  ---------  -----------  ----------------------
     91.7      42354454238        524  80829111.1  100103438.0      2584  100122844   36353596.6  poll                  
      6.3       2904339587      10778    269469.3     152797.0       390   20315425     629880.6  ioctl                 
      0.5        208793539         52   4015260.4     172039.0      4815   64956875   11671099.1  pthread_cond_wait     
      0.4        202200048     350912       576.2        416.0       241      19417        576.6  write                 
      0.3        144057691         15   9603846.1      33589.0     22638   75295218   22021416.2  waitpid               
      0.2         89260811     153970       579.7        575.0       207      35539        180.1  read                  
      0.2         88890494         80   1111131.2       3801.5      1409   65089654    7362694.9  futex                 
      0.1         68454069        809     84615.7      15427.0      5447   20687282     736496.5  open64                
      0.1         64035558          1  64035558.0   64035558.0  64035558   64035558          0.0  system                
      0.0         22684002        619     36646.2      31691.0      3060    1094661      98830.7  mmap64                
      0.0         17859914        557     32064.5      29573.0     14215     100715      12250.8  pthread_create        
      0.0          3019593         90     33551.0      11836.0        68     250363      65617.8  sem_timedwait         
      0.0          1681600        240      7006.7       4898.0       833      69492       7002.1  fopen                 
      0.0          1463696        148      9889.8       8576.5      1229      36673       7332.9  mmap                  
      0.0           603898        252      2396.4       1391.5       460      25332       3363.4  fclose                
      0.0           451588        903       500.1        491.0       140       9721        348.8  fcntl                 
      0.0           424619       1034       410.7         45.0        32      20352       1495.2  fread                 
      0.0           405021         66      6136.7       5752.5      1267      31604       4830.0  munmap                
      0.0           340391         46      7399.8       4860.0      1706      49200       8146.6  open                  
      0.0           211457         32      6608.0       2110.0       893      57461      13029.2  fopen64               
      0.0           201046       2166        92.8         34.0        27      16664        722.1  fwrite                
      0.0           197083          1    197083.0     197083.0    197083     197083          0.0  sem_wait              
      0.0           131861          2     65930.5      65930.5     65267      66594        938.3  nanosleep             
      0.0           111212         52      2138.7       1753.5      1260       6883       1178.0  pthread_cond_signal   
      0.0            67564         50      1351.3         49.5        34      33852       6410.4  fgets                 
      0.0            49903         16      3118.9       2147.0      1137       8901       2208.3  pipe                  
      0.0            44954         15      2996.9       1847.0       450       8524       2470.7  pread                 
      0.0            38748        149       260.1         86.0        20      11369       1010.1  fflush                
      0.0            30778         67       459.4        190.0       153       7308        919.1  sigaction             
      0.0            29330          4      7332.5       7618.0      4080      10014       2846.3  socket                
      0.0            22395          4      5598.8       5685.0      3250       7775       2341.6  pipe2                 
      0.0            21819          1     21819.0      21819.0     21819      21819          0.0  writev                
      0.0            19833          2      9916.5       9916.5      8450      11383       2073.9  connect               
      0.0             8520          2      4260.0       4260.0      2145       6375       2991.1  getc                  
      0.0             6308         22       286.7        196.5       172       1165        278.1  signal                
      0.0             5413          1      5413.0       5413.0      5413       5413          0.0  fputs                 
      0.0             5319          2      2659.5       2659.5      1761       3558       1270.7  bind                  
      0.0             4935         13       379.6        417.0       112        562        144.1  pthread_cond_broadcast
      0.0             4777          1      4777.0       4777.0      4777       4777          0.0  pthread_mutex_lock    
      0.0             3020          2      1510.0       1510.0       966       2054        769.3  listen                
      0.0             2395         64        37.4         23.0        22        489         63.0  pthread_mutex_trylock 
      0.0             1079          2       539.5        539.5       213        866        461.7  dup2                  

[5/8] Executing 'cuda_api_sum' stats report

 Time (%)  Total Time (ns)  Num Calls   Avg (ns)     Med (ns)    Min (ns)   Max (ns)   StdDev (ns)          Name         
 --------  ---------------  ---------  -----------  -----------  ---------  ---------  -----------  ---------------------
     96.4        313649310          1  313649310.0  313649310.0  313649310  313649310          0.0  cuCtxCreate_v2       
      1.3          4196897          3    1398965.7     665997.0       5892    3525008    1870555.2  cuMemAlloc_v2        
      0.9          2771005          1    2771005.0    2771005.0    2771005    2771005          0.0  cuMemHostAlloc       
      0.7          2189499         32      68421.8       6764.5       1382     281578     111468.4  cuStreamCreate       
      0.3          1044927          2     522463.5     522463.5       8806    1036121     726421.4  cuMemFree_v2         
      0.1           471519          8      58939.9      39709.5      35193     139481      37058.6  cuMemGetInfo_v2      
      0.1           169480          3      56493.3      24871.0      23268     121341      56165.4  cuModuleUnload       
      0.0           123617          7      17659.6      21876.0       6218      29633       8817.8  cuLaunchKernel       
      0.0           123118          1     123118.0     123118.0     123118     123118          0.0  cuModuleLoadData     
      0.0           111001        408        272.1        167.5        125       3371        324.5  cuCtxSetCurrent      
      0.0           104353          5      20870.6      21713.0       5882      36136      12115.3  cuMemcpyHtoDAsync_v2 
      0.0           102707          2      51353.5      51353.5      44324      58383       9941.2  cuModuleLoadFatBinary
      0.0            94464         10       9446.4       7211.0       5096      20145       5151.8  cudaLaunchKernel     
      0.0            76442        365        209.4        171.0         93       1128        119.7  cuGetProcAddress     
      0.0            52374         22       2380.6       2348.5        620       7999       1796.9  cuEventQuery         
      0.0            34525          1      34525.0      34525.0      34525      34525          0.0  cuMemcpyHtoD_v2      
      0.0            34424          5       6884.8       6555.0       4920       9080       1736.5  cuMemcpyDtoHAsync_v2 
      0.0            33815         20       1690.8        705.0        415       9897       2495.1  cuEventRecord        
      0.0            31452          1      31452.0      31452.0      31452      31452          0.0  cuMemcpyDtoH_v2      
      0.0            22809          6       3801.5       3727.0       2496       5134        910.6  cuStreamSynchronize  
      0.0            16871          1      16871.0      16871.0      16871      16871          0.0  cuMemsetD32_v2       
      0.0            14969         34        440.3        342.5        230       1397        293.2  cuEventCreate        
      0.0            14276         10       1427.6       1510.5        876       2024        362.8  cuStreamWaitEvent    
      0.0             5617          2       2808.5       2808.5       2370       3247        620.1  cuInit               

[6/8] Executing 'cuda_gpu_kern_sum' stats report

 Time (%)  Total Time (ns)  Instances  Avg (ns)  Med (ns)  Min (ns)  Max (ns)  StdDev (ns)                                                  Name                                                
 --------  ---------------  ---------  --------  --------  --------  --------  -----------  ----------------------------------------------------------------------------------------------------
     29.9            28161          5    5632.2    5600.0      5536      5792         96.0  void tensorflow::functor::ColumnReduceKernel<float *, float *, tensorflow::functor::MaxPropagateNaN…
     28.0            26367          5    5273.4    5280.0      5247      5312         27.0  void tensorflow::functor::CleanupSegments<float *, float *, tensorflow::functor::MaxPropagateNaN>(T…
     24.9            23391          5    4678.2    4607.0      4577      4992        175.9  Abs_GPU_DT_FLOAT_DT_FLOAT_kernel                                                                    
     12.3            11584          1   11584.0   11584.0     11584     11584          0.0  main_kColReduction_reduce__4_1_0___thin_1                                                           
      4.8             4544          1    4544.0    4544.0      4544      4544          0.0  main_kColReduction_reduce__4_1_0___thin                                                             

[7/8] Executing 'cuda_gpu_mem_time_sum' stats report

 Time (%)  Total Time (ns)  Count  Avg (ns)  Med (ns)  Min (ns)  Max (ns)  StdDev (ns)      Operation     
 --------  ---------------  -----  --------  --------  --------  --------  -----------  ------------------
     80.1            73152      6   12192.0   11808.0     11712     13632        765.6  [CUDA memcpy HtoD]
     16.3            14848      6    2474.7    2272.0      2272      3040        329.9  [CUDA memcpy DtoH]
      3.7             3360      1    3360.0    3360.0      3360      3360          0.0  [CUDA memset]     

[8/8] Executing 'cuda_gpu_mem_size_sum' stats report

 Total (MB)  Count  Avg (MB)  Med (MB)  Min (MB)  Max (MB)  StdDev (MB)      Operation     
 ----------  -----  --------  --------  --------  --------  -----------  ------------------
      1.229      6     0.205     0.205     0.205     0.205        0.000  [CUDA memcpy HtoD]
      0.002      6     0.000     0.000     0.000     0.000        0.000  [CUDA memcpy DtoH]
      0.001      1     0.001     0.001     0.001     0.001        0.000  [CUDA memset]     

Generated:
    /root/.cache/bazel/_bazel_root/54ece412abe75fa85ab728a2c061e33c/execroot/org_disc_compiler/bazel-out/k8-opt/bin/mlir/disc/tests/tensorflow_ops/max.cpp.test.runfiles/org_disc_compiler/report89.nsys-rep
    /root/.cache/bazel/_bazel_root/54ece412abe75fa85ab728a2c061e33c/execroot/org_disc_compiler/bazel-out/k8-opt/bin/mlir/disc/tests/tensorflow_ops/max.cpp.test.runfiles/org_disc_compiler/report89.sqlite
