#include <algorithm>
#include <iostream>

#include "cutlass/array.h"
#include "cutlass/complex.h"
#include "cutlass/cutlass.h"
#include "cutlass/epilogue/thread/activation.h"
#include "cutlass/epilogue/thread/linear_combination_generic.h"
#include "cutlass/epilogue/thread/scale_type.h"
#include "cutlass/gemm/device/default_gemm_configuration.h"
#include "cutlass/gemm/device/gemm_universal.h"
#include "cutlass/util/host_tensor.h"
#include "cutlass/util/reference/host/tensor_fill.h"

class SpecializedGemmFusion {
 public:
  template <typename T>
  struct ElementwiseUnaryOp {
    CUTLASS_HOST_DEVICE
    T operator()(T const& input) const {
      //////////// To update according to the problem ////////////
      return input + T(1);
      ////////////////////////////////////////////////////////////
    }
  };

  template <typename T>
  struct EpilogueFunctor {
    //////////// To update according to the problem ////////////
    static const bool kIsHeavy = false;
    ////////////////////////////////////////////////////////////

    CUTLASS_HOST_DEVICE
    T operator()(T const& scalar) const {
      ElementwiseUnaryOp<T> op;

      return op(scalar);
    }

    using Params = cutlass::epilogue::thread::LinearCombinationGenericParams<T>;

    CUTLASS_HOST_DEVICE
    T operator()(T const& scalar, Params const& params_) const {
      return this->operator()(scalar);
    }
  };

  template <typename T, int N>
  struct EpilogueFunctor<cutlass::Array<T, N>> {
    //////////// To update according to the problem ////////////
    static const bool kIsHeavy = false;
    ////////////////////////////////////////////////////////////

    CUTLASS_HOST_DEVICE
    cutlass::Array<T, N> operator()(cutlass::Array<T, N> const& frag) const {
      cutlass::Array<T, N> result;
      ElementwiseUnaryOp<T> op;

      CUTLASS_PRAGMA_UNROLL
      for (int i = 0; i < N; ++i) {
        result[i] = op(frag[i]);
      }

      return result;
    }

    using Params = cutlass::epilogue::thread::LinearCombinationGenericParams<T>;

    CUTLASS_HOST_DEVICE
    cutlass::Array<T, N> operator()(cutlass::Array<T, N> const& frag,
                                    Params const& params_) const {
      return this->operator()(frag);
    }
  };

 public:
  SpecializedGemmFusion(int64_t batch_size, int64_t m, int64_t n, int64_t k,
                        const void* A, bool a_transpose, const void* B,
                        bool b_transpose, const void* C, void* D,
                        const void* alpha, const void* beta)
      : batch_size_(batch_size),
        m_(m),
        n_(n),
        k_(k),
        A_(A),
        a_transpose_(a_transpose),
        B_(B),
        b_transpose_(b_transpose),
        C_(C),
        D_(D),
        alpha_(alpha),
        beta_(beta) {}
  bool run();

 private:
  int64_t batch_size_;
  int64_t m_;
  int64_t n_;
  int64_t k_;
  const void* A_;
  bool a_transpose_;
  const void* B_;
  bool b_transpose_;
  const void* C_;
  void* D_;
  const void* alpha_;
  const void* beta_;
};

bool SpecializedGemmFusion::run() {
  // >>>>>>>>>>>>>>>>>>>>>> Deterministic config <<<<<<<<<<<<<<<<<<<<<< //
  // The following configuration are determistic currently.
  constexpr cutlass::FloatRoundStyle Round =
      cutlass::FloatRoundStyle::round_to_nearest;
  constexpr cutlass::ComplexTransform TransformA =
      cutlass::ComplexTransform::kNone;
  constexpr cutlass::ComplexTransform TransformB =
      cutlass::ComplexTransform::kNone;
  // >>>>>>>>>>>>>>>>>>>>>> ==================== <<<<<<<<<<<<<<<<<<<<<< //

  // >>>>>>>>>>>>>>>>>>>>>> Generate by compiler <<<<<<<<<<<<<<<<<<<<<< //
  // The following code will be generated by the compiler according to the
  // problem.
  using ElementA = cutlass::half_t;
  using LayoutA = cutlass::layout::ColumnMajor;
  using ElementB = cutlass::half_t;
  using LayoutB = cutlass::layout::ColumnMajor;
  using ElementOutput = cutlass::half_t;
  using LayoutOutput = cutlass::layout::ColumnMajor;
  using ElementAccumulator = float;
  using OperatorClass = cutlass::arch::OpClassTensorOp;
  using ArchTag = cutlass::arch::Sm80;

  constexpr cutlass::epilogue::thread::ScaleType::Kind Scale =
      cutlass::epilogue::thread::ScaleType::NoBetaScaling;
  constexpr bool IsHeavy = false;
  // Number of elements computed per operation, i.e., per vectorized memory
  // access. Usually it is 128/sizeof_bits<ElementOutput_>, but we use 64 or 32
  // sometimes when there are not enough data to store.
  constexpr int Count = 128 / cutlass::sizeof_bits<ElementOutput>::value;
  // Data type of epilogue.
  using ElementComputeEpilogue = ElementOutput;
  using EpilogueOutputOp = cutlass::epilogue::thread::LinearCombinationGeneric<
      EpilogueFunctor, ElementOutput, Count, ElementAccumulator,
      ElementComputeEpilogue, Scale, Round, IsHeavy>;

  using GemmConfiguration = cutlass::gemm::device::DefaultGemmConfiguration<
      OperatorClass, ArchTag, ElementA, ElementB, ElementOutput,
      ElementAccumulator>;
  constexpr int AlignmentA = GemmConfiguration::kAlignmentA;
  constexpr int AlignmentB = GemmConfiguration::kAlignmentB;
  using Operator = GemmConfiguration::Operator;

  constexpr bool GatherA = false;
  constexpr bool GatherB = false;
  constexpr bool ScatterD = false;
  using PermuteDLayout = cutlass::layout::NoPermute;
  // >>>>>>>>>>>>>>>>>>>>>> ==================== <<<<<<<<<<<<<<<<<<<<<< //

  // >>>>>>>>>>>>>>>>>>>> Generate GEMM parameters <<<<<<<<<<<<<<<<<<<< //
  // Most of the parameters are determistic. Some are tunable, e.g., splitk.
  cutlass::gemm::GemmUniversalMode mode;
  if (batch_size_ > 1) {
    mode = cutlass::gemm::GemmUniversalMode::kBatched;
  } else {
    // >>>>>>>>>>>>>>>>>>> Generate tunable parameter <<<<<<<<<<<<<<<<<<< //
    // TODO: implement the parameter selection logic.
    if (true) {
      mode = cutlass::gemm::GemmUniversalMode::kGemm;
    }
    // >>>>>>>>>>>>>>>>>>>>>> ==================== <<<<<<<<<<<<<<<<<<<<<< //
  }

  cutlass::gemm::GemmCoord problem_size(m_, n_, k_);

  typename EpilogueOutputOp::Params epilogue(
      *reinterpret_cast<const ElementComputeEpilogue*>(alpha_),
      *reinterpret_cast<const ElementComputeEpilogue*>(beta_));

  // Currently, it requires the batch dimension to be the most significant
  // dim.
  long long int batch_stride_A =
      static_cast<long long int>(m_) * static_cast<long long int>(k_);
  long long int batch_stride_B =
      static_cast<long long int>(k_) * static_cast<long long int>(n_);
  // Currently, we assume C_ is always nullptr.
  long long int batch_stride_C = 0;
  long long int batch_stride_D =
      static_cast<long long int>(m_) * static_cast<long long int>(n_);

  int const lda = a_transpose_ ? k_ : m_;
  int const ldb = b_transpose_ ? n_ : k_;
  int const ldc = 0;
  int const ldd = m_;

  int const* ptr_gather_A_indices = nullptr;
  int const* ptr_gather_B_indices = nullptr;
  int const* ptr_scatter_D_indices = nullptr;

  // >>>>>>>>>>>>>>>>>>>>>> ==================== <<<<<<<<<<<<<<<<<<<<<< //

  // >>>>>>>>>>>>>>>>>>> Generate tunable parameter <<<<<<<<<<<<<<<<<<< //
  // TODO: implement the parameter selection logic.
  if (true) {
    using ThreadblockShape = cutlass::gemm::GemmShape<128, 128, 32>;
    using WarpShape = cutlass::gemm::GemmShape<64, 64, 32>;
    using InstructionShape = cutlass::gemm::GemmShape<16, 8, 16>;
    using ThreadblockSwizzle =
        cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<8>;
    constexpr int Stages = 3;

    using Gemm = cutlass::gemm::device::GemmUniversal<
        ElementA, LayoutA, ElementB, LayoutB, ElementOutput, LayoutOutput,
        ElementAccumulator, OperatorClass, ArchTag, ThreadblockShape, WarpShape,
        InstructionShape, EpilogueOutputOp, ThreadblockSwizzle, Stages,
        AlignmentA, AlignmentB, Operator, TransformA, TransformB, GatherA,
        GatherB, ScatterD, PermuteDLayout>;

    typename Gemm::Arguments arguments{mode,
                                       problem_size,
                                       static_cast<int>(batch_size_),
                                       epilogue,
                                       A_,
                                       B_,
                                       C_,
                                       D_,
                                       batch_stride_A,
                                       batch_stride_B,
                                       batch_stride_C,
                                       batch_stride_D,
                                       lda,
                                       ldb,
                                       ldc,
                                       ldd,
                                       ptr_gather_A_indices,
                                       ptr_gather_B_indices,
                                       ptr_scatter_D_indices};

    size_t workspace_size = Gemm::get_workspace_size(arguments);
    cutlass::device_memory::allocation<uint8_t> workspace(workspace_size);

    Gemm gemm_op;
    cutlass::Status status = gemm_op.can_implement(arguments);
    if (status != cutlass::Status::kSuccess) {
      return false;
    }

    status = gemm_op.initialize(arguments, workspace.get());
    if (status != cutlass::Status::kSuccess) {
      return false;
    }

    status = gemm_op();
    if (status != cutlass::Status::kSuccess) {
      return false;
    }
  }

  return true;
}

bool gemmFusion(int64_t batch_size, int64_t m, int64_t n, int64_t k,
                const void* A, bool a_transpose, const void* B,
                bool b_transpose, const void* C, void* D, const void* alpha,
                const void* beta) {
  SpecializedGemmFusion specialization(batch_size, m, n, k, A, a_transpose, B,
                                       b_transpose, C, D, alpha, beta);
  return specialization.run();
}

int main() {
  // Prepare
  const int length_m = 128;
  const int length_n = 128;
  const int length_k = 128;
  const int batch_size = 2;

  using ElementA = cutlass::half_t;
  using LayoutA = cutlass::layout::ColumnMajor;
  using ElementB = cutlass::half_t;
  using LayoutB = cutlass::layout::ColumnMajor;
  using ElementOutput = cutlass::half_t;
  using LayoutOutput = cutlass::layout::ColumnMajor;
  cutlass::HostTensor<ElementA, LayoutA> tensor_a(
      {length_m, length_k * batch_size});
  cutlass::HostTensor<ElementB, LayoutB> tensor_b(
      {length_k, length_n * batch_size});
  cutlass::HostTensor<ElementOutput, LayoutOutput> tensor_d(
      {length_m, length_n * batch_size});

  // Fill input and output matrices on host using CUTLASS helper functions
  for (int b = 0; b < batch_size; b++) {
    for (int k = 0; k < length_k; k++) {
      for (int m = 0; m < length_m; m++) {
        tensor_a.host_view().at({m, b * length_k + k}) = b * 1000 + k;
      }
    }
  }
  // Fill identity for each gemm in the batch for B.
  for (int n = 0; n < length_n; n++) {
    for (int b = 0; b < batch_size; b++) {
      for (int k = 0; k < length_k; k++) {
        bool is_one =
            length_n < length_k ? ((k % length_n) == n) : ((n % length_k) == k);
        tensor_b.host_view().at({k, b * length_n + n}) = (is_one ? 1 : 0);
      }
    }
  }
  cutlass::reference::host::TensorFill(
      tensor_d.host_view());  // fill with zeros

  // Copy data from host to GPU
  tensor_a.sync_device();
  tensor_b.sync_device();
  tensor_d.sync_device();

  bool a_transpose = false;
  bool b_transpose = false;
  void const* ptr_A = tensor_a.device_data();
  void const* ptr_B = tensor_b.device_data();
  void* ptr_D = tensor_d.device_data();
  using ElementComputeEpilogue = ElementOutput;
  ElementComputeEpilogue alpha = ElementComputeEpilogue(1);
  ElementComputeEpilogue beta = ElementComputeEpilogue(0);

  bool succeed =
      gemmFusion(batch_size, length_m, length_n, length_k, ptr_A, a_transpose,
                 ptr_B, b_transpose, nullptr, ptr_D, &alpha, &beta);

  auto error = cudaDeviceSynchronize();

  tensor_d.sync_host();
  // print partial.
  if (true) {
    for (int b = 0; b < batch_size; b++) {
      std::cout << "[ZZ] mat " << b << std::endl;
      for (int row = 0; row < 8; row++) {
        for (int col = 0; col < 8; col++) {
          std::cout << tensor_d.host_view().at(
                           {row, b * length_n + col})  // col major
                    << " ";
        }
        std::cout << std::endl;
      }
    }
  }

  // print all.
  if (false) {
    for (int b = 0; b < batch_size; b++) {
      for (int row = 0; row < length_m; row++) {
        for (int col = 0; col < length_n; col++) {
          std::cout << tensor_d.host_view().at({row, b * length_n + col})
                    << " ";
        }
      }
      std::cout << std::endl;
    }
  }

  return 0;
}
